,Topic,Count,Name,Representation,Representative_Docs
0,-1,7179,-1_classification_datasets_dataset_neural,"classification,datasets,dataset,neural,learning,networks,prediction,deep,feature,features","Modern machine learning (ML) and deep learning (DL) techniques using high-dimensional data representations have helped accelerate the materials discovery process by efficiently detecting hidden patterns in existing datasets and linking input representations to output properties for a better understanding of the scientific phenomenon. While a deep neural network comprised of fully connected layers has been widely used for materials property prediction, simply creating a deeper model with a large number of layers often faces with vanishing gradient problem, causing a degradation in the performance, thereby limiting usage. In this paper, we study and propose architectural principles to address the question of improving the performance of model training and inference under fixed parametric constraints. Here, we present a general deep-learning framework based on branched residual learning (BRNet) with fully connected layers that can work with any numerical vector-based representation as input to build accurate models to predict materials properties. We perform model training for materials properties using numerical vectors representing different composition-based attributes of the respective materials and compare the performance of the proposed models against traditional ML and existing DL architectures. We find that the proposed models are significantly more accurate than the ML/DL models for all data sizes by using different composition-based attributes as input. Further, branched learning requires fewer parameters and results in faster model training due to better convergence during the training phase than existing neural networks, thereby efficiently building accurate models for predicting materials properties. © 2023, The Author(s).,Introduction: Feature selection in the face of high-dimensional data can reduce overfitting and learning time, and at the same time improve the accuracy and efficiency of the system. Since there are many irrelevant and redundant features in breast cancer diagnosis, removing such features leads to more accurate prediction and reduced decision time when dealing with large-scale data. Meanwhile, ensemble classifiers are powerful techniques to improve the prediction performance of classification models, where several individual classifier models are combined to achieve higher accuracy. Methods: In this paper, an ensemble classifier algorithm based on multilayer perceptron neural network is proposed for the classification task, in which the parameters (e.g., number of hidden layers, number of neurons in each hidden layer, and weights of links) are adjusted based on an evolutionary approach. Meanwhile, this paper uses a hybrid dimensionality reduction technique based on principal component analysis and information gain to address this problem. Results: The effectiveness of the proposed algorithm was evaluated based on the Wisconsin breast cancer database. In particular, the proposed algorithm provides an average of 17% better accuracy compared to the best results obtained from the existing state-of-the-art methods. Conclusion: Experimental results show that the proposed algorithm can be used as an intelligent medical assistant system for breast cancer diagnosis. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Cardiovascular diseases are causing more deaths across the globe. With innovations in Artificial Intelligence (AI) predicting such diseases early is very important research area. With learning based approaches that exploit knowledge from given samples, it is possible to improve disease prediction process. There are many aspects to proper healthcare such as preventing diseases with suitable diet and lifestyle, early detection of diseases if any and efficient treatment. Data is being accumulated in every domain. However, the healthcare industry is on top of the list as it provides large volumes of data pertaining to human health, diet and drug aspects. The existing literature has not shown adequate research in this direction. The Healthcare industry has an unprecedented impact on the well-being of people across the globe. In the recent observations by World Health Organization (WHO), data science approach towards disease prediction greatly complements existing Clinical Decision Support Systems (CDSSs).This research paper presents a comprehensive study on the application of data science techniques for disease prediction and drug recommendation in healthcare, focusing on a case study involving cardiovascular diseases. The primary objective of this study is to develop a robust predictive model that identifies the likelihood of cardiovascular diseases in patients, and subsequently recommends drug interventions for optimal treatment outcomes. Here we propose Disease Prediction and Drug Recommendation Framework (DPDRF). The framework is realized by defining an algorithm known as Cardio Disease Prediction and Drug Recommendation (CDP-DR). The Disease Prediction and Drug Recommendation algorithm in turn uses different supervised machine learning (ML) algorithms such as Random Forest (RF), Logistic Regression (LR), Support Vector Machine (SVM), Decision Tree (DT), Stochastic Gradient Descent (SGD), Gradient Boosting, and Extreme Gradient Boosting (XGB). Another algorithm known as Entropy and Gain based Hybrid Feature Selection (EG-HFS) is defined to leverage quality of training leading to performance enhancement of prediction models. The experimental results with cardio disease prediction as a case study revealed that the proposed framework is useful in disease prediction and drug recommendations by using different prediction models. Highest accuracy achieved by the proposed system is 96.23%. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
1,0,329,0_renewable_photovoltaic_solar_forecasting,"renewable,photovoltaic,solar,forecasting,forecast,prediction,energy,pv,electricity,optimization","The use of renewable energy sources is becoming increasingly widespread around the world due to various factors, the most relevant of which is the high environmental friendliness of these types of energy resources. However, the large-scale involvement of green energy leads to the creation of distributed energy networks that combine several different generation methods, each of which has its own specific features, and as a result, the data collection and processing necessary to optimize the operation of such energy systems become more relevant. Development of new technologies for the more optimal use of RES is one of the main tasks of modern research in the field of energy, where an important place is assigned to the use of technologies based on artificial intelligence, allowing researchers to significantly increase the efficiency of the use of all types of RES within energy systems. This paper proposes to consider the methodology of application of modern approaches to the assessment of the amount of energy obtained from renewable energy sources based on artificial intelligence technologies, approaches used for data processing and for optimization of the control processes for operating energy systems with the integration of renewable energy sources. The relevance of the work lies in the formation of a general approach applied to the evaluation of renewable energy sources such as solar and wind energy based on the use of artificial intelligence technologies. As a verification of the approach considered by the authors, a number of models for predicting the amount of solar power generation using photovoltaic panels have been implemented, for which modern machine-learning methods have been used. As a result of testing for quality and accuracy, the best results were obtained using a hybrid forecasting model, which combines the joint use of a random forest model applied at the stage of the normalization of the input data, exponential smoothing model, and LSTM model. © 2024 by the authors.,The scarcity of energy resources and global warming over the past few decades have prompted the widespread adoption of renewable energy sources. Among the potential renewable energy sources, solar energy has emerged as one of the most promising renewable energy sources. However, the uncertainty and fluctuations of solar power generation create negative impacts on the stability and reliability of the electric grid, planning of operation, economic feasibility, and investment. Therefore, accurate prediction of solar power generation is crucial to ensure the stability of the power grid and promote a large-scale investment in a solar energy system. A large number of research studies have been conducted on predicting solar power generation under different perspectives. However, no existing study analyses and predicts power generation of multi-solar energy sites by only one prediction model. The integration of multiple sites into one predictive model will reduce the number of required models for each site, thereby saving the computing resources and required calculation time. This paper proposes a novel methodology to group multiple solar sites and develop an integrated model by using a machine learning algorithm to predict power generation of each group. Firstly, the K-means clustering algorithm is utilized to cluster multiple solar sites which have similar power generation properties into one group. Then, a machine learning algorithm has been developed to predict power generation in a computationally fast and reliable manner. The proposed approach is verified by the real data of 223 solar sites in Taiwan. The experimental results show that the training time can be reduced by 93.2% without reducing the accuracy of the prediction model. Therefore, the cluster-based prediction approach gives better performance as compared with existing models. © The Author(s) 2023.,Renewable energy generation sources are the need of the hour to mitigate the challenges of fossil fuels, environmental impacts, large transmission and distribution losses, and variable consumption pattern. Renewable energy such as solar PV generation or wind generation is most unpredicted generation and depends upon weather constraints. A promising prediction of generation is a suitable solution for this problem. A machine learning-based conventional neural network algorithm is proposed to forecast the generation and calculate the errors in generation forecast. The generated energy is supplied to consumers, but the consumers consume electricity by their choice and an energy management model is required at the consumer end to supply the available predicted generation to develop a self-sustainable microgrid. The energy consumption at consumer end is essential to predict, and based on the predicted consumption at hourly bases, the cost model for the consumers is developed. The consumption profile is forecasted using long short-term memory as it is a good tool for time-series forecasting using available sequential load consumption data. The demand is forecasted and analyzed for selected load categories (i.e. fixed, non-shift-able, and shift-able loads). The evaluation matrix mean square error (MSE), mean absolute error, and root mean square error (RMSE) are used for the error calculations. The RMSE for solar and wind power generation forecasting have been 2.472 and 3.034, respectively. The RMSE for fixed, non-shiftable, and shiftable demand forecasting have been 0.027, 0.067, and 0.21, respectively. The results show that the proposed methods for predictions of solar, wind power generation and energy consumption forecast achieve better accuracy. The presented methodology can be implemented with energy storage for renewable energy production and demand management for energy from sustainable energy resources. © 2023 Taylor & Francis Group, LLC."
2,1,278,1_cnn_crop_crops_agriculture,"cnn,crop,crops,agriculture,agricultural,classification,convolutional,rice,neural,dataset","crop diseases pose a serious death trap to food safety, but their rapid disease diagnosis remains burdensome in many parts of the world due to the lack of the necessary foundation. These days, deep learning models have shown better performance than hi-tech machine learning techniques in several fields, with computer vision being one of the most noticeable cases. Agronomy is one of the domains in which deep learning concepts have been used for disease identification on different parts of plants. Having a disease is very normal and common, but prompt disease recognition and early avoidance of crop diseases are crucial for refining production. Although the standard convolutional neural network models identify the disease very accurately but require a higher computation cost and a large number of parameters. This requires a model to be developed which should be efficient and need to generate less no of parameters. This research work proposed a model to identify the diseases of plant leaves with greater accuracy and efficiency compared to the existing approaches. The standard models like AlexNet, VGG, and GoogleNet along with the proposed model were trained on the Night shed plants leaf which is available in the plant village. It has 9 categorical classes of diseases and healthy plant leaves. A range of parameters, including batch size, dropout, learning rate, and activation function were used to evaluate the models' performance or achievement. The proposed model achieved a disease classification accuracy rate of 93% to 95%. According to the findings of the accuracy tests, the suggested model is promising and may have a significant influence on the speed and accuracy with which disease-infected leaves are identified. © 2023, Ismail Saritas. All rights reserved.,Crop diseases cause a substantial loss in the quantum and quality of agricultural production. Regular monitoring may help in early stage disease detection an d thereby reduction in crop loss. An automatic plant disease identification system based on visual symptoms can provide a smart agriculture solution to such problems. Various solutions for plant disease identification have been provided by researchers using image processing, machine learning and deep learning techniques. In this paper a lightweight Convolutional Neural Network ‘VGG-ICNN’ is introduced for the identification of crop diseases using plant-leaf images. VGG-ICNN consists of around 6 million parameters that are substantially fewer than most of the available high performing deep learning models. The performance of the model is evaluated on five different public datasets covering a large number of crop varieties. These include multiple crop species datasets: PlantVillage and Embrapa with 38 and 93 categories, respectively, and single crop datasets: Apple, Maize, and Rice, each with four, four, and five categories, respectively. Experimental results demonstrate that the method outperforms some of the recent deep learning approaches on crop disease identification, with 99.16% accuracy on the PlantVillage dataset. The model is also shown to perform consistently well on all the five datasets, as compared with some recent lightweight CNN models. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Plant health is an important factor in agricultural production as it mostly affected by plant diseases. Due to plant diseases, the growth and crop yield gets affected which results in negative impact on agriculture in terms of economic loss to farmers. In plant disease management, early and accurate disease detection can control its spreading and avoid unnecessary loss to farmers. Traditionally, plant disease detection has been carried out through visual inspection by human experts. This method is based on subjective perception hence it has risk for error in detecting accurate disease. In recent past, researchers have proposes numerous machine learning approaches to detect the plant diseases. Due to advancement in artificial intelligence and electronic gadgets technology, there is large scope for improvement in neural network algorithms for detecting plant diseases early and accurately by extracting leaves features efficiently. To detect tomato plant diseases, the novel convolutional neural network (CNN) model has been proposed in this paper. The hierarchical mixed pooling technique for smoothing to sharpening approach has been used in proposed CNN model. The system uses tomato plant leaf images obtained from Kaggle dataset. The system has been trained with 1000 images of healthy leaf and 1000 images each for nine different diseases frequently occurs in tomato plant. The different training models has been framed and experimented to identify efficient hierarchy of pooling techniques. The CNN training model 3 exhibit smoothing to sharpening approach with “Average-Max-GlobalMax” mixed pooling hierarchy and depicts better performance with a training loss 28.88%, a validation loss 12.61%, a training accuracy 96.46%, and a validation accuracy 95.41% at 20 epochs. Also, the performance of designed system have been evaluated with different state-of-art deep learning algorithms and compared with proposed CNN model. © 2023 University of Bahrain. All rights reserved."
3,2,224,2_iot_intrusion_cybersecurity_intrusions,"iot,intrusion,cybersecurity,intrusions,ddos,attacks,attack,cloud,malware,sdn","Intrusion detection systems (IDS) play a critical role in safeguarding computer networks against unauthorized access and malicious activities. However, traditional IDS approaches face challenges in accurately detecting complex and evolving cyber threats. The proposed framework leverages the power of deep learning to automatically extract meaningful features from network traffic data, enabling more accurate and robust intrusion detection. The proposed deep convolutional neural network (DCNN) has been trained on large-scale datasets, incorporating both normal and malicious network traffic, to enable effective discrimination between normal and anomalous behavior. To evaluate the performance of the framework, a comprehensive performance evaluation approach is developed, considering key metrics such as detection accuracy, false positive rate, and computational efficiency. Additionally, GPU has been utilized for boosting the performance of the model, demonstrating the effectiveness and superiority of the deep CNN-based intrusion detection system over traditional methods. The novelty of this study lies in the development of a dependable intrusion detection system that harnesses the potential of DCNN for network traffic analysis. The proposed framework is evaluated with four publicly available IDS datasets, namely ISCX-IDS 2012, DDoS (Kaggle), CICIDS2017, and CICIDS2018. Our results demonstrate the effectiveness of the optimized DCNN model in improving IDS performance and accuracy. With detection accuracy levels ranging from 99.79% to 100%, our results underscore the model's efficacy, offering a dependable and efficient approach for the detection of cyber threats. The outcomes of this study have significant implications for network security, providing valuable insights for practitioners and researchers working towards building robust and intelligent intrusion detection systems. © 2023 The Author(s),Deep learning (DL) techniques are being widely researched for their effectiveness in detecting cyber intrusions against the Internet of Things (IoT). Time sensitive Critical Infrastructures (CIs) that rely on IoT require rapid detection of cyber intrusions close to the constrained devices in order to prevent service delays. Deep learning techniques perform better in detecting attacks compared to shallow machine learning algorithms and can be used for intrusion detection. However, communication overheads due to large volume of IoT data and computation requirements for deep learning models prevents effective application of deep learning models closer to the constrained devices. Existing IDS techniques are either based on shallow learning algorithms or not trained on relevant IoT datasets and furthermore not designed for distributed fog-cloud deployment. To counter these issues, we propose a novel fog-cloud based IoT intrusion detection framework which incorporates a distributed processing by splitting the dataset according to attack class and a feature selection step on time-series IoT data. This is followed by a deep learning Recurrent Neural Network (SimpleRNN and Bi-directional Long Short-Term Memory (LSTM)) for attack detection. The effectiveness of the proposed approach was evaluated using the high-dimensional BoT-IoT dataset which contains large volumes of realistic IoT attack traffic. Results show that feature selection methods significantly reduced the dataset size by 90% under the computation requirements without compromising on the attack detection ability. The models built on reduced dataset achieved higher recall rate compared to models trained on full feature set without loosing class differentiation ability. The SimpleRNN and Bi-LSTM models also did not suffer any underfitting or overfitting with the reduced feature space. The proposed deep learning based IoT intrusion detection framework is suitable for fog-cloud based deployment and can scale well even with large volumes of IoT data. © 2023 Elsevier B.V.,With the rapid development of the Internet of Things (IoT), there are several challenges pertaining to security in IoT applications. Compared with the characteristics of the traditional Internet, the IoT has many problems, such as large assets, complex and diverse structures, and lack of computing resources. Traditional network intrusion detection systems cannot meet the security needs of IoT applications. In view of this situation, this study applies cloud computing and machine learning to the intrusion detection system of IoT to improve detection performance. Usually, traditional intrusion detection algorithms require considerable time for training, and these intrusion detection algorithms are not suitable for cloud computing due to the limited computing power and storage capacity of cloud nodes; therefore, it is necessary to study intrusion detection algorithms with low weights, short training time, and high detection accuracy for deployment and application on cloud nodes. An appropriate classification algorithm is a primary factor for deploying cloud computing intrusion prevention systems and a prerequisite for the system to respond to intrusion and reduce intrusion threats. This paper discusses the problems related to IoT intrusion prevention in cloud computing environments. Based on the analysis of cloud computing security threats, this study extensively explores IoT intrusion detection, cloud node monitoring, and intrusion response in cloud computing environments by using cloud computing, an improved extreme learning machine, and other methods. We use the Multi-Feature Extraction Extreme Learning Machine (MFE-ELM) algorithm for cloud computing, which adds a multi-feature extraction process to cloud servers, and use the deployed MFE-ELM algorithm on cloud nodes to detect and discover network intrusions to cloud nodes. In our simulation experiments, a classical dataset for intrusion detection is selected as a test, and test steps such as data preprocessing, feature engineering, model training, and result analysis are performed. The experimental results show that the proposed algorithm can effectively detect and identify most network data packets with good model performance and achieve efficient intrusion detection for heterogeneous data of the IoT from cloud nodes. Furthermore, it can enable the cloud server to discover nodes with serious security threats in the cloud cluster in real time, so that further security protection measures can be taken to obtain the optimal intrusion response strategy for the cloud cluster. © 2022 Chongqing University of Posts and Telecommunications"
4,3,221,3_educational_students_student_teaching,"educational,students,student,teaching,pedagogical,study,education,learners,classroom,curriculum","The necessity for the development and enhancement of teacher commitment to satisfying students' learning needs in response to the COVID crisis is increasingly highlighted. It is not known, however, how to increase commitment in schoolteachers to boost online teaching in light of the fact that they, too, are struggling to cope with the rapid, unexpected change. A total of 601 teachers from primary and secondary schools across China participated in this study, with an average teaching experience of 15.9 years. Structural equation modelling was used to verify the significance of contextual, cognitive, affective and behavioural factors in boosting teachers' commitment to online teaching. The findings demonstrated that teacher agency played a complete mediating role in the predicting power of other factors to teacher commitment. Therefore, it was recommended that attention be paid to the practice and opportunities for teacher agentic actions, which necessitates real encounters with online teaching, allowing teachers to act meaningfully and initiate a new set of teaching strategies. Practitioner notes What is already known about this topic The large-scale transition to emergency online teaching serves as the catalyst for creating a blended or hybrid model of education provision in the long term. How hard teachers work to perform at their best and overcome obstacles to support students' learning needs in new environment relies on the intensity of teacher commitment to change. Online and blended learning requires teachers to not only be prepared for a diverse learning environment but also to build and rebuild their own identity as future teachers. What this paper adds This study adds to our knowledge of how traditional F2F classroom teachers reinvented their roles and responsibilities in response to the pandemic-driven challenges based on real-world experiences. As a result of the COVID-19 lockdown school closures, schoolteachers' commitment to enhancing online teaching efforts has increased. The study highlights the complete mediating role of teacher agency in the predicting power of cognitive and affective factors to teacher commitment. Implications for practice and/or policy To learn more about how to be a good online teacher, future teachers need greater deliberate effort in diverse online teaching activities. Future teachers should be equipped with not only new technological and remote instructional strategies and skills, but also with confidence in, value for, and actual experiences with online teaching in a technology-rich environment. For teachers to obtain hands-on experience in integrating technology with distance teaching pedagogy at a time of rapid change, schools should have some days online and offer blended learning opportunities wherever possible. © 2023 British Educational Research Association.,This study aimed to explore the integration of metacognition in online science education for college students and tested the feasibility of the learning model on students’ high order thinking skills (HOTS). The analyze, design, develop, implement, and evaluate (ADDIE) model was employed in this study. A needs analysis was conducted through interviews and questionnaire surveys to 21 science lecturers from primary school teacher education study programs at seven state universities and 14 private universities in Indonesia. In the development phase, the effectiveness of the model was examined through an experimental study involving three groups of students: experimental group (41 students), control group 1 (39 students), and control group 2 (39 students). The experimental study was performed using the randomized pretest-posttest comparison group design. The research hypothesis was investigated using a general linear model and multivariate analysis of variance. Through awareness-building, essential questioning, planning, monitoring, evaluating, and reflecting, this study successfully integrated metacognition into online science education. The model's learning syntax incorporated both synchronous and asynchronous learning activities. Virtual and contextual projects are critical components of this approach because they demonstrate how metacognition is regulated. Expert judgment indicated that the model under development was highly feasible. The experimental study established that the learning model had a considerable effect on students’ HOTS, which rose by 75% (a large effect) due to the model’s implementation. © 2022, Institute of Advanced Engineering and Science. All rights reserved.,Recent years have seen a sharp increase in the transition from traditional teaching methods to online distance teaching and learning in all educational institutions including schools and higher education institutions. This change means that student-teachers need to have online learning experience and acquire professional skills needed to become online distance teachers. In the last two decades, research has investigated various aspects of online teaching but a main subject which has not been studied in depth is the learners’ feeling of belonging in an online distance course, especially among student-teachers. The research investigated the contribution of student-teachers’ self-perceived readiness for learning online and a collaborative online learning environment to the learners’ sense of belonging. For this purpose, quantitative and qualitative data were gathered from pre- and post-tests and personal and collaborative blogs from 172 student-teachers’ studying in two large teacher-education colleges in the center of Israel, to measure their self-perceived readiness for online collaborative learning, their sense of belonging and the learning experiences and changes they underwent along the course. It was found that a student-teacher, who had previously enjoyed a good experience in a collaborative online course, and had strong technological literacy for online learning, performed better in an online course, operated optimally in collaborative task performance, and felt a strong sense of belonging to the group at the course’s end. These findings can inform future teacher training, the formation of new pedagogic models and teaching methods for collaborative online environments and their implementation in teaching-learning. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
5,4,193,4_chatbots_educational_academic_chatbot,"chatbots,educational,academic,chatbot,students,essays,study,education,writing,programming","Natural language processing (NLP) has been studied in computing for decades. Recent technological advancements have led to the development of sophisticated artificial intelligence (AI) models, such as Chat Generative Pre-trained Transformer (ChatGPT). These models can perform a range of language tasks and generate human-like responses, which offers exciting prospects for academic efficiency. This manuscript aims at (i) exploring the potential benefits and threats of ChatGPT and other NLP technologies in academic writing and research publications; (ii) highlights the ethical considerations involved in using these tools, and (iii) consider the impact they may have on the authenticity and credibility of academic work. This study involved a literature review of relevant scholarly articles published in peer-reviewed journals indexed in Scopus as quartile 1. The search used keywords such as “ChatGPT,” “AI-generated text,” “academic writing,” and “natural language processing.” The analysis was carried out using a quasi-qualitative approach, which involved reading and critically evaluating the sources and identifying relevant data to support the research questions. The study found that ChatGPT and other NLP technologies have the potential to enhance academic writing and research efficiency. However, their use also raises concerns about the impact on the authenticity and credibility of academic work. The study highlights the need for comprehensive discussions on the potential use, threats, and limitations of these tools, emphasizing the importance of ethical and academic principles, with human intelligence and critical thinking at the forefront of the research process. This study highlights the need for comprehensive debates and ethical considerations involved in their use. The study also recommends that academics exercise caution when using these tools and ensure transparency in their use, emphasizing the importance of human intelligence and critical thinking in academic work. © 2023 Institute of Sport. All rights reserved.,Large Language Models (LLMs) and conversational-style generative artificial intelligence (AI) are causing major disruption to higher education pedagogy. The emergence of tools like ChatGPT has raised concerns about plagiarism detection but also presents opportunities for educators to leverage AI to build supportive learning environments. In this commentary, we explore the potential of AI-augmented teaching and learning practice in higher education, discussing both the productive affordances and challenges associated with these technologies. We offer instructional advice for writing instructional text to guide the generation of quality outputs from AI models, as well as a case study to illustrate using AI for assessment design. Ultimately, we suggest that AI should be seen as one tool among many that can be used to enhance teaching and learning outcomes in higher education. Practitioner Notes 1. Learning to write effective instructional prompts for AI models will help augment learning and teaching practice. 2. AI models offer the potential for significant productive affordances, including personalised feedback, adaptive learning pathways, and enhanced student engagement. 3. To successfully integrate AI into higher education, institutions must prioritise faculty development programs that provide training and support for educators to effectively use these technologies in the classroom. 4. Institutions must ensure that AI is used in a way that aligns with their values and mission and that students are informed about how their data is being used. 5. It is important to recognise that AI is not a panacea for all of the challenges facing higher education. Rather, it should be seen as one tool among many that can be used to enhance teaching and learning outcomes. © 2023, University of Wollongong. All rights reserved.,In recent years, the rise of advanced artificial intelligence technologies has had a profound impact on many fields, including education and research. One such technology is ChatGPT, a powerful large language model developed by OpenAI. This technology offers exciting opportunities for students and educators, including personalized feedback, increased accessibility, interactive conversations, lesson preparation, evaluation, and new ways to teach complex concepts. However, ChatGPT poses different threats to the traditional education and research system, including the possibility of cheating on online exams, human-like text generation, diminished critical thinking skills, and difficulties in evaluating information generated by ChatGPT. This study explores the potential opportunities and threats that ChatGPT poses to overall education from the perspective of students and educators. Furthermore, for programming learning, we explore how ChatGPT helps students improve their programming skills. To demonstrate this, we conducted different coding-related experiments with ChatGPT, including code generation from problem descriptions, pseudocode generation of algorithms from texts, and code correction. The generated codes are validated with an online judge system to evaluate their accuracy. In addition, we conducted several surveys with students and teachers to find out how ChatGPT supports programming learning and teaching. Finally, we present the survey results and analysis. © 2023 by the authors."
6,5,161,5_traffic_congestion_prediction_road,"traffic,congestion,prediction,road,forecasting,transportation,forecast,networks,neural,predict","Intelligent Transportation Systems (ITS) are becoming increasingly important as traditional traffic management systems struggle to handle the rapid growth of vehicles on the road. Accurate traffic prediction is a critical component of ITS, as it can help improve traffic management, avoid congested roads, and allocate resources more efficiently for connected vehicles. However, modeling traffic in a large and interconnected road network is challenging because of its complex spatio-temporal data. While classical statistics and machine learning methods have been used for traffic prediction, they have limited ability to handle complex traffic data, leading to unsatisfactory accuracy. In recent years, deep learning methods, such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), have shown superior capabilities for traffic prediction. However, most CNN-based models are built for Euclidean grid-structured data, while traffic road network data are irregular and better formatted as graph-structured data. Graph Convolutional Neural Networks (GCNs) have emerged to extend convolution operations to more general graph-structured data. This paper reviews recent developments in traffic prediction using deep learning, focusing on GCNs as a promising technique for handling irregular, graph-structured traffic data. We also propose a novel GCN-based method that leverages attention mechanisms to capture both local and long-range dependencies in traffic data with Kalman Filter, and we demonstrate its effectiveness through experiments on real-world datasets where the model achieved around 5% higher accuracy compared to the original model. © (2023). All Rights Reserved.,Large-scale and diversified traffic data resources strongly support research into estimating urban traffic states and predicting traffic flow. There are many studies on traffic prediction, but there is still not a universally applicable real-world traffic flow prediction method. This paper regards urban road sections as a microscopic traffic system. Based on a deep understanding of the traffic state of road sections, it proposes a pertinent traffic flow prediction framework based on the traffic factor state network (TFSN) framework by combining model-driven methods with machine learning to identify traffic patterns in road sections. For different road traffic patterns, it proves mathematically that the state of traffic flow in each period tends to be the state of the corresponding period with greater probability. According to different road patterns and traffic states, suitable traffic flow modeling and prediction methods were selected. The case shows that this method can improve the accuracy of traffic flow predictions. The research results demonstrate that the average absolute percentage error of traffic flow predictions in urban sections selected with different characteristics and models is reduced by 7.51% compared with the direct prediction error method, verifying the effectiveness and usability of the proposed prediction framework. © 2023 Elsevier B.V.,Purpose: The study aims to propose an intelligent real-time traffic model to address the traffic congestion problem. The proposed model assists the urban population in their everyday lives by assessing the probability of road accidents and accurate traffic information prediction. It also helps in reducing overall carbon dioxide emissions in the environment and assists the urban population in their everyday lives by increasing overall transportation quality. Design/methodology/approach: This study offered a real-time traffic model based on the analysis of numerous sensor data. Real-time traffic prediction systems can identify and visualize current traffic conditions on a particular lane. The proposed model incorporated data from road sensors as well as a variety of other sources. It is difficult to capture and process large amounts of sensor data in real time. Sensor data is consumed by streaming analytics platforms that use big data technologies, which is then processed using a range of deep learning and machine learning techniques. Findings: The study provided in this paper would fill a gap in the data analytics sector by delivering a more accurate and trustworthy model that uses internet of things sensor data and other data sources. This method can also assist organizations such as transit agencies and public safety departments in making strategic decisions by incorporating it into their platforms. Research limitations/implications: The model has a big flaw in that it makes predictions for the period following January 2020 that are not particularly accurate. This, however, is not a flaw in the model; rather, it is a flaw in Covid-19, the global epidemic. The global pandemic has impacted the traffic scenario, resulting in erratic data for the period after February 2020. However, once the circumstance returns to normal, the authors are confident in their model’s ability to produce accurate forecasts. Practical implications: To help users choose when to go, this study intended to pinpoint the causes of traffic congestion on the highways in the Bay Area as well as forecast real-time traffic speeds. To determine the best attributes that influence traffic speed in this study, the authors obtained data from the Caltrans performance measurement system (PeMS), reviewed it and used multiple models. The authors developed a model that can forecast traffic speed while accounting for outside variables like weather and incident data, with decent accuracy and generalizability. To assist users in determining traffic congestion at a certain location on a specific day, the forecast method uses a graphical user interface. This user interface has been designed to be readily expanded in the future as the project’s scope and usefulness increase. The authors’ Web-based traffic speed prediction platform is useful for both municipal planners and individual travellers. The authors were able to get excellent results by using five years of data (2015–2019) to train the models and forecast outcomes for 2020 data. The authors’ algorithm produced highly accurate predictions when tested using data from January 2020. The benefits of this model include accurate traffic speed forecasts for California’s four main freeways (Freeway 101, I-680, 880 and 280) for a specific place on a certain date. The scalable model performs better than the vast majority of earlier models created by other scholars in the field. The government would benefit from better planning and execution of new transportation projects if this programme were to be extended across the entire state of California. This initiative could be expanded to include the full state of California, assisting the government in better planning and implementing new transportation projects. Social implications: To estimate traffic congestion, the proposed model takes into account a variety of data sources, including weather and incident data. According to traffic congestion statistics, “bottlenecks” account for 40% of traffic congestion, “traffic incidents” account for 25% and “work zones” account for 10% (Traffic Congestion Statistics). As a result, incident data must be considered for analysis. The study uses traffic, weather and event data from the previous five years to estimate traffic congestion in any given area. As a result, the results predicted by the proposed model would be more accurate, and commuters who need to schedule ahead of time for work would benefit greatly. Originality/value: The proposed work allows the user to choose the optimum time and mode of transportation for them. The underlying idea behind this model is that if a car spends more time on the road, it will cause traffic congestion. The proposed system encourages users to arrive at their location in a short period of time. Congestion is an indicator that public transportation needs to be expanded. The optimum route is compared to other kinds of public transit using this methodology (Greenfield, 2014). If the commute time is comparable to that of private car transportation during peak hours, consumers should take public transportation. © 2022, Emerald Publishing Limited."
7,6,153,6_nlp_corpus_text_annotated,"nlp,corpus,text,annotated,clinical,structured,entity,learning,entities,record","OBJECTIVE: This work aims to explore the value of Dutch unstructured data, in combination with structured data, for the development of prognostic prediction models in a general practitioner (GP) setting. MATERIALS AND METHODS: We trained and validated prediction models for 4 common clinical prediction problems using various sparse text representations, common prediction algorithms, and observational GP electronic health record (EHR) data. We trained and validated 84 models internally and externally on data from different EHR systems. RESULTS: On average, over all the different text representations and prediction algorithms, models only using text data performed better or similar to models using structured data alone in 2 prediction tasks. Additionally, in these 2 tasks, the combination of structured and text data outperformed models using structured or text data alone. No large performance differences were found between the different text representations and prediction algorithms. DISCUSSION: Our findings indicate that the use of unstructured data alone can result in well-performing prediction models for some clinical prediction problems. Furthermore, the performance improvement achieved by combining structured and text data highlights the added value. Additionally, we demonstrate the significance of clinical natural language processing research in languages other than English and the possibility of validating text-based prediction models across various EHR systems. CONCLUSION: Our study highlights the potential benefits of incorporating unstructured data in clinical prediction models in a GP setting. Although the added value of unstructured data may vary depending on the specific prediction task, our findings suggest that it has the potential to enhance patient care. © The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association.,Background: Longitudinal data on key cancer outcomes for clinical research, such as response to treatment and disease progression, are not captured in standard cancer registry reporting. Manual extraction of such outcomes from unstructured electronic health records is a slow, resource-intensive process. Natural language processing (NLP) methods can accelerate outcome annotation, but they require substantial labeled data. Transfer learning based on language modeling, particularly using the Transformer architecture, has achieved improvements in NLP performance. However, there has been no systematic evaluation of NLP model training strategies on the extraction of cancer outcomes from unstructured text. Results: We evaluated the performance of nine NLP models at the two tasks of identifying cancer response and cancer progression within imaging reports at a single academic center among patients with non-small cell lung cancer. We trained the classification models under different conditions, including training sample size, classification architecture, and language model pre-training. The training involved a labeled dataset of 14,218 imaging reports for 1112 patients with lung cancer. A subset of models was based on a pre-trained language model, DFCI-ImagingBERT, created by further pre-training a BERT-based model using an unlabeled dataset of 662,579 reports from 27,483 patients with cancer from our center. A classifier based on our DFCI-ImagingBERT, trained on more than 200 patients, achieved the best results in most experiments; however, these results were marginally better than simpler “bag of words” or convolutional neural network models. Conclusion: When developing AI models to extract outcomes from imaging reports for clinical cancer research, if computational resources are plentiful but labeled training data are limited, large language models can be used for zero- or few-shot learning to achieve reasonable performance. When computational resources are more limited but labeled training data are readily available, even simple machine learning architectures can achieve good performance for such tasks. © 2023, BioMed Central Ltd., part of Springer Nature.,Introduction: In the medical field, electronic medical records contain a large amount of textual information, and the unstructured nature of this information makes data extraction and analysis challenging. Therefore, automatic extraction of entity information from electronic medical records has become a significant issue in the healthcare domain. Methods: To address this problem, this paper proposes a deep learning-based entity information extraction model called Entity-BERT. The model aims to leverage the powerful feature extraction capabilities of deep learning and the pre-training language representation learning of BERT(Bidirectional Encoder Representations from Transformers), enabling it to automatically learn and recognize various entity types in medical electronic records, including medical terminologies, disease names, drug information, and more, providing more effective support for medical research and clinical practices. The Entity-BERT model utilizes a multi-layer neural network and cross-attention mechanism to process and fuse information at different levels and types, resembling the hierarchical and distributed processing of the human brain. Additionally, the model employs pre-trained language and sequence models to process and learn textual data, sharing similarities with the language processing and semantic understanding of the human brain. Furthermore, the Entity-BERT model can capture contextual information and long-term dependencies, combining the cross-attention mechanism to handle the complex and diverse language expressions in electronic medical records, resembling the information processing method of the human brain in many aspects. Additionally, exploring how to utilize competitive learning, adaptive regulation, and synaptic plasticity to optimize the model's prediction results, automatically adjust its parameters, and achieve adaptive learning and dynamic adjustments from the perspective of neuroscience and brain-like cognition is of interest. Results and discussion: Experimental results demonstrate that the Entity-BERT model achieves outstanding performance in entity recognition tasks within electronic medical records, surpassing other existing entity recognition models. This research not only provides more efficient and accurate natural language processing technology for the medical and health field but also introduces new ideas and directions for the design and optimization of deep learning models. Copyright © 2023 Lu, Jiang, Shi, Zhong, Gu, Huangfu and Gong."
8,7,147,7_cnn_pneumonia_radiologists_chest,"cnn,pneumonia,radiologists,chest,lung,coronavirus,scan,scans,imaging,radiography","Pneumonia is a life-threatening disease. Computer tomography (CT) imaging is broadly used for diagnosing pneumonia. To assist radiologists in accurately and efficiently detecting pneumonia from CT scans, many deep learning methods have been developed. These methods require large amounts of annotated CT scans, which are difficult to obtain due to privacy concerns and high annotation costs. To address this problem, we develop a three-level optimization based method which leverages CT data from a source domain to mitigate the lack of labeled CT scans in a target domain. Our method automatically identifies and downweights low-quality source CT data examples which are noisy or have large domain discrepancy with target data, by minimizing the validation loss of a target model trained on reweighted source data. On a target dataset with 2218 CT scans and a source dataset with 349 CT images, our method achieves an F1 score of 91.8% in detecting pneumonia and an F1 score of 92.4% in detecting other types of pneumonia, which are significantly better than those achieved by state-of-the-art baseline methods. © 2023, The Author(s).,Combating the covid19 scourge is a prime concern for the human race today. Rapid diagnosis is critical to identify the infection accurately. Due to the prevalence of public health crisis, reaction-based blood tests are the customary approach for identifying covid19. As a result, scientists are analyzing screening methods like deep layered machine learning on chest radiographs. Despite their usefulness, these approaches have large computational costs, rendering them unworkable in practice. This study's main goal is to establish an accurate yet efficient method for predicting SARS-CoV-2 infection (Severe Acute Respiratory Syndrome CoronaVirus 2) using chest radiography pictures. We utilized and enhanced the graph-based family of neural networks to achieve the stated goal. The IsoCore algorithm is trained on a collection of X-ray images separated into four categories: healthy, Covid19, viral pneumonia, and bacterial pneumonia. The IsoCore model has 5 to 10 times fewer parameters than the other tested designs. It attains an overall accuracy of 99.79%. We believe the acquired results are the most ideal in the deep inference domain at this time. This proposed model might be employed by doctors via phones. © The Authors.,The paper uses convolutional neural networks (CNNs) to analyze radiography pictures and discriminate between areas afflicted by pneumonia and normal lung tissue. A sizable dataset of annotated chest X-rays is used to train the deep learning model, which enables it to pick up on complex patterns and characteristics linked to pneumonia. Pneumonia is a significant respiratory disease that affects a large number of individuals worldwide. Timely and accurate diagnosis of pneumonia plays a crucial role in effective treatment and management of the disease. We evaluate the performance of several up-to-date convolution neural network (CNN) architectures, namely ResNet-50, VGG-16, and DenseNet-121, then compare their results with traditional machine learning classifiers. Recent advancements in deep learning methods have shown accurate results in the investigation and diagnosis of medical image data, including the detection of pneumonia. This paper examines different deep-learning methods for categorizing pneumonia from lung X-ray imagery. Our results show that deep learning techniques performed better than conventional machine learning techniques in classifying pneumonia, with an estimated accuracy of 95% across all of the examined CNN models. These results demonstrate the potential of deep learning algorithms to significantly improve the accuracy and effectiveness of pneumonia diagnosis, supporting physicians in making knowledgeable decisions about patient care. © Copyright by International Academic Publishing House (IAPH)"
9,8,143,8_hypergraph_graphs_subgraph_networks,"hypergraph,graphs,subgraph,networks,graph,nodes,embeddings,subgraphs,vertex,embedding","Unsupervised graph representation learning aims to learn low-dimensional node embeddings without supervision while preserving graph topological structures and node attributive features. Previous Graph Neural Networks (GNN) require a large number of labeled nodes, which may not be accessible in real-world applications. To this end, we present a novel unsupervised graph neural network model with Cluster-aware Self-training and Refining (CLEAR). Specifically, in the proposed CLEAR model, we perform clustering on the node embeddings and update the model parameters by predicting the cluster assignments. To avoid degenerate solutions of clustering, we formulate the graph clustering problem as an optimal transport problem and leverage a balanced clustering strategy. Moreover, we observe that graphs often contain inter-class edges, which mislead the GNN model to aggregate noisy information from neighborhood nodes. Therefore, we propose to refine the graph topology by strengthening intra-class edges and reducing node connections between different classes based on cluster labels, which better preserves cluster structures in the embedding space. We conduct comprehensive experiments on two benchmark tasks using real-world datasets. The results demonstrate the superior performance of the proposed model over baseline methods. Notably, our model gains over 7% improvements in terms of accuracy on node clustering over state-of-the-arts. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.,Graph embedding is an important technique used for representing graph structure data that preserves intrinsic features in a low-dimensional space suitable for graph-based applications. Graphs containing node attributes and weighted links are commonly employed to model various real-world problems and issues in computer science. In recent years, a hot research topic has been the exploitation of diverse information, including node attributes and topological semantic information, in graph embedding. However, due to limitations in deep learning based on neural networks, such information has not been fully utilized nor adequately integrated in existing models, leaving graph embedding unsatisfactory, especially for large resource graphs (e.g., knowledge graphs and task interaction graphs). In this study, we introduce a resource-centric graph embedding approach based on deep random forests learning, which reconstructs graphs using a deep autoencoder to achieve high effectiveness. To accomplish this, our approach employs three key components. The first component is a preprocessor driven by graph similarity, alongside modularity and self-attention modules, to comprehensively integrate graph representation. The second component utilizes local graph information structures to enhance the raw graph. Finally, we integrate diverse information using multi-grained scanning and dual-level cascade forests in the deep learning extractor and generator, ultimately producing the final graph embedding. Experimental results on seven real-world scenarios show that our approach outperforms state-of-the-art embedding methods. © 2023 Elsevier Ltd,Information aggregation and propagation over networks via graph neural networks (GNNs) plays an important role in node or graph representation learning, which currently depend on the calculation with a fixed adjacency matrix, facing over-smoothing problem, and difficulty to stack multiple layers for high-level representations. In contrast, Transformer calculates an importance score for each node to learn its embedding via the attention mechanism and has achieved great successes in many natural language processing (NLP) and computer vision (CV) tasks. However, Transformer is inflexible to extend to graphs, as its input and output must have the same dimension. It will also become intractable to allocate attention over a large-scale graph due to distractions. Moreover, most graph Transformers are trained in supervised ways, which consume additional resources to annotate samples with potentially wrong labels and have limited generalization of representations. Therefore, this article attempts to build a new Sparse Graph Transformer with Contrastive learning for graph representation learning, called SGTC. Specifically, we first employ centrality measures to remove the redundant topological information from input graph according to the influences of nodes and edges, then disturb the pruned graph to get two different augmentation views, and learn node representations in a contrastive manner. Besides, a novel sparse attention mechanism is also proposed to capture structural features of graphs, which effectively save memory and training time. SGTC can produce low-dimensional and high-order node representations, which have better generalization for multiple tasks. The proposed model is evaluated on three downstream tasks over six networks, and experimental results confirm its superior performance against the state-of-the-art baselines. © 2014 IEEE."
10,9,141,9_electroencephalogram_electroencephalography_eeg_braincomputer,"electroencephalogram,electroencephalography,eeg,braincomputer,bci,classifier,classification,epileptic,eegbased,epilepsy","Introduction: Motor imagery electroencephalography (MI-EEG) has significant application value in the field of rehabilitation, and is a research hotspot in the brain-computer interface (BCI) field. Due to the small training sample size of MI-EEG of a single subject and the large individual differences among different subjects, existing classification models have low accuracy and poor generalization ability in MI classification tasks. Methods: To solve this problem, this paper proposes a electroencephalography (EEG) joint feature classification algorithm based on instance transfer and ensemble learning. Firstly, the source domain and target domain data are preprocessed, and then common space mode (CSP) and power spectral density (PSD) are used to extract spatial and frequency domain features respectively, which are combined into EEG joint features. Finally, an ensemble learning algorithm based on kernel mean matching (KMM) and transfer learning adaptive boosting (TrAdaBoost) is used to classify MI-EEG. Results: To validate the effectiveness of the algorithm, this paper compared and analyzed different algorithms on the BCI Competition IV Dataset 2a, and further verified the stability and effectiveness of the algorithm on the BCI Competition IV Dataset 2b. The experimental results show that the algorithm has an average accuracy of 91.5% and 83.7% on Dataset 2a and Dataset 2b, respectively, which is significantly better than other algorithms. Discussion: The statement explains that the algorithm fully exploits EEG signals and enriches EEG features, improves the recognition of the MI signals, and provides a new approach to solving the above problem. Copyright © 2023 Wang, Dai, Liu, Chen, Hu, Hu and Li.,Electroencephalogram (EEG) signals have been widely studied in human emotion recognition. The majority of existing EEG emotion recognition algorithms utilize dozens or hundreds of electrodes covering the whole scalp region (denoted as full-channel EEG devices in this paper). Nowadays, more and more portable and miniature EEG devices with only a few electrodes (denoted as few-channel EEG devices in this paper) are emerging. However, emotion recognition from few-channel EEG data is challenging because the device can only capture EEG signals from a portion of the brain area. Moreover, existing full-channel algorithms cannot be directly adapted to few-channel EEG signals due to the significant inter-variation between full-channel and few-channel EEG devices. To address these challenges, we propose a novel few-channel EEG emotion recognition framework from the perspective of knowledge transfer. We leverage full-channel EEG signals to provide supplementary information for few-channel signals via a transfer learning-based model CD-EmotionNet, which consists of a base emotion model for efficient emotional feature extraction and a cross-device transfer learning strategy. This strategy helps to enhance emotion recognition performance on few-channel EEG data by utilizing knowledge learned from full-channel EEG data. To evaluate our cross-device EEG emotion transfer learning framework, we construct an emotion dataset containing paired 18-channel and 5-channel EEG signals from 25 subjects, as well as 5-channel EEG signals from 13 other subjects. Extensive experiments show that our framework outperforms state-of-the-art EEG emotion recognition methods by a large margin. IEEE,Electroencephalography (EEG) is often used to evaluate several types of neurological brain disorders because of its noninvasive and high temporal resolution. In contrast to electrocardiography (ECG), EEG can be uncomfortable and inconvenient for patients. Moreover, deep-learning techniques require a large dataset and a long time for training from scratch. Therefore, in this study, EEG–EEG or EEG–ECG transfer learning strategies were applied to explore their effectiveness for the training of simple cross-domain convolutional neural networks (CNNs) used in seizure prediction and sleep staging systems, respectively. The seizure model detected interictal and preictal periods, whereas the sleep staging model classified signals into five stages. The patient-specific seizure prediction model with six frozen layers achieved 100% accuracy for seven out of nine patients and required only 40 s of training time for personalization. Moreover, the cross-signal transfer learning EEG–ECG model for sleep staging achieved an accuracy approximately 2.5% higher than that of the ECG model; additionally, the training time was reduced by >50%. In summary, transfer learning from an EEG model to produce personalized models for a more convenient signal can both reduce the training time and increase the accuracy; moreover, challenges such as data insufficiency, variability, and inefficiency can be effectively overcome. © 2023 by the authors."
11,10,139,10_neuroprotective_hippocampus_alzheimers_hippocampal,"neuroprotective,hippocampus,alzheimers,hippocampal,microglia,neurotrophic,neurons,neuronal,brain,cognitive","Long-term high-fat diet (HFD) in adolescents leads to impaired hippocampal function and increases the risk of cognitive impairment. Studies have shown that HFD activates hippocampal microglia and induces hippocampal inflammation, which is an important factor for cognitive impairment. Electroacupuncture stimulation (ES), a nerve stimulation therapy, is anti-inflammatory. This study explored its therapeutic potential and mechanism of action in obesity-related cognitive impairment. 4-week-old C57 mice were given either normal or HFD for 22 weeks. At 19 weeks, some of the HFD mice were treated with ES and nigericin sodium salt. The cognitive behavior was assessed through Morris water maze test at 23 weeks. Western blotting was used to detect the expression levels of pro-inflammatory molecules IL-1? and IL-1R, synaptic plasticity related proteins synaptophysin and Postsynaptic Density-95 (PSD-95), and apoptotic molecules (Caspase-3 and Bcl-2), in the hippocampus. The number, morphology, and status of microglia, along with the brain-derived neurotrophic factor?BDNF? content, were analyzed using immunofluorescence. ES treatment improved cognitive deficits in HFD model mice, and decreased the expressions of microglial activation marker, CD68, and microglial BDNF. Inhibition of proinflammatory cytokine, IL-1?, and IL-1R promoted PSD-95 and synaptophysin expressions. Peripheral NLRP3 inflammasome agonist injections exacerbated the cognitive deficits in HFD mice and promoted the expressions of IL-1? and IL-1R in the hippocampus. The microglia showed obvious morphological damage and apoptosis. Collectively, our findings suggest that ES inhibits inflammation, regulates microglial BDNF, and causes remodeling of hippocampal function in mice to counteract obesity-like induced cognitive impairment. Overexcitation of peripheral inflammasome complexes induces hippocampal microglia apoptosis, which hinders the effects of ES. © 2023 Elsevier B.V.,Purpose: To elucidate the potential mechanisms of QFY for the treatment of Alzheimer’s Disease (AD), and explore the effective substances of QFY. Materials and Methods: UPLC-LTQ-Orbitrap-MS was used to identify the chemical constituents of the serum samples and the cerebrospinal fluid samples of rats after QFY administration. Network pharmacology was used to predict potential targets and pathways of QFY against AD. The AD mice model was established by subcutaneous injection of D-gal for 8 consecutive weeks. New object recognition (NOR) and Morris water maze test (MWM) were used to evaluate the learning and memory abilities of mice. Moreover, the levels of TNF-?, IL-1?, and IL-18 in the brain hippocampus of mice were determined by ELISA. The expression of Bax, Bcl-2, Caspase-1, PSD95, SYP, ICAM-1 and MCP-1 proteins in the hippocampus was detected by Western blotting. Furthermore, qRT-PCR was used to detect the gene expressions of PSD95, SYP, M1 and M2 polarization markers of microglia, including iNOS, CD16, ARG-1, and IL-10 in the hippocampus. Results: A total of 51 prototype compounds were detected in rat serum and 15 prototype components were identified in rat cerebrospinal fluid. Behavioral experiments revealed that QFY significantly increased the recognition index, decreased the escape latency, increased the platform crossing times and increased the residence time in the target quadrant. QFY also could alleviate the ultrastructural pathological changes in the hippocampus of AD mice. Meanwhile, QFY treatment suppressed the expression of inflammatory factors, such as TNF-?, IL-1?, and IL-18. QFY improved the synaptic plasticity of the hippocampus in D-gal model mice by significantly increasing the expression of proteins and mRNAs of PSD95 and SYP. Conclusion: QFY could effectively improve the learning and memory impairment of D-gal-induced AD mice by inhibiting the excessive activation of microglia, enhancing the expression of M2 microglia, inhibiting the increase of inflammatory factors, cell adhesion factors and chemokines, anti-apoptosis, and improving synaptic plasticity. © 2023 Lei et al.,Ethnopharmacological relevance: Epimedium brevicornu Maxim. is a traditional medicinal Chinese herb that is enriched with flavonoids, which have remarkably high medicinal value. Icariin (ICA) is a marker compound isolated from the total flavonoids of Epimedium brevicornu Maxim. It has been shown to improve Neurodegenerative disease, therefore, ICA is probably a potential drug for treating AD. Materials and methods: The 6–8-week-old SPF-class male ICR mice were randomly divided into 8 groups for modeling, and then the mice were administered orally with ICA for 21 days. The behavioral experiments were conducted to evaluate if learning and memory behavior were absent in mice, confirming that infusion of Amyloid ?-protein (A?)1-42 caused significant memory impairment. The morphological changes and damage of neurons in the mice's brains were observed by HE and Nissl staining. The spinous protrusions (dendritic spines) on neuronal dendrites were investigated by Golgi-Cox staining. The molecular mechanism of ICA was examined by Western Blot. The protein docking of ICA and Donepezil with BDNF were analyzed to determine their interaction. Results: The behavioral experimental results showed that in A?1-42-induced AD mice, the learning and memory abilities were improved after using ICA. At the same time, the low, medium, and high doses of ICA could reduce the content of A?1-42 in the hippocampus of AD mice, repair neuronal damage, enhance synaptic plasticity, as well as increase the expression of BDNF, Tr?B, CREB, Akt, GAP43, PSD95, and SYN proteins in the hippocampus of mice. However, the effect with high doses of ICA is more pronounced. The high-dose administration of ICA has the best therapeutic effect on AD mice. After administering the inhibitor k252a, the therapeutic effect of ICA was reversed. The macromolecular docking results of ICA and BDNF protein demonstrated a strong interaction of ?7.8 kcal/mol, which indicates that ICA plays a therapeutic role in AD mice by regulating the BDNF-Tr?B signaling pathway. Conclusions: The results confirm that ICA can repair neuronal damage, enhance synaptic plasticity, as well as ultimately improve learning and memory impairment through the regulation of the BDNF-Tr?B signaling pathway. © 2023 Elsevier B.V."
12,11,135,11_offloading_scheduling_edge_5g,"offloading,scheduling,edge,5g,iot,caching,network,mobile,networks,servers","In view of the randomness distribution of multiple users in the dynamic large-scale Internet of Things (IoT) scenario, comprehensively formulating available resources for fog nodes in the area and achieving computation services at low cost have become great challenges. As a result, this paper studies an efficient and intelligent computation offloading mechanism with resource allocation. Specifically, an optimization problem is formulated to minimize the total energy consumption of all tasks under the joint optimization of computation offloading decisions, bandwidth resources and transmission power. Meanwhile, a Twin Delayed Deep Deterministic Policy Gradient-based Intelligent Computation Offloading (TD3PG-ICO) algorithm is proposed to solve this optimization problem. By combining the concept of the actor critic algorithm, the proposed algorithm designs two independent critic networks that can avoid the subjective prediction of a single critic network and better guide the policy network to generate the global optimal computation offloading policy. Additionally, this algorithm introduces a continuous variable discretization operation to select the target offloading node with random probability. The available resources of the target node are dynamically allocated to improve the model decision-making effect. Finally, the simulation results show that this proposed algorithm has faster convergence speed and good robustness. It can always approach the greedy algorithm with respect to the lowest total energy consumption. Furthermore, compared with full local and Deep Q-learning Network (DQN)-based computation offloading schemes, the total energy consumption can be reduced by an average of 15.53% and 6.41%, respectively. © 2022 Chongqing University of Posts and Telecommunications,Due to the performance and resource limitations of wireless devices at the edge of the network, the intrusion detection system deployed on the mobile edge network will cause severe packet loss when faced with large traffic. Based on this, a collaborative intrusion detection system (CIDS) architecture applied to mobile edge computing is proposed, which can offload part of the detection tasks to an intrusion detection system with better performance and resources on the edge server. On this basis, a task offloading scheduling algorithm based on Deep Q Network (DQN) is proposed. First, the time delay, energy consumption, and offloading decision models are established. Then, the task scheduling process is described as a Markov decision process and the relevant space and value function are established. Finally, the problem of excessive state and action space in Q-learning is solved by the Deep Q Network. Experiments have shown that our proposed scheme enables the system to have advantages over the comparative algorithms in terms of response time, energy consumption, and packet loss rate, etc. © 2022 Elsevier Ltd,In mobile edge computing (MEC), offloading computing tasks from edge clients to edge nodes can reduce the burden on edge clients, especially for delay-sensitive tasks, they must be completed within the deadline. However, when the edge nodes receive a large number of tasks, the waiting time of the tasks may be too long, and some tasks may even be dropped due to timeout. To address these problems, we model task offloading as a long-term optimization problem based on Markov decision process (MDP), we consider task queuing models on edge clients and edge nodes to optimize distributed task offloading schemes, and the current workload of edge nodes prediction model for dynamic task scheduling to avoid excessive workload of edge nodes. A distributed dynamic task offloading algorithm based on deep reinforcement learning is proposed, and a recurrent neural network controlled by Gated Recurrent Unit (GRU) and Dueling-DQN and Double-DQN (DDQN) techniques enable each client to make its own offloading decisions without knowing other information. In order to improve the training efficiency and the stability of the strategy, a queue selection algorithm is proposed to reduce the action space. Experimental results show that, compared with some existing algorithms, the proposed algorithm can effectively predict the workload of edge nodes and make reasonable offloading decisions, significantly reducing the average energy consumption and delay of edge clients, as well as the ratio of dropped tasks. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
13,12,131,12_retina_cnn_retinal_retinopathy,"retina,cnn,retinal,retinopathy,vision,diabetic,macular,diabetes,convolutional,classification","Diabetic retinopathy occurs due to damage to the blood vessels in the retina, and it is a major health problem in recent years that progresses slowly without recognizable symptoms. Optical coherence tomography (OCT) is a popular and widely used noninvasive imaging modality for the diagnosis of diabetic retinopathy. Accurate and early diagnosis of this disease using OCT images is crucial for the prevention of blindness. In recent years, several deep learning methods have been very successful in automating the process of detecting retinal diseases from OCT images. However, most methods face reliability and interpretability issues. In this study, we propose a deep residual network for the classification of four classes of retinal diseases, namely diabetic macular edema (DME), choroidal neovascularization (CNV), DRUSEN and NORMAL in OCT images. The proposed model is based on the popular architecture called ResNet50, which eliminates the vanishing gradient problem and is pre-trained on large dataset such as ImageNet and trained end-to-end on the publicly available OCT image dataset. We removed the fully connected layer of ResNet50 and placed our new fully connected block on top to improve the classification accuracy and avoid overfitting in the proposed model. The proposed model was trained and evaluated using different performance metrics, including receiver operating characteristic (ROC) curve on a dataset of 84,452 OCT images with expert disease grading as DRUSEN, CNV, DME and NORMAL. The proposed model provides an improved overall classification accuracy of 99.48% with only 5 misclassifications out of 968 test samples and outperforms existing methods on the same dataset. The results show that the proposed model is well suited for the diagnosis of retinal diseases in ophthalmology clinics. Graphical abstract: [Figure not available: see fulltext.] © 2022, International Association of Scientists in the Interdisciplinary Areas.,Diabetic retinopathy (DR) is the damage to the micro-vascular system in the retina, due to prolonged diabetes mellitus. Diagnosis and treatment of DR entail screening of retinal fundus images of diabetic patients. The manual inspection of pathological changes in retinal images is a skill-based task that involves lots of effort and time. Therefore, computer-aided detection and diagnosis of DR have been extensively explored for the past few decades. In recent years with the development of different benchmark deep convolutional neural networks (CNN), deep learning and machine learning have been efficiently and effectively adapted to different DR classification tasks. The success of CNNs largely relies on how good they are in extracting discriminative features from the fundus images. However, to the best of our knowledge, till date no study has been conducted to evaluate the feature extraction capabilities of all the benchmark CNNs to support the DR classification tasks and to find the best training-hyper-parameters for each of them in fundus retinal image-based DR classification tasks. In this work, we try to find the best benchmark CNN, which can be used as the backbone feature extractor for the DR classification tasks using fundus retinal images. We also aim to find the optimal hyper-parameters for training each of the benchmark CNN family, particularly when they are applied to the DR gradation tasks using retinal image datasets with huge class-imbalance and limited samples of higher severity classes. To address the cause, we conduct a detailed comprehensive comparative study on the performances of almost all the benchmark CNNs and their variants proposed during 2014 to 2019, for the DR gradation tasks on common standard retinal datasets. We have also conducted a comprehensive optimal training hyper-parameter search for each of the benchmark CNN family for the fundus image-based DR classification tasks. The benchmark CNNs are transfer learned and end-to-end trained in an incremental fashion on a class-balanced dataset curated from the train set of the EyePACS dataset. The benchmark models are evaluated on APTOS, MESSIDOR-1, and MESSIDOR-2 datasets to test their cross-dataset generalization. Experimental results show that features extracted by EfficientNetB1 have outperformed features of all the other CNN models in DR classification tasks on all three test datasets. MobileNet-V3-Large also shows promising performance on MESSIDOR-1 dataset. The success of EfficientNetB1 and MobileNet-V3-Large indicates that comparatively shallower and light-weighted CNNs tend to extract more discriminative and expressive features from fundus images for DR stage detection. In future, researchers can explore different preprocessing and post-processing techniques and incorporate novel architectural components on these networks to further improve the classification accuracy and robustness. © 2022, King Fahd University of Petroleum & Minerals.,Recently, there has been a considerable rise in the number of diabetic patients suffering from diabetic retinopathy (DR). DR is one of the most chronic diseases and makes the key cause of vision loss in middle-aged people in the developed world. Initial detection of DR becomes necessary for decreasing the disease severity by making use of retinal fundus images. This article introduces a Deep Learning Enabled Large Scale Healthcare Decision Making for Diabetic Retinopathy (DLLSHDM-DR) on Retinal Fundus Images. The proposed DLLSHDM-DR technique intends to assist physicians with the DR decision-making method. In the DLLSHDM-DR technique, image preprocessing is initially performed to improve the quality of the fundus image. Besides, the DLLSHDM-DR applies HybridNet for producing a collection of feature vectors. For retinal image classification, the DLLSHDM-DR technique exploits the Emperor Penguin Optimizer (EPO) with a Deep Recurrent Neural Network (DRNN). The application of the EPO algorithm assists in the optimal adjustment of the hyperparameters related to the DRNN model for DR detection showing the novelty of our work. To assuring the improved performance of the DLLSHDM-DR model, a wide range of experiments was tested on the EyePACS dataset. The comparison outcomes assured the better performance of the DLLSHDM-DR approach over other DL models. © 2023 CRL Publishing. All rights reserved."
14,13,126,13_texture_features_defects_feature,"texture,features,defects,feature,segmentation,inspection,defect,classification,defectfree,images","Defect detection is one of the key factors in fabric quality control. To improve the speed and accuracy of denim fabric defect detection, this paper proposes a defect detection algorithm based on cascading feature extraction architecture. Firstly, this paper extracts these weight parameters of the pre-trained VGG16 model on the large dataset ImageNet and uses its portability to train the defect detection classifier and the defect recognition classifier respectively. Secondly, retraining and adjusting partial weight parameters of the convolution layer were retrained and adjusted from of these two training models on the high-definition fabric defect dataset. The last step is merging these two models to get the defect detection algorithm based on cascading architecture. Then there are two comparative experiments between this improved defect detection algorithm and other feature extraction methods, such as VGG16, ResNet-50, and Xception. The results of experiments show that the defect detection accuracy of this defect detection algorithm can reach 94.3% and the speed is also increased by 1–3 percentage points. © 2023 KIPS,Industrial defect detection methods based on deep learning can reduce the cost of traditional manual quality inspection, improve the accuracy and efficiency of detection, and are widely used in industrial fields. Traditional computer defect detection methods focus on manual features and require a large amount of defect data, which has some limitations. This paper proposes a texture surface defect detection method based on convolutional neural network and wavelet analysis: TSDNet. The approach combines wavelet analysis with patch extraction, which can detect and locate many defects in a complex texture background; a patch extraction method based on random windows is proposed, which can quickly and effectively extract defective patches; and a judgment strategy based on a sliding window is proposed to improve the robustness of CNN. Our method can achieve excellent detection accuracy on DAGM 2007, a micro-surface defect database and KolektorSDD dataset, and can find the defect location accurately. The results show that in the complex texture background, the method can obtain high defect detection accuracy with only a small amount of training data and can accurately locate the defect position. © 2023 by the authors.,The occurrence of anomalies on the surface of industrial products can lead to issues such as decreased product quality, reduced production efficiency, and safety hazards. Early detection and resolution of these problems are crucial for ensuring the quality and efficiency of production. The key challenge in applying deep learning to surface defect detection of industrial products is the scarcity of defect samples, which will make supervised learning methods unsuitable for surface defect detection problems. Therefore, it is a reasonable solution to use anomaly detection methods to deal with surface defect detection. Among image-based anomaly detection, reconstruction-based methods are the most commonly used. However, reconstruction-based approaches lack the involvement of defect samples in the training process, posing the risk of a perfect reconstruction of defects by the reconstruction network. In this paper, we propose a reconstruction-based defect detection algorithm that addresses these challenges by utilizing more realistic synthetic anomalies for training. Our model focuses on creating authentic synthetic defects and introduces an auto-encoder image reconstruction network with deep feature consistency constraints, as well as a defect separation network with a large receptive field. We conducted experiments on the challenging MVTec anomaly detection dataset and our trained model achieved an AUROC score of 99.70% and an average precision (AP) score of 99.87%. Our method surpasses recently proposed defect detection algorithms, thereby enhancing the accuracy of surface defect detection in industrial products. © 2024 by the authors."
15,14,124,14_segmentation_supervised_dataset_deep,"segmentation,supervised,dataset,deep,annotations,mri,annotation,learning,datasets,images","Accurate and massive medical image annotation data is crucial for diagnosis, surgical planning, and deep learning in the development of medical images. However, creating large annotated datasets is challenging because labeling medical images is complicated, laborious, and time-consuming and requires expensive and professional medical skills. To significantly reduce the cost of labeling, an interactive image annotation framework based on composite geodesic distance is proposed, and medical images are labeled through segmentation. This framework uses Attention U-net to obtain initial segmentation based on adding user interaction to indicate incorrect segmentation. Another Attention U-net takes the user's interaction with the initial segmentation as input. It uses a composite geodesic distance transform to convert the user's interaction into constraints, giving accurate segmentation results. To further improve the labeling efficiency for large datasets, this paper validates the proposed framework against the segmentation background of a self-built prostate MRI image datasets. Experimental results show that the proposed method achieves higher accuracy in less interactive annotation and less time than traditional interactive annotation methods with better Dice and Jaccard results. This has important implications for improving medical diagnosis, surgical planning, and the development of deep-learning models in medical imaging. © 2023 Elsevier Ltd,Background and objective: Deep learning-based methods for fast target segmentation of magnetic resonance imaging (MRI) have become increasingly popular in recent years. Generally, the success of deep learning methods in medical image segmentation tasks relies on a large amount of labeled data. The time-consuming and labor-intensive problem of data annotation is a major challenge in medical image segmentation tasks. The aim of this work is to enhance the segmentation of MR images using a semi-supervised learning-based method using a small amount of labeled data and a large amount of unlabeled data. Methods: To utilize the effective information of the unlabeled data, we designed the method of guiding the Student segmentation model simultaneously by the Dual-Teacher structure of CNN and transformer forming the subject network. Both Teacher A and Student models are CNNs, and the TA-S module they form is a mean teacher structure with added data noise. In the TB-S module formed by the combination of Student and Teacher B models, their backbone networks CNN and transformer capture the local and global information of the image at the same time, respectively, to create pseudo labels for each other and perform cross-supervision. The Dual-Teacher guides the Student through synchronous training and performs knowledge rectification and communication with each other through consistent regular constraints, which better utilizes the valid information in the unlabeled data. In addition, the segmentation predictions of Teacher A and Student and Teacher A and Teacher B are screened for uncertainty assessment during the training process to enhance the prediction accuracy and generalization of the model. This method uses the mechanism of simultaneous training of the synthetic structure composed of TA-S and TB-S modules to jointly guide the optimization of the Student model to obtain better segmentation ability. Results: We evaluated the proposed method on a publicly available MRI dataset from a cardiac segmentation competition organized by MICCAI in 2017. Compared with several existing state-of-the-art semi-supervised segmentation methods, the method achieves better segmentation results in terms of Dice coefficient and HD distance evaluation metrics of 0.878 and 4.9 mm and 0.886 and 5.0 mm, respectively, using a training set containing only 10% and 20% of labeled data. Conclusion: This method fuses CNN and transformer to design a new Teacher-Student semi-supervised learning optimization strategy, which greatly improves the utilization of a large number of unlabeled medical images and the effectiveness of model segmentation results. © 2022 Elsevier B.V.,The availability of large, high-quality annotated datasets in the medical domain poses a substantial challenge in segmentation tasks. To mitigate the reliance on annotated training data, self-supervised pre-training strategies have emerged, particularly employing contrastive learning methods on dense pixel-level representations. In this work, we proposed to capitalize on intrinsic anatomical similarities within medical image data and develop a semantic segmentation framework through a self-supervised fusion network, where the availability of annotated volumes is limited. In a unified training phase, we combine segmentation loss with contrastive loss, enhancing the distinction between significant anatomical regions that adhere to the available annotations. To further improve the segmentation performance, we introduce an efficient parallel transformer module that leverages Multiview multiscale feature fusion and depth-wise features. The proposed transformer architecture, based on multiple encoders, is trained in a self-supervised manner using contrastive loss. Initially, the transformer is trained using an unlabeled dataset. We then fine-tune one encoder using data from the first stage and another encoder using a small set of annotated segmentation masks. These encoder features are subsequently concatenated for the purpose of brain tumor segmentation. The multiencoder-based transformer model yields significantly better outcomes across three medical image segmentation tasks. We validated our proposed solution by fusing images across diverse medical image segmentation challenge datasets, demonstrating its efficacy by outperforming state-of-the-art methodologies. IEEE"
16,15,123,15_sentiment_sentiments_nlp_classification,"sentiment,sentiments,nlp,classification,twitter,lstm,tweets,reviews,corpus,text","Sentiment Analysis is a method to identify, extract, and quantify people’s feelings, opinions, or attitudes. The wealth of online data motivates organizations to keep tabs on customers’ opinions and feelings by turning to sentiment analysis tasks. Along with the sentiment analysis, the emotion analysis of written reviews is also essential to improve customer satisfaction with restaurant service. Due to the availability of massive online data, various computerized methods are proposed in the literature to decipher text sentiments. The majority of current methods rely on machine learning, which necessitates the pre-training of large datasets and incurs substantial space and time complexity. To address this issue, we propose a novel unsupervised sentiment classification model. This study presents an unsupervised mathematical optimization framework to perform sentiment and emotion analysis of reviews. The proposed model performs two tasks. First, it identifies a review’s positive and negative sentiment polarities, and second, it determines customer satisfaction as either satisfactory or unsatisfactory based on a review. The framework consists of two stages. In the first stage, each review’s context, rating, and emotion scores are combined to generate performance scores. In the second stage, we apply a non-cooperative game on performance scores and achieve Nash Equilibrium. The output from this step is the deduced sentiment of the review and the customer’s satisfaction feedback. The experiments were performed on two restaurant review datasets and achieved state-of-the-art results. We validated and established the significance of the results through statistical analysis. The proposed model is domain and language-independent. The proposed model ensures rational and consistent results. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,With the recent expansion of social media in the form of social networks, online portals, and microblogs, users have generated a vast number of opinions, reviews, ratings, and feedback. Businesses, governments, and individuals benefit greatly from this information. While this information is intended to be informative, a large portion of it necessitates the use of text mining and sentiment analysis models. It is a matter of concern that reviews on social media lack text context semantics. A model for sentiment classification for customer reviews based on manifold dimensions and manifold modeling is presented to fully exploit the sentiment data provided in reviews and handle the issue of the absence of text context semantics. This paper uses a deep learning framework to model review texts using two dimensions of language texts and ideogrammatic icons and three levels of documents, sentences, and words for a text context semantic analysis review that enhances the precision of the sentiment categorization process. Observations from the experiments show that the proposed model outperforms the current sentiment categorization techniques by more than 8.86%, with an average accuracy rate of 97.30%. © 2023 by the authors.,With the exponential growth of social media platforms and online communication, the necessity of using automated sentiment analysis techniques has significantly increased. Deep learning techniques have emerged in extracting complex patterns and features from unstructured text data, which makes them a powerful tool for sentiment analysis. This research article presents a comprehensive review of sentiment analysis using deep learning techniques. We discuss various aspects of sentiment analysis, including data preprocessing, feature extraction, model architectures, and evaluation metrics. We explore the use of recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformer models in sentiment analysis tasks. We examine the utilization of RNNs, incorporating long short-term memory (LSTM) and gated recurrent unit (GRU), to model sequential dependencies in text data. Furthermore, we discuss the recent advancements in sentiment analysis achieved through a transformer. The findings from this review can facilitate the development of more accurate and efficient sentiment analysis models, enabling organizations to gain valuable insights from large volumes of textual data in several domains, such as social media, market analysis, and customer reviews. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
17,16,123,16_federated_distributed_networks_learning,"federated,distributed,networks,learning,collaborative,servers,collaboratively,edge,centralized,sharing","Full leverage of the huge volume of data generated on a large number of user devices for providing intelligent services in the 6G network calls for Ubiquitous Intelligence (UI). A key to developing UI lies in the involvement of the large number of network devices, which contribute their data to collaborative Machine Learning (ML) and provide their computational resources to support the learning process. Federated Learning (FL) is a new ML method that enables data owners to collaborate in model training without exposing private data, which allows user devices to contribute their data to developing UI. Edge computing deploys cloud-like capabilities at the network edge, which enables network devices to offer their computational resources for supporting FL. Therefore, a combination of FL and edge computing may greatly facilitate the development of ubiquitous intelligence in the 6G network. In this article, we present a comprehensive survey of the recent developments in technologies for combining FL and edge computing with a holistic vision across the fields of FL and edge computing. We conduct our survey from both the perspective of an FL framework deployed in an edge computing environment (FL in Edge) and the perspective of an edge computing system providing a platform for FL (Edge for FL). From the FL in Edge perspective, we first identify the main challenges to FL in edge computing and then survey the representative technical strategies for addressing the challenges. From the Edge for FL perspective, we first analyze the key requirements for edge computing to support FL and then review the recent advances in edge computing technologies that may be exploited to meet the requirements. Then we discuss open problems and identify some possible directions for future research on combining FL and edge computing, with the hope of arousing the research community's interest in this emerging and exciting interdisciplinary field.  © ; 2023 IEEE.,The Federated learning (FL) technique resolves the issue of training machine learning (ML) techniques on distributed networks, including the huge volume of modern smart devices. FL clients frequently use Wi-Fi and have to interact in unstable network surroundings. However, as the present FL aggregation approaches receive and send a large number of weights, accuracy can be decreased considerably in unstable network surroundings. Therefore, this study presents a Quantum with Metaheuristics Algorithm Based Minimization of Communication Costs in Federated Learning (QMAMCC-FL) technique. The presented QMAMCC-FL technique is designed a federated hybrid convolutional neural network with a gated recurrent unit (HCNN-GRU) model with a quantum Aquila optimization (QAO) algorithm. The QMAMCC-FL technique upgrades the global model via weight collection of the learned model, which is commonly used in FL. The proposed model can be employed to increase the performance of network communication and reduce the size of data transmitted from clients to servers such as smartphones and tablets. The experimental analysis of the QMAMCC-FL approach is tested, and the outcomes show better performance over other existing models.  © 2013 IEEE.,Federated Learning (FL) is a very effective distributed machine learning framework that enables a large number of devices to jointly train models without sharing raw data. However, the process of iterative learning and data communication in FL can be time-consuming, depending heavily on the number of clients participating in training and the number of local iterations between two consecutive global aggregations (communication period). In this paper, we analyze the runtime of the FL framework and propose a joint optimization problem, which considers both the number of local clients involved in global training and the communication period to minimize the error-runtime convergence of FedAvg. By theoretical analysis, we obtain the optimal solutions of this optimization problem in closed forms under the assumption of different probability distributions of the client's local computation time. In addition, we design an adaptive control algorithm that can dynamically select the number of clients and communication period during FL training to speed up the convergence of the model. Experimental results validate our theoretical analysis and show that the proposed algorithm has outstanding performance compared with other related FL control algorithms. © 2023 Elsevier Ltd"
18,17,123,17_code_snippets_programming_compiler,"code,snippets,programming,compiler,api,developers,syntax,program,github,corpus","Software defects not only reduce operational reliability but also significantly increase overall maintenance costs. Consequently, it is necessary to predict software defects at an early stage. Existing software defect prediction studies work with artificially designed metrics or features extracted from source code by machine learning-based approaches to perform classification. However, these methods fail to make full use of the defect-related information other than code, such as comments in codes and commit messages. Therefore, in this paper, additional information extracted from natural language text is combined with the programming language codes to enrich the semantic features. A novel model based on Transformer architecture and multi-channel CNN, PM2-CNN, is proposed for software defect prediction. Pretrained language model and CNN-based classifier are utilized in the model to obtain context-sensitive representations and capture the local correlation of sequences. A large and widely used dataset is utilized to verify the effectiveness of the proposed method. The results show that the proposed method has improvements in generic evaluation metrics compared with the optimal baseline method. Accordingly, external information can have a positive impact on software defect prediction, and our model effectively incorporates such information to improve detection performance. © 2023 Elsevier Inc.,In software engineering, software personnel faced many large-scale software and complex systems, these need programmers to quickly and accurately read and understand the code, and efficiently complete the tasks of software change or maintenance tasks. Code-NN is the first model to use deep learning to accomplish the task of code summary generation, but it is not used the structural information in the code itself. In the past five years, researchers have designed different code summarization systems based on neural networks. They generally use the end-to-end neural machine translation framework, but many current research methods do not make full use of the structural information of the code. This paper raises a new model called G-DCS to automatically generate a summary of java code; the generated summary is designed to help programmers quickly comprehend the effect of java methods. G-DCS uses natural language processing technology, and training the model uses a code corpus. This model could generate code summaries directly from the code files in the coded corpus. Compared with the traditional method, it uses the information of structural on the code. Through Graph Convolutional Neural Network (GCN) extracts the structural information on the code to generate the code sequence, which makes the generated code summary more accurate. The corpus used for training was obtained from GitHub. Evaluation criteria using BLEU-n. Experimental results show that our approach outperforms models that do not utilize code structure information. © 2023 Taiwan Academic Network Management Committee. All rights reserved.,Code search aims to retrieve code snippets from a large-scale codebase, where the semantics of the searched code match developers’ query intent. Code is a low-level implementation of programming intents, but query is always expressed as clear and high-level semantics, which makes it difficult for DL-based approaches to learn the semantic relationship between them. Through a large-scale empirical analysis on more than 2.2 million pairs of Java code and description, we found that the semantics of code and query can be aligned by enriching code with the descriptions of other code in terms of similar implementation. Based on the finding, we propose a code semantic enrichment approach for deep code search, named SemEnr. Specifically, we first enrich semantics for all code snippets in the training and testing data. We estimated the syntactic similarity of each code snippet from the training data and retrieved the most similar one for each. Thereafter, the semantics of one code snippet is represented by its code tokens and the description of the retrieved most similar code. During the model training, we used the attention mechanism to embed pairs of enriched code and query into the shared high-dimensional vector space. To enhance the quality of our learned representations, we integrated a multi-perspective co-attention mechanism, employing Convolutional Neural Networks (CNNs) to capture local correlations between code and query. Finally, we evaluated the effectiveness of our approach by performing experiments on two extensively used Java datasets. Our experimental results reveal that SemEnr achieves an MRR of 0.698 and 0.631, outperforming the best baseline CAT (a state-of-the-art DL-based model) by 19.93% and 18.83%, respectively. In addition, we conducted a user study involving 50 real-world queries to assess SemEnr's performance, and the findings suggest that SemEnr outperformed baseline models by returning more relevant code snippets. © 2023 Elsevier Inc."
19,18,118,18_electrocardiogram_arrhythmia_arrhythmias_ecg,"electrocardiogram,arrhythmia,arrhythmias,ecg,cardiac,ecgs,classification,cardiovascular,cardiologists,neural","The proliferation of wearable devices has allowed the collection of electrocardiogram (ECG) recordings daily to monitor heart rhythm and rate. For example, 24-hour Holter monitors, cardiac patches, and smartwatches are widely used for ECG gathering and application. An automatic atrial fibrillation (AF) detector is required for timely ECG interpretation. Deep learning models can accurately identify AFs if large amounts of annotated data are available for model training. However, it is impractical to request sufficient labels for ECG recordings for an individual patient to train a personalized model. We propose a Siamese-network-based approach for transfer learning to address this issue. A pre-trained Siamese convolutional neural network is created by comparing two labeled ECG segments from the same patient. We sampled 30-second ECG segments with a 50% overlapping window from the ECG recordings of patients in the MIT-BIH Atrial Fibrillation Database. Subsequently, we independently detected the occurrence of AF in each patient in the Long-Term AF Database. By fine-tuning the model with the 1, 3, 5, 7, 9, or 11 ECG segments ranging from 30 to 180 s, our method achieved macro-F1 scores of 96.84%, 96.91%, 96.97%, 97.02%, 97.05%, and 97.07%, respectively. © 2023 Elsevier B.V.,This paper presents a new spline-based modeling method of electrocardiogram (ECG) signal that can reproduce normal as well as abnormal ECG beats. Large volume ECG data is required for automatic machine learning diagnostic systems, medical education, research and testing purposes but due to privacy issues, access to this medical data is very difficult. Given this, modeling an ECG signal is a very challenging task in the field of biomedical signal processing. Spline-based modeling is the latest and one of the most efficient methods with very low computational complexity in the domain of ECG signal generation. In this paper, healthy ECG and arrhythmia conditions have been considered for the synthetic generation, (namely Atrial fibrillation and Congestive heart failure ECG beats) because these are the leading causes of death globally. To validate the performance of the presented modeling method, it is tested on 100 signals, also the percentage root mean square difference (PRD) and the root mean square error (RMSE) have been determined. These calculated values are analyzed and the results are found to be very promising and show that the presented method is one of the best methods in the field of synthetic ECG signal generation. A comparison amongst relevant existing techniques and the proposed method is also presented. The performance merit values PRD and RMSE, for the proposed method obtained are 38.99 and 0.10092, respectively, which are lower than the values obtained in other compared methods. To ensure fidelity of the proposed modeling technique with respect to IEC60601 standard, few Conformance Testing Services (CTS)database signals have also been modelled with a very close resemblance with the standard signals. © 2023 IOP Publishing Ltd.,Cardiovascular diseases are serious threats to human health. Electrocardiogram (ECG) is of great significance for clinical diagnosis and follow-up treatment of arrhythmias. However, the ECG signal is inevitably contaminated by a large amount of noise during ECG acquisition and transmission, which affects the results of arrhythmia detection. Noise reduction was often applied before classification in previous studies, which may cause the loss of some heart beats. Therefore, the preprocessing of noise reduction is omitted and noisy signals are directly classified in this paper. A novel deep learning model, deep shrinkage network, is developed in this paper to improve feature extraction ability and arrhythmia detection accuracy of noisy ECG recordings. Soft thresholding is introduced into deep fully convolutional neural network (DFCNN) to eliminate noise. ECG spectrograms are used as the input of the proposed network, and the Focal Loss function is employed to solve the problem of data imbalance. By training the original data from the MIT-BIH Arrhythmia Database, an overall accuracy of 99.74% is achieved. Significant advantages are also shown in the detection task of noisy ECG signals with different SNRs, demonstrating the effectiveness of the proposed network. ICIC International ©2023."
20,19,105,19_interprofessional_nursing_nurses_study,"interprofessional,nursing,nurses,study,nurse,practices,teaching,patients,students,clinical","Background: There are still concerns about the effectiveness of clinical education models which are done with the aim of reducing the theoretical-practical gap in nursing. In this article, we intend to describe an innovative model to create an integration and structured relationship between educational and healthcare provider institutions. The basis of this work is the full-time presence of nursing teacher in the clinical settings and the development of their role to improve the education of students and nurses and the quality of nursing services. Methods: This was a participatory action research. This action research was implemented in four steps of problem identification, planning, action and reflection. Interviews, focus groups and observation were used for the qualitative part. Clinical Learning Environment Inventory (CLEI), Job Satisfaction in Nursing Instrument questionnaires and Patient Satisfaction with Nursing Care Quality Questionnaire were completed before and after the study. Qualitative content analysis, paired and independent t test were used for data analysis. Results: The academic-practice integration Model of TPSN is a dynamic and interactive model for accountability in nursing Discipline. Unlike the medical education model that includes patients, students, and physicians as the three points of a triangle, this model, which is shaped like a large triangle, places the person in need of care and treatment (patient, client, family, or society) in the center of the triangle, aiming to focus on the healthcare receiver. The model consists of three components (Mentoring component, Preceptorship component, and integrated clinical education component). Each of the components of this model alone will not be able to eliminate the ultimate goal of bridging the theory-practice gap. Conclusions: A new and innovative model was proposed to reduce the theory-practice gap in the present study. This model increases the collaboration between educational institutions and healthcare settings compared with the previous models. The TPSN model helps students, nurses, and nursing instructors integrate theoretical knowledge with clinical practice and act as professional nurses. © 2022, The Author(s).,Understanding the effects of racism on public health is necessary to stimulate structural and systemic change to improve the health outcomes of patients. As a healthcare team, racism is best addressed through interprofessional collaboration to develop equitable and sustainable solutions that transform the health and wellbeing of patients and communities. As a part of the collective effort to properly educate our health professional students about the declarations by local and county health departments that “racism as a public health crisis,” we sought to utilize an interprofessional collaboration model. A steering committee of faculty, staff, and students at The Ohio State University created a new longitudinal interprofessional education (IPE) exercise titled Personal and Collective Responsibility for Health Equity: Anti-Racism in Action (ARIA). The participating Colleges/Schools of Dentistry, Medicine, Nursing, Optometry, Pharmacy, Public Health, School of Health and Rehabilitation Sciences (SHRS), Social Work, and Veterinary Medicine worked with leaders from the University's Office of Interprofessional Practice and Education to develop this 5-week program to engage a cohort of approximately 1300 students in a virtual learning experience during the Spring of 2021, when COVID-19 restricted in-person instruction. Ultimately, 200-interprofessional teams of 5–7 students were involved in this learning experience. Students individually selected resources (readings or videos) from a comprehensive resource kit provided by the steering team, to learn about and reflect on the differential types of racism, how it impacts well-being, health care delivery, and how health professionals can collaborate to advance health equity. The 200 interprofessional teams met twice virtually during the 5-week module and were provided with discussion questions and short assignments. Each team then contextualized, designed and submitted a final poster of a project to describe how an interprofessional approach to racism could further racial equity in health and healthcare delivery as applied to a selected perceived health equity issue, problem or dilemma. Student survey data was used to describe the effect of this module on student learning and attitudes. Students generally agreed that the module helped them to achieve the learning objectives. A thematic analysis of open-ended responses revealed that students generally had a positive response to the content on racism and the opportunity to learn interprofessionally, and they had specific suggestions for how to improve the experience. The results were utilized to re-design the activity for the following year and may be useful to other institutions wishing to address racism through interprofessional education. © 2023 Elsevier Inc.,Interprofessional education and collaborative practice requirements of professional health programs have resulted in collaborative learning programming in nearly all health professions. Due to recent initiatives for contemporary education of health professionals on pain science education, an academic health science center at a large public southeastern university created a focused interprofessional learning activity on pain science education within its curriculum. The learning objectives for this session aligned with both the core competencies from the Interprofessional Education Collaborative and the International Association for the Study of Pain. In a two-year period, nearly 750 students participated in this interprofessional pain science education session from medicine, nursing, pharmacy, physical therapy, and behavioral health. The session involved a standardized patient encounter for a patient presenting with chronic pain. Both faculty observers and the standardized patients assessed student teams on their interprofessional competencies during the activity, who then discussed findings prior to faculty-student debriefings. Faculty and standardized patient raters agreed that student groups demonstrated very strong skills in communication, information sharing, and solicitation of patient/client input during the event. Other skills were met with some level of disagreement. This session identifies the potential impact of interprofessional learning that can transpire through a standardized patient experience with modeling of activities in association with contemporary pain science education competencies. Future models of this program should include further assessment of interprofessional learning from students of all health care disciplines as well as continued solicitation of input from the patient or standardized patients. © 2022 Elsevier Inc."
21,20,105,20_precipitation_rainfall_climate_meteorological,"precipitation,rainfall,climate,meteorological,forecast,forecasts,weather,forecasting,convective,downscale","Climate change is expected to alter the magnitude and spatiotemporal patterns of hydro-climate variables such as precipitation, which has significant impacts on the ecosystem, human societies and water security. Global Climate Models are the major tools to simulate historical as well as future precipitation. However, due to imperfect model structures, parameters and boundary conditions, direct model outputs are subject to large uncertainty, which needs serious evaluation and bias correction before usage. In this study, seasonal precipitation predictions from 30 Coupled Model Inter-comparison Project Phase 6 (CMIP6) models and Climate Research Unit observations are used to evaluate historical precipitation climatology in global continents during 1901–2014. A grid based model heterogeneity oriented Convolutional Neural Network (CNN) is proposed to correct the ensemble mean precipitation bias ratio. Besides, regression based Linear Scaling (LS), distribution based Quantile Mapping (QM) and spatial correlation CNN bias correction approaches are employed for comparison. Results of model performance evaluation indicate that generally precipitation prediction is more reliable in JJA than DJF on the global scale. Most models tend to have larger bias ratio for extreme precipitation. In addition, current CMIP6 models still have certain issues in accurate simulation of precipitation in mountainous regions and the regions affected by complex climate systems. Moreover, the proposed grid based model heterogeneity oriented CNN has better performance in ensemble mean bias correction than LS, QM, and spatial correlation CNN, which could consider the relative model performance and capture the features similar to actual climate dynamics. © 2023. American Geophysical Union. All Rights Reserved.,To examine the characteristics of future precipitation under climate change is of great significance to urban water security. In this paper, multiple machine learning techniques, i.e., statistical downscaling model (SDSM), support vector machine (SVM), and multilayer perceptron (MLP), were used to downscale large-scale climatic variables simulated by the General Circulation Models (GCMs) to precipitation on a local scale. It was demonstrated in Shenzhen city, China, through multisite downscaling schemes based on projections from the Max Planck Institute Earth System Model (MPI-ESM1.2-HR), Meteorological Research Institute Earth System Model Version 2.0 (MRI-ESM2.0), and Beijing Climate Center Climate System Model (BCC-CSM2-MR). The obtained results showed that the downscaled precipitation would provide good monthly simulations against observations at 10 discrete stations. Regardless of superior performance of SVM and MLP over SDSM, the daily precipitation simulations should be further improved, and downscaling of heavy daily precipitations would be promoted by quantile mapping corrections. Due to the relatively poor simulation performance of BCC-CSM2-MR, the other two climate models were considered under the Shared Socioeconomic Pathways (SSP1-2.6, SSP2-4.5, and SSP5-8.5 scenarios) for ensemble precipitation projections for 2015-2100. Under the SSP1-2.6 scenario, the amounts of annual average precipitation for 10 stations were estimated to be higher relative to the historical period (2.7%-17%), and 9 out of 10 stations presented an increasing trend. However, downward trends also existed at three stations when it comes to scenarios SSP2-4.5 and SSP5-8.5. Moreover, a significantly positive trend was found to dominate the trend changes of annual extreme daily precipitation during 2015-2050, but the detected trends at stations were greatly dependent on the downscaling techniques and climate models. Besides, the increase in daily extreme precipitations for various return periods as well as statistically different precipitation characteristics for discrete stations would further shed light on urgent demands on urban resilient strategies for climate change adaptation. © 2022 American Society of Civil Engineers.,This study assesses the suitability of convolutional neural networks (CNNs) for downscaling precipitation over East Africa in the context of seasonal forecasting. To achieve this, we design a set of experiments that compare different CNN configurations and deployed the best-performing architecture to downscale one-month lead seasonal forecasts of June–July–August–September (JJAS) precipitation from the Nanjing University of Information Science and Technology Climate Forecast System version 1.0 (NUIST-CFS1.0) for 1982–2020. We also perform hyper-parameter optimization and introduce predictors over a larger area to include information about the main large-scale circulations that drive precipitation over the East Africa region, which improves the downscaling results. Finally, we validate the raw model and downscaled forecasts in terms of both deterministic and probabilistic verification metrics, as well as their ability to reproduce the observed precipitation extreme and spell indicator indices. The results show that the CNN-based downscaling consistently improves the raw model forecasts, with lower bias and more accurate representations of the observed mean and extreme precipitation spatial patterns. Besides, CNN-based downscaling yields a much more accurate forecast of extreme and spell indicators and reduces the significant relative biases exhibited by the raw model predictions. Moreover, our results show that CNN-based downscaling yields better skill scores than the raw model forecasts over most portions of East Africa. The results demonstrate the potential usefulness of CNN in downscaling seasonal precipitation predictions over East Africa, particularly in providing improved forecast products which are essential for end users. © 2024, Institute of Atmospheric Physics/Chinese Academy of Sciences, and Science Press."
22,21,104,21_pollution_pollutants_emissions_meteorological,"pollution,pollutants,emissions,meteorological,pollutant,atmospheric,ozone,aerosol,aerosols,meteorology","Large-scale restrictions on anthropogenic activities in China in 2020 due to the Corona Virus Disease 2019 (COVID-19) indirectly led to improvements in air quality. Previous studies have paid little attention to the changes in nitrogen dioxide (NO2), fine particulate matter (PM2.5) and ozone (O3) concentrations at different levels of anthropogenic activity limitation and their interactions. In this study, machine learning models were used to simulate the concentrations of three pollutants during periods of different levels of lockdown, and compare them with observations during the same period. The results show that the difference between the simulated and observed values of NO2 concentrations varies at different stages of the lockdown. Variation between simulated and observed O3 and PM2.5 concentrations were less distinct at different stages of lockdowns. During the most severe period of the lockdowns, NO2 concentrations decreased significantly with a maximum decrease of 65.28 %, and O3 concentrations increased with a maximum increase of 75.69 %. During the first two weeks of the lockdown, the titration reaction in the atmosphere was disrupted due to the rapid decrease in NO2 concentrations, leading to the redistribution of Ox (NO2 + O3) in the atmosphere and eventually to the production of O3 and secondary PM2.5. The effect of traffic restrictions on the reduction of NO2 concentrations is significant. However, it is also important to consider the increase in O3 due to the constant volatile organic compounds (VOCs) and the decrease in NOx (NO+NO2). Traffic restrictions had a limited effect on improving PM2.5 pollution, so other beneficial measures were needed to sustainably reduce particulate matter pollution. Research on COVID-19 could provide new insights into future clean air action. © 2023,The spatial distribution characteristics of multi-air pollutants and their impacts are difficult to quantify effectively. As PM2.5 and NO2 are the main air pollutants, it is of great significance to explore the spatial causes of their pollution and their interaction mechanism. This study used machine learning (LightGBM) and hot spot analysis to map the spatial distribution of PM2.5 and NO2 in Southwest Fujian (SWFJ) in 2018 and their key pollution areas. Then, the factors and interactive detection of geographical detectors were used to conduct a detailed analysis of the quantitative impact of potential factors such as human activities, terrain, air pollutants, and meteorology on PM2.5 and NO2 pollution. From this we can learn that 1. LightGBM has good stability for drawing the spatial distribution of PM2.5 and NO2. 2. The spatial mechanism of PM2.5 and NO2 can be effectively interpreted from a massive data and macro perspective. 3. A large amount of evidence shows that potential factors such as human activities, topography, air pollutants and meteorology have direct or indirect effects on PM2.5 and NO2 pollution in the SWFJ area. This includes the direct impact of local road traffic emissions on the distribution of PM2.5 and NO2 pollution, the digestion of both by vegetation, the mutual transformation of atmospheric pollutants themselves, and the impact of meteorological conditions. This study not only confirms the effectiveness of machine learning combined with geographical detectors to promote the study of regional air pollution mechanisms, but also confirms the feasibility of exploring the spatial distribution mechanisms of various air pollutants. Therefore, this study is of great significance for explaining the spatial distribution of PM2.5 and NO2, and can also provide reference for policy formulation to reduce regional PM2.5 and NO2 concentrations. © 2023 Elsevier Ltd,Poor air quality has various detrimental physical and mental effects on human health and quality of life. In particular, PM2.5 air pollution has been associated with cardiovascular and respiratory problems. Therefore, air quality management is an essential issue for densely populated cities to reduce or prevent the adverse effects of air pollution. Considering this, reliable models for predicting pollution levels for pollutants like PM2.5 are critical tools for decision-making. For this purpose, this study presents three kinds of deep learning (DL) algorithms (LSTM, RNN, and GRU) that utilize a time-windowing strategy to predict the hourly concentration of PM2.5 in the Istanbul metropolitan. The models were trained and tested using large data sets that envelope air quality parameters (PM2.5, SO2, NO, NO2, NOX, and O3) and meteorological factors (temperature, wind speed, relative humidity, and air pressure) for about five years. The experimental results demonstrate that the LSTM+LSTM model performs significantly better with an R2 of 0.98 and 0.97 at the significance level (p < 0.05) for training and test sets compared to other deep learning algorithms. In addition, data for one year from several stations located in nine different districts of Istanbul were used to evaluate the proposed model's generalization ability. As a result, the proposed LSTM+LSTM model has a good generalization ability with an R2 accuracy rate of 0.90 (p < 0.05) and above for all stations and can be used for non-linear, non-stationary multidimensional time series data. Furthermore, the results were compared to other studies in the literature; it was found that the proposed LSTM+LSTM model performed better in predicting PM2.5 concentrations. © 2023 Elsevier B.V."
23,22,101,22_forecasting_stocks_forecast_forecasts,"forecasting,stocks,forecast,forecasts,prediction,stock,predictability,predict,lstm,trading","Predicting stock market fluctuations is a difficult task due to its intricate and ever-changing nature. To address this challenge, we propose an approach to minimize forecasting errors by utilizing a classification-based technique, which is a widely used set of algorithms in the field of machine learning. Our study focuses on the potential effectiveness of this approach in improving stock market predictions. Specifically, we introduce a new method to predict stock returns using an Extra Trees Classifier. Technical indicators are used as inputs to train our model while the target is the percentage difference between the closing price and the closing price after 10 trading days for 120 companies from various industries. The 10-day time frame strikes a good balance between accuracy and practicality for traders, avoiding the low accuracy of short time frames and the impracticality of longer ones. The Extra Trees Classifier algorithm is ideal for stock market predictions because of its ability to handle large data sets with a high number of input features and improve model robustness by reducing overfitting. Our results show that our Extra Trees Classifier model outperforms the more traditional Random Forest method, achieving an accuracy of 86.1%. These findings suggest that our model can effectively predict significant price changes in the stock market with high precision. Overall, our study provides valuable insights into the potential of classification-based techniques in enhancing stock market predictions. © 2023 by the author.,Using machine learning coupled with stock price data to predict stock price trends has attracted increasing attention from data mining and machine learning communities. An accurate prediction results can help investors reduce investment risks and improve investment returns. The research on correlation stocks is one of the most important directions among many studies. Due to the high volatility and randomness of stock data, the correlation between stocks changes over time, which makes the stock correlation in static correlation stock sets often inconsistent with reality. Furthermore, various raw data related to stocks contain sufficient stock history information to analyze the future trend of stocks, but traditional prediction models cannot make good use of this information, which restricts the learning ability of the model and reduces the prediction accuracy. In this paper, we propose a stock prediction model combining multi-view stock data features with dynamic market correlation information (MDF-DMC). The model extracts stock trend features by combining multi-view raw data of a single stock with a Multi-layer Perceptron Mixer (MLP-Mixer); The improved Transformer encoder learns the correlation between the stock to be predicted and all the selected stocks in the stock market dynamically and extracts the features of the market correlation. We have conducted a large number of experiments on a total of 578 stocks in the stock markets of China and the United States, and the results show that our model has achieved excellent accuracy and returns across all data sets. © 2023 Elsevier Ltd,As the center of the financial market, the stock market is popular with the public attention of investors. It is of great significance for investors that an effective analytic method of stock public opinion is proposed. As the main communication platform, the forum not only provides the investors with investment information but also comments related to the stock market. In view of the defects of text emotions and investment problems, this paper proposes a framework based on web crawler and deep learning technologies including one-dimensional convolutional neural networks (1DCNN) and long short-term memory (LSTM), to evaluate the stock market volatility. Among them, the extracted features include not only the stock price but also the text information. Firstly, we develop the crawler technology to grab large-scale text data from the internet and they are manually labeled their emotions by analyzing the relevant financial knowledge. Secondly, as the character-level text classification method, the 1DCNN is designed for text sentiment classification to detect the reliability of text annotation. Finally, considering the time sequence of price and the continuity of post influence, the emotional and technical features are combined to estimate the fluctuation of the stock market in different industries by the LSTM model. We test four evaluation indexes, the classification accuracy of the model is 74.38%, the accuracy rate is 76.83%, the recall rate is 70%, and the F1 value is 72.8%. The results show that the combination of characteristics of internet public opinion more effectively evaluates the changes in the stock market. © 2022, King Fahd University of Petroleum & Minerals."
24,23,100,23_molecular_molecules_chemistry_densities,"molecular,molecules,chemistry,densities,models,atoms,computationally,energies,computational,learning","Electronic properties and absorption spectra are the grounds to investigate molecular electronic states and their interactions with the environment. Modeling and computations are required for the molecular understanding and design strategies of photo-active materials and sensors. However, the interpretation of such properties demands expensive computations and dealing with the interplay of electronic excited states with the conformational freedom of the chromophores in complex matrices (i.e., solvents, biomolecules, crystals) at finite temperature. Computational protocols combining time dependent density functional theory and ab initio molecular dynamics (MD) have become very powerful in this field, although they require still a large number of computations for a detailed reproduction of electronic properties, such as band shapes. Besides the ongoing research in more traditional computational chemistry fields, data analysis and machine learning methods have been increasingly employed as complementary approaches for efficient data exploration, prediction and model development, starting from the data resulting from MD simulations and electronic structure calculations. In this work, dataset reduction capabilities by unsupervised clustering techniques applied to MD trajectories are proposed and tested for the ab initio modeling of electronic absorption spectra of two challenging case studies: a non-covalent charge-transfer dimer and a ruthenium complex in solution at room temperature. The K-medoids clustering technique is applied and is proven to be able to reduce by ?100 times the total cost of excited state calculations on an MD sampling with no loss in the accuracy and it also provides an easier understanding of the representative structures (medoids) to be analyzed on the molecular scale. © 2023 by the authors.,Machine learning potentials (MLPs) are poised to combine the accuracy of ab initio predictions with the computational efficiency of classical molecular dynamics (MD) simulation. While great progress has been made over the last two decades in developing MLPs, there is still much to be done to evaluate their model transferability and facilitate their development. In this work, we construct two deep potential (DP) models for liquid water near graphene surfaces, Model S and Model F, with the latter having more training data. A concurrent learning algorithm (DP-GEN) is adopted to explore the configurational space beyond the scope of conventional ab initio MD simulation. By examining the performance of Model S, we find that an accurate prediction of atomic force does not imply an accurate prediction of system energy. The deviation from the relative atomic force alone is insufficient to assess the accuracy of the DP models. Based on the performance of Model F, we propose that the relative magnitude of the model deviation and the corresponding root-mean-square error of the original test dataset, including energy and atomic force, can serve as an indicator for evaluating the accuracy of the model prediction for a given structure, which is particularly applicable for large systems where density functional theory calculations are infeasible. In addition to the prediction accuracy of the model described above, we also briefly discuss simulation stability and its relationship to the former. Both are important aspects in assessing the transferability of the MLP model. © 2023 Author(s).,According to density functional theory, any chemical property can be inferred from the electron density, making it the most informative attribute of an atomic structure. In this work, we demonstrate the use of established physical methods to obtain important chemical properties from model-predicted electron densities. We introduce graph neural network architectural choices that provide physically relevant and useful electron density predictions. Despite not being trained to predict atomic charges, the model is able to predict atomic charges with an error of an order of magnitude lower than that of a sum of atomic charge densities. Similarly, the model predicts dipole moments with half the error of the sum of the atomic charge densities method. We demonstrate that larger data sets lead to more useful predictions for these tasks. These results pave the way for an alternative path in atomistic machine learning where data-driven approaches and existing physical methods are used in tandem to obtain a variety of chemical properties in an explainable and self-consistent manner. © 2023 The Authors. Published by American Chemical Society."
25,24,98,24_turbulent_aerodynamic_turbulence_aerodynamics,"turbulent,aerodynamic,turbulence,aerodynamics,flows,reynolds,flow,eddy,modeling,simulations","A super-resolution reconstruction model for the subgrid scale (SGS) turbulent flow field in large-eddy simulation (LES) is proposed, and it is called the meta-learning deep convolutional neural network (MLDCNN). Direct numerical simulation (DNS) data of isotropic turbulence are used as the dataset of the model. The MLDCNN is an unsupervised learning model, which only includes high-resolution DNS data without manually inputting preprocessed low-resolution data. In this model, the training process adopts the meta-learning method. First, in the a priori test, the SGS turbulent flow motions in the filtered DNS (FDNS) flow field are reconstructed, and the energy spectrum and probability density function of the velocity gradient of the DNS flow field are reconstructed with high accuracy. Then, in the a posteriori test, the super-resolution reconstruction of the LES flow field is carried out. The difficulty of LES flow field reconstruction is that it contains filtering loss and subgrid model errors relative to the DNS flow field. The super-resolution reconstruction of the LES flow field achieves good results through this unsupervised learning model. The proposed model makes a good prediction of small-scale motions in the LES flow field. This work improves the prediction accuracy of LES, which is crucial for the phenomena dominated by small-scale motions, such as relative motions of particles suspended in turbulent flows.  © 2022 Author(s).,A wall model for large-eddy simulation (LES) is proposed by devising the flow as a combination of building blocks. The core assumption of the model is that a finite set of simple canonical flows contains the essential physics to predict the wall shear stress in more complex scenarios. The model is constructed to predict zero/favourable/adverse mean pressure gradient wall turbulence, separation, statistically unsteady turbulence with mean flow three-dimensionality, and laminar flow. The approach is implemented using two types of artificial neural networks: A classifier, which identifies the contribution of each building block in the flow, and a predictor, which estimates the wall shear stress via a combination of the building-block flows. The training data are obtained directly from wall-modelled LES (WMLES) optimised to reproduce the correct mean quantities. This approach guarantees the consistency of the training data with the numerical discretisation and the gridding strategy of the flow solver. The output of the model is accompanied by a confidence score in the prediction that aids the detection of regions where the model underperforms. The model is validated in canonical flows (e.g. laminar/turbulent boundary layers, turbulent channels, turbulent Poiseuille-Couette flow, turbulent pipe) and two realistic aircraft configurations: The NASA Common Research Model High-lift and NASA Juncture Flow experiment. It is shown that the building-block-flow wall model outperforms (or matches) the predictions by an equilibrium wall model. It is also concluded that further improvements in WMLES should incorporate advances in subgrid-scale modelling to minimise error propagation to the wall model. © The Author(s), 2023. Published by Cambridge University Press.,Near-wall flow simulation remains a central challenge in aerodynamics modelling: Reynolds-averaged Navier-Stokes predictions of separated flows are often inaccurate, and large-eddy simulation (LES) can require prohibitively small near-wall mesh sizes. A deep learning (DL) closure model for LES is developed by introducing untrained neural networks into the governing equations and training in situ for incompressible flows around rectangular prisms at moderate Reynolds numbers. The DL-LES models are trained using adjoint partial differential equation (PDE) optimization methods to match, as closely as possible, direct numerical simulation (DNS) data. They are then evaluated out-of-sample - for aspect ratios, Reynolds numbers and bluff-body geometries not included in the training data - and compared with standard LES models. The DL-LES models outperform these models and are able to achieve accurate LES predictions on a relatively coarse mesh (downsampled from the DNS mesh by factors of four or eight in each Cartesian direction). We study the accuracy of the DL-LES model for predicting the drag coefficient, near-wall and far-field mean flow, and resolved Reynolds stress. A crucial challenge is that the LES quantities of interest are the steady-state flow statistics; for example, a time-averaged velocity component. Calculating the steady-state flow statistics therefore requires simulating the DL-LES equations over a large number of flow times through the domain. It is a non-trivial question whether an unsteady PDE model with a functional form defined by a deep neural network can remain stable and accurate on, especially when trained over comparatively short time intervals. Our results demonstrate that the DL-LES models are accurate and stable over long time horizons, which enables the estimation of the steady-state mean velocity, fluctuations and drag coefficient of turbulent flows around bluff bodies relevant to aerodynamics applications. © The Author(s), 2023. Published by Cambridge University Press."
26,25,97,25_videotext_videos_multimodal_captions,"videotext,videos,multimodal,captions,captioning,attention,retrieval,video,caption,imagetext","Video moment retrieval aims to locate the timestamps best matching the query description within an untrimmed video. However, existing video moment retrieval approaches typically suffer from two major limitations: (1)Utilize only negative moment-sentence pairs sampled from intra-videos, which may overfit the bias of the dataset and not have an excellent understanding of the video and query due to the dataset size and annotation biases. (2)Decouple the video and the query, perform unimodal learning separately, and then concatenate them together as multimodal fusion features. In this paper, we propose a novel approach named Momentum Contrastive Matching Network(MCMN). Inspired by MoCo, we propose the Momentum Cross-modal Contrast for cross-modal learning to enable large-scale negative sample interactions, which contributes to the generation of more precise and discriminative representations, and use temporal decay to model key attenuation in the memory queue when computing the contrastive loss. In addition, we use an attention module to adaptively generate clip-specific word embeddings to achieve semantic alignment from a temporal perspective, which are considered to be more important for finding relevant video contents with large boundary ambiguities. Experimental results on the three major video moment retrieval benchmark datasets, including TACoS, Charades-STA, and ActivityNet Captions demonstrate that MCMN surpasses previous methods and reaches state-of-the-art with disparate visual features. IEEE,The explosive growth of videos on the Internet makes it a great challenge to use texts to retrieve the videos we need. The general method of text-video retrieval is to project them into a common semantic space to calculate the similarity score. The key technologies of a retrieval model are how to get strong feature representations of text and video and bridge the semantic gap between the two modalities. Moreover, most existing methods do not consider the strong consistency of text-video positive sample pairs. Considering the above problems, we proposed a text-video retrieval method based on enhanced self-attention and multi-task learning in this paper. Firstly, while encoding, the extracted text feature vectors and the extracted video feature vectors are input into Transformer based on enhanced self-attention mechanism for encoding and fusion. Then the text representations and video representations are projected into a common semantic space. Finally, by introducing multi-task learning in the common semantic space, our proposed approach combines the semantic similarity measurement task and the semantic consistency judgement task to optimize the common space through semantic consistency constraints. Our method obtains better retrieval performance on the MSR-Video to Text (MSRVTT), Large Scale Movie Description Challenge (LSMDC), and ActivityNet datasets than some existing approaches, which proves the effectiveness of our proposed strategies. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Video captioning requires that the model has the abilities of video understanding, video-Text alignment, and text generation. Due to the semantic gap between vision and language, conducting video-Text alignment is a crucial step to reduce the semantic gap, which maps the representations from the visual to the language domain. However, the existing methods often overlook this step, so the decoder has to directly take the visual representations as input, which increases the decoder's workload and limits its ability to generate semantically correct captions. In this paper, we propose a video-Text alignment module with a retrieval unit and an alignment unit to learn video-Text aligned representations for video captioning. Specifically, we firstly propose a retrieval unit to retrieve sentences as additional input which is used as the semantic anchor between visual scene and language description. Then, we employ an alignment unit with the input of the video and retrieved sentences to conduct the video-Text alignment. The representations of two modal inputs are aligned in a shared semantic space. The obtained video-Text aligned representations are used to generate semantically correct captions. Moreover, retrieved sentences provide rich semantic concepts which are helpful for generating distinctive captions. Experiments on two public benchmarks, i.e., VATEX and MSR-VTT, demonstrate that our method outperforms state-of-The-Art performances by a large margin. The qualitative analysis shows that our method generates correct and distinctive captions.  © 2023 Association for Computing Machinery."
27,26,94,26_fmri_cortex_connectome_connectivity,"fmri,cortex,connectome,connectivity,neuroimaging,neurobiological,networks,brain,graph,cortical","Brain functional connectivity (FC) networks inferred from functional magnetic resonance imaging (fMRI) have shown altered or aberrant brain functional connectome in various neuropsychiatric disorders. Recent application of deep neural networks to connectome-based classification mostly relies on traditional convolutional neural networks (CNNs) using input FCs on a regular Euclidean grid to learn spatial maps of brain networks neglecting the topological information of the brain networks, leading to potentially sub-optimal performance in brain disorder identification. We propose a novel graph deep learning framework that leverages non-Euclidean information inherent in the graph structure for classifying brain networks in major depressive disorder (MDD). We introduce a novel graph autoencoder (GAE) architecture, built upon graph convolutional networks (GCNs), to embed the topological structure and node content of large fMRI networks into low-dimensional representations. For constructing the brain networks, we employ the Ledoit-Wolf (LDW) shrinkage method to efficiently estimate high-dimensional FC metrics from fMRI data. We explore both supervised and unsupervised techniques for graph embedding learning. The resulting embeddings serve as feature inputs for a deep fully-connected neural network (FCNN) to distinguish MDD from healthy controls (HCs). Evaluating our model on resting-state fMRI MDD dataset, we observe that the GAE-FCNN outperforms several state-of-the-art methods for brain connectome classification, achieving the highest accuracy when using LDW-FC edges as node features. The graph embeddings of fMRI FC networks also reveal significant group differences between MDD and HCs. Our framework demonstrates the feasibility of learning graph embeddings from brain networks, providing valuable discriminative information for diagnosing brain disorders. IEEE,To learn multiscale functional connectivity patterns of the aging brain, we built a brain age prediction model of functional connectivity measures at seven scales on a large fMRI dataset, consisting of resting-state fMRI scans of 4186 individuals with a wide age range (22 to 97 years, with an average of 63) from five cohorts. We computed multiscale functional connectivity measures of individual subjects using a personalized functional network computational method, harmonized the functional connectivity measures of subjects from multiple datasets in order to build a functional brain age model, and finally evaluated how functional brain age gap correlated with cognitive measures of individual subjects. Our study has revealed that functional connectivity measures at multiple scales were more informative than those at any single scale for the brain age prediction, the data harmonization significantly improved the brain age prediction performance, and the data harmonization in the functional connectivity measures' tangent space worked better than in their original space. Moreover, brain age gap scores of individual subjects derived from the brain age prediction model were significantly correlated with clinical and cognitive measures. Overall, these results demonstrated that multiscale functional connectivity patterns learned from a large-scale multi-site rsfMRI dataset were informative for characterizing the aging brain and the derived brain age gap was associated with cognitive and clinical measures. © 2023,Background: Although stratifying autism spectrum disorder (ASD) into different subtypes is a common effort in the research field, few papers have characterized the functional connectivity alterations of ASD subgroups classified by their clinical presentations. Methods: This is a case-control rs-fMRI study, based on large samples of open database (Autism Brain Imaging Data Exchange, ABIDE). The rs-MRI data from n = 415 ASD patients (males n = 357), and n = 574 typical development (TD) controls (males n = 410) were included. Clinical features of ASD were extracted and classified using data from each patient's Autism Diagnostic Interview-Revised (ADI-R) evaluation. Each subtype of ASD was characterized by local functional connectivity using regional homogeneity (ReHo) for assessment, remote functional connectivity using voxel-mirrored homotopic connectivity (VMHC) for assessment, the whole-brain functional connectivity, and graph theoretical features. These identified imaging properties from each subtype were integrated to create a machine learning model for classifying ASD patients into the subtypes based on their rs-fMRI data, and an independent dataset was used to validate the model. Results: All ASD participants were classified into Cluster-1 (patients with more severe impairment) and Cluster-2 (patients with moderate impairment) according to the dimensional scores of ADI-R. When compared to the TD group, Cluster-1 demonstrated increased local connection and decreased remote connectivity, and widespread hyper- and hypo-connectivity variations in the whole-brain functional connectivity. Cluster-2 was quite similar to the TD group in both local and remote connectivity. But at the level of whole-brain functional connectivity, the MCC-related connections were specifically impaired in Cluster-2. These properties of functional connectivity were fused to build a machine learning model, which achieved ?75% for identifying ASD subtypes (Cluster-1 accuracy = 81.75%; Cluster-2 accuracy = 76.48%). Conclusions: The stratification of ASD by clinical presentations can help to minimize disease heterogeneity and highlight the distinguished properties of brain connectivity in ASD subtypes. © 2023 The Authors"
28,27,92,27_recognition_classification_learning_attention,"recognition,classification,learning,attention,classes,fewshot,zeroshot,training,images,datasets","Deep neural networks have achieved promising progress in remote sensing (RS) image classification, for which the training process requires abundant samples for each class. However, it is time-consuming and unrealistic to annotate labels for each RS category, given the fact that the RS target database is increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel classes that are not seen during training, which provides a promising solution for the aforementioned problem. However, previous ZSL models mainly depend on manually-labeled attributes or word embeddings extracted from language models to transfer knowledge from seen classes to novel classes. Those class embeddings may not be visually detectable and the annotation process is time-consuming and labor-intensive. Besides, pioneer ZSL models use convolutional neural networks pre-trained on ImageNet, which focus on the main objects appearing in each image, neglecting the background context that also matters in RS scene classification. To address the above problems, we propose to collect visually detectable attributes automatically. We predict attributes for each class by depicting the semantic-visual similarity between attributes and images. In this way, the attribute annotation process is accomplished by machine instead of human as in other methods. Moreover, we propose a Deep Semantic-Visual Alignment (DSVA) that take advantage of the self-attention mechanism in the transformer to associate local image regions together, integrating the background context information for prediction. The DSVA model further utilizes the attribute attention maps to focus on the informative image regions that are essential for knowledge transfer in ZSL, and maps the visual images into attribute space to perform ZSL classification. With extensive experiments, we show that our model outperforms other state-of-the-art models by a large margin on a challenging large-scale RS scene classification benchmark. Moreover, we qualitatively verify that the attributes annotated by our network are both class discriminative and semantic related, which benefits the zero-shot knowledge transfer. © 2023 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS),Few-shot object detection (FSOD) aims to detect objects belonging to novel classes with few training samples. With the small number of novel class samples, the visual information extracted is insufficient to accurately represent the object itself, presenting significant intra-class variance and confusion between classes of similar samples, resulting in large errors in the detection results of the novel class samples. We propose a few-shot object detection framework to achieve effective classification and detection by embedding semantic information and contrastive learning. Firstly, we introduced a semantic fusion (SF) module, which projects semantic spatial information into visual space for interaction, to compensate for the lack of visual information and further enhance the representation of feature information. To further improve the classification performance, we embed the memory contrastive proposal (MCP) module to adjust the distribution of the feature space by calculating the contrastive loss between the class-centered features of previous samples and the current input features to obtain a more discriminative embedding space for better intra-class aggregation and inter-class separation for subsequent classification and detection. Extensive experiments on the PASCAL VOC and MS-COCO datasets show that the performance of our proposed method is effectively improved. Our proposed method improves nAP50 over the baseline model by 4.5% and 3.5%. © 2023 by the authors.,With the advent of large-scale datasets, significant advancements have been made in image semantic segmentation. However, the annotation of these datasets necessitates substantial human and financial resources. Therefore, the focus of research has shifted towards few-shot semantic segmentation, which leverages a small number of labeled samples to effectively segment unknown categories. The current mainstream methods are to use the meta-learning framework to achieve model generalization, and the main challenges are as follows. (1) The trained model will be biased towards the seen class, so the model will misactivate the seen class when segmenting the unseen class, which makes it difficult to achieve the idealized class agnostic effect. (2) When the sample size is limited, there exists an intra-class gap between the provided support images and the query images, significantly impacting the model’s generalization capability. To solve the above two problems, we propose a network with prototype complementarity characteristics (PCNet). Specifically, we first generate a self-support query prototype based on the query image. Through the self-distillation, the query prototype and the support prototype perform feature complementary learning, which effectively reduces the influence of the intra-class gap on the model generalization. A standard semantic segmentation model is introduced to segment the seen classes during the training process to achieve accurate irrelevant class shielding. After that, we use the rough prediction map to extract its background prototype and shield the background in the query image by the background prototype. In this way, we obtain more accurate fine-grained segmentation results. The proposed method exhibits superiority in extensive experiments conducted on the PASCAL- (Formula presented.) and COCO- (Formula presented.) datasets. We achieve new state-of-the-art results in the few-shot semantic segmentation task, with an mIoU of 71.27% and 51.71% in the 5-shot setting, respectively. Comprehensive ablation experiments and visualization studies show that the proposed method has a significant effect on small-sample semantic segmentation. © 2023 by the authors."
29,28,91,28_forestry_forest_forests_lidar,"forestry,forest,forests,lidar,vegetation,canopy,elevation,landsat,sentinel2,trees","Forest canopy height is defined as the distance between the highest point of the tree canopy and the ground, which is considered to be a key factor in calculating above-ground biomass, leaf area index, and carbon stock. Large-scale forest canopy height monitoring can provide scientific information on deforestation and forest degradation to policymakers. The Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) was launched in 2018, with the Advanced Topographic Laser Altimeter System (ATLAS) instrument taking on the task of mapping and transmitting data as a photon-counting LiDAR, which offers an opportunity to obtain global forest canopy height. To generate a high-resolution forest canopy height map of Jiangxi Province, we integrated ICESat-2 and multi-source remote sensing imagery, including Sentinel-1, Sentinel-2, the Shuttle Radar Topography Mission, and forest age data of Jiangxi Province. Meanwhile, we develop four canopy height extrapolation models by random forest (RF), Support Vector Machine (SVM), K-nearest neighbor (KNN), Gradient Boosting Decision Tree (GBDT) to link canopy height in ICESat-2, and spatial feature information in multi-source remote sensing imagery. The results show that: (1) Forest canopy height is moderately correlated with forest age, making it a potential predictor for forest canopy height mapping. (2) Compared with GBDT, SVM, and KNN, RF showed the best predictive performance with a coefficient of determination (R2) of 0.61 and a root mean square error (RMSE) of 5.29 m. (3) Elevation, slope, and the red-edge band (band 5) derived from Sentinel-2 were significantly dependent variables in the canopy height extrapolation model. Apart from that, Forest age was one of the variables that the RF moderately relied on. In contrast, backscatter coefficients and texture features derived from Sentinel-1 were not sensitive to canopy height. (4) There is a significant correlation between forest canopy height predicted by RF and forest canopy height measured by field measurements (R2 = 0.69, RMSE = 4.02 m). In a nutshell, the results indicate that the method utilized in this work can reliably map the spatial distribution of forest canopy height at high resolution. © 2023 by the authors.,Continuous mapping of the height and canopy cover of forests is vital for measuring forest biomass, monitoring forest degradation and restoration. In this regard, the contribution of Light Detection and Ranging (LiDAR) sensors, which were developed to obtain detailed data on forest composition across large geographical areas, is immense. Accordingly, this study aims to predict forest canopy cover and height in tropical forest areas utilizing Global Ecosystem Dynamics Investigation (GEDI) LIDAR, multisensor images, and random forest regression. To achieve this, we gathered predictor variables from the Shuttle Radar Topography Mission (SRTM) digital elevation model (DEM), Sentinel-2 multispectral datasets, and Sentinel-1 synthetic aperture radar (SAR) backscatters. The model's accuracy was evaluated based on a validation dataset of GEDI Level 2A and Level 2B. The random forest method was used the combination of data layers from Sentinel-1, Sentinel-2, and topographic measurements to model forest canopy cover and height. The produced canopy height and cover maps had a resolution of 30 m with R2 = 0.86 and an RMSE of 3.65 m for forest canopy height and R2 = 0.87 and an RMSE of 0.15 for canopy cover for the year 2022. These results suggest that combining multiple variables and data sources improves canopy cover and height prediction accuracy compared to relying on a single data source. The output of this study could be helpful in creating forest management plans that support sustainable utilization of the forest resources. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.,With the implementation of large-scale ecological restoration projects, Southwest China has become one of the fastest forest growth areas in the world in terms of vegetation cover and above-ground biomass (AGB). It is expected to be a potential area for achieving the carbon neutrality target in China. Accurate estimation of forest AGB is becoming an increasingly urgent necessity for carbon neutrality and forest management. However, due to the complex geological background, there is significant uncertainty in estimating forest AGB in the southwestern region, which generally results in underestimating carbon sinks from forest restoration. To address the issue, we propose a method by incorporating forest age information and stack learning technique to estimate forest AGB. Based on remote sensing, forest inventory and in situ forest biomass data, three fundamental methods (Multiple regression, Random forest and Support vector machine) are employed and compared to build the AGB estimation model with vegetation indices, texture feature factors and forest age. Optimal basic models are further enhanced by integration learning to improve the estimation performance and then applied to the study area Guangxi to obtain regional AGB information of different forest types. The results show that: (1) forest age plays a vital role in reducing the uncertainty of AGB estimation. By incorporating forest age information, R2 of AGB estimation is improved by 0.07–0.27 and RMSE is decreased by 16.35%–47.47% for different forest types; (2) with R2 value >0.78, random forest model outperforms support vector machine and multiple linear regression models. Compared with the single optimal model, integration model by stack learning further enhances R2 of estimation by 0.02–0.03 and decreases RMSE by 5.20%–14.89%. (3) The total forest AGB in Guangxi is 988.17 Tg and the average forest AGB level is 73.30 t/ha. Natural broadleaf forest has the highest AGB level (86.75 t/ha), followed by natural coniferous forest (81.19 t/ha), planted coniferous forest (63.23 t/ha) and planted eucalyptus forest (49.71 t/ha). AGB level in karst areas is lower than that in non-karst areas due to soil and water constraints. The majority of plantation forests in Guangxi is in the early and middle stages of forest succession, with the rapid growth of forest AGB, and has hence significant potential as carbon sinks. This study indicates that stack learning and incorporation of forest age could significantly decrease the uncertainty of forest AGB estimation. Our study helps to provide more accurate AGB information for karst ecological project management and regional carbon neutrality assessment. © 2023 John Wiley & Sons Ltd."
30,29,90,29_vegetation_crops_crop_multispectral,"vegetation,crops,crop,multispectral,agricultural,chlorophyll,hyperspectral,plants,planting,uav","Context or problem: Nitrogen is one of the important elements of crops, which plays a decisive role in crop growth and development and the formation of yields. Monitoring of rice organ-scale nitrogen concentration based on the unmanned aerial vehicle (UAV) images is of great significance for rice field management and yield prediction. Objective or research question: Previous studies have focused on the use of traditional statistical methods and chlorophyll-related vegetation indices to construct plant nitrogen concentration, with models lacking generalizability. Methods: In this study, rice field trials of two varieties (NJ9108, YD6) and nitrogen fertilizer treatments (N0-N3: 0, 105, 210 and 315 kg/ha) were conducted for 3 years with manual sampling and UAV digital and hyperspectral images during key fertility periods. Based on the data of the whole growth periods and combined with vegetation indices (VIs), color indices (CIs), hyperspectral parameters (HPs), texture indices (TIs) and machine-learning algorithms, monitoring models of nitrogen concentration at the organ scale of rice were constructed and used to estimate the N content of multiple organs (leaf and stem) of rice at different periods. Field experiments were used to collect the multi-organ nitrogen concentration of rice and the remote sensing (RS) data of UAV during the critical growth period of the two years (2021, 2022), and machine-learning algorithms were used to construct the estimation models. Results: The results showed that VIs had good correlations with leaf nitrogen concentration (LNC), stem nitrogen concentration (SNC) and plant nitrogen concentration (PNC), with correlation coefficients (r) of 0.86, 0.74 and 0.81, respectively. Machine learning estimation models combining multiple types of RS indices were more accurate than single parameter models constructed by traditional statistical methods, with the LNC optimal model (R2 = 0.8, RMSE = 3.83 mg/g), the SNC optimal model (R2 = 0.7, RMSE = 2.43 mg/g) and the PNC optimal model (R2 = 0.7, RMSE = 3.19 mg/g). Validated using data from 2020, the machine-learning models were far more accurate than traditional methods. Conclusions: These results show that the use of multi-source remote sensing data based on machine-learning algorithms can effectively estimate the nitrogen concentration of organs in rice. Implications: This study provides an accurate, stable and universal method for estimating rice nitrogen concentration in rice organs, which can be used as a reference for estimating rice nitrogen concentration in large fields using UAV RS technology. © 2023 Elsevier B.V.,Leaf chlorophyll is an important dynamic biochemical parameter to assess crop phenology, health stress, and yield. Optical remote sensing data is a widely used technique for the estimation of vegetation leaf chlorophyll. Drone technology is a promising solution for high-resolution monitoring of the leaf chlorophyll content and employed in this study. Image fusion was also accomplished to provide high-resolution multi-spectral images by combining Sentinel-2A and drone imagery. Support vector machine regression was adopted to determine the best vegetation indices for the retrieval of leaf chlorophyll content. Random forest regression and support vector machine regression algorithms were adopted to develop the best models for chlorophyll retrieval. The 34 models derived from drone data and the 46 models derived from fusion data were evaluated for chlorophyll retrieval. It was found that the best support vector machine model, based on anthocyanin content index and enhanced vegetation index (M31), had the best correlation coefficient (r = 0.732) and root mean square error (1.93). The random forest regression models M32 (based on chlorophyll vegetation index and visible atmospherically resistant index) and M16 (based on canopy chlorophyll content index and visible atmospherically resistant index) for drone data were found to be the best, which performed equally well in terms of correlation coefficients (r = 0.77) and root mean square error (1.51). The multiple vegetation indices of drone-based leaf chlorophyll content random forest regression models (M16 and M32) provided higher performance than single vegetation indices-based linear (simple linear fit) and nonlinear leaf chlorophyll content models. Out of 46 models of fusion products, the random forest regression M14 (green normalized difference vegetation index with narrow near infrared band and visible atmospherically resistant index) offered the best performance for leaf chlorophyll content retrieval, as can be observed through a comparison of correlation (r = 0.77) and RMSE (=1.71) values. It was also investigated that inversion performances (r ? 0.8 and RMSD ? 1.4 with standard deviation ? 2.6) of fusion data-based models in cross-applicability and transferability were found to be suitable for large-scale inversion of Sentinel-2A data products. © 2023,Leaf Area Index (LAI) is one of the indicators used to measure the growth status of rice fields. Rapid, accurate, and large-scale monitoring of LAI plays an important role in ensuring stable grain yield increase. In recent years, the spectral saturation problem and the parameter adjustment problem of machine learning algorithms have become the main limitations to improve the accuracy of LAI estimation. High-resolution Unmanned Aerial Vehicles (UAVs) images contain not only rich spectral information, but also texture information reflecting the crop canopy structure. Therefore, in order to fully understand the role of spectral information and texture information fusion in rice LAI estimation, this study used the hyperspectral sensor carried by the UAVs to obtain the spectral images of rice canopy of different varieties and different growth stages. Rice canopy reflectance and 8 basic texture features based on Gray-level Co-occurrence Matrix (GLCM) were extracted from hyperspectral images to calculate vegetation indexs (VIs) and combined texture features. Normalized difference texture index (NDTI), Non-linear texture index (NLTI), Enhanced vegetation texture index (EVTI), and Modified triangular texture index (MTTI) were calculated using two and three GLMC-based texture features to explore the effect of combinations of different basic texture features on LAI sensitivity. Two rice LAI estimation models were developed for single spectral indicators and combined with texture indicators, respectively. The results show that: (1) After preprocessing and feature band screening, the optimal spectral band, vegetation index, and trilateral parameters were obtained. When the combined spectral parameters (SP) of the three were used as the only input to the model, R2 showed an increasing trend throughout the growth period. The best results were achieved using the support vector regression (SVR) combined with the pelican optimization algorithm (POA) in the pre jointing stage: R2= 0.839, RMSE = 0.107, and MAPE = 7.02%. (2) When texture information based on hyperspectral images was incorporated into the model input, the results showed that the models based on spectral indicators combined with texture measurements were all superior to those using spectral indicators alone, with the best model having a coefficient of determination: R2 = 0.917, RMSE = 0.078, and MAPE = 4.19%, which has promising applications in crop growth index detection. This technology can quickly and effectively monitor the growth status of crops in the field, providing a theoretical basis for estimating crop yield in the later stage. © 2023"
31,30,89,30_remotesensing_encoder_sensing_segmentation,"remotesensing,encoder,sensing,segmentation,images,convolutional,land,features,multimodal,pixel","Although deep learning-based methods for semantic segmentation have achieved prominent performance in the general image domain, semantic segmentation for high-resolution remote sensing images remains highly challenging. One challenge is the large image size. High-resolution remote sensing images can have very high spatial resolution, resulting in images with hundreds of millions of pixels. This makes it difficult for deep learning models to process the images efficiently, as they typically require large amounts of memory and computational resources. Another challenge is the complexity of the objects and scenes in the images. High-resolution remote sensing images often contain a wide variety of objects, such as buildings, roads, trees, and water bodies, with complex shapes and textures. This requires deep learning models to be able to capture a wide range of features and patterns to segment the objects accurately. Moreover, remote sensing images can suffer from various types of noise and distortions, such as atmospheric effects, shadows, and sensor noises, which can also increase difficulty in segmentation tasks. To deal with the aforementioned challenges, we propose a new, mixed deep learning model for semantic segmentation on high-resolution remote sensing images. Our proposed model adopts our newly designed local channel spatial attention, multi-scale attention, and 16-piece local channel spatial attention to effectively extract informative multi-scale features and improve object boundary discrimination. Experimental results with two public benchmark datasets show that our model can indeed improve overall accuracy and compete with several state-of-the-art methods. © 2023 by the authors.,Semantic segmentation of high-resolution remote sensing images has emerged as one of the foci of research in the remote sensing field, which can accurately identify objects on the ground and determine their localization. In contrast, the traditional deep learning-based semantic segmentation, on the other hand, requires a large amount of annotated data, which is unsuitable for high-resolution remote sensing tasks with limited resources. It is therefore important to build a semantic segmentation method for high-resolution remote sensing images. In this paper, it is proposed an improved U-Net model based on transfer learning to solve the semantic segmentation problem of high-resolution remote sensing images. The model is based on the symmetric encoder–decoder structure of U-Net. For the encoder, transfer learning is applied and VGG16 is used as the backbone of the feature extraction network, and in the decoder, after upsampling using bilinear interpolation, it is performed multiscale fusion with the feature maps of the corresponding layers of the encoder in turn and is finally obtained the predicted value of each pixel to achieve precise localization. To verify the efficacy of the proposed network, experiments are performed on the ISPRS Vaihingen dataset. The experiments show that the applied method has achieved high-quality semantic segmentation results on the high-resolution remote sensing dataset, and the MIoU is 1.70%, 2.20%, and 2.33% higher on the training, validation, and test sets, respectively, and the IoU is 4.26%, 6.89%, and 5.44% higher for the automotive category compared to the traditional U-Net. © 2023, The Author(s).,There are many problems with remote sensing images, such as large data scales, complex illumination conditions, occlusion, and dense targets. The existing semantic segmentation methods for remote sensing images are not accurate enough for small and irregular target segmentation results, and the edge extraction results are poor. The authors propose a remote sensing image segmentation method based on a DCNN and multiscale feature fusion. Firstly, an end-to-end remote sensing image segmentation model using complete residual connection and multiscale feature fusion was designed based on a deep convolutional encoder–decoder network. Secondly, weighted high-level features were obtained using an attention mechanism, which better preserved the edges, texture, and other information of remote sensing images. The experimental results on ISPRS Potsdam and Urban Drone datasets show that compared with the comparison methods, this method has better segmentation effect on small and irregular objects and achieves the best segmentation performance while ensuring the computation speed. © 2023 IGI Global. All rights reserved."
32,31,88,31_mammography_mammograms_cnn_breast,"mammography,mammograms,cnn,breast,classification,neural,cancer,dataset,learning,detection","The most lethal and devastating form of cancer, breast cancer, is often first detected when a lump appears in the breast. The cause can be attributed to a typical proliferation of cells in the mammary glands. Early breast cancer detection improves survival. Breast cancer screening and early detection are commonly carried out using imaging techniques such as mammography and ultrasound. Convolutional neural networks (CNNs) can identify breast cancer on mammograms. Layers of artificial neurons detect patterns and properties in images to help identify abnormalities more accurately. CNNs may be trained on large datasets to improve accuracy and handle more complex visual information than traditional methods. We introduced a unique approach termed BreastNet-SVM with the objective of automating the identification and categorization of breast cancer in mammograms. This study uses a nine-layer model with two fully connected layers to retrieve data features. Furthermore, we utilized support vector machines (SVM) for classification purposes. To conduct this experiment, we used a well-known benchmark dataset Digital Database for Screening Mammography (DDSM). It is shown that the suggested model has a 99.16% accuracy rate, a 97.13% sensitivity rate, and a 99.30% specificity rate. The top approaches for detecting breast cancer were compared to the recommended BreastNet-SVM model. In terms of accuracy, the proposed BreastNet-SVM model fared better in experimental results on a DDSM dataset.  © 2013 IEEE.,Breast cancer is responsible for the deaths of hundreds of women every year. The manual identification of breast cancer has more difficulties, and have the possibility of error. Many imaging approaches are being researched for their potential to identify breast cancer (BC). Incorrect identification might sometimes result in unneeded therapy and diagnosis. Because of this, accurate identification of breast cancer may save a great number of patients from needing unneeded surgery and biopsies. Deep learning's (DL) performance in the processing of medical images has substantially increased as a result of recent breakthroughs in the sector. Because of their improved capacity to anticipate outcomes, deep learning algorithms are able to reliably detect BC from ultrasound pictures. Transfer learning is a kind of machine learning that reuses knowledge representations from public models that were built with the use of large-scale datasets. Transfer learning has been shown to often result in overfitting. The primary purpose of this research is to develop and provide suggestions for a deep learning model that is effective and reliable in the detection and classification of breast cancer. A tissue biopsy is obtained from the suspicious region in order to ascertain the nature of a breast tumor and whether or not it is cancerous. Tumors may take any of these forms. When the images have been reconstructed with the help of a variational autoencoder (VAE) and a denoising variational autoencoder (DVAE), a convolutional neural network (CNN) model is used. This will be the case because it opens up a new area of the field to be investigated. The histological subtypes of breast cancer are used in conjunction with the degree of differentiation to execute the task of breast cancer categorization.  © 2023 - IOS Press. All rights reserved.,Breast cancer develops in breast cells. It is the most common type of cancer in women and the second most lethal disease after lung cancer. The presence of breast masses is an important symptom for detecting breast cancer in its early stages. This study proposes a hybrid features extraction method to improve the automatic detection of breast cancer by combining three feature extraction methods: Kinetic Features, convolutional neural network deep learning features, and the newly proposed Quantum Chebyshev polynomials model. The long short-term memory model is used as a classifier in this study to detect breast cancer automatically, which could reduce human errors in the diagnosis process. The experimental results using a large publicly available dataset achieved a detection accuracy of 99.50% for hybrid features in post-contrast 2, potentially reducing human errors in the diagnosis process. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
33,32,88,32_exam_examination_examinations_exams,"exam,examination,examinations,exams,chatgpt,chatgpts,questions,medical,chatgpt4,study","BACKGROUND: This study aimed to assess the performance of ChatGPT, a large language model (LLM), on the Italian State Exam for Medical Residency (SSM) test to determine its potential as a tool for medical education and clinical decision-making support. MATERIALS AND METHODS: A total of 136 questions were obtained from the official SSM test. ChatGPT responses were analyzed and compared to the performance of medical doctors who took the test in 2022. Questions were classified into clinical cases (CC) and notional questions (NQ). RESULTS: ChatGPT achieved an overall accuracy of 90.44%, with higher performance on clinical cases (92.45%) than on notional questions (89.15%). Compared to medical doctors' scores, ChatGPT performance was higher than 99.6% of the participants. CONCLUSIONS: These results suggest that ChatGPT holds promise as a valuable tool in clinical decision-making, particularly in the context of clinical reasoning. Further research is needed to explore the potential applications and implementation of large language models (LLMs) in medical education and medical practice.,Background: Chat Generative Pre-trained Transformer (ChatGPT), OpenAI Limited Partnership, San Francisco, CA, USA is an artificial intelligence language model gaining popularity because of its large database and ability to interpret and respond to various queries. Although it has been tested by researchers in different fields, its performance varies depending on the domain. We aimed to further test its ability in the medical field. Methods: We used questions from Taiwan's 2022 Family Medicine Board Exam, which combined both Chinese and English and covered various question types, including reverse questions and multiple-choice questions, and mainly focused on general medical knowledge. We pasted each question into ChatGPT and recorded its response, comparing it to the correct answer provided by the exam board. We used SAS 9.4 (Cary, North Carolina, USA) and Excel to calculate the accuracy rates for each question type. Results: ChatGPT answered 52 questions out of 125 correctly, with an accuracy rate of 41.6%. The questions' length did not affect the accuracy rates. These were 45.5%, 33.3%, 58.3%, 50.0%, and 43.5% for negative-phrase questions, multiple-choice questions, mutually exclusive options, case scenario questions, and Taiwan's local policy-related questions, with no statistical difference observed. Conclusion: ChatGPT's accuracy rate was not good enough for Taiwan's Family Medicine Board Exam. Possible reasons include the difficulty level of the specialist exam and the relatively weak database of traditional Chinese language resources. However, ChatGPT performed acceptably in negative-phrase questions, mutually exclusive questions, and case scenario questions, and it can be a helpful tool for learning and exam preparation. Future research can explore ways to improve ChatGPT's accuracy rate for specialized exams and other domains. © 2023 Wolters Kluwer Health. All rights reserved.,Background: Chat Generative Pre-Trained Transformer (ChatGPT) is an artificial learning and large language model tool developed by OpenAI in 2022. It utilizes deep learning algorithms to process natural language and generate responses, which renders it suitable for conversational interfaces. ChatGPT’s potential to transform medical education and clinical practice is currently being explored, but its capabilities and limitations in this domain remain incompletely investigated. The present study aimed to assess ChatGPT’s performance in medical knowledge competency for problem assessment in obstetrics and gynecology (OB/GYN). Methods: Two datasets were established for analysis: questions (1) from OB/GYN course exams at a German university hospital and (2) from the German medical state licensing exams. In order to assess ChatGPT’s performance, questions were entered into the chat interface, and responses were documented. A quantitative analysis compared ChatGPT’s accuracy with that of medical students for different levels of difficulty and types of questions. Additionally, a qualitative analysis assessed the quality of ChatGPT’s responses regarding ease of understanding, conciseness, accuracy, completeness, and relevance. Non-obvious insights generated by ChatGPT were evaluated, and a density index of insights was established in order to quantify the tool’s ability to provide students with relevant and concise medical knowledge. Results: ChatGPT demonstrated consistent and comparable performance across both datasets. It provided correct responses at a rate comparable with that of medical students, thereby indicating its ability to handle a diverse spectrum of questions ranging from general knowledge to complex clinical case presentations. The tool’s accuracy was partly affected by question difficulty in the medical state exam dataset. Our qualitative assessment revealed that ChatGPT provided mostly accurate, complete, and relevant answers. ChatGPT additionally provided many non-obvious insights, especially in correctly answered questions, which indicates its potential for enhancing autonomous medical learning. Conclusion: ChatGPT has promise as a supplementary tool in medical education and clinical practice. Its ability to provide accurate and insightful responses showcases its adaptability to complex clinical scenarios. As AI technologies continue to evolve, ChatGPT and similar tools may contribute to more efficient and personalized learning experiences and assistance for health care providers. Copyright © 2023 Riedel, Kaefinger, Stuehrenberg, Ritter, Amann, Graf, Recker, Klein, Kiechle, Riedel and Meyer."
34,33,87,33_mimo_channels_5g_channel,"mimo,channels,5g,channel,mmwave,antennas,antenna,wireless,ghz,communications","Massive multiple-input and multiple-output (M-MIMO) is considered a vital technology for enhancement of energy efficiency and link capacity in fifth-generation (5G) communication systems. The accessibility of the downlink channel state information (CSI) at the base station (BS) is necessary to access the potential advantages of millimeter wave (mmWave) and M-MIMO systems. The CSI matrix is usually large due to the massive scale antennas present at the BS. To carry out the downlink precoding computations, the downlink channel responses need to be estimated and fed back to the base station. However, in absence of channel reciprocity in frequency division duplexing, acquiring accurate CSI is a challenge. This paper proposes a novel network based on deep learning, VAECNN-Net. The network is the combination of convolutional neural network and variational autoencoder, hence, termed VAECNN-Net. The network compresses the CSI at the user equipment side and is reliably retrieved at the base station. The performance analysis is carried out on a synthetic data set for a single-user mmWave M-MIMO system and is also validated for the COST 2100 channel model. It is observed that the proposed network has lower computational complexity than existing models such as CsiNet, TransNet, and CsiNet+DNN. The network also provides superior cosine similarity and normalized mean square error (NMSE) performance compared to existing techniques. The model is compared with the baseline network CsiNet and it also shows superior performance in terms of NMSE for the generated data set. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,In mm-wave massive multiple-input-multiple-output (MIMO) systems, accurate channel state information (CSI) at the base station (BS) is the key knowledge to obtain the performance gain. Consequently, the user is not only required to complete the channel estimation, but also to feedback CSI to the BS. However, large-scale antenna arrays result in a substantial feedback overhead, which poses a challenging issue. Furthermore, the accuracy requirements of channel estimation and CSI feedback depend on the computing capacity of the user. In this paper, we propose a joint channel estimation and multiple-compression-rate feedback (JCEMF) scheme, and adopt centralized learning (CL) and federated learning (FL) strategies for the scheme. According to the limited computational resources available to users, the JCEMF scheme enables various lengths of feedback bits to change the feedback overhead. Additionally, the users in FL train the local models using their own datasets and upload the local model updates to the BS, thereby reducing communication overhead and protecting data privacy. Specifically, an estimation network is designed for the user to estimate the channel from the received signal. In the CSI feedback process, we introduce an MCRF network, which can achieve CSI compression and reconstruction with different numbers of feedback bits. Simulation results verify that the proposed approach shows good performance of joint channel estimation and multiple-compression-rate CSI feedback in different channel conditions. © 2024 Elsevier B.V.,The latest research for applying deep learning in wireless communications gives several opportunities to reduce complex signal processing. The channel estimation is important to study the nature of the varying channel and to calculate channel state information (CSI) value which is utilized at the receiver to nullify the interference which occurs during multipath transmission. In the current article, considering the massive Multiple Input Multiple Output (MIMO) channel model, a DL approach is developed with a fully connected neural network (NN) architecture which is used to estimate the channel with minimum error. The proposed DL architecture uses an openly available channel dataset. Further, using generated pilot symbols of lengths 2 and 4, the performance of DL-based Fully connected NN (DL-FCNN) is analyzed to estimate the channel in uplink massive MIMO communication. The obtained results demonstrate that the channel estimation performance was calculated in terms of normalized mean square error((NMSE) for different values of SNR added at receiver base station (BS) to the signals over the range of BS antennas. Also, the channel estimation error over a large number of BS antennas for massive MIMO scenarios is observed, and it is observed that the NMSE reduces with a greater number of antennas. Hence, it can be inferred that the DL models will be the future for most physical layer signal processing techniques such as channel estimation, modulation detection, etc. within massive MIMO networks. © 2022 Taylor & Francis Group, LLC."
35,34,87,34_pointnet_lidar_pointbased_cloud,"pointnet,lidar,pointbased,cloud,clouds,points,3d,pointvoxel,segmentation,sphere2vec","As 3D acquisition technology develops and 3D sensors become increasingly affordable, large quantities of 3D point cloud data are emerging. How to effectively learn and extract the geometric features from these point clouds has become an urgent problem to be solved. The point cloud geometric information is hidden in disordered, unstructured points, making point cloud analysis a very challenging problem. To address this problem, we propose a novel network framework, called Tree Graph Network (TGNet), which can sample, group, and aggregate local geometric features. Specifically, we construct a Tree Graph by explicit rules, which consists of curves extending in all directions in point cloud feature space, and then aggregate the features of the graph through a cross-attention mechanism. In this way, we incorporate more point cloud geometric structure information into the representation of local geometric features, which makes our network perform better. Our model performs well on several basic point clouds processing tasks such as classification, segmentation, and normal estimation, demonstrating the effectiveness and superiority of our network. Furthermore, we provide ablation experiments and visualizations to better understand our network. © 2023 Tech Science Press. All rights reserved.,In view of the difficulty of using raw 3D point clouds for component detection in the railway field, this paper designs a point cloud segmentation model based on deep learning together with a point cloud preprocessing mechanism. First, a special preprocessing algorithm is designed to resolve the problems of noise points, acquisition errors, and large data volume in the actual point cloud model of the bolt. The algorithm uses the point cloud adaptive weighted guided filtering for noise smoothing according to the noise characteristics. Then retaining the key points of the point cloud, this algorithm uses the octree to partition the point cloud and carries out iterative farthest point sampling in each partition for obtaining the standard point cloud model. The standard point cloud model is then subjected to hierarchical multi-scale feature extraction to obtain global features, which are combined with local features through a self-attention mechanism, while linear interpolation is used to further expand the perceptual field of local features of the model as a basis for segmentation, and finally the segmentation is completed. Experiments show that the proposed algorithm could deal with the scattered bolt point cloud well, realize the segmentation of train bolt and background, and could achieve high segmentation accuracy, which has important practical significance for train safety detection. © 2023 by the authors.,Point cloud data have been widely explored due to its superior accuracy and robustness under various adverse situations. Meanwhile, deep neural networks (DNNs) have achieved very impressive success in various applications such as surveillance and autonomous driving. The convergence of point cloud and DNNs has led to many deep point cloud models, largely trained under the supervision of large-scale and densely-labelled point cloud data. Unsupervised point cloud representation learning, which aims to learn general and useful point cloud representations from unlabelled point cloud data, has recently attracted increasing attention due to the constraint in large-scale point cloud labelling. This paper provides a comprehensive review of unsupervised point cloud representation learning using DNNs. It first describes the motivation, general pipelines as well as terminologies of the recent studies. Relevant background including widely adopted point cloud datasets and DNN architectures is then briefly presented. This is followed by an extensive discussion of existing unsupervised point cloud representation learning methods according to their technical approaches. We also quantitatively benchmark and discuss the reviewed methods over multiple widely adopted point cloud datasets. Finally, we share our humble opinion about several challenges and problems that could be pursued in the future research in unsupervised point cloud representation learning.  © 1979-2012 IEEE."
36,35,84,35_quantum_quantumclassical_qubits_entanglement,"quantum,quantumclassical,qubits,entanglement,qml,unitary,variational,computing,learning,hamiltonian","Variational quantum algorithms have the potential for significant impact on high-dimensional optimization, with applications in classical combinatorics, quantum chemistry, and condensed matter. Nevertheless, the optimization landscape of these algorithms is generally nonconvex, leading the algorithms to converge to local, rather than global, minima and the production of suboptimal solutions. In this work, we introduce a variational quantum algorithm that couples classical Markov chain Monte Carlo techniques with variational quantum algorithms, allowing the former to provably converge to global minima and thus assure solution quality. Due to the generality of our approach, it is suitable for a myriad of quantum minimization problems, including optimization and quantum state preparation. Specifically, we devise a Metropolis-Hastings method that is suitable for variational quantum devices and use it, in conjunction with quantum optimization, to construct quantum ensembles that converge to Gibbs states. These performance guarantees are derived from the ergodicity of our algorithm’s state space and enable us to place analytic bounds on its time-complexity. We demonstrate both the effectiveness of our technique and the validity of our analysis through quantum circuit simulations for MaxCut instances, solving these problems deterministically and with perfect accuracy, as well as large-scale quantum Ising and transverse field spin models of up to 50 qubits. Our technique stands to broadly enrich the field of variational quantum algorithms, improving and guaranteeing the performance of these promising, yet often heuristic, methods. © 2022 The Author(s). Published by IOP Publishing Ltd.,The neural network and quantum computing are both significant and appealing fields, with their interactive disciplines promising for large-scale computing tasks that are untackled by conventional computers. However, both developments are restricted by the scope of the hardware development. Nevertheless, many neural network algorithms had been proposed before GPUs became powerful enough for running very deep models. Similarly, quantum algorithms can also be proposed as knowledge reserve before real quantum computers are easily accessible. Specifically, taking advantage of both the neural networks and quantum computation and designing quantum deep neural networks (QDNNs) for acceleration on the Noisy Intermediate-Scale Quantum (NISQ) processors is also an important research problem. As one of the most widely used neural network architectures, convolutional neural network (CNN) remains to be accelerated by quantum mechanisms, with only a few attempts having been demonstrated. In this article, we propose a new hybrid quantum-classical circuit, namely, Quantum Fourier Convolutional Network (QFCN). Our model achieves exponential speedup compared with classical CNN theoretically and improves over the existing best result of quantum CNN. We demonstrate the potential of this architecture by applying it on different deep learning tasks, including traffic prediction and image classification.  © 2023 Association for Computing Machinery.,The advent of noisy intermediate-scale quantum computers has put the search for possible applications to the forefront of quantum information science. One area where hopes for an advantage through near-term quantum computers are high is quantum machine learning, where variational quantum learning models based on parametrized quantum circuits are discussed. In this work, we introduce the concept of a classical surrogate, a classical model which can be efficiently obtained from a trained quantum learning model and reproduces its input-output relations. As inference can be performed classically, the existence of a classical surrogate greatly enhances the applicability of a quantum learning strategy. However, the classical surrogate also challenges possible advantages of quantum schemes. As it is possible to directly optimize the Ansatz of the classical surrogate, they create a natural benchmark the quantum model has to outperform. We show that large classes of well-analyzed reuploading models have a classical surrogate. We conducted numerical experiments and found that these quantum models show no advantage in performance or trainability in the problems we analyze. This leaves only generalization capability as a possible point of quantum advantage and emphasizes the dire need for a better understanding of inductive biases of quantum learning models. © 2023 American Physical Society."
37,36,84,36_feature_features_bearings_fault,"feature,features,bearings,fault,faults,machinery,bearing,faultrelated,diagnostic,machines","Ensuring safe machine operation in industrial environments requires accurate bearing fault diagnosis. However, maintaining consistent data distribution between input training and test sets in practical online engineering tests based on depth models can be challenging. Inconsistencies in data distribution can hinder internal feature extraction and significantly affect the accuracy of online diagnosis. Additionally, different types of fault data are strongly nonlinearly coupled in space, requiring a deep model with powerful feature learning capabilities to reduce coupling effects between data. However, in practical engineering, deep models face difficulties in learning-rich fault features when few labeled training samples are available, leading to overfitting and poor generalization issues. To overcome these challenges, this study proposes an intelligent diagnosis method based on the Mel-scale frequency cepstrum coefficient (MFCC) and deep convolutional neural network with spatial adjacent region dropout (DCNN-SARD). Firstly, MFCC is utilized to extract sign fault features from vibration signals in different frequency bands, thus reducing the effect of inconsistent data distribution on fault diagnosis accuracy. Secondly, according to the different fault types, the obtained signature fault features are organized into one-dimensional vectors to construct block sample datasets. Finally, the fault features and input to the next convolution are enriched by creating a spatial descent region on the output feature map of the DCNN and randomly dropping the activation of neighboring kernels. This improves the generalizability of the model without increasing its structural complexity. The effectiveness of the proposed algorithm is experimentally validated on two common bearing datasets, and its reliability is verified on a large gas turbine bearing dataset. © 2023, The Author(s), under exclusive licence to The Brazilian Society of Mechanical Sciences and Engineering.,In recent years, rolling bearing fault diagnosis technology based on deep learning (DL) has provided a more intelligent and reliable method for the safe operation of mechanical systems due to its powerful feature learning ability. However, in real industrial scenarios, the acquisition of fault samples is limited and knotty, which makes it difficult for DL methods that require a large number of fault samples to be successful. To overcome the above problems, in this article, a novel meta-learning network with adaptive input (AI) and attention mechanism is proposed for rolling bearing fault diagnosis with small samples under different working conditions. First, inspired by the envelope demodulation signal processing method, an AI length selection strategy considering the different working conditions is proposed, which improves the disadvantages of Gram angle field (GAF) 2-D coding method with the traditional fixed input. Second, the residual structure and attention mechanism are introduced to make the network have stronger feature extraction and generalization performance and further improve the classification accuracy. Finally, the effectiveness of the method is verified on the Case Western Reserve University (CWRU) bearing fault datasets and the high-speed train axle box bearing fault datasets conducted by us. The results show that the proposed improved model-agnostic meta-learning (MAML) network is superior to the other four mainstream meta-learning methods under the same conditions, and satisfactory fault diagnosis results can be obtained on the bearing fault datasets under different working conditions.  © 2023 IEEE.,Fault diagnosis is an essential process for the health maintenance of rotating machinery. With the development of AI technology, many deep learning-based methods have been applied to fault diagnosis to enhance the intelligence level of equipment maintenance. Such methods normally need a large amount of labeled data for model training. However, label acquisition is a difficult task that requires extensive human labor. To address these issues, a fault diagnosis method based on feature extraction via an unsupervised graph neural network is proposed in this paper. In the proposed method, the K-nearest neighbor approach is adopted to construct a fault graph from the collected signals, thereby providing extra relationship information for fine feature mining. Then, the GraphSAGE model is trained on the constructed graph in an unsupervised way, that is, it does not need labeled data, to extract features of each signal sample. Based on the extracted features, some traditional classifiers are adopted to identify the fault types. The proposed model is evaluated on a rolling bearing dataset provided by the University of Paderborn and a motor rotor dataset collected by a constructed motor rotor system. Compared with some traditional deep learning-based fault diagnosis methods, the proposed model can achieve more accurate diagnoses even when there are only a few labeled samples. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
38,37,83,37_cnn_cracks_crack_segmentation,"cnn,cracks,crack,segmentation,recognition,neural,convolutional,convolution,images,dataset","In dam engineering, the presence of cracks and crack width are important indicators for diagnosing the health of dams. The accurate measurement of cracks facilitates the safe use of dams. The manual detection of such defects is unsatisfactory in terms of cost, safety, accuracy, and the reliability of evaluation. The introduction of deep learning for crack detection can overcome these issues. However, the current deep learning algorithms possess a large volume of model parameters, high hardware requirements, and difficulty toward embedding in mobile devices such as drones. Therefore, we propose a lightweight MobileNetV2_DeepLabV3 image segmentation network. Furthermore, to prevent interference by noise, light, shadow, and other factors for long-length targets when segmenting, the atrous spatial pyramid pooling (ASPP) module parameters in the DeepLabV3+ network structure were modified, and a multifeature fusion structure was used instead of the parallel structure in ASPP, allowing the network to obtain richer crack features. We collected the images of dam cracks from different environments, established segmentation datasets, and obtained segmentation models through network training. Experiments show that the improved MobileNetV2_DeepLabV3 algorithm exhibited a higher crack segmentation accuracy than the original MobileNetV2_DeepLabV3 algorithm; the average intersection rate attained 83.23%; and the crack detail segmentation was highly accurate. Compared with other semantic segmentation networks, its training time was at least doubled, and the total parameters were reduced by more than 2 to 7 times. After extracting cracks through the semantic segmentation, we proposed to use the method of inscribed circle of crack outline to calculate the maximum width of the detected crack image and to convert it into the actual width of the crack. The maximum relative error rate was 11.22%. The results demonstrated the potential of innovative deep learning methods for dam crack detection. Copyright © 2023 Zihao Wu et al.,Crack detection is a crucial task in assessing the condition of concrete structures. Herein, a novel deep learning method based on convolutional neural networks, referred to as R-FPANet, is proposed for crack detection. The R-FPANet performs automatic segmentation and quantification of crack morphology at the pixel level. In this methodology, the modularization concept based on the following three modules is adopted: ResNet-50 is chosen as the backbone to extract features from images, the Feature Pyramid Network with Dense Block is integrated to promote the fusion of both shallow and deep features as well as enhance feature reuse, and self-attention mechanisms such as Channel Attention Module and Position Attention Module are introduced to strengthen the dependency between features. Based on the crack segmentation results, a suitably established framework is developed for quantitative analysis of the major geometric parameters, including crack area, crack length, crack mean width and crack max-width at the pixel level. To verify the effectiveness of the proposed method, a large-scale concrete crack image dataset was produced and carefully labeled at the pixel level and then utilized to train the model. Finally, our experiments reveal that the proposed approach achieves an Intersection over Union of 83.07%, further indicating that the segmentation performance of the proposed method is better than the state-of-the-art models and also confirming that the crack quantification results are close to reality. Overall, the proposed method performs well, contributing to crack detection and quantification with great potential for practical use. © 2023 Institution of Structural Engineers,Crack detection is vital to maintain the structural safety of in-service bridges, which is an increasing demand in the industrial community. The deep learning-based crack detection is an emerging method that provides a novel way to deal with this problem. Although, the deep neural networks (DNN) can learn to detect cracks themselves, they require large numbers of crack images to learn the features of cracks in the real world. Besides, except for the crack forms, there always exist all kinds of noise motifs that will disturb the correct detection of crack regions. The lack of crack sample images has been an obstacle for the improvement of deep learning-based crack detection method. This paper proposes a generative adversarial network (GAN) based method to establish a synthesized crack image dataset with pixel-wise annotations, which provides a novel way aside from the traditional data augmentation method. The Deep Convolutional GAN (DCGAN) model was adopted for the generation of synthesized crack annotations while the Pixel2Pixel model was used to generate the corresponding synthesized crack images. The generated crack annotations and crack images during the training epochs were demonstrated to show how the GANs learn to generate the synthesized images. Moreover, comparative study was conducted to validate the performance of the synthesized crack image dataset for training the crack detection DNN. The results showed the DNN trained by the synthesized images can achieve 74.34% of the MeanIoU that was reached by the same DNN model trained with real images. As for the way of using the synthesized and real crack images for training a crack detection DNN, the way that use the synthesized crack images for pre-training and then use the real images for fine-tuning is better than the way that directly mix the synthesized and real crack images for training. This work provides reference for the GAN-based establishment of crack image dataset and the evaluation of the image quality for training crack detection DNNs. © 2023 Elsevier Ltd"
39,38,82,38_pdes_pde_discretization_nonlinear,"pdes,pde,discretization,nonlinear,modeling,sparse,models,neural,partial,optimization","Efficient solution of partial differential equations (PDEs) of physical laws is of interest for manifold applications in computer science and image analysis. However, conventional domain discretization techniques for numerical solving PDEs such as Finite Difference (FDM), Finite Element (FEM) methods are unsuitable for real-time applications and are also quite laborious in adaptation to new applications, especially for non-experts in numerical mathematics and computational modeling. More recently, alternative approaches to solving PDEs using the so-called Physically Informed Neural Networks (PINNs) received increasing attention because of their straightforward application to new data and potentially more efficient performance. In this work, we present a novel data-driven approach to solve 2D Laplace PDE with arbitrary boundary conditions using deep learning models trained on a large set of reference FDM solutions. Our experimental results show that both forward and inverse 2D Laplace problems can efficiently be solved using the proposed PINN approach with nearly real-time performance and average accuracy of 94% for different types of boundary value problems compared to FDM. In summary, our deep learning based PINN PDE solver provides an efficient tool with various applications in image analysis and computational simulation of image-based physical boundary value problems. © 2023, The Author(s).,In this work, an unsupervised data-driven method based on sequential singular value filtering (Seq-SVF) is proposed to simultaneously identify multiple partial differential equations from observed data considering potential noises. This method is aimed to extend the Sparse Identification of Nonlinear Dynamics (SINDy) to the identification of general nonlinear partial differential equations by transforming the paradigm based on regression to an unsupervised paradigm. To discover the complex coupled equations of vector or tensor forms without prior knowledge, the techniques of singular value decomposition (SVD) and strong rank-revealing QR factorization (sRRQR) are applied to the data matrix, which ensures that the method can automatically identify the number and the corresponding linearly independent terms as the left-hand terms of governing equations. To balance the complexity and the precision of modeling, a strategy for filtering singular values is designed to determine the sparse structure of governing equations from a large number of nonlinear basis functions. We show the success of the method to extract explicit and succinct models from many complex linear, nonlinear, and multiphysics mechanical systems, and the examples show more accuracy compared with using traditional sparse learning methods. © 2023 Elsevier B.V.,Full-field discrete measurements of the continuous spatiotemporal response of physical processes often generate large datasets. Such continuous spatiotemporal dynamic models are represented by partial differential equations (PDEs). In the past, attempts have been made to identify the PDE models from the measured response by inferring its parameters via regression or deep learning-based techniques. But the previously presented regression-based methods fail to estimate the parameters of the higher-order PDE models in the presence of moderate noise. Likewise, the deep learning-based methods lack the much-needed property of repeatability and robustness in the identification of PDE models from the measured response. The proposed method of SimultaNeous Basis Function Approximation and Parameter Estimation (SNAPE) addresses such drawbacks by simultaneously fitting basis functions to the measured response and estimating the parameters of both ordinary and partial differential equations. The domain knowledge of the general multidimensional process is used as a constraint in the formulation of the optimization framework. The alternating direction method of multipliers (ADMM) algorithm is used to simultaneously optimize the loss function over the parameter space of the PDE model and coefficient space of the basis functions. The proposed method not only infers the parameters but also estimates a continuous function that approximates the solution to the PDE model. SNAPE not only demonstrates its applicability on various complex dynamic systems that encompass wide scientific domains including Schrödinger equation, chaotic duffing oscillator, and Navier–Stokes equation but also estimates an analytical approximation to the process response. The method systematically combines the knowledge of well-established scientific theories and the concepts of data science to infer the properties of the process from the observed data. © 2022 Elsevier Ltd"
40,39,81,39_hyperspectral_spatialspectral_multispectral_classification,"hyperspectral,spatialspectral,multispectral,classification,spectral,spectralspatial,cnn,features,cnns,convolutional","Hyperspectral imagery gives details of spectral information through hundreds of spectral bands also known as dimensionality. The bands with continuous spectral information model are capable of classifying various materials of interest. The enhanced dimensionality of such data allows for major changes in data relevant information, but it also presents a challenge to traditional methods for accurate hyperspectral image analysis so-called ""curse of dimensionality.""The hyperspectral images are used to identify objects on the earth's surface. Due to a large number of spectral bands, classifying objects using hyperspectral imagery has become more challenging. Feature reduction and extraction techniques are used to address these high-dimensionality issues. However, there are various challenges dealing with data classification with performance and computational time. As a result, a technique for hyperspectral image classification based on a convolutional neural network (CNN) along with geometric transformation and polygons segmentation process has been proposed. A CNN is used to convert compressed features into high-level features that were utilized to classify objects into buildings and road networks. The main objective of this paper is to design an automated building footprint extraction and road detection from hyperspectral imagery using CNN. The polygons segmentation is used for the extraction and detection of spectral features in hyperspectral data. CNN is used to classify these extracted spectral features, such as building footprints and road detection, using different kernels. When comparing the proposed techniques with other support vector machine and fully convolutional network methods, the experimental results show that CNN provides 97% classification accuracy.  © 2022 SPIE and IS&T.,Aimed at the hyperspectral image (HSI) classification under the condition of limited samples, this paper designs a joint spectral–spatial classification network based on metric meta-learning. First, in order to fully extract HSI fine features, the squeeze and excitation (SE) attention mechanism is introduced into the spectrum dimensional channel to selectively extract useful HSI features to improve the sensitivity of the network to information features. Second, in the part of spatial feature extraction, the VGG16 model parameters trained in advance on the HSRS-SC dataset are used to realize the transfer and learning of spatial feature knowledge, and then, the higher-level abstract features are extracted to mine the intrinsic attributes of ground objects. Finally, the gated feature fusion strategy is introduced to connect the extracted spectral and spatial feature information on HSI for mining more abundant feature information. In this paper, a large number of experiments are carried out on the public hyperspectral dataset, including Pavia University and Salinas. The results show that the meta-learning method can achieve fast learning of new categories with only a small number of labeled samples and has good generalization ability for different HSI datasets. Copyright © 2023 Wu, Li and Wang.,Currently, long-range spectral and spatial dependencies have been widely demonstrated to be essential for hyperspectral image (HSI) classification. Due to the transformer superior ability to exploit long-range representations, the transformer-based methods have exhibited enormous potential. However, existing transformer-based approaches still face two crucial issues that hinder the further performance promotion of HSI classification: 1) treating HSI as 1-D sequences neglects spatial properties of HSI and 2) the dependence between spectral and spatial information is not fully considered. To tackle the above problems, a large kernel spectral-spatial attention network (LKSSAN) is proposed to capture the long-range 3-D properties of HSI, which is inspired by the visual attention network (VAN). Specifically, a spectral-spatial attention module (SSAM) is first proposed to effectively exploit discriminative 3-D spectral-spatial features while keeping the 3-D structure of HSI. This module introduces the large kernel attention (LKA) and convolutional feed-forward (CFF) to flexibly emphasize, model, and exploit the long-range 3-D feature dependencies with lower computational pressure. Finally, the features from the SSAM are fed into the classification module for the optimization of 3-D spectral-spatial representation. To verify the effectiveness of the proposed classification method, experiments are executed on four widely used HSI datasets. The experiments demonstrate that LKSSAN is indeed an effective way for long-range 3-D feature extraction of HSI.  © 1980-2012 IEEE."
41,40,80,40_actions_videos_action_attention,"actions,videos,action,attention,supervised,recognition,spatiotemporal,video,representations,motion","In recent years, spatial-temporal graph convolutional networks have played an increasingly important role in skeleton-based human action recognition. However, there are still three major limitations to most ST-GCN-based approaches: (1) They only use a single joint scale to extract action features, or process joint and skeletal information separately. As a result, action features cannot be extracted dynamically through the mutual directivity between the scales. (2) These models treat the contributions of all joints equally in training, which neglects the problem that some joints with difficult loss-reduction are critical joints in network training. (3) These networks rely heavily on a large amount of labeled data, which remains costly. To address these problems, we propose a Tohjm-trained multiscale spatial-temporal graph convolutional neural network for semi-supervised action recognition, which contains three parts: encoder, decoder and classifier. The encoder’s core is a correlated joint–bone–body-part fusion spatial-temporal graph convolutional network that allows the network to learn more stable action features between coarse and fine scales. The decoder uses a self-supervised training method with a motion prediction head, which enables the network to extract action features using unlabeled data so that the network can achieve semi-supervised learning. In addition, the network is also capable of fully supervised learning with the encoder, decoder and classifier. Our proposed time-level online hard joint mining strategy is also used in the decoder training process, which allows the network to focus on hard training joints and improve the overall network performance. Experimental results on the NTU-RGB + D dataset and the Kinetics-skeleton dataset show that the improved model achieves good performance for action recognition based on semi-supervised training, and is also applicable to the fully supervised approach. © 2022 by the authors.,Temporal action proposal (TAP) aims to detect the action instances’ starting and ending times in untrimmed videos, which is fundamental and critical for large-scale video analysis and human action understanding. The main challenge of the temporal action proposal lies in modeling representative temporal relations in long untrimmed videos. Existing state-of-the-art methods achieve temporal modeling by building local-level, proposal-level, or global-level temporal dependencies. Local methods lack a wider receptive field, while proposal and global methods lack the focalization of learning action frames and contain background distractions. In this paper, we propose that learning semantic-level affinities can capture more practical information. Specifically, by modeling semantic associations between frames and action units, action segments (foregrounds) can aggregate supportive cues from other co-occurring actions, and nonaction clips (backgrounds) can learn the discriminations between them and action frames. To this end, we propose a novel framework named the Mask-Guided Network (MGNet) to build semantic-level temporal associations for the TAP task. Specifically, we first propose a Foreground Mask Generation (FMG) module to adaptively generate the foreground mask, representing the locations of the action units throughout the video. Second, we design a Mask-Guided Transformer (MGT) by exploiting the foreground mask to guide the self-attention mechanism to focus on and calculate semantic affinities with the foreground frames. Finally, these two modules are jointly explored in a unified framework. MGNet models the intra-semantic similarities for foregrounds, extracting supportive action cues for boundary refinement; it also builds the inter-semantic distances for backgrounds, providing the semantic gaps to suppress false positives and distractions. Extensive experiments are conducted on two challenging datasets, ActivityNet-1.3 and THUMOS14, and the results demonstrate that our method achieves superior performance. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,As the cornerstone of human-behavior analysis in video understanding, temporal action proposal generation aims to predict the starting and ending time of human action instances in untrimmed videos. Although large achievements in temporal action proposal generation have been achieved, most previous studies ignore the variability of action frequency in raw videos, leading to unsatisfying performances on high-action-frequency videos. In fact, there exists two main issues which should be well addressed: data imbalance between high and low action-frequency videos, and inferior detection of short actions in high-action-frequency videos. To address the above issues, we propose an effective framework by adapting to the variability of action frequency, namely Action Frequency Adaptive Network (AFAN), which can be flexibly built upon any temporal action proposal generation method. AFAN consists of two modules: Learning From Experts (LFE) and Fine-Grained Processing (FGP). The LFE first trains a series of action proposal generators on different subsets of imbalanced data as experts and then teaches a unified student model via knowledge distillation. To better detect short actions, FGP first finds out high-action-frequency videos and then performs fine-grained detection. Extensive experimental results on four benchmark datasets (ActivityNet-1.3, HACS, THUMOS14 and FineAction) demonstrate the effectiveness and generalizability of the proposed AFAN, especially for high-action-frequency videos.  © 1999-2012 IEEE."
42,41,78,41_teaching_educational_students_courses,"teaching,educational,students,courses,classroom,student,assessment,learners,evaluation,learning","The current world is in an era of the explosive growth in information and rapid renewal. This objective reality puts new demands on education reform. Educational systems and educational models must undergo major changes to meet the needs of creative talent. With the wide application of modern computer technology and network technology, the traditional education structure based on classrooms is constantly changing. In the traditional education process, it emphasizes directly instilling a large amount of knowledge to the students to obtain the quality of teaching. In the network environment, it focuses on cultivating students. Learning methods allow students to have independent knowledge acquisition and also update skills. Online teaching provides a broader and free platform and environment for students' learning. It not only provides rich and colorful teaching resources, but also broadens and breaks the time and space constraints of traditional teaching. The current main method of management of teaching resources is the component teaching resource library, which uses information technology to carry out unified maintenance and management of the teaching resources with increased benefits, and realizes rapid retrieval and convenient use of teaching resources. The numerical verifications are conducted in the final section of the research to verify the performance. © 2021 John Wiley & Sons, Ltd.,With the continuous development of computers, colleges and universities need to attach importance to the combination of student teaching and computer-aided design. The teaching of parametric design is used to scientifically divide different teaching contents to help students meet the needs of the digital era. Use online resources to transfer offline teaching content to online to increase offline design analysis and discussion time. Therefore, how to improve the informatization level of university education management has become one of the important challenges that Chinese education departments are facing and must solve at this stage. For computer-aided design online courses is conducive to improving the quality of online teaching of such courses. Based on the implementation process and effect analysis of computer-aided design online course teaching, the formative multiple evaluation index of course effect is proposed. Based on the potential information of multimedia, a large number of researchers have begun to conduct data mining research on it and accumulated a large amount of data. The ROF-LGB model, a comprehensive classification model based on rotating forest and LightGBM, attempts to deeply explore the potential information of the actual teaching management data set. In this process, some thoughts on educational information management are put forward for readers' reference. © 2023 CAD Solutions, LLC.,The COVID-19 pandemic posed a considerable challenge to education and teaching. The concept of “Internet + Education” accelerated the construction and popularization of a large number of online teaching platforms, accompanied by the emergence of various massive open online courses and online classroom teaching platforms. Thus, evaluating the teaching quality of online courses has become essential. Diversified scientific evaluations of traditional teaching fail owing to problems such as single evaluation subjects, imperfect evaluation standards, and low evaluation efficiency. In this study, first, documents on evaluation index systems and evaluation methods for identifying the teaching effect of online courses are classified, and observation points of an online course teaching effect evaluation in 16 aspects before, during, and after class are proposed. Second, the index weights are determined based on analytic hierarchy process, and a fuzzy comprehensive evaluation model is established to evaluate the online course teaching effect in six application-oriented universities in Henan Province, China. Results show that the observation points of the online course teaching effect evaluation proposed in this study are scientific and reasonable. Specifically, among the observation point indices, X-16, X-2, and X-3 have substantial weight, and the overall average score of the online course teaching effect of the case study subjects is 3.756, with a –24.88% room for improvement compared with the full score of 5. The method can effectively evaluate the teaching quality of online courses scientifically and accurately. The results also have important reference value for elevating the quality assurance standards of online teaching, thereby perfecting the dynamic process of online teaching quality assurance and realizing the effectiveness of online teaching results. © 2023 by the authors of this article. Published under CC-BY."
43,42,74,42_covid_pandemic_mortality_coronavirus,"covid,pandemic,mortality,coronavirus,covid19,hospitalized,prediction,outbreak,predict,predictors","Objective COVID-19 would kill fewer people if health programmes can predict who is at higher risk of mortality because resources can be targeted to protect those people from infection. We predict mortality in a very large population in Mexico with machine learning using demographic variables and pre-existing conditions. Design Cohort study. Setting March 2020 to November 2021 in Mexico, nationally represented. Participants 1.4 million laboratory-confirmed patients with COVID-19 in Mexico at or over 20 years of age. Primary and secondary outcome measures Analysis is performed on data from March 2020 to November 2021 and over three phases: (1) from March to October in 2020, (2) from November 2020 to March 2021 and (3) from April to November 2021. We predict mortality using an ensemble machine learning method, super learner, and independently estimate the adjusted mortality relative risk of each pre-existing condition using targeted maximum likelihood estimation. Results Super learner fit has a high predictive performance (C-statistic: 0.907), where age is the most predictive factor for mortality. After adjusting for demographic factors, renal disease, hypertension, diabetes and obesity are the most impactful pre-existing conditions. Phase analysis shows that the adjusted mortality risk decreased over time while relative risk increased for each pre-existing condition. Conclusions While age is the most important predictor of mortality, younger individuals with hypertension, diabetes and obesity are at comparable mortality risk as individuals who are 20 years older without any of the three conditions. Our model can be continuously updated to identify individuals who should most be protected against infection as the pandemic evolves.  © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ.,Introduction. The risk of infectious disease transmission, including COVID-19, is disproportionately high in correctional facilities due to close living conditions, relatively low levels of vaccination, and reduced access to testing and treatment. While much progress has been made on describing and mitigating COVID-19 and other infectious disease risk in jails and prisons, there are open questions about which data can best predict future outbreaks. Methods. We used facility data and demographic and health data collected from 24 prison facilities in the Pennsylvania Department of Corrections from March 2020 to May 2021 to determine which sources of data best predict a coming COVID-19 outbreak in a prison facility. We used machine learning methods to cluster the prisons into groups based on similar facility-level characteristics, including size, rurality, and demographics of incarcerated people. We developed logistic regression classification models to predict for each cluster, before and after vaccine availability, whether there would be no cases, an outbreak defined as 2 or more cases, or a large outbreak, defined as 10 or more cases in the next 1, 2, and 3 d. We compared these predictions to data on outbreaks that occurred. Results. Facilities were divided into 8 clusters of sizes varying from 1 to 7 facilities per cluster. We trained 60 logistic regressions; 20 had test sets with between 35% and 65% of days with outbreaks detected. Of these, 8 logistic regressions correctly predicted the occurrence of an outbreak more than 55% of the time. The most common predictive feature was incident cases among the incarcerated population from 2 to 32 d prior. Other predictive features included the number of tests administered from 1 to 33 d prior, total population, test positivity rate, and county deaths, hospitalizations, and incident cases. Cumulative cases, vaccination rates, and race, ethnicity, or age statistics for incarcerated populations were generally not predictive. Conclusions. County-level measures of COVID-19, facility population, and test positivity rate appear as potential promising predictors of COVID-19 outbreaks in correctional facilities, suggesting that correctional facilities should monitor community transmission in addition to facility transmission to inform future outbreak response decisions. These efforts should not be limited to COVID-19 but should include any large-scale infectious disease outbreak that may involve institution-community transmission. The risk of infectious disease transmission, including COVID-19, is disproportionately high in correctional facilities. We used machine learning methods with data collected from 24 prison facilities in the Pennsylvania Department of Corrections to determine which sources of data best predict a coming COVID-19 outbreak in a prison facility. Key predictors included county-level measures of COVID-19, facility population, and the test positivity rate in a facility. Fortifying correctional facilities with the ability to monitor local community rates of infection (e.g., though improved interagency collaboration and data sharing) along with continued testing of incarcerated people and staff can help correctional facilities better predict—and respond to—future infectious disease outbreaks. © The Author(s) 2024.,Background: At the end of 2019, the coronavirus disease 2019 (COVID-19) pandemic increased the hospital burden of COVID-19 caused by the SARS-Cov-2 and became the most significant health challenge for nations worldwide. The severity and high mortality of COVID-19 have been correlated with various demographic characteristics and clinical manifestations. Prediction of mortality rate, identification of risk factors, and classification of patients played a crucial role in managing COVID-19 patients. Our purpose was to develop machine learning (ML)-based models for the prediction of mortality and severity among patients with COVID-19. Identifying the most important predictors and unraveling their relationships by classification of patients to the low-, moderate- and high-risk groups might guide prioritizing treatment decisions and a better understanding of interactions between factors. A detailed evaluation of patient data is believed to be important since COVID-19 resurgence is underway in many countries. Results: The findings of this study revealed that the ML-based statistically inspired modification of the partial least square (SIMPLS) method could predict the in-hospital mortality among COVID-19 patients. The prediction model was developed using 19 predictors including clinical variables, comorbidities, and blood markers with moderate predictability (Q2 = 0.24) to separate survivors and non-survivors. Oxygen saturation level, loss of consciousness, and chronic kidney disease (CKD) were the top mortality predictors. Correlation analysis showed different correlation patterns among predictors for each non-survivor and survivor cohort separately. The main prediction model was verified using other ML-based analyses with a high area under the curve (AUC) (0.81?0.93) and specificity (0.94?0.99). The obtained data revealed that the mortality prediction model can be different for males and females with diverse predictors. Patients were classified into four clusters of mortality risk and identified the patients at the highest risk of mortality, which accentuated the most significant predictors correlating with mortality. Conclusion: An ML model for predicting mortality among hospitalized COVID-19 patients was developed considering the interactions between factors that may reduce the complexity of clinical decision-making processes. The most predictive factors related to patient mortality were identified by assessing and classifying patients into different groups based on their sex and mortality risk (low-, moderate-, and high-risk groups). Copyright © 2023 Banoei, Rafiepoor, Zendehdel, Seyyedsalehi, Nahvijou, Allameh and Amanpour."
44,43,72,43_sar_radar_radars_ships,"sar,radar,radars,ships,aperture,detection,recognition,maritime,ship,classification","With the development of synthetic aperture radar (SAR) technology, more SAR datasets with high resolution and large scale have been obtained. Research using SAR images to detect and monitor marine targets has become one of the most important marine applications. In recent years, deep learning has been widely applied to target detection. However, it was difficult to use deep learning to train an SAR ship detection model in complex scenes. To resolve this problem, an SAR ship detection method combining YOLOv4 and the receptive field block (CY-RFB) was proposed in this paper. Extensive experimental results on the SAR-Ship-Dataset and SSDD datasets demonstrated that the proposed method had achieved supreme detection performance compared to the state-of-The-Art ship detection methods in complex scenes, whether they were in offshore or inshore scenes of SAR images.  © 2023 Society of Photo-Optical Instrumentation Engineers (SPIE).,With the continuous development of earth observation technology, space-based synthetic aperture radar (SAR) has become an important source of information for maritime surveillance, and ship classification in SAR images has also become a hot research direction in the field of maritime ship monitoring. In recent years, the remote sensing community has proposed several solutions to the problem of ship object classification in SAR images. However, it is difficult to obtain an adequate amount of labeled SAR samples for training classifiers, which limits the application of machine learning, particularly deep learning methods, in SAR image ship object classification. In contrast, as a real-time automatic tracking system for monitoring ships at sea, a ship automatic identification system (AIS) can provide a large amount of relatively easy-to-obtain labeled ship samples. Therefore, to solve the problem of SAR image ship classification and improve the classification performance of learning models with limited samples, we proposed a SAR image ship classification method based on multiple classifiers ensemble learning (MCEL) and AIS data transfer learning. The core idea of our method is to transfer the MCEL model trained on AIS data to SAR image ship classification, which mainly includes three steps: first, we use the acquired global space-based AIS data to build a dataset for ship object classification models training; then, the ensemble learning model is constructed by combining multiple base classifiers; and finally, the trained classification model is transferred to SAR images for ship type prediction. Experiments show that the proposed method achieves a classification accuracy of 85.00% for the SAR ship classification, which is better than the performance of each base classifier. This proves that AIS data transfer learning can effectively solve the problem of SAR ship classification with limited samples, and has important application value in maritime surveillance. © 2022 by the authors.,Deep learning-based synthetic aperture radar (SAR) ship detection methods are significant in signal processing and radar imaging. However, these approaches always require large-scale SAR ship images with labels to train the model. Due to the inaccessibility of SAR sensors, it is difficult to acquire enough SAR images. Annotating ship targets also demands resources and manpower. To tackle this issue, we propose a novel SAR image generation method named SARGAN for SAR ship detection task. Given the position and category, SARGAN can generate realistic SAR images with SAR ship targets, land, and background in the desired location. In the SARGAN, there are five components: target encoder, scene constructor, SAR image generator, and target and image discriminators. The target encoder is introduced to predict the latent vector for each target, while the scene constructor integrates all targets in the entire scene using convolutional LSTM. We improve the structure of the SAR image generator by adding operations to generate high-quality images. The image and target discriminators are responsible for distinguishing between real and fake samples, with the latter also predicting the category. To promote the generation of diverse and realistic SAR ship images, multiple loss functions are employed for training. Additionally, we have annotated the lands and background in the high-resolution SAR images dataset (HRSID) and combined them with labeled ships to create a new dataset for training and testing of SARGAN. Extensive experiments demonstrate that SARGAN outperforms other SAR image generation methods, and the generated SAR ship images are highly conducive for SAR ship detection task.  © 2023 IEEE."
45,44,71,44_concrete_concretes_cement_prediction,"concrete,concretes,cement,prediction,predicting,predict,compressive,regression,steel,strength","Concrete compressive strength (CCS) is the most crucial structural engineering designing conventional concrete and high-performance concrete (HPC) structures. Accurately predicting high-performance concrete (HPC) compressive strength is crucial when considering this parameter for cost–benefit analysis and time-saving. Recent studies have found that the deep learning (DL) model is more popular, since it has a higher prediction accuracy than conventional machine learning (ML) techniques. This study proposes four deep learning approaches: BiLSTM, CNN, GRU, and LSTM models, which is rarely seen in the literature. The model is developed using a large database, including details about cement, fly ash, coarse aggregate, sand, water, age, and blast furnace slag as input variables and compressive strength as an output variable. In this research, 80% of the dataset is used for training, while the remaining 20% is used as a testing dataset. Deep learning models result showed an R-square value of the above around 0.960 at the training phase and almost overhead 0.940 at the testing phase. But GRU model performs better than other models, where the R-square value was the significant level of the overhead of 0.990 at the training phase and also the above almost 0.961 at the testing phase, which is a high-accuracy result for both phases. Thus, this research provides a novel and effective method for predicting HPC's compressive strength, which can help develop sustainable infrastructures without requiring time-consuming and costly experiments. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.,This paper focuses on the compressive strength of Glass fiber reinforced polymer (GFRP)-confined reinforced concrete columns. Data from 114 sets of GFRP-confined reinforced concrete columns were collected to evaluate the researchers’ and proposed model. A data-driven machine learning model was used to model the compressive strength of the GFRP-confined reinforced concrete columns and investigate the importance and sensitivity of the parameters affecting the compressive strength. The results show that the researchers’ model facilitates the study of the compressive strength of confined columns but suffers from a large coefficient of variation and too high or conservative estimation of compressive strength. The back propagation (BP) neural network has the best accuracy and robustness in predicting the compressive strength of the confined columns, with the coefficient of variation of only 14.22%, and the goodness of fit for both the training and testing sets above 0.9. The parameters that have an enormous influence on compressive strength are the concrete strength and FRP thickness, and all the parameters, except the fracture strain of FRP, are positively or inversely related to the compressive strength. © 2023 by the authors.,Assessment of concrete strength in existing structures is a common engineering problem. Several attempts in the literature showed the potential of ML methods for predicting concrete strength using concrete properties and NDT values as inputs. However, almost all such ML efforts based on NDT data trained models to predict concrete strength for a specific concrete mix design. We trained a global ML-based model that can predict concrete strength for a wide range of concrete types. This study uses data with high variability for training a metaheuristic-guided ANN model that can cover most concrete mixes used in practice. We put together a dataset that has large variations of mix design components. Training an ANN model using this dataset introduced significant test errors as expected. We optimized hyperparameters, architecture of the ANN model and performed feature selection using genetic algorithm. The proposed model reduces test errors from 9.3 MPa to 4.8 MPa. © 2023 The Authors"
46,45,71,45_adversarial_attacks_attacker_attackers,"adversarial,attacks,attacker,attackers,attack,softmax,defenses,attacking,security,evasion","Recent improvements in deep learning models and their practical applications have raised concerns about the robustness of these models against adversarial examples. Adversarial training (AT) has been shown effective in reaching a robust model against the attack that is used during training. However, it usually fails against other attacks, i.e., the model overfits to the training attack scheme. In this paper, we propose a new method for generating adversarial perturbations during training that mitigates the mentioned issue. More specifically, we minimize the perturbation ?p norm while maximizing the classification loss in the Lagrangian form to craft adversarial examples. We argue that crafting adversarial examples based on this scheme results in enhanced attack generalization in the learned model. We compare our final model robust accuracy with the closely related state-of-the-art AT methods against attacks that were not used during training. This comparison demonstrates that our average robust accuracy against unseen attacks is 5.9% higher in the CIFAR-10 dataset and 3.2% higher in the ImageNet-100 dataset than corresponding state-of-the-art methods. We also demonstrate that our attack is faster than other attack schemes that are designed for unseen attack generalization and conclude that the proposed method is feasible for large datasets. Our code is available at https://github.com/rohban-lab/Lagrangian_Unseen . © 2023, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.,The outstanding performance of deep neural networks has promoted deep learning applications in a broad set of domains. However, the potential risks caused by adversarial samples have hindered the large-scale deployment of deep learning. In these scenarios, adversarial perturbations, imperceptible to human eyes, significantly decrease the model's final performance. Many papers have been published on adversarial attacks and their countermeasures in the realm of deep learning. Most focus on evasion attacks, where the adversarial examples are found at test time, as opposed to poisoning attacks where poisoned data is inserted into the training data. Further, it is difficult to evaluate the real threat of adversarial attacks or the robustness of a deep learning model, as there are no standard evaluation methods. Hence, with this article, we review the literature to date. Additionally, we attempt to offer the first analysis framework for a systematic understanding of adversarial attacks. The framework is built from the perspective of cybersecurity to provide a lifecycle for adversarial attacks and defenses.  © 2022 Association for Computing Machinery.,Deep neural networks (DNNs) are widely used to handle many difficult tasks, such as image classification and malware detection, and achieve outstanding performance. However, recent studies on adversarial examples, which have maliciously undetectable perturbations added to their original samples that are indistinguishable by human eyes but mislead the machine learning approaches, show that machine learning models are vulnerable to security attacks. Though various adversarial retraining techniques have been developed in the past few years, none of them is scalable. In this paper, we propose a new iterative adversarial retraining approach to robustify the model and to reduce the effectiveness of adversarial inputs on DNN models. The proposed method retrains the model with both Gaussian noise augmentation and adversarial generation techniques for better generalization. Furthermore, the ensemble model is utilized during the testing phase in order to increase the robust test accuracy. The results from our extensive experiments demonstrate that the proposed approach increases the robustness of the DNN model against various adversarial attacks, specifically, fast gradient sign attack, Carlini and Wagner (C&W) attack, Projected Gradient Descent (PGD) attack, and DeepFool attack. To be precise, the robust classifier obtained by our proposed approach can maintain a performance accuracy of 99% on average on the standard test set. Moreover, we empirically evaluate the runtime of two of the most effective adversarial attacks, i.e., C&W attack and BIM attack, to find that the C&W attack can utilize GPU for faster adversarial example generation than the BIM attack can. For this reason, we further develop a parallel implementation of the proposed approach. This parallel implementation makes the proposed approach scalable for large datasets and complex models. © 2022, The Author(s)."
47,46,71,46_microstructures_microstructure_microstructural_multiscale,"microstructures,microstructure,microstructural,multiscale,modeling,tensile,materials,deformations,elastic,polycrystalline","Inspired by natural materials, hierarchical architected materials can achieve enhanced properties including achieving tailored mechanical responses. However, the design space for such materials is often exceedingly large, and both predicting mechanical properties of complex hierarchically organized materials and designing such materials for specific target properties can be extremely difficult. In this paper we report a deep learning approach using an attention-based diffusion model, capable of providing both, forward predictions of nonlinear mechanical properties as a function of the hierarchical material structure as well as solving inverse design problems in order to discover hierarchical microstructure candidates for a specified nonlinear mechanical response. We exemplify the method for a system of compressively loaded four-hierarchy level materials derived from a family of honeycomb structures, where patterns of distributed buckling events are unitary deformation events that control small- and large-scale deformation behavior. Our model offers exquisite single-shot end-to-end performance in both forward and inverse directions across the entire range of deformation regime, and is capable of rapidly discovering multiple solutions that satisfy a design objective in accordance with the known physical behavior elucidated by, and validated with, coarse-grained simulations. The model provides an effective way towards biologically inspired materials design for high-throughput discovery in order to achieve diverse nonlinear constitutive relationships. © 2023 Elsevier Ltd,The design of optimal microstructures requires first, the identification of microstructural features that influence the material's properties and, then, a search for a combination of these features that give rise to desired properties. For microstructures with complex morphologies, where the number of features is large, deriving these structure–property relationships is a challenging task. To address this challenge, we propose a generative machine learning model that can automatically identify low-dimensional descriptors of microstructural features that can be used to establish structure–property relationships. Based on this model, we present an integrated, data-driven framework for microstructure characterization, reconstruction, and design that is applicable to heterogeneous materials with polycrystalline microstructures. The proposed method is evaluated on a case study of designing dual-phase steel microstructures created with the multi-level Voronoi tessellation method. To this end, we train a variational autoencoder to identify the descriptors from these synthetic dual-phase steel microstructures. Subsequently, we employ Bayesian optimization to search for the optimal combination of the descriptors and generate microstructures with specific yield stress and low susceptibility for damage initiation. The presented results show how microstructure descriptors, determined by the variational autoencoder model, act as design variables for an optimization algorithm that identifies microstructures with desired properties. © 2023 Elsevier Ltd,Two-scale simulations are often employed to analyze the effect of the microstructure on a component's macroscopic properties. Understanding these structure–property relations is essential in the optimal design of materials for specific applications. However, these two-scale simulations are typically computationally expensive and infeasible in multi-query contexts such as optimization and material design. To make such analyses amenable, the microscopic simulations can be replaced by inexpensive-to-evaluate surrogate models. Such surrogate models must be able to handle microstructure parameters in order to be used for material design. A previous work focused on the construction of an accurate surrogate model for microstructures under varying loading and material parameters by combining proper orthogonal decomposition and Gaussian process regression. However, that method works only for a fixed geometry, greatly limiting the design space. This work hence focuses on extending the methodology to treat geometrical parameters. To this end, a method that transforms different geometries onto a parent domain is presented, that then permits existing methodologies to be applied. We propose to solve an auxiliary problem based on linear elasticity to obtain the geometrical transformations. The method has a good reducibility and can therefore be quickly solved for many different geometries. Using these transformations, combined with the nonlinear microscopic problem, we derive a fast-to-evaluate surrogate model with the following key features: (1) the predictions of the effective quantities are independent of the auxiliary problem, (2) the predicted stress fields automatically fulfill the microscopic balance laws and are periodic, (3) the method is non-intrusive, (4) the stress field for all geometries can be recovered, and (5) the sensitivities are available and can be readily used for optimization and material design. The proposed methodology is tested on several composite microstructures, where rotations and large variations in the shape of inclusions are considered. Finally, a two-scale example is shown, where the surrogate model achieves a high accuracy and significant speed up, thus demonstrating its potential in two-scale shape optimization and material design problems. © 2022 The Author(s)"
48,47,70,47_clustering_cluster_graphbased_clusters,"clustering,cluster,graphbased,clusters,embedding,views,regularization,similarity,graphs,unsupervised","Multiview clustering (MVC) aims to exploit heterogeneous information from different sources and was extensively investigated in the past decade. However, far less attention has been paid to handling large-scale multiview data. In this brief, we fill this gap and propose a fast multiview clustering by an optimal graph mining model to handle large-scale data. We mine a consistent clustering structure from landmark-based graphs of different views, from which the optimal graph based on the one-hot encoding of cluster labels is recovered. Our model is parameter-free, so intractable hyperparameter tuning is avoided. An efficient algorithm of linear complexity to the number of samples is developed to solve the optimization problems. Extensive experiments on real-world datasets of various scales demonstrate the superiority of our proposal. IEEE,Cluster assignment of large and complex datasets is a crucial but challenging task in pattern recognition and computer vision. In this study, we explore the possibility of employing fuzzy clustering in a deep neural network framework. Thus, we present a novel evolutionary unsupervised learning representation model with iterative optimization. It implements the deep adaptive fuzzy clustering (DAFC) strategy that learns a convolutional neural network classifier from given only unlabeled data samples. DAFC consists of a deep feature quality-verifying model and a fuzzy clustering model, where deep feature representation learning loss function and embedded fuzzy clustering with the weighted adaptive entropy is implemented. We joint fuzzy clustering to the deep reconstruction model, in which fuzzy membership is utilized to represent a clear structure of deep cluster assignments and jointly optimize for the deep representation learning and clustering. Also, the joint model evaluates current clustering performance by inspecting whether the resampled data from estimated bottleneck space have consistent clustering properties to improve the deep clustering model progressively. Experiments on various datasets show that the proposed method obtains a substantially better performance for both reconstruction and clustering quality compared to the other state-of-the-art deep clustering methods, as demonstrated with the in-depth analysis in the extensive experiments. IEEE,Fuzzy clustering algorithms have been widely used to reveal the possible hidden structure of data. However, with the increasing of data amount, large scale data has brought genuine challenges for fuzzy clustering. Most fuzzy clustering algorithms suffer from the long time-consumption problem since a large amount of distance calculations are involved to update the solution per iteration. To address this problem, we introduce the popular anchor graph technique into fuzzy clustering and propose a scalable fuzzy clustering algorithm referred to as Scalable Fuzzy Clustering with Anchor Graph (SFCAG). The main characteristic of SFCAG is that it addresses the scalability issue plaguing fuzzy clustering from two perspectives: anchor graph construction and membership matrix learning. Specifically, we select a small number of anchors and construct a sparse anchor graph, which is beneficial to reduce the computational complexity. We then formulate a trace ratio model, which is parameter-free, to learn the membership matrix of anchors to speed up the clustering procedure. In addition, the proposed method enjoys linear time complexity with the data size. Extensive experiments performed on both synthetic and real world datasets demonstrate the superiority (both effectiveness and scalability) of the proposed method over some representative large scale clustering methods.  © 1989-2012 IEEE."
49,48,69,48_hospitalizations_hospitalization_hospital_mortality,"hospitalizations,hospitalization,hospital,mortality,predicting,icu,sepsis,prediction,hospitals,boosting","The recent COVID-19 pandemic has affected health systems across the world. Especially, Intensive Care Units (ICUs) have played a pivotal role in the treatment of critically-ill patients. At the same time however, the increasing number of admissions due to the vast prevalence of the virus have caused several problems for ICU wards such as overburdening of staff and shortages of medical resources. These issues might have affected the quality of healthcare services provided directly impacting a patient’s survival. The objective of this research is to leverage Machine Learning (ML) on hospital data in order to support hospital managers and practitioners with the treatment of COVID-19 patients. This is accomplished by providing more detailed inference about a patient’s likelihood of ICU admission, mortality and in case of hospitalization the length of stay (LOS). In this pursuit, the outcome variables are in three separate models predicted by five different ML algorithms: eXtreme Gradient Boosting (XGB), K-Nearest Neighbor (KNN), Random Forest (RF), bagged-CART (b-CART), and LogitBoost (LB). With the exception of KNN, the studied models show good predictive capabilities when evaluating relevant accuracy scores, such as area under the curve. By implementing an ensemble stacking approach (either a Neural Net or a General Linear Model) on top of the aforementioned ML algorithms the performance is further boosted. Ultimately, for the prediction of admission to the ICU, the ensemble stacking via a Neural Net achieved the best result with an accuracy of over 95%. For mortality at the ICU, the vanilla XGB performed slightly better (1% difference with the meta-model). To predict large length of stays both ensemble stacking approaches yield comparable results. Besides it direct implications for managing COVID-19 patients, the approach presented serves as an example how data can be employed in future pandemics or crises. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Background: Risk stratification plays an essential role in the decision making for sepsis management, as existing approaches can hardly satisfy the need to assess this heterogeneous population. We aimed to develop and validate a machine learning model to predict in-hospital mortality in critically ill patients with sepsis. Methods: Adult patients fulfilling the definition of Sepsis-3 were included at a large tertiary medical center. Relevant clinical features were extracted within the first 24 h in ICU, re-classified into different genres, and utilized for model development under three strategies: “Basic + Lab”, “Basic + Intervention”, and “Whole” feature sets. Extreme gradient boosting (XGBoost) was compared with logistic regression (LR) and established severity scores. Temporal validation was conducted using admissions from 2017 to 2019. Results: The final cohort included 24,272 patients, of which 4013 patients formed the test cohort for temporal validation. The trained and fine-tuned XGBoost model with the whole feature set showed the best discriminatory ability in the test cohort with AUROC as 0.85, significantly higher than the XGBoost “Basic + Lab” model (0.83), the LR “Whole” model (0.82), SOFA (0.63), SAPS-II (0.73), and LODS score (0.74). The performance in varying subgroups remained robust, and predictors, such as increased urine output and supplemental oxygen therapy, were crucially correlated with improved survival when interpretability was explored. Conclusions: We developed and validated a novel XGBoost-based model and demonstrated significantly improved performance to LR and other scores in predicting the mortality risks of sepsis patients in the hospital using features in the first 24 h. © 2023 by the authors.,An ICU is a critical care unit that provides advanced medical support and continuous monitoring for patients with severe illnesses or injuries. Predicting the mortality rate of ICU patients can not only improve patient outcomes, but also optimize resource allocation. Many studies have attempted to create scoring systems and models that predict the mortality of ICU patients using large amounts of structured clinical data. However, unstructured clinical data recorded during patient admission, such as notes made by physicians, is often overlooked. This study used the MIMIC-III database to predict mortality in ICU patients. In the first part of the study, only eight structured variables were used, including the six basic vital signs, the GCS, and the patient’s age at admission. In the second part, unstructured predictor variables were extracted from the initial diagnosis made by physicians when the patients were admitted to the hospital and analyzed using Latent Dirichlet Allocation techniques. The structured and unstructured data were combined using machine learning methods to create a mortality risk prediction model for ICU patients. The results showed that combining structured and unstructured data improved the accuracy of the prediction of clinical outcomes in ICU patients over time. The model achieved an AUROC of 0.88, indicating accurate prediction of patient vital status. Additionally, the model was able to predict patient clinical outcomes over time, successfully identifying important variables. This study demonstrated that a small number of easily collectible structured variables, combined with unstructured data and analyzed using LDA topic modeling, can significantly improve the predictive performance of a mortality risk prediction model for ICU patients. These results suggest that initial clinical observations and diagnoses of ICU patients contain valuable information that can aid ICU medical and nursing staff in making important clinical decisions. © 2023 by the authors."
50,49,68,49_tomography_denoising_mri_imaging,"tomography,denoising,mri,imaging,sparseview,learningbased,deep,neural,sparse,reconstructed","Acceleration in MRI has garnered much attention from the deep-learning community in recent years, particularly for imaging large anatomical volumes such as the abdomen or moving targets such as the heart. A variety of deep learning approaches have been investigated, with most existing works using convolutional neural network (CNN)-based architectures as the reconstruction backbone, paired with fixed, rather than learned, k-space undersampling patterns. In both image domain and k-space, CNN-based architectures may not be optimal for reconstruction due to its limited ability to capture long-range dependencies. Furthermore, fixed undersampling patterns, despite ease of implementation, may not lead to optimal reconstruction. Lastly, few deep learning models to date have leveraged temporal correlation across dynamic MRI data to improve reconstruction. To address these gaps, we present a dual-domain (image and k-space), transformer-based reconstruction network, paired with learning-based undersampling that accepts temporally correlated sequences of MRI images for dynamic reconstruction. We call our model DuDReTLU-net. We train the network end-to-end against fully sampled ground truth dataset. Human cardiac CINE images undersampled at different factors (5?100) were tested. Reconstructed images were assessed both visually and quantitatively via the structural similarity index, mean squared error, and peak signal-to-noise. Experimental results show superior performance of DuDReTLU-net over state-of-the-art methods (LOUPE, k-t SLR, BM3D-MRI) in accelerated MRI reconstruction; ablation studies show that transformer-based reconstruction outperformed CNN-based reconstruction in both image domain and k-space; dual-domain reconstruction architectures outperformed single-domain reconstruction architectures regardless of reconstruction backbone (CNN or transformer); and dynamic sequence input leads to more accurate reconstructions than single frame input. We expect our results to encourage further research in the use of dual-domain architectures, transformer-based architectures, and learning-based undersampling, in the setting of accelerated MRI reconstruction. The code for this project is made freely available at https://github.com/william2343/dual-domain-mri-recon-nets (Hong et al., 2022). © 2023 Elsevier Ltd,Objective. Low-dose CT (LDCT) is an important research topic in the field of CT imaging because of its ability to reduce radiation damage in clinical diagnosis. In recent years, deep learning techniques have been widely applied in LDCT imaging and a large number of denoising methods have been proposed. However, One major challenge of supervised deep learning-based methods is the exactly geometric pairing of datasets with different doses. Therefore, the aim of this study is to develop an unsupervised learning-based LDCT imaging method to address the aforementioned challenges. Approach. In this paper, we propose an unsupervised learning-based dual-domain method for LDCT denoising, which consists of two stages: the first stage is projection domain denoising, in which the unsupervised learning method Noise2Self is applied to denoise the projection data with statistically independent and zero-mean noise. The second stage is an iterative enhancement approach, which combines the prior information obtained from the generative model with an iterative reconstruction algorithm to enhance the details of the reconstructed image. Main results. Experimental results show that our proposed method outperforms the comparison method in terms of denoising effect. Particularly, in terms of SSIM, the denoised results obtained using our method achieve the highest SSIM. Significance. In conclusion, our unsupervised learning-based method can be a promising alternative to the traditional supervised methods for LDCT imaging, especially when the availability of the labeled datasets is limited. © 2023 Institute of Physics and Engineering in Medicine.,Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable self-supervised learning for MR image reconstruction by combining a self-supervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of Noise2Noise in MR reconstruction by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the proposed method requires a reduced amount of training data to achieve high reconstruction quality among the state-of-art of MR reconstruction utilizing the Noise2Noise method. IEEE"
51,50,64,50_lstm_news_tweets_classification,"lstm,news,tweets,classification,twitter,headlines,content,clickbait,disinformation,features","The proliferation of social networks has presented a significant challenge in combating the pervasive issue of fake news within modern societies. Due to the large amount of information and news produced daily in text, audio, and video, the validation and verification of this information have become crucial tasks. Leveraging advancements in artificial intelligence, distinguishing between fake news and factual information through automatic fake news detection systems has become more feasible. Automatic fake news detection has been explored from diverse perspectives, employing various feature extraction and classification models. Nonetheless, empirical evaluations, categorization, and comparisons of existing techniques for handling this problem remain limited. In this paper, we revisit the definitions and perspectives of fake news and propose an updated taxonomy for the field based on multiple criteria: (1) Type of features used in fake news detection; (2) Fake news detection perspectives; (3) Feature representation methods; and (4) Classification approaches. Moreover, we conduct an extensive empirical study to evaluate several feature representation techniques and classification approaches based on accuracy and computational cost. Our experimental results demonstrate that the optimal feature extraction techniques vary depending on the characteristics of the dataset. Notably, context-dependent models based on transformer models consistently exhibit superior performance. Additionally, employing transformer models as feature extraction methods, rather than solely fine-tuning the network for the downstream task, improves overall performance. Through extensive error analysis, we identify that a combination of feature representation methods and classification algorithms, including classical ones, offer complementary aspects and should be considered for achieving better generalization performance while maintaining a relatively low computational cost. For further details, including source codes, figures, and datasets, please refer to our project's GitHub repository: [https://github.com/FFarhangian/Fake-news-detection-Comparative-Study]. © 2023 Elsevier B.V.,The use of digital media, such as social networks, has promoted the spreading of fake news on a large scale. Therefore, several Machine Learning techniques, such as artificial neural networks, have been used for fake news detection and classification. These techniques are widely used due to their learning capabilities. Besides, models based on artificial neural networks can be easily integrated into social media and websites to spot fake news early and avoid their propagation. Nevertheless, most fake news classification models are available only for English news, limiting the possibility of detecting fake news in other languages, such as Spanish. For this reason, this study proposes implementing a web service that integrates a deep learning model for the classification of fake news in Spanish. To determine the best model, the performance of several neural network architectures, including MLP, CNN, and LSTM, was evaluated using the F1 score., and LSTM using the F1 score. The LSTM architecture was the best, with an F1 score of 0.746. Finally, the efficiency of web service was evaluated, applying temporal behavior as a metric, resulting in an average response time of 1.08 seconds © 2023, International Journal of Advanced Computer Science and Applications.All Rights Reserved.,As the number of people using social networks increases, more people are using social media platforms to meet their news needs. Users think that it is easier to follow the agenda by accessing news, especially on Twitter, rather than newspaper news pages. However, fake news is increasingly appearing on social media, and it is not always possible for people to obtain correct news from partial news pages or short Twitter posts. Understanding whether the news shared on Twitter is true or not is an important problem. Detecting fake tweets is of great importance in Turkish as well as in any language. In this study, fake news obtained from verification platforms on Twitter and real news obtained from the Twitter accounts of mainstream newspapers were labeled and, preprocessed using the Zemberek natural language processing tool developed for the Turkish language, and a dataset named TR_FaRe_News was created. Then, the TR_FaRe_News dataset was explored using ensemble methods and BoW, TF-IDF, and Word2Vec vectorization methods for fake news detection. Then a pre-trained BERT deep learning model was fine-tuned, and variations of the model extended with Bi-LSTM and Convolutional Neural Network (CNN) layers with the frozen and unfrozen parameters methods were explored. The performance evaluation was conducted using seven comparable datasets, namely BuzzFeedNews, GossipCop, ISOT, LIAR, Twitter15, and Twitter16, including an LLM-generated fake news dataset. Analyzing Turkish tweets and using fake news datasets generated by LLM is considered an important contribution. Accuracy values between 90 and 94% were obtained with the BERT and BERTurk + CNN models with 94% accuracy.  © 2013 IEEE."
52,51,64,51_renewable_photovoltaic_learning_scheduling,"renewable,photovoltaic,learning,scheduling,pv,vpps,dqn,reinforcement,dispatch,energy","Renewable energy sources (RES) are increasingly being developed and used to address the energy crisis and protect the environment. However, the large-scale integration of wind and solar energy into the power grid is still challenging and limits the adoption of these new energy sources. Microgrids (MGs) are small-scale power generation and distribution systems that can effectively integrate renewable energy, electric loads, and energy storage systems (ESS). By using MGs, it is possible to consume renewable energy locally and reduce energy losses from long-distance transmission. This paper proposes a deep reinforcement learning (DRL)-based energy management system (EMS) called DRL-MG to process and schedule energy purchase requests from customers in real-time. Specifically, the aim of this paper is to enhance the quality of service (QoS) for customers and reduce their electricity costs by proposing an approach that utilizes a Deep Q-learning Network (DQN) model. The experimental results indicate that the proposed method outperforms commonly used real-time scheduling methods significantly. © 2023 The Authors. IET Generation, Transmission & Distribution published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.,Taking into account the challenges of obtaining accurate physical parameters and uncertainties arising from the integration of a large number of sources and loads, this paper proposes a real-time voltage control method for AC/DC distribution networks. The method utilizes model-free generation and coordinated control of multiple converters, and employs a combination of agent modeling and multi-agent soft actor critic (MASAC) techniques for modeling and solving the problem. Firstly, a complex nonlinear mapping relationship between bus power and voltage is established by training an power-voltage model, to address the issue of obtaining physical parameters in AC/DC distribution networks. Next, a Markov decision process is established for the voltage control problem, with multiple intelligent agents distributed to control the active and reactive power at each converter, in response to the uncertainties of photovoltaic (PV) and load variations. Using the MASAC method, a centralized training strategy and decentralized execution policy are implemented to achieve distributed control of the converters, with each converter making optimal decisions based on its local observation state. Finally, the proposed method is verified by numerical simulations, demonstrating its sound effectiveness and generalization ability. Copyright © 2023 Zhao, Han, Wang, Dong and Qian.,The development of distributed renewable energy resources and smart energy management are efficient approaches to decarbonizing building energy systems. Reinforcement learning (RL) is a data-driven control algorithm that trains a large amount of data to learn control policy. However, this learning process generally presents low learning efficiency using real-world stochastic data. To address this challenge, this study proposes a model-based RL approach to optimize the operation of existing zero-energy houses considering PV generation consumption and energy costs. The model-based approach takes advantage of the inner understanding of the system dynamics; this knowledge improves the learning efficiency. A reward function is designed considering the physical constraints of battery storage, photovoltaic (PV) production feed-in profit, and energy cost. Measured data of a zero-energy house are used to train and test the proposed RL agent control, including Q-learning, deep Q network (DQN), and deep deterministic policy gradient (DDPG) agents. The results show that the proposed RL agents can achieve fast convergence during the training process. In comparison with the rule-based strategy, test cases verify the cost-effectiveness performances of proposed RL approaches in scheduling operations of the hybrid energy system under different scenarios. The comparative analysis of test periods shows that the DQN agent presents better energy cost-saving performances than Q-learning while the Q-learning agent presents more flexible action control of the battery with the fluctuation of real-time electricity prices. The DDPG algorithm can achieve the highest PV self-consumption ratio, 49.4%, and the self-sufficiency ratio reaches 36.7%. The DDPG algorithm outperforms rule-based operation by 7.2% for energy cost during test periods. © 2023 by the authors."
53,52,64,52_prognosis_prognostic_carcinoma_mortality,"prognosis,prognostic,carcinoma,mortality,predicting,cancer,metastasis,prediction,predict,predictive","Background: Individualized therapeutic strategies can be carried out under the guidance of expected lifespan, hence survival prediction is important. Nonetheless, reliable survival estimation in individuals with bone metastases from cancer of unknown primary (CUP) is still scarce. The objective of the study is to construct a model as well as a web-based calculator to predict three-month mortality among bone metastasis patients with CUP using machine learning-based techniques. Methods: This study enrolled 1010 patients from a large oncological database, the Surveillance, Epidemiology, and End Results (SEER) database, in the United States between 2010 and 2018. The entire patient population was classified into two cohorts at random: a training cohort (n=600, 60%) and a validation cohort (410, 40%). Patients from the validation cohort were used to validate models after they had been developed using the four machine learning approaches of random forest, gradient boosting machine, decision tree, and eXGBoosting machine on patients from the training cohort. In addition, 101 patients from two large teaching hospital were served as an external validation cohort. To evaluate each model’s ability to predict the outcome, prediction measures such as area under the receiver operating characteristic (AUROC) curves, accuracy, and Youden index were generated. The study’s risk stratification was done using the best cut-off value. The Streamlit software was used to establish a web-based calculator. Results: The three-month mortality was 72.38% (731/1010) in the entire cohort. The multivariate analysis revealed that older age (P=0.031), lung metastasis (P=0.012), and liver metastasis (P=0.008) were risk contributors for three-month mortality, while radiation (P=0.002) and chemotherapy (P<0.001) were protective factors. The random forest model showed the highest area under curve (AUC) value (0.796, 95% CI: 0.746-0.847), the second-highest precision (0.876) and accuracy (0.778), and the highest Youden index (1.486), in comparison to the other three machine learning approaches. The AUC value was 0.748 (95% CI: 0.653-0.843) and the accuracy was 0.745, according to the external validation cohort. Based on the random forest model, a web calculator was established: https://starxueshu-codeok-main-8jv2ws.streamlitapp.com/. When compared to patients in the low-risk groups, patients in the high-risk groups had a 1.99 times higher chance of dying within three months in the internal validation cohort and a 2.37 times higher chance in the external validation cohort (Both P<0.001). Conclusions: The random forest model has promising performance with favorable discrimination and calibration. This study suggests a web-based calculator based on the random forest model to estimate the three-month mortality among bone metastases from CUP, and it may be a helpful tool to direct clinical decision-making, inform patients about their prognosis, and facilitate therapeutic communication between patients and physicians. Copyright © 2022 Cui, Wang, Shi, Ye, Lei and Wang.,Background: Identifying female individuals at highest risk of developing life-threatening breast cancers could inform novel stratified early detection and prevention strategies to reduce breast cancer mortality, rather than only considering cancer incidence. We aimed to develop a prognostic model that accurately predicts the 10-year risk of breast cancer mortality in female individuals without breast cancer at baseline. Methods: In this model development and validation study, we used an open cohort study from the QResearch primary care database, which was linked to secondary care and national cancer and mortality registers in England, UK. The data extracted were from female individuals aged 20–90 years without previous breast cancer or ductal carcinoma in situ who entered the cohort between Jan 1, 2000, and Dec 31, 2020. The primary outcome was breast cancer-related death, which was assessed in the full dataset. Cox proportional hazards, competing risks regression, XGBoost, and neural network modelling approaches were used to predict the risk of breast cancer death within 10 years using routinely collected health-care data. Death due to causes other than breast cancer was the competing risk. Internal–external validation was used to evaluate prognostic model performance (using Harrell's C, calibration slope, and calibration in the large), performance heterogeneity, and transportability. Internal–external validation involved dataset partitioning by time period and geographical region. Decision curve analysis was used to assess clinical utility. Findings: We identified data for 11 626 969 female individuals, with 70 095 574 person-years of follow-up. There were 142 712 (1·2%) diagnoses of breast cancer, 24 043 (0·2%) breast cancer-related deaths, and 696 106 (6·0%) deaths from other causes. Meta-analysis pooled estimates of Harrell's C were highest for the competing risks model (0·932, 95% CI 0·917–0·946). The competing risks model was well calibrated overall (slope 1·011, 95% CI 0·978–1·044), and across different ethnic groups. Decision curve analysis suggested favourable clinical utility across all age groups. The XGBoost and neural network models had variable performance across age and ethnic groups. Interpretation: A model that predicts the combined risk of developing and then dying from breast cancer at the population level could inform stratified screening or chemoprevention strategies. Further evaluation of the competing risks model should comprise effect and health economic assessment of model-informed strategies. Funding: Cancer Research UK. © 2023 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license,Purpose: This study aims to develop a prediction model to categorize the risk of early death among breast cancer patients with bone metastases using machine learning models. Methods: This study examined 16,189 bone metastatic breast cancer patients between 2010 and 2019 from a large oncological database in the United States. The patients were divided into two groups at random in a 90:10 ratio. The majority of patients (n = 14,582, 90%) were served as the training group to train and optimize prediction models, whereas patients in the validation group (n = 1,607, 10%) were utilized to validate the prediction models. Four models were introduced in the study: the logistic regression model, gradient boosting tree model, decision tree model, and random forest model. Results: Early death accounted for 17.4% of all included patients. Multivariate analysis demonstrated that older age; a separated, divorced, or widowed marital status; nonmetropolitan counties; brain metastasis; liver metastasis; lung metastasis; and histologic type of unspecified neoplasms were significantly associated with more early death, whereas a lower grade, a positive estrogen receptor (ER) status, cancer-directed surgery, radiation, and chemotherapy were significantly the protective factors. For the purpose of developing prediction models, the 12 variables were used. Among all the four models, the gradient boosting tree had the greatest AUC [0.829, 95% confident interval (CI): 0.802–0.856], and the random forest (0.828, 95% CI: 0.801–0.855) and logistic regression (0.819, 95% CI: 0.791–0.847) models came in second and third, respectively. The discrimination slopes for the three models were 0.258, 0.223, and 0.240, respectively, and the corresponding accuracy rates were 0.801, 0.770, and 0.762, respectively. The Brier score of gradient boosting tree was the lowest (0.109), followed by the random forest (0.111) and logistic regression (0.112) models. Risk stratification showed that patients in the high-risk group (46.31%) had a greater six-fold chance of early death than those in the low-risk group (7.50%). Conclusion: The gradient boosting tree model demonstrates promising performance with favorable discrimination and calibration in the study, and this model can stratify the risk probability of early death among bone metastatic breast cancer patients. Copyright © 2022 Xiong, Cao, Shi, Long, Liu and Lei."
54,53,64,53_robot_robotics_robotic_robots,"robot,robotics,robotic,robots,planning,autonomous,reinforcement,learning,locomotion,learned","Recent advances in deep reinforcement learning (RL) based techniques combined with training in simulation have offered a new approach to developing robust controllers for legged robots. However, the application of such approaches to real hardware has largely been limited to quadrupedal robots with direct-drive actuators and light-weight bipedal robots with low gear-ratio transmission systems. Application to real, life-sized humanoid robots has been less common arguably due to a large sim2real gap. In this paper, we present an approach for effectively overcoming the sim2real gap issue for humanoid robots arising from inaccurate torque-tracking at the actuator level. Our key idea is to utilize the current feedback from the actuators on the real robot, after training the policy in a simulation environment artificially degraded with poor torque-tracking. Our approach successfully trains a unified, end-to-end policy in simulation that can be deployed on a real HRP-5P humanoid robot to achieve bipedal locomotion. Through ablations, we also show that a feedforward policy architecture combined with targeted dynamics randomization is sufficient for zero-shot sim2real success, thus eliminating the need for computationally expensive, memory-based network architectures. Finally, we validate the robustness of the proposed RL policy by comparing its performance against a conventional model-based controller for walking on uneven terrain with the real robot.  © 2013 IEEE.,In recent years, data-driven learning methods have been widely studied for autonomous robot skill learning. However, these methods rely on large amounts of robot-environment interaction data for training, which largely prevents them from being applied to real-world robots. To address this problem, this article proposes a novel simulation-reality closed-loop learning framework for autonomous robot skill learning that can improve data efficiency, enhance policy stability, and achieve effective policy simulation-to-reality (sim2real) transfer. First, a hybrid control model combining the asymmetric deep deterministic policy gradients (Asym-DDPGs) model and the forward prediction control (FPC) model is proposed to learn vision-based manipulation policies in simulations, which can decompose complex tasks to improve learning efficiency. Second, a novel pixel-level domain adaptation method named Position-CycleGAN is designed to translate real images to simulated images while also preserving the task-related information. The policy trained in simulations can be directly migrated into real robots in a reverse reality-to-simulation manner using the Position-CycleGAN model. The experimental results validate the effectiveness of the proposed framework. This work provides an efficient and feasible path for achieving autonomous skill learning.  © 2016 IEEE.,Mobile robots are playing an increasingly significant role in social life and industrial production, such as searching and rescuing robots, autonomous exploration of sweeping robots, and so on. Improving the accuracy of autonomous navigation of mobile robots is a hot issue to be solved. However, traditional navigation methods are unable to realize crash-free navigation in an environment with dynamic obstacles, more and more scholars are gradually using autonomous navigation based on deep reinforcement learning (DRL) to replace overly conservative traditional methods. But on the other hand, DRL's training time is too long, and the lack of long-term memory easily leads the robot to a dead end, which makes its application in the actual scene more difficult. To shorten training time and prevent mobile robots from getting stuck and spinning around, we design a new robot autonomous navigation framework which combines the traditional global planning and the local planning based on DRL. Therefore, the entire navigation process can be transformed into first using traditional navigation algorithms to find the global path, then searching for several high-value landmarks on the global path, and then using the DRL algorithm to move the mobile robot toward the designated landmarks to complete the final navigation, which makes the robot training difficulty greatly reduced. Furthermore, in order to improve the lack of long-term memory in deep reinforcement learning, we design a feature extraction network containing memory modules to preserve the long-term dependence of input features. Through comparing our methods with traditional navigation methods and reinforcement learning based on end-to-end depth navigation methods, it shows that while the number of dynamic obstacles is large and obstacles are rapidly moving, our proposed method is, on average, 20% better than the second ranked method in navigation efficiency (navigation time and navigation paths' length), 34% better than the second ranked method in safety (collision times), 26.6% higher than the second ranked method in success rate, and shows strong robustness. Copyright © 2023 Wang, Sun, Xie, Bin and Xiao."
55,54,63,54_galaxies_astronomers_astronomy_galactic,"galaxies,astronomers,astronomy,galactic,astronomical,milky,stellar,galaxy,stars,luminosity","Emission-line galaxy classification plays an important role in comprehending the formation and evolution of galaxies. The widely used optical spectral classification method for galaxies is the BPT diagram, which classifies emission-line galaxies on the basis of precise spectral line measurements. Various classical machine learning methods have been utilized to classify galaxy spectra. Deep learning (DL) is more feasible for a huge amount of data, as it can learn patterns autonomously from the original data. This study aims to explore the possibility of applying DL to classify galaxy spectra and improve classification efficiency. A one-dimensional convolutional neural network model called GalSpecNet was constructed to classify emission-line galaxy spectra, which recognizes star-forming, composite, active galactic nucleus (AGN), and normal galaxies with an accuracy of over 93 per cent. This study employs the Gradient-weighted Class Activation Mapping to elucidate the decision-making process of the model by inspecting spectral features that the model prioritizes for each type of galaxy. The findings suggest that the model considers features highly consistent with the conventional BPT method. Subsequently, we applied the model to the cross-matched galaxies of Sloan Digital Sky Survey Data Release 16 (DR16) and Large Sky Area Multi-Object Fiber Spectroscopic Telescope DR8 and present a catalogue comprising of 41 699 star-forming candidates and 55 103 AGN candidates. The catalogue is publicly available. © 2023 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.,Stellar atmospheric parameters (effective temperature, surface gravity, and metallicity) are fundamental for understanding the formation and evolution of stars and galaxies. Photometric data can provide a low-cost way to estimate these parameters, but traditional methods based on photometric magnitudes have many limitations. In this paper, we propose a novel model called Bayesian Convit, which combines an approximate Bayesian framework with a deep-learning method, namely Convit, to derive stellar atmospheric parameters from Sloan Digital Sky Survey images of stars and effectively provide corresponding confidence levels for all the predictions. We achieve high accuracy for T eff and [Fe/H], with ?(T eff) = 172.37 K and ?([Fe/H]) = 0.23 dex. For log g , which is more challenging to estimate from image data, we propose a two-stage approach: (1) classify stars into two categories based on their log g values (>4 dex or <4 dex) and (2) regress separately these two subsets. We improve the estimation accuracy of stars with log g > 4 dex significantly to ? ( log g > 4 ) = 0.052 dex, which are comparable to those based on spectral data. The final joint result is ? ( log g ) = 0.41 dex. Our method can be applied to large photometric surveys like Chinese Space Station Telescope and Large Synoptic Survey Telescope. © 2023. The Author(s). Published by the American Astronomical Society.,Stellar classification is a central topic in astronomical research that relies mostly on the use of spectra. However, with the development of large sky surveys, spectra are becoming increasingly scarce compared to photometric images. Numerous observed stars lack spectral types. In Sloan Digital Sky Survey (SDSS), there are more than hundreds of millions of such stars. In this paper, we propose a convolutional neural network-based stellar classification network (SCNet) in an attempt to solve the stellar classification task from photometric images alone, distinguishing between seven classes, i.e. O, B, A, F, G, K, and M. A total of 46 245 identified stellar objects were collected from the SDSS as the training samples for our network. Compared to many typical classification networks in deep learning, SCNet achieves the best classification accuracy of 0.861. When we allow an error to be within three neighbouring subtypes for SCNet, the accuracy even reaches 0.907. We apply the final SCNet model to 50 245 638 SDSS stars without corresponding spectra and present a new star classification catalogue, containing 7438 O-type stars, 31 433 B-type stars, 201 189 A-type stars, 910 007 F-type stars, 10 986 055 G-type stars, 18 941 155 K-type stars, and 19 168 361 M-type stars. © 2023 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society."
56,55,63,55_recommender_recommendation_recommendations_personalized,"recommender,recommendation,recommendations,personalized,factorization,collaborative,ratings,embeddings,rating,useritem","Graph-based recommender system has attracted widespread attention and produced a series of research results. Because of the powerful high-order connection modeling capabilities of the Graph Neural Network, the performance of these graph-based recommender systems are far superior to those of traditional neural network-based collaborative filtering models. However, from both analytical and empirical perspectives, the apparent performance improvement is accompanied with a significant time overhead, which is noticeable in large-scale graph topologies. More importantly, the intrinsic data-sparsity problem substantially limits the performance of graph-based recommender systems, which compelled us to revisit graph-based recommendation from a novel perspective. In this article, we focus on analyzing the time complexity of graph-based recommender systems to make it more suitable for real large-scale application scenarios. We propose a novel end-to-end graph recommendation model called the Collaborative Variational Graph Auto-Encoder (CVGA), which uses the information propagation and aggregation paradigms to encode user-item collaborative relationships on the user-item interaction bipartite graph. These relationships are utilized to infer the probability distribution of user behavior for parameter estimation rather than learning user or item embeddings. By doing so, we reconstruct the whole user-item interaction graph according to the known probability distribution in a feasible and elegant manner. From the perspective of the graph auto-encoder, we convert the graph recommendation task into a graph generation problem and are able to do it with approximately linear time complexity. Extensive experiments on four real-world benchmark datasets demonstrate that CVGA can be trained at a faster speed while maintaining comparable performance over state-of-the-art baselines for graph-based recommendation tasks. Further analysis shows that CVGA can effectively mitigate the data sparsity problem and performs equally well on large-scale datasets.  © 2023 Association for Computing Machinery.,Deep learning has recently gained a lot of grip in recommender systems. Hybrid recommendation systems, content-based recommendation and Deep learning were all used in multiple ways. Big data has been doing this for nearly 10 years, and the amount of available data on the network will be quickly growing. When challenged with complicated and huge data sets, it's indeed difficult for many people to obtain the necessary data rapidly. At this point, the recommendation system, including its features, is one of the most significant techniques for communicating with the large data overload issue. The development of recommendation algorithms has been aided by the growth of the e-commerce industry in particular. Traditional single recommendation algorithms are plagued by data sparsity, long-tail items and cold start. At this point, hybrid recommendation algorithms can efficiently keep away from some of the flaws of single algorithms. In response to these concerns, this paper proposes an experimental hybrid technique for enhancing the quality of the personalized product recommendation system algorithm that depends on deep learning IA-CNN to give back for a single collaborative model's limitations. To generalize and categorize the output results, first, the system employs a comprehensive approach, fusing productand user-based collaborative filtering strategies. That is the methodology that the algorithm uses. Improved deep learning techniques are then used to capture nonlinear interactions between users and products that are more detailed and abstract. Finally, we devised experiments to test the algorithm's efficacy. Tests on the Amazon product rating dataset are performed against the benchmark algorithm, and the outcomes show that the proposed IA-CNN algorithm outperforms the on the test dataset, the benchmark algorithm was used for rating prediction. © 2023, Ismail Saritas. All rights reserved.,Recommendation systems have become more important since the widespread use of the Internet. The large amount of information means that it is difficult for users to discover what they really need. However, users' preferences can change over time because of the age and the impact of social networks. Recommendation systems must capture users' preferences and recommend suitable items, which is a difficult challenge. This study proposes a novel recommendation system that adapts to the changing preferences of users. By learning and adapting to users' changing preferences better recommendations are provided. This study uses context factors as additional information to model users' preferences more accurately. For the proposed recommendation system, the context is based on users' most recent interaction items. This study proposes two novel attention mechanisms for the recommendation system. The contextual item attention module captures contextual information, changing pattern in users' preference and the importance of items. The multi-head attention module extends the diversity of users' preferences and adapts to changing preferences. The recommendation performance is improved using additional item's temporal information to model the contextual item's representation. Experiments compare the proposed algorithm with several state-of-the-art recommendation methods using three real-world datasets. The experimental results demonstrate that the proposed context-aware recommendation model outperforms traditional methods and demonstrate the effectiveness with which contextual information is captured by the attention mechanism.  © 1989-2012 IEEE."
57,56,62,56_privacy_privacypreserving_federated_private,"privacy,privacypreserving,federated,private,anonymity,secure,encryption,adversary,sharing,verifiable","Federated learning (FL) enables multiple worker devices share local models trained on their private data to collaboratively train a machine learning model. However, local models are proved to imply the information about the private data and, thus, introduce much vulnerabilities to inference attacks where the adversary reconstructs or infers the sensitive information about the private data (e.g., labels, memberships, etc.) from the local models. To address this issue, existing works proposed homomorphic encryption, secure multiparty computation (SMC), and differential privacy methods. Nevertheless, the homomorphic encryption and SMC-based approaches are not applicable to large-scale FL scenarios as they incur substantial additional communication and computation costs and require secure channels to delivery keys. Moreover, differential privacy brings a substantial tradeoff between privacy budget and model performance. In this article, we propose a novel FL framework, which can protect the data privacy of worker devices against the inference attacks with minimal accuracy cost and low computation and communication cost, and does not rely on the secure pairwise communication channels. The main idea is to generate the lightweight keys based on computational Diffie-Hellman (CDH) problem to encrypt the local models, and the FL server can only get the sum of the local models of all worker devices without knowing the exact local model of any specific worker device. The extensive experimental results on three real-world data sets validate that the proposed FL framework can protect the data privacy of worker devices, and only incurs a small constant of computation and communication cost and a drop in test accuracy of no more than 1%.  © 2014 IEEE.,Consumer electronic devices often involve processing and analyzing a large amount of user personal data. Nevertheless, owing to apprehensions regarding privacy and security, users are hesitant to transmit this sensitive data to centralized cloud servers for training. The combination of mobile edge computing and federated learning (FL) enables local devices to access computational power and storage resources, allowing them to engage in distributed learning and model training while safeguarding user privacy. However, these resources are not unlimited. Furthermore, as artificial intelligence technology progresses, inference attacks have become a major threat to privacy in traditional federated learning. To address these challenges, we propose an innovative federated deep learning algorithm, called Fed-PEMC. This algorithm combines local differential privacy and model compression techniques. By leveraging deep reinforcement learning for model compression, Fed-PEMC reduces model size while maintaining model accuracy, improving communication efficiency. We also introduce customized label sampling to accelerate model training. Before uploading the model, we implement local differential privacy protection on the compressed model, reducing privacy budget and addressing privacy leakage caused by inference attacks. Theoretical analysis and experimental results validate that Fed-PEMC adheres to (&#x03F5;, &#x03B4;)-differential privacy and exhibits a communication cost linked to the model size. Experimental results show that compared to baseline algorithms, Fed-PEMC excels in ensuring privacy, maintaining model accuracy, and optimizing communication efficiency, and Fed-PEMC outperforms the baseline solution DP-Fed by 2.27 and 2.02 percentage points in testing accuracy on the Mnist and Cifar10 datasets, respectively. IEEE,There are several unsolved problems in federated learning, such as the security concerns and communication costs associated with it. Differential privacy (DP) offers effective privacy protection by introducing noise to parameters based on rigorous privacy definitions. However, excessive noise addition can potentially compromise the accuracy of the model. Another challenge in federated learning is the issue of high communication costs. Training large-scale federated models can be slow and expensive in terms of communication resources. To address this, various model pruning algorithms have been proposed. To address these challenges, this paper introduces a communication-efficient, privacy-preserving FL algorithm based on two-stage gradient pruning and differentiated differential privacy, named IsmDP-FL. The algorithm leverages a two-stage approach, incorporating gradient pruning and differentiated differential privacy. In the first stage, the trained model is subject to gradient pruning, followed by the addition of differential privacy to the important parameters selected after pruning. Non-important parameters are pruned by a certain ratio, and differentiated differential privacy is applied to the remaining parameters in each network layer. In the second stage, gradient pruning is performed during the upload to the server for aggregation, and the final result is returned to the client to complete the federated learning process. Extensive experiments demonstrate that the proposed method ensures a high communication efficiency, maintains the model privacy, and reduces the unnecessary use of the privacy budget. © 2023 by the authors."
58,57,62,57_diabetes_insulin_glycemic_prediction,"diabetes,insulin,glycemic,prediction,health,glucose,obesity,predict,predictive,cardiovascular","Background: Diabetes mellitus (DM) is a chronic metabolic disease that could produce severe complications threatening life. Its early detection is thus quite important for the timely prevention and treatment. Normally, fasting blood glucose (FBG) by physical examination is used for large-scale screening of DM; however, some people with normal fasting glucose (NFG) actually have suffered from diabetes but are missed by the examination. This study aimed to investigate whether common physical examination indexes for diabetes can be used to identify the diabetes individuals from the populations with NFG. Methods: The physical examination data from over 60,000 individuals with NFG in three Chinese cohorts were used. The diabetes patients were defined by HbA1c ? 48 mmol/mol (6.5%). We constructed the models using multiple machine learning methods, including logistic regression, random forest, deep neural network, and support vector machine, and selected the optimal one on the validation set. A framework using permutation feature importance algorithm was devised to discover the personalized risk factors. Results: The prediction model constructed by logistic regression achieved the best performance with an AUC, sensitivity, and specificity of 0.899, 85.0%, and 81.1% on the validation set and 0.872, 77.9%, and 81.0% on the test set, respectively. Following feature selection, the final classifier only requiring 13 features, named as DRING (diabetes risk of individuals with normal fasting glucose), exhibited reliable performance on two newly recruited independent datasets, with the AUC of 0.964 and 0.899, the balanced accuracy of 84.2% and 81.1%, the sensitivity of 100% and 76.2%, and the specificity of 68.3% and 86.0%, respectively. The feature importance ranking analysis revealed that BMI, age, sex, absolute lymphocyte count, and mean corpuscular volume are important factors for the risk stratification of diabetes. With a case, the framework for identifying personalized risk factors revealed FBG, age, and BMI as significant hazard factors that contribute to an increased incidence of diabetes. DRING webserver is available for ease of application (http://www.cuilab.cn/dring). Conclusions: DRING was demonstrated to perform well on identifying the diabetes individuals among populations with NFG, which could aid in early diagnosis and interventions for those individuals who are most likely missed. © 2023, BioMed Central Ltd., part of Springer Nature.,Diabetes is a complex disease that can lead to serious health complications if left unmanaged. Early detection and treatment of diabetes is crucial, and data analysis and predictive techniques can play a significant role. Data mining techniques, such as classification and prediction models, can be used to analyse various aspects of data related to diabetes, and extract useful information for early detection and prediction of the disease. XGBoost classifier is a machine learning algorithm that effectively predicts diabetes with high accuracy. This algorithm uses a gradient-boosting framework and can handle large and complex datasets with high-dimensional features. However, it is important to note that the choice of the best algorithm for predicting diabetes may depend on the specific characteristics of the data and the research question being addressed. In addition to predicting diabetes, data analysis and predictive techniques can also be used to identify risk factors for diabetes and its complications, monitor disease progression, and evaluate the effectiveness of treatments. These techniques can provide valuable insights into the underlying mechanisms of the disease and help healthcare providers make informed decisions about patient care. Data analysis and predictive techniques have the potential to significantly improve the early detection and management of diabetes, a fast-growing chronic disease that notable health hazards. The XGBoost classifier showed the most effectiveness, with an accuracy rate of 89%. © 2023 by the author.,Objective: Diabetes mellitus is a global epidemic disease. Long-time exposure of patients to hyperglycemia can lead to various type of chronic tissue damage. Early diagnosis of and screening for diabetes are crucial to population health. Methods: We collected the national physical examination data in Xinjiang, China, in 2020 (a total of more than 4 million people). Three types of physical examination indices were analyzed: questionnaire, routine physical examination and laboratory values. Integrated learning, deep learning and logistic regression methods were used to establish a risk model for type-2 diabetes mellitus. In addition, to improve the convenience and flexibility of the model, a diabetes risk score card was established based on logistic regression to assess the risk of the population. Results: An XGBoost-based risk prediction model outperformed the other five risk assessment algorithms. The AUC of the model was 0.9122. Based on the feature importance ranking map, we found that hypertension, fasting blood glucose, age, coronary heart disease, ethnicity, parental diabetes mellitus, triglycerides, waist circumference, total cholesterol, and body mass index were the most important features of the risk prediction model for type-2 diabetes. Conclusions: This study established a diabetes risk assessment model based on multiple ethnicities, a large sample and many indices, and classified the diabetes risk of the population, thus providing a new forecast tool for the screening of patients and providing information on diabetes prevention for healthy populations. © 2023, The Author(s)."
59,58,61,58_agents_agent_multiagent_reinforcement,"agents,agent,multiagent,reinforcement,ai,multirobot,planning,learning,coordination,learn","Many research results have emerged in the past decade regarding multi-agent reinforcement learning. These include the successful application of asynchronous advantage actor-critic, double deep Q-network and other algorithms in multi-agent environments, and the more representative multi-agent training method based on the classical centralized training distributed execution algorithm QMIX. However, in a large-scale multi-agent environment, training becomes a major challenge due to the exponential growth of the state-action space. In this article, we design a training scheme from small-scale multi-agent training to large-scale multi-agent training. We use the transfer learning method to enable the training of large-scale agents to use the knowledge accumulated by training small-scale agents. We achieve policy transfer between tasks with different numbers of agents by designing a new dynamic state representation network, which uses a self-attention mechanism to capture and represent the local observations of agents. The dynamic state representation network makes it possible to expand the policy model from a few agents (4 agents, 10 agents) task to large-scale agents (16 agents, 50 agents) task. Furthermore, we conducted experiments in the famous real-time strategy game Starcraft II and the multi-agent research platform MAgent. And also set unmanned aerial vehicles trajectory planning simulations. Experimental results show that our approach not only reduces the time consumption of a large number of agent training tasks but also improves the final training performance. © The Author(s) 2023.,Cooperative multi-agent reinforcement learning (MARL) has achieved significant results, most notably by leveraging the representation-learning abilities of deep neural networks. However, large centralized approaches quickly become infeasible as the number of agents scale, and fully decentralized approaches can miss important opportunities for information sharing and coordination. Furthermore, not all agents are equal—in some cases, individual agents may not even have the ability to send communication to other agents or explicitly model other agents. This paper considers the case where there is a single, powerful, central agent that can observe the entire observation space, and there are multiple, low-powered local agents that can only receive local observations and are not able to communicate with each other. The central agent’s job is to learn what message needs to be sent to different local agents based on the global observations, not by centrally solving the entire problem and sending action commands, but by determining what additional information an individual agent should receive so that it can make a better decision. In this work, we present our MARL algorithm hammer, describe where it would be most applicable, and implement it in the cooperative navigation and multi-agent walker domains. Empirical results show that (1) learned communication does indeed improve system performance, (2) results generalize to heterogeneous local agents, and (3) results generalize to different reward structures. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Large-scale multiagent reinforcement learning requires huge computation and space costs, and the too-long execution process makes it hard to train policies for agents. This work proposes a concept of fuzzy agent, which is a new paradigm for training homogeneous agents. Aiming at a lightweight and affordable reinforcement learning mechanism for large-scale homogeneous multiagent systems, we break the one-to-one correspondence between agent and policy, designing abstract agents as the substitute for the multiagent to interact with the environment. The Markov decision process models for these abstract agents are conducted by fuzzy logic, which also acts on the behavior mapping from abstract agent to entity. Specifically, just the abstract agents execute their policy at a time step, and the concrete behaviors are generated by simple matrix operations. The proposal has lower space and computation complexities because the number of abstract agents is far less than that of entities, and the coupling among agents is retained implicitly. Compared with other approximation and simplification methods, the proposed fuzzy agent not only greatly reduces required computing resources but also ensures the effectiveness of the learned policies. Several experiments are conducted to validate our method. The results show that the proposal outperforms the baseline methods, while it has satisfactory zero-shot and few-shot transfer abilities.  © 1993-2012 IEEE."
60,59,61,59_controller_adaptive_controllers_control,"controller,adaptive,controllers,control,maneuvers,reinforcement,optimization,autonomous,dynamic,trajectory","Complex neural network structures may not be appropriate for adaptive control applications, as large number of variable adaptive parameters have to be updated in each time step, thus increasing the computational burden, which makes the real-time implementation of the controller difficult. In this study, the ability of the neural networks to approximate non-linear dynamic systems is used to derive a neuro-based adaptive control with minimalistic architecture. A linear neural identifier is devised, which emulates a local linear model of the system by online adjustment of its parameters. Stability and optimal rate of convergence is ensured through an adaptive learning rate, determined using Lyapunov stability theorems. The novelty of the control scheme lies in its minimalistic neural structure comprising of a single-linear neuron and, therefore, does not impose excessive computational burden on the system, making it feasible for real-time application. To assert our claims, benchmark examples from different domains are used to illustrate the effectiveness of the proposed controller. The neuro-controller is used in a water-lift plant to control the height of the water in a storage tank. Another example of a moving cart holding an inverted pendulum, an inherently unstable system that forms the basis of the robot-control mechanism, is also used. The controller is also tested on a complex non-linear higher-order power system to enhance stability by effectively damping the electromechanical oscillations. The superior performance of the controller is demonstrated by comparing with other recently reported controllers. Additional advantages of the proposed scheme include model-free control and requirement of only local measurements. The proposed method has potential applications in control problems that require adaptability, computational simplicity, and quick response. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,This paper mainly focuses on the development of a learning-based controller for a class of uncertain mechanical systems modeled by the Euler-Lagrange formulation. The considered system can depict the behavior of a large class of engineering systems, such as vehicular systems, robot manipulators and satellites. All these systems are often characterized by highly nonlinear characteristics, heavy modeling uncertainties and unknown perturbations, therefore, accurate-model-based nonlinear control approaches become unavailable. Motivated by the challenge, a reinforcement learning (RL) adaptive control methodology based on the actor-critic framework is investigated to compensate the uncertain mechanical dynamics. The approximation inaccuracies caused by RL and the exogenous unknown disturbances are circumvented via a continuous robust integral of the sign of the error (RISE) control approach. Different from a classical RISE control law, a tanh(·) function is utilized instead of a sign(·) function to acquire a more smooth control signal. The developed controller requires very little prior knowledge of the dynamic model, is robust to unknown dynamics and exogenous disturbances, and can achieve asymptotic output tracking. Eventually, co-simulations through ADAMS and MATLAB/Simulink on a three degrees-of-freedom (3-DOF) manipulator and experiments on a real-time electromechanical servo system are performed to verify the performance of the proposed approach. © 2023 China Ordnance Society,This paper investigates the robust optimal control problem of a class of continuous-time, partially linear, interconnected systems. In addition to the dynamic uncertainties resulted from the interconnected dynamic system, unknown bounded disturbances are taken into account throughout the learning process, wherein the system&#x2019;s dynamics and the disturbances are assumed unknown. These challenges lead the collected online data to be imperfect. In this scenario, traditional data-driven control techniques, such as adaptive dynamic programming (ADP) and robust ADP, encounter a challenge in approximating the optimal control policy precisely due to imperfect data and computational errors. In this paper, a novel data-driven robust policy iteration method is proposed to simultaneously solve the robust optimal control problems. Without relying on the knowledge of the system&#x2019;s dynamics, the external disturbances or the complete state, the implementation of the proposed method only needs to access the input and partial state information. Based on the small-gain theorem, the notions of strong unboundedness observability and input-to-output stability, it is guaranteed that the learned robust optimal control gain is stabilizing and that the solution of the closed-loop system is uniformly ultimately bounded despite the existence of dynamic uncertainties and unknown external disturbances. The simulation results reveal the efficiency and practicality of the proposed data-driven control method. <italic>Note to Practitioners</italic>&#x2014;This work is motivated by the use of reinforcement learning to improve the quality of designing adaptive optimal controllers for engineering applications. Adaptive dynamic programming methods, in particular policy iteration (PI), are widely used in solving optimal control problems. However, due to the iterative nature of PI, the approximated optimal control policy may be inaccurate and imprecise. Especially, when using imperfect system&#x2019;s measurements instead of the modelling information. This can result in causing the learned control policy to deviate from the actual optimal policy. This becomes more challenging in the existence of dynamic uncertainties and unknown external disturbances which corrupt the measurements and result in imperfect data. This work investigates the conditions on the uncertainties such that the proposed novel data-driven PI algorithm is robust to system&#x2019;s uncertainties, unknown external disturbances and imperfect measurements. The approximated robust optimal control policy performs robustly in the existences of imperfect data and uncertainties, and at the same time is close enough to the optimal control policy. IEEE"
61,60,60,60_recognition_facial_faces_affectnet,"recognition,facial,faces,affectnet,face,classifier,dcnn,features,feature,trained","The ability to recognize facial expressions using computer vision is a crucial task that has numerous potential applications. Although deep neural networks have achieved high performance, their use in the recognition of facial expressions is still challenging. This is because different facial expressions have varying degrees of similarities among themselves, and numerous variations cause diversity in the same facial images. In this study, we propose a novel divide-and-conquer-based learning strategy to improve the performance of facial expression recognition (FER). The face area in an image was detected using MobileNet, and a ResNet-18 model was employed as a backbone deep neural network for recognizing facial expressions. Subsequently, groups containing similar facial expressions were categorized by analyzing the confusion matrix, which represents the inference results of the trained ResNet-18 model, and these similar facial expression groups were then utilized to re-train the deep learning model. In the experiments, the proposed method was evaluated using two thermal (Tufts and RWTH) and two RGB (RAF and FER2013) datasets. The results demonstrate improved FER performance, with an accuracy of 97.75% for Tufts, 86.11% for RWTH, 90.81% for RAF, and 77.83% for FER2013. As such, the proposed method can accurately classify large amounts of facial expression data.  © 2013 IEEE.,To better apply deep convolutional neural networks for expression recognition in UAV-enabled B5G/6G networks, we propose a deep network expression recognition method based on transfer learning and fine-tuning on facial expression datasets. Initially, we train our model on a large-scale facial attribute dataset and subsequently fine-tune it on a facial expression dataset. This strategy effectively lowers the high costs and dependence associated with annotating facial expression datasets, while enhancing the accuracy and training speed of our model. This method is particularly suited for UAVs equipped with on-board cameras and image processing capabilities, enabling real-time expression recognition for various applications such as crowd monitoring, search and rescue, and human-UAV interaction. Compared to traditional methods that train exclusively on facial expression datasets, our method significantly reduces the number of training iterations on facial expression datasets and significantly improves the generalization ability of the model, especially for UAV applications. We use a large-scale facial attribute dataset, which is more closely related to the facial expression recognition task, as our source dataset, forming a contrast with methods that typically use a facial recognition dataset as the transfer learning source dataset. The experimental results on the CK + dataset, integrated with UAV-enabled B5G/6G networks, show that our method achieves a facial expression recognition accuracy of 97.6%, a significant improvement over the 97.3% accuracy rate of methods that only train on facial expression datasets, with less training time as well. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The generation of a large human-labelled facial expression dataset is challenging due to ambiguity in labelling the facial expression class, and annotation cost. However, facial expression recognition (FER) systems demand discriminative feature representation, and require many training samples to establish stronger decision boundaries. Recently, FER approaches have used data augmentation techniques to increase the number of training samples for model generation. However, these augmented samples are derived from existing training data, and therefore have limitations for developing an accurate FER system. To achieve meaningful facial expression representations, we introduce an augmentation technique based on deep learning and genetic algorithms for FER. The proposed approach exploits the hypothesis that augmenting the feature-set is better than augmenting the visual data for FER. By evaluating this relationship, we discovered that the genetic evolution of discriminative features for facial expression is significant in developing a robust FER approach. In this approach, facial expression samples are generated from RGB visual data from videos considering human face frames as regions of interest. The face detected frames are further processed to extract key-frames within particular intervals. Later, these key-frames are convolved through a deep convolutional network for feature generation. A genetic algorithm’s fitness function is gauged to select optimal genetically evolved deep facial expression receptive fields to represent virtual facial expressions. The extended facial expression information is evaluated through an extreme learning machine classifier. The proposed technique has been evaluated on five diverse datasets i.e. JAFFE, CK+, FER2013, AffectNet and our application-specific Instructor Facial Expression (IFEV) dataset. Experimentation results and analysis show the promising accuracy and significance of the proposed technique on all these datasets. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
62,61,59,61_buildings_building_lidar_imagery,"buildings,building,lidar,imagery,heights,sensing,roof,images,architectural,3d","Accurate building extraction for high-resolution remote sensing images is critical for topographic mapping, urban planning, and many other applications. Its main task is to label each pixel point as a building or non-building. Although deep-learning-based algorithms have significantly enhanced the accuracy of building extraction, fully automated methods for building extraction are limited by the requirement for a large number of annotated samples, resulting in a limited generalization ability, easy misclassification in complex remote sensing images, and higher costs due to the need for a large number of annotated samples. To address these challenges, this paper proposes an improved interactive building extraction model, ARE-Net, which adopts a deep interactive segmentation approach. In this paper, we present several key contributions. Firstly, an adaptive-radius encoding (ARE) module was designed to optimize the interaction features of clicks based on the varying shapes and distributions of buildings to provide maximum a priori information for building extraction. Secondly, a two-stage training strategy was proposed to enhance the convergence speed and efficiency of the segmentation process. Finally, some comprehensive experiments using two models of different sizes (HRNet18s+OCR and HRNet32+OCR) were conducted on the Inria and WHU building datasets. The results showed significant improvements over the current state-of-the-art method in terms of (Formula presented.). The proposed method achieved performance enhancements of 7.98% and 13.03% with HRNet18s+OCR and 7.34% and 15.49% with HRNet32+OCR on the WHU and Inria datasets, respectively. Furthermore, the experiments demonstrated that the proposed ARE-Net method significantly reduced the annotation costs while improving the convergence speed and generalization performance. © 2023 by the authors.,Building heights are one of the crucial data for comprehending the functions of urban systems. Employing optical remote sensing imagery, the shadow-based method is one of the most promising methods which have been proposed for estimating building height. However, the existing shadow-based studies for building height estimation are restricted to a small area due to the lack of building height annotations and ignorance of building azimuth variations. The Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) allows large-scale building height retrieval in the along-track direction and thus can be taken as ground truth building height annotations to support the shadow-based algorithms for large-scale building height extraction. Here, we proposed an approach for extracting building height by combining Google Earth Satellite (GES) images and ICESat-2 photons. Building and shadow instances were first extracted using a U-Net deep learning framework. Based on the building height annotations retrieved from ICESat-2 photons, an improved shadow-based building height estimation model by minimizing the global error across all sample buildings was developed. A typical urban area located in the city center of Shanghai, China with an area of around 90 km2 was selected to validate the proposed method. In total 15,966 buildings were successfully extracted and the results indicated that the estimated building heights have high accuracy with an absolute mean error of 4.08 m. Moreover, the proposed method shows a better performance compared to the existing shadow-based method and existing building height datasets. The method holds great potential for large-scale building-level height retrieval which contributes to further studies of urban morphologies. © 2023 The Author(s),Building extraction from very high-resolution remote sensing images is a fundamental task and is widely used in applications, such as change detection, disaster assessment, and real-time update of geographic information databases. However, due to the complexity of the geographical environment and the diversity of target features, accurate automatic building extraction remains very challenging. With the fast development of deep learning techniques, convolutional neural networks (CNN) have been widely used in remote sensing research and have achieved considerable results. But for large urban area-based building detection tasks, the CNN-based method usually gets into local optima and generates many false positive detections around building boundaries. To avoid the local optima and be aware of nonlocal information, this article proposes a hybrid feature extraction model based on the combination of the CNN and Transformer to realize the automatic building detection from very high-resolution remote sensing images. Meanwhile, a multiconstraint weighting mechanism is proposed to enhance the ability of the model to recognize the regular geometric boundaries of buildings. Comprehensive experiments are conducted on the three different datasets. The proposed MC-TRANSU achieves the best F1-score and intersection over union, compared with the state-of-the-art methods, such as SegNet, TransUnet, and Swin-Unet, and the detection accuracy improved around 5%. Quantitative and qualitative results verify the superiority and effectiveness of our model.  © 2008-2012 IEEE."
63,62,59,62_reidentification_supervised_discriminative_pose,"reidentification,supervised,discriminative,pose,normalization,person,feature,identity,cameras,datasets","Person Re-identification (P-ReID) task searches for true matches of a given query from a large repository of non-overlapping camera’s images/videos. In smart cities surveillance, P-ReID is challenging due to variation in human’s appearance, illumination affects, and difference in viewpoints. The mainstream approaches achieve P-ReID by implementing supervised learning strategies, requiring exhaustive manual annotation, which is probably erroneous due to human involvement. In addition, the employed methods use high-dimensional feature maps to identify a person, which is not a realistic approach in terms of storage resources and computational complexity. To tackle these issues, we incorporate learned features and deep autoencoder under the umbrella of a unified framework for P-ReID. First, we apply a unique image patching strategy by dividing the input image into two parts (upper and lower) and acquire learned features from fully connected layer of a pretrained Convolutional Neural Network (CNN) model for both patches. To achieve efficient and high performance, the proposed framework utilizes a self-tuned autoencoder to acquire low-dimensional representative features. The obtained features are matched with the patterns of database via cosine similarity measurement to re-identify a person’s appearance. The proposed framework provides a trade-off between time complexity and accuracy, where a lightweight model can be incorporated with reduced number of autoencoder layers to obtain fast and comparatively flexible results. The major novelty of the proposed framework includes implementation of a hybrid network mechanism for P-ReID, which shows convincing real-time results and best fits for smart cities surveillance. The proposed framework is tested over several P-ReID datasets to prove its influence over the existing works with reduced computational complexity. © 2021, Springer Science+Business Media, LLC, part of Springer Nature.,Person re-identification (ReID) aims at identifying the same person’s images across different cameras. However large domain gaps between source and target domains, as well as lack of label information in the target domain poses a huge challenge for unsupervised domain adaptive the ReID model. This paper tackles the challenge through three aspects : (1) we design a robust visual-spatiotemporal fusion model, which improves the quality of pseudo labels based on visual probability evaluation, spatiotemporal probability evaluation and visual-spatiotemporal fusion. (2) we propose a novel sampling strategy for deep mutual information estimation and maximization algorithm (DIM), which employs data augmentation and dynamic storage stack to improve the reliability of the selected samples. (3) We combine the DIM loss and other supervised losses together to construct a new multi-objective loss function and present a corresponding dynamic adjustment strategy for the weights of loss functions, which contribute to stable the convergence of the training process. As shown in experimental results, our model has achieved excellent results on two ReID datasets, Market-1501 and DukeMTMC-ReID. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons could have the same attribute, and persons' appearances look different, e.g., with viewpoint changes. Recent reID methods focus on learning person features discriminative only for a particular factor of variations (e.g., human pose), which also requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to factorize person images into identity-related and -unrelated features. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose). To this end, we propose a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN). It disentangles identity-related and -unrelated features from person images through an identity-shuffling technique that exploits identification labels alone without any auxiliary supervisory signals. We restrict the distribution of identity-unrelated features, or encourage the identity-related and -unrelated features to be uncorrelated, facilitating the disentanglement process. Experimental results validate the effectiveness of IS-GAN, showing state-of-the-art performance on standard reID benchmarks, including Market-1501, CUHK03 and DukeMTMC-reID. We further demonstrate the advantages of disentangling person representations on a long-term reID task, setting a new state of the art on a Celeb-reID dataset. Our code and models are available online: https://cvlab-yonsei.github.io/projects/ISGAN/.  © 1979-2012 IEEE."
64,63,58,63_materials_prediction_semiconductors_thermoelectrics,"materials,prediction,semiconductors,thermoelectrics,conductivity,thermal,thermoelectric,ceramics,semiconductor,ferroelectrics","Progress in the application of machine learning (ML) methods to materials design is hindered by the lack of understanding of the reliability of ML predictions, in particular, for the application of ML to small data sets often found in materials science. Using ML prediction for transparent conductor oxide formation energy and band gap, dilute solute diffusion, and perovskite formation energy, band gap, and lattice parameter as examples, we demonstrate that (1) construction of a convex hull in feature space that encloses accurately predicted systems can be used to identify regions in feature space for which ML predictions are highly reliable; (2) analysis of the systems enclosed by the convex hull can be used to extract physical understanding; and (3) materials that satisfy all well-known chemical and physical principles that make a material physically reasonable are likely to be similar and show strong relationships between the properties of interest and the standard features used in ML. We also show that similar to the composition-structure-property relationships, inclusion in the ML training data set of materials from classes with different chemical properties will not be beneficial for the accuracy of ML prediction and that reliable results likely will be obtained by ML model for narrow classes of similar materials even in the case where the ML model will show large errors on the data set consisting of several classes of materials. © 2023 American Chemical Society.,Double half-Heusler alloys are promising materials for applications as magnetocaloric materials, topological insulators, but especially thermoelectric materials. Four different elements in their composition provide a wide range of possible compositions, which, on the other hand, is difficult to study directly by applying traditional first-principles approaches to large number of compositions. In this work, based on the gradient boosting method, regression models are constructed that allow rapid prediction of the lattice thermal conductivity, as well as a number of other thermal and elastic properties, based on the composition and crystal structure of a compound. This made it possible for the first time to calculate the lattice thermal conductivity, as well as Grüneisen parameter, Debye temperature, and elastic moduli for a number of double half-Heusler compounds. We observe that the predicted thermal conductivity is in better agreement with the experimental data than the results of density functional theory calculations available in the literature. Half-Heusler compounds with thermal conductivity values lower than those previously known have been found. In addition, we have analyzed the importance of various features for predicting each of the studied properties, and the effect of the crystallographic symmetry of the compound on the prediction accuracy. © 2023 Elsevier B.V.,High-throughput screening and material informatics have shown a great power in the discovery of novel materials, including batteries, high entropy alloys, and photocatalysts. However, the lattice thermal conductivity (?) oriented high-throughput screening of advanced thermal materials is still limited to the intensive use of first principles calculations, which is inapplicable to fast, robust, and large-scale material screening due to the unbearable computational cost demanding. In this study, 15 machine learning algorithms are utilized for fast and accurate ? prediction from basic physical and chemical properties of materials. The well-trained models successfully capture the inherent correlation between these fundamental material properties and ? for different types of materials. Moreover, deep learning combined with a semi-supervised technique shows the capability of accurately predicting diverse ? values spanning 4 orders of magnitude, especially the power of extrapolative prediction on 3716 new materials. The developed models provide a powerful tool for large-scale advanced thermal functional materials screening with targeted thermal transport properties. © 2023 The Royal Society of Chemistry."
65,64,58,64_saliency_salient_attention_convolutional,"saliency,salient,attention,convolutional,vision,supervised,detection,features,feature,segmentation","Weakly supervised salient object detection (SOD) is a challenging task and has drawn much attention from several research perspectives, it has revealed two problems while driving the rapid development of saliency detection. (1) Large divergence in the characteristics of saliency regions in terms of location, shape and size makes them difficult to recognize. (2) The properties of convolutional neural networks dictate that it is insensitive to various transformations, which will lead to hardly balance the application of various disturbances. To tackle these limitations, this paper proposes a novel seminar learning framework with consistent transformation ensembling (SLF-CT) for scribble supervised SOD. The framework consists of the teacher–student model and the student–student model for segmenting the salient objects. Specifically, we first design a cross attention guided network (CAGNet) as a baseline model for saliency prediction. Then we assign CAGNet to the teacher–student model, where the teacher network is based on the exponential moving average and guides the training of the student network. Moreover, we adopt multiple pseudo labels to transfer the information among students from different conditions. To further enhance the regularization of the network, a consistency transformation mechanism is also incorporated, which encourages the saliency prediction and input image of the network to be consistent. The experimental results demonstrate that the proposed approach performs favorably comparable with the state-of-the-art weakly supervised methods. As far as we know, the proposed approach is the first application of seminar learning in the SOD area. © 2023,The human visual system has limited capacity in simultaneously processing multiple visual inputs. Consequently, humans rely on shifting their attention from one location to another. When viewing an image of complex scenes, psychology studies and behavioural observations show that humans prioritise and sequentially shift attention among multiple visual stimuli. In this paper, we propose to predict the saliency rank of multiple objects by inferring human attention shift. We first construct a new large-scale salient object ranking dataset, with the saliency rank of objects defined by the order that an observer attends to these objects via attention shift. We then propose a new deep learning-based model to leverage both bottom-up and top-down attention mechanisms for saliency rank prediction. Our model includes three novel modules: Spatial Mask Module (SMM), Selective Attention Module (SAM) and Salient Instance Edge Module (SIEM). SMM integrates bottom-up and semantic object properties to enhance contextual object features, from which SAM learns the dependencies between object features and image features for saliency reasoning. SIEM is designed to improve segmentation of salient objects, which helps further improve their rank predictions. Experimental results show that our proposed network achieves state-of-the-art performances on the salient object ranking task across multiple datasets. Code and data are available at https://github.com/SirisAvishek/Attention_Shift_Ranks . © 2023, The Author(s).,Co-saliency detection targets at segmenting the common salient objects in a group of relevant images. The current co-salient object detection methods based on deep learning have two limitations: (1) There is only a single target in training images, which can not provide adversarial samples for the model, making the model have poor generalization performance. When facing the interference of unknown class targets, similar salient objects, noisy background environments and so on, the model is greatly limited; (2) The existing methods usually use convolution neural networks (CNNs) to extract features. However, the CNNs can not obtain a large receptive field which makes the model unable to fully model the long-range dependencies, resulting in poor discriminative capability of the model. To this end, we propose a co-saliency detection transformer guided by intra-group adversarial mixup. Aiming at building the co-saliency detection network from a perspective of sequence-to-sequencc and training the model on mixup adversarial data, making the model more generic. Our network mainly contains two parts, a mixup subnetwork and a co-saliency detection transformer. Specifically, in the mixup sub-network, we propose an object refinement module: we set input class activation maps(CAMs) as guidance to segment salient objects with smooth edges as the adversarial objects in an unsupervised way; a distance adjusting module: the adversarial objects are mixed into another group of images with the minimum overlap, constructing the mixed training data. In the co-saliency detection transformer, we construct the model from sequence-to-sequence. In this part, we design a task injector, which can inject group information and saliency information into the feature sequence, and we adopt self-attention to fully capture global information between features. Finally, we mix the group information and saliency information by self-attention, further enhancing the discriminative capability of the feature and generating the Precise results of co-saliency detection. Extensive experiments are carried out on three benchmark datasets including Cosal2015, CoCA, and CoS()D3k, demonstrating superiority of our method to state-of-the-art methods. © 2023 Science Press. All rights reserved."
66,65,58,65_battery_batteries_lithium_lithiumion,"battery,batteries,lithium,lithiumion,prediction,lstm,electrochemical,learning,charging,predict","Due to the large share of production costs, the lifespan of an electric vehicle's (EV) lithium-ion traction battery should be as long as possible. The optimisation of the EV's operating strategy with regard to battery life requires a regular evaluation of the battery's state-of-health (SOH). Yet the SOH, the remaining battery capacity, cannot be measured directly through sensors but requires the elaborate conduction of special characterisation tests. Considering the limited number of test facilities as well as the rapidly growing number of EVs, time-efficient and scalable SOH estimation methods are urgently needed and are the object of investigation in this work. The developed virtual SOH experiment originates from the incremental capacity measurement and solely relies on the commonly logged battery management system (BMS) signals to train the digital battery twins. The first examined dataset with identical load profiles for new and aged battery state serves as proof of concept. The successful SOH estimation based on the second dataset that consists of varying load profiles with increased complexity constitutes a step towards the application on real driving cycles. Assuming that the load cycles contain pauses and start from the fully charged battery state, the SOH estimation succeeds either through a steady shift of the load sequences (variant one) with an average deviation of 0.36% or by random alignment of the dataset's subsequences (variant two) with 1.04%. In contrast to continuous capacity tests, the presented framework does not impose restrictions to small currents. It is entirely independent of the prevailing and unknown ageing condition due to the application of battery models based on the novel encoder–decoder architecture and thus provides the cornerstone for a scalable and robust estimation of battery capacity on a pure data basis. © 2022 Elsevier Ltd,With the continuous concern on the safety of battery systems, accurate and rapid assessment of battery degradation is essential for practical applications. In this paper, a transferable attention network model based on deep learning is developed to evaluate battery degradation, which can simultaneously predict state of health (SOH) and remaining useful life (RUL) for lithium-ion batteries. First, degradation patterns of the cells are identified by K-means clustering based on the difference of the cells at their early cycles. Secondly, the attention mechanisms are designed to suppress noises in extracted feature maps and trace the interaction between long- and short-term degradation modes. Thirdly, the common knowledge represented by the reference cells and the unique degradation features of the target cell are fused by transfer learning, then SOH and RUL prediction are realized through multi-task learning. Finally, a large-scale battery dataset containing different cycle conditions is used to verify the accuracy and generalization of the developed method. The results show that the developed method achieves accurate SOH and RUL prediction with the average root mean square error within 0.14% and six cycles, respectively. © 2023 Elsevier Ltd,State-of-health (SOH) of lithium-ion batteries plays a vital role in the safe and reliable operation of electric vehicles. However, most of the existing SOH estimation methods still require a large number of battery aging data while the established model usually lacks generalization. Here, we build a data-driven transfer learning model to obtain more generality on the SOH estimation. First, potential health features are extracted from battery charging data and then pruned via the importance function. Second, support vector regression (SVR) is employed to establish source models with different battery data, which takes selected features as the input and capacity as the output. Third, the transfer-stacking (TS) method is utilized to combine all source models. A TS-SVR method for SOH estimation is then established only using the first 30% of target battery data after solving the optimization problem of assigning weight to each source model. Finally, the proposed algorithm is verified by three different battery datasets and shows better estimation performance than the comparative algorithms. It is proved that the proposed method uses only a small amount of target battery data while together with the source battery data can achieve an accurate SOH estimation during its life cycle.  © 1982-2012 IEEE."
67,66,58,66_seismic_cnn_neural_deep,"seismic,cnn,neural,deep,inversion,geologic,denoising,geological,learning,wavefield","Recently, seismic inversion has made extensive use of supervised learning methods. The traditional deep learning inversion network can utilize the temporal correlation in the vertical direction. Still, it does not consider the spatial correlation in the horizontal direction of seismic data. Each seismic trace is inverted independently, which leads to noise and large geological variations in seismic data, thus leading to lateral discontinuity. Given this, the proposed method uses the spatial correlation of the seismic data in the horizontal direction. In the network training stage, several seismic traces centered on the well-side trace and the corresponding logging curve form a set of training sample pairs for training, to enhance the lateral continuity and anti-noise performance. Additionally, Attention U-Net is introduced in acoustic impedance inversion. Attention U-Net adds attention gate (AG) model to the skip connection between the encoding and decoding layers of the U-Net network, which can give different weights to different features, so the model can focus on the features related to the inversion task and avoid the influence of irrelevant data and noise during the inversion process. The performance of the proposed method is evaluated using the Marmousi2 model and the SEAM model and compared with other methods. The experimental results show that the proposed method has the advantages of high accuracy of acoustic impedance value inversion, good transverse continuity of inversion results, and strong anti-noise performance. Copyright © 2023 Ning, Li, Wei and Yang.,Seismic inversion allows the prediction of subsurface properties from seismic reflection data and is a key step in reservoir modeling and characterization. With the generalization of machine learning in geophysics, deep learning methods have been proposed as efficient seismic inversion methods. However, most of these methods lack a probabilistic approach to deal with the uncertainties inherent in the seismic inversion problem and/or rely on complete and representative training data, which often is partially or scarcely available. We have explored the ability of deep convolutional neural networks to extract meaningful and complex representations from spatially structured data, combined with geostatistical simulation, to efficiently invert poststack seismic data directly for high-resolution models of acoustic impedance. Our model incorporates physics constraints and sparse direct measurements while leveraging the use of imprecise but widely distributed indirect measurements as represented by the seismic data. The models generated with geostatistical simulation provide additional information with higher spatial resolution than the original seismic data and allow assessing uncertainty in the model predictions by generating multiple realizations of the subsurface properties. Our method can (1) provide an uncertainty assessment of the predictions, (2) model the complex and nonlinear relationship between data and model, (3) extend the seismic bandwidth at low and high ends of the frequency parameters spectrum, and (4) lessen the need for large, annotated training data. Our method is applied to a 1D synthetic example and a real 3D application example from a Brazilian reservoir. The predicted impedance models are compared with those obtained from a full iterative geostatistical seismic inversion. Our method allows retrieving similar models but has the advantage of generating alternative solutions in greater numbers, providing a larger exploration of the model parameter space in less time than the iterative geostatistical seismic inversion. © 2023 Society of Exploration Geophysicists. All rights reserved.,We propose to use a Few-Shot Learning (FSL) method for the pre-stack seismic inversion problem in obtaining a high resolution reservoir model from recorded seismic data. Recently, artificial neural network (ANN) demonstrates great advantages for seismic inversion because of its powerful feature extraction and parameter learning ability. Hence, ANN method could provide a high resolution inversion result that are critical for reservoir characterization. However, the ANN approach requires plenty of labeled samples for training in order to obtain a satisfactory result. For the common problem of scarce samples in the ANN seismic inversion, we create a novel pre-stack seismic inversion method that takes advantage of the FSL. The results of conventional inversion are used as the auxiliary dataset for ANN based on FSL, while the well log is regarded the scarce training dataset. According to the characteristics of seismic inversion (large amount and high dimensional), we construct an arch network (A-Net) architecture to implement this method. An example shows that this method can improve the accuracy and resolution of inversion results. © 2022 The Authors"
68,67,57,67_federated_dataset_datasets_learning,"federated,dataset,datasets,learning,privacy,distributed,sharing,collaborative,fedsurf,biomedical","Federated learning (FL), a relatively new area of research in medical image analysis, enables collaborative learning of a federated deep learning model without sharing the data of participating clients. In this paper, we propose FedDropoutAvg, a new federated learning approach for detection of tumor in images of colon tissue slides. The proposed method leverages the power of dropout, a commonly employed scheme to avoid overfitting in neural networks, in both client selection and federated averaging processes. We examine FedDropoutAvg against other FL benchmark algorithms for two different image classification tasks using a publicly available multi-site histopathology image dataset. We train and test the proposed model on a large dataset consisting of 1.2 million image tiles from 21 different sites. For testing the generalization of all models, we select held-out test sets from sites that were not used during training. We show that the proposed approach outperforms other FL methods and reduces the performance gap (to less than 3% in terms of AUC on independent test sites) between FL and a central deep learning model that requires all data to be shared for centralized training, demonstrating the potential of the proposed FedDropoutAvg model to be more generalizable than other state-of-the-art federated models. To the best of our knowledge, ours is the first study to effectively utilize the dropout strategy in a federated setting for tumor detection in histology images. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,With the advent of the Internet of Things (IoT), Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) algorithms, the landscape of data-driven medical applications has emerged as a promising avenue for designing robust and scalable diagnostic and prognostic models from medical data. This has gained a lot of attention from both academia and industry, leading to significant improvements in healthcare quality. However, the adoption of AI-driven medical applications still faces tough challenges, including meeting security, privacy, and quality of service (QoS) standards. Recent developments in Federated Learning (FL) have made it possible to train complex machine-learned models in a distributed manner and has become an active research domain, particularly processing the medical data at the edge of the network in a decentralized way to preserve privacy and address security concerns. To this end, in this paper, we explore the present and future of FL technology in medical applications where data sharing is a significant challenge. We delve into the current research trends and their outcomes, unravelling the complexities of designing reliable and scalable FL models. Our paper outlines the fundamental statistical issues in FL, tackles device-related problems, addresses security challenges, and navigates the complexity of privacy concerns, all while highlighting its transformative potential in the medical field. Our study primarily focuses on medical applications of FL, particularly in the context of global cancer diagnosis. We highlight the potential of FL to enable computer-aided diagnosis tools that address this challenge with greater effectiveness than traditional data-driven methods. Recent literature has shown that FL models are robust and generalize well to new data, which is essential for medical applications. We hope that this comprehensive review will serve as a checkpoint for the field, summarizing the current state-of-the-art and identifying open problems and future research directions. Authors,Brain tumor segmentation in medical imaging is a critical task for diagnosis and treatment while preserving patient data privacy and security. Traditional centralized approaches often encounter obstacles in data sharing due to privacy regulations and security concerns, hindering the development of advanced AI-based medical imaging applications. To overcome these challenges, this study proposes the utilization of federated learning. The proposed framework enables collaborative learning by training the segmentation model on distributed data from multiple medical institutions without sharing raw data. Leveraging the U-Net-based model architecture, renowned for its exceptional performance in semantic segmentation tasks, this study emphasizes the scalability of the proposed approach for large-scale deployment in medical imaging applications. The experimental results showcase the remarkable effectiveness of federated learning, significantly improving specificity to 0.96 and the dice coefficient to 0.89 with the increase in clients from 50 to 100. Furthermore, the proposed approach outperforms existing convolutional neural network (CNN)- and recurrent neural network (RNN)-based methods, achieving higher accuracy, enhanced performance, and increased efficiency. The findings of this research contribute to advancing the field of medical image segmentation while upholding data privacy and security. © 2023 by the authors."
69,68,57,68_topics_topic_corpus_texts,"topics,topic,corpus,texts,topical,summarization,dirichlet,keywords,citations,classification","Background: Topic models are a class of unsupervised machine learning models, which facilitate summarization, browsing and retrieval from large unstructured document collections. This study reviews several methods for assessing the quality of unsupervised topic models estimated using non-negative matrix factorization. Techniques for topic model validation have been developed across disparate fields. We synthesize this literature, discuss the advantages and disadvantages of different techniques for topic model validation, and illustrate their usefulness for guiding model selection on a large clinical text corpus. Design, setting and data: Using a retrospective cohort design, we curated a text corpus containing 382,666 clinical notes collected between 01/01/2017 through 12/31/2020 from primary care electronic medical records in Toronto Canada. Methods: Several topic model quality metrics have been proposed to assess different aspects of model fit. We explored the following metrics: reconstruction error, topic coherence, rank biased overlap, Kendall’s weighted tau, partition coefficient, partition entropy and the Xie-Beni statistic. Depending on context, cross-validation and/or bootstrap stability analysis were used to estimate these metrics on our corpus. Results: Cross-validated reconstruction error favored large topic models (K ? 100 topics) on our corpus. Stability analysis using topic coherence and the Xie-Beni statistic also favored large models (K = 100 topics). Rank biased overlap and Kendall’s weighted tau favored small models (K = 5 topics). Few model evaluation metrics suggested mid-sized topic models (25 ? K ? 75) as being optimal. However, human judgement suggested that mid-sized topic models produced expressive low-dimensional summarizations of the corpus. Conclusions: Topic model quality indices are transparent quantitative tools for guiding model selection and evaluation. Our empirical illustration demonstrated that different topic model quality indices favor models of different complexity; and may not select models aligning with human judgment. This suggests that different metrics capture different aspects of model goodness of fit. A combination of topic model quality indices, coupled with human validation, may be useful in appraising unsupervised topic models. © 2023, The Author(s).,Topic discovery involves identifying the main ideas within large volumes of textual data. It indicates recurring topics in documents, providing an overview of the text. Current topic discovery models receive the text, with or without pre-processing, including stop word removal, text cleaning, and normalization (lowercase conversion). A topic discovery process that receives general domain text with or without processing generates general topics. General topics do not offer detailed overviews of the input text, and manual text categorization is tedious and time-consuming. Extracting topics from text with an automatic classification task is necessary to generate specific topics enriched with top words that maintain semantic relationships among them. Therefore, this paper presents an approach that integrates text classification for topic discovery from large amounts of English textual data, such as 20-Newsgroups and Reuters Corpora. We rely on integrating automatic text classification before the topic discovery process to obtain specific topics for each class with relevant semantic relationships between top words. Text classification performs a word analysis that makes up a document to decide what class or category to identify; then, the proposed integration provides latent and specific topics depicted by top words with high coherence from each obtained class. Text classification accomplishes this with a convolutional neural network (CNN), incorporating an embedding model based on semantic relationships. Topic discovery over categorized text is realized with latent Dirichlet analysis (LDA), probabilistic latent semantic analysis (PLSA), and latent semantic analysis (LSA) algorithms. An evaluation process for topic discovery over categorized text was performed based on the normalized topic coherence metric. The 20-Newsgroups corpus was classified, and twenty topics with the ten top words were identified for each class. The normalized topic coherence obtained was 0.1723 with LDA, 0.1622 with LSA, and 0.1716 with PLSA. The Reuters Corpus was also classified, and twenty and fifty topics were identified. A normalized topic coherence of 0.1441 was achieved when applying the LDA algorithm, obtaining 20 topics for each class; with LSA, the coherence was 0.1360, and with PLSA, it was 0.1436. © 2023 by the authors.,Topic modelling is an important approach of unsupervised machine learning that allows automatically extracting the main ""topics""from large collections of documents. In addition, topic modelling is able to identify the topic proportions of each individual document, which can be helpful for organizing the collections. Many topic modelling algorithms have been proposed to date, including several that leverage advanced techniques such as variational inference and deep autoencoders. However, to date topic modelling has made limited use of reinforcement learning, a framework that has obtained vast success in many other unsupervised learning tasks. For this reason, in this article we propose training a neural topic model using a reinforcement learning objective and minimizing the objective with the recently-proposed REBAR gradient estimator. Experiments performed over two probing datasets have shown that the proposed model has achieved improvements over all the compared models in terms of both model perplexity and topic coherence, and produced topics that appear qualitatively informative and consistent.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM."
70,69,57,69_memory_quantization_throughput_compression,"memory,quantization,throughput,compression,hardware,neural,gpu,accelerator,gpus,sparse","Recently, low bit-width quantization (e.g., INT8) has been commonly used in deep neural network inference acceleration, but fewer researchers have focused on low-precision training quantization techniques. Considering that the backward propagation in deep neural network training is more computationally intensive and has a heavier energy overhead than the inference process, the quantization of backward propagation is of great interest for the training of very large-scale neural networks as well as for low-power devices with online training requirements. However, the shape specificity and continuous variability of the gradient distribution make gradient quantization difficult, and many studies propose various complex quantization methods for the gradient to reduce training accuracy loss. In this paper, we propose two innovative techniques mainly for INT8 quantization training, including the Data-aware Dynamic Segmentation Quantization scheme to quantize various special gradient distributions and the Update Direction Periodic Search strategy to achieve lower quantization errors. Then, we build a distribution-aware INT8 quantization training framework based on these two methods and conduct experiments on various models and tasks. Experimental results show that our proposed INT8 quantization training method achieves a negligible loss in final training accuracy compared to the full-precision floating-point counterpart on different models, including ResNet, MobileNetV2, VGG, AlexNet, and LSTM. By replacing floating-point computing with 8-bit integer computing for network training, this INT8 quantization training framework provides the possibility of deploying online training directly on low-power devices in the future. © 2023 Elsevier B.V.,The proliferation of deep learning algorithms has catalyzed their utilization to solve a multitude of real-world problems. Algorithms such as deep neural networks (DNNs) are compute- and power-intensive, thereby accentuating the development of hardware platforms like DNN inference accelerators. However, inference execution of large DNNs in resource-constrained environments induces energy bottlenecks in these accelerators. Since large DNNs consist of hundreds of millions of trained parameters, accessing them from the accelerator memory incurs substantial energy. To address this challenge, we propose HardCompress, which, to the best of our knowledge, is the first low-power solution that uses traditional compression strategies pertaining to commercial DNN accelerators in resource-constrained IoT edge devices. The three-step approach involves hardware-based post-quantization trimming of weights, followed by their dictionary-based compression and subsequent decompression by a low-power hardware engine during inference in the accelerator. We evaluate the proposed solution on lightweight networks trained on the MNIST dataset, the compact model trained on the CIFAR-10 dataset, and large DNNs trained on the ImageNet dataset. Performance of HardCompress at different quantization levels has been analyzed. Furthermore, to quantify the effectiveness of the proposed solution, an energy framework that contrasts the DRAM energies of the original and HardCompressed models has been developed. Finally, a fault injection framework which compares the fault resilience of the original model with its HardCompressed counterpart is also proposed. Our results exhibit that HardCompress, without any performance degradation in large DNNs, furnishes a maximum compression of 99.27%, equivalent to 137× reduction in memory footprint and 0.07 J for 8-bit quantization in the systolic array-based DNN accelerator. Furthermore, our proposed low-power decompression engine incurs an area overhead of only 0.02%; thus, enabling HardCompress' utilization in resource-constrained environments. © 1982-2012 IEEE.,Large language models (LLMs) have sparked a new revolution in the field of natural language processing (NLP), and have garnered tremendous attention in both academic research and everyday life, thanks to their unprecedented performance in a wide range of applications. However, their deployment remains a significant challenge, primarily due to their intensive computational and memory requirements. Hardware acceleration and efficient quantization are promising solutions to address the two issues. In this paper, a quantization and hardware architecture co-design is presented for matrix-vector multiplications (MVMs) of LLMs. During quantization, we uniformly group weights and activations to ensure workload balance for hardware. To enhance the performance of quantization, we further propose two approaches called channel sorting and channel selection, which can be applied simultaneously. To support the proposed quantization scheme, we develop two precision-scalable MVM hardware architectures. They are specifically designed for high speed and high energy efficiency, respectively. Experimental results show that our proposed quantization scheme achieves state-of-the-art performance among all the reported post-training schemes that quantize both weights and activations into integers. Compared to MVM architecture of the state-of-the-art LLM accelerator OliVe, our design exhibits significant advantages in terms of area efficiency and energy efficiency. IEEE"
71,70,57,70_manufacturing_steel_printing_alloy,"manufacturing,steel,printing,alloy,prediction,molding,mechanical,thermal,aluminum,machine","This paper explores the potential of using Chat Generative Pre-trained Transformer (ChatGPT), a Large Language Model (LLM) developed by OpenAI, to address the main challenges and improve the efficiency of the Gcode generation process in Additive Manufacturing (AM), also known as 3D printing. The Gcode generation process, which controls the movements of the printer's extruder and the layer-by-layer build process, is a crucial step in the AM process and optimizing the Gcode is essential for ensuring the quality of the final product and reducing print time and waste. ChatGPT can be trained on existing Gcode data to generate optimized Gcode for specific polymeric materials, printers, and objects, as well as analyze and optimize the Gcode based on various printing parameters such as printing temperature, printing speed, bed temperature, fan speed, wipe distance, extrusion multiplier, layer thickness, and material flow. Here the capability of ChatGPT in performing complex tasks related to AM process optimization was demonstrated. In particular performance tests were conducted to evaluate ChatGPT's expertise in technical matters, focusing on the evaluation of printing parameters and bed detachment, warping, and stringing issues for Fused Filament Fabrication (FFF) methods using thermoplastic polyurethane polymer as feedstock material. This work provides effective feedback on the performance of ChatGPT and assesses its potential for use in the AM field. The use of ChatGPT for AM process optimization has the potential to revolutionize the industry by offering a user-friendly interface and utilizing machine learning algorithms to improve the efficiency and accuracy of the Gcode generation process and optimal printing parameters. Furthermore, the real-time optimization capabilities of ChatGPT can lead to significant time and material savings, making AM a more accessible and cost-effective solution for manufacturers and industry. © 2023 Kingfa Scientific and Technological Co. Ltd.,Metal 3D printing has gained a lot of attention among industries since it offers a practical solution to problems rising during the manufacturing of parts and components with complex geometry. This is an additive technology that eliminated several fabrication steps and at the same time reduces material waste during the manufacturing process. However, in all additive manufacturing technologies, the final properties of the parts are determined by the operational process parameters. In this study, several machine learning algorithms were examined to characterize the effects of the printing process parameters on relative density, hardness, yield strength, and tensile strength in manufactured parts. It was possible by using “Big Data” collected from a large number of previously published articles on the application of laser powder bed fusion (LPBF) for the 3D printing of 316L stainless steel samples. Among different process parameters, laser power, laser energy density, and scanning speed were proven to have the largest effects directly on the physical and mechanical properties of LPBF-processed parts. Six different classification models and five support vector machine regression-based models were tested to find the most accurate prediction algorithm. To validate the obtained results from the applied machine learning models, a set of 316L specimens were produced using LPBF technology using a random set of process parameters. The physical and mechanical properties of 3D printed samples were tested and compared to the ones predicted from the optimum models from machine learning analysis. The results were in great agreement, which shows the high accuracy of the developed machine learning algorithms in this study. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,The temperature history of an additively manufactured part plays a critical role in determining process-structure-property relationships in fusion-based additive manufacturing (AM) processes. Therefore, fast thermal simulation methods are needed for a variety of AM tasks, from temperature history prediction for part design and process planning to in situ temperature monitoring and control during manufacturing. However, conventional numerical simulation methods fall short in satisfying the strict requirements of time efficiency in these applications due to the large space and time scales of the required multiscale simulation. While data-driven surrogate models are of interest for their rapid computation capabilities, the performance of these models relies on the size and quality of the training data, which is often prohibitively expensive to create. Physics-informed neural networks (PINNs) mitigate the need for large datasets by imposing physical principles during the training process. This work investigates the use of a PINN to predict the time-varying temperature distribution in a part during manufacturing with laser powder bed fusion (L-PBF). Notably, the use of the PINN in this study enables the model to be trained solely on randomly synthesized data. These training data are both inexpensive to obtain, and the presence of stochasticity in the dataset improves the generalizability of the trained model. Results show that the PINN model achieves higher accuracy than a comparable artificial neural network trained on labeled data. Further, the PINN model trained in this work maintains high accuracy in predicting temperature for laser path scanning strategies unseen in the training data. © 2023 by ASME."
72,71,56,71_cnn_bridges_damage_neural,"cnn,bridges,damage,neural,bridge,vibrationbased,wavelet,damagesensitive,vibration,deep","Gathering properly labeled, adequately rich, and case-specific data for successfully training a purely data-driven or hybrid model for structural health monitoring (SHM) applications is a challenging task. We posit that a Transfer Learning (TL) method that utilizes available data in any relevant source domain and directly applies to the target domain through domain adaptation can provide substantial remedies to address this issue. Accordingly, we present a novel TL method that differentiates between the source's no-damage and damage cases and utilizes a domain adaptation (DA) technique. The DA module transfers the accumulated knowledge in contrasting no-damage and damage cases in the source domain to the target domain, given only the target's no-damage case. High-dimensional features allow employing signal processing domain knowledge to devise a generalizable DA approach. The Generative Adversarial Network (GAN) architecture is adopted for learning since its optimization process accommodates high-dimensional inputs in a zero-shot setting. At the same time, its training objective conforms seamlessly with the case of no-damage and damage data in SHM since its discriminator network differentiates between real (no-damage) and fake (possibly unseen damage) data. An extensive set of experimental results demonstrates the method's success in transferring knowledge on differences between no-damage and damage cases across three strongly heterogeneous independent target structures. The area under the Receiver Operating Characteristics curves (Area Under the Curve — AUC) is used to evaluate the differentiation between no-damage and damage cases in the target domain, reaching values as high as 0.95. With no-damage and damage cases discerned from each other, zero-shot structural damage detection is carried out. The mean F1 scores for all damages in the three independent datasets are 0.971, 0.986, and 0.975. The success of the proposed TL approach is expected to pave the way for further improvements in the accuracy and generalizability of data-driven SHM applications, thereby offering new possibilities for large-scale SHM applications in urban settings. © 2023 Elsevier Ltd,Structural health monitoring (SHM) approaches have offered potential solutions for monitoring and analyzing the behavior of transport infrastructures. Recently, vibration-based damage detection using deep learning approaches has received considerable attention in the SHM community. Previous studies on structural damage detection have employed supervised deep learning models, which require large amounts of labeled data. However, acquiring labeled data in practical engineering is challenging, costly, and sometimes impractical. This study uses a deep autoencoder model to extract the damage-sensitive features from acceleration data in the frequency domain without any labels. For this, a numerical example of a concrete highway bridge model subjected to a single-vehicle load under varying temperatures, low-extent damages, vehicle speeds, road surface conditions, and measurement noises was used to evaluate the effectiveness of the proposed method. This study demonstrated that the trained model is sensitive to damage in terms of reconstruction loss. In addition, the damage index (DI) between different damage-sensitive features was calculated using the Gaussian process-based z-scores. The results show that the proposed method has good damage detection capability, with the model struggling only at a higher speed (V3 = 8.33 m/s) with poor road surface roughness, where damage becomes evident after 10% or 15% damage severity. These results emphasize the potential of using the proposed method in practical engineering. © 2023 Institution of Structural Engineers,In cases with a large number of sensors and complex spatial distribution, correctly learning the spatial characteristics of the sensors is vital for structural damage identification. Graph convolutional neural networks (GCNs), unlike other methods, have the ability to learn the spatial characteristics of the sensors, which is targeted at the above problems in structural damage identification. However, under the influence of environmental interference, sensor instability, and other factors, part of the vibration signal can easily change its fundamental characteristics, and there is a possibility of misjudging structural damage. Therefore, on the basis of building a high-performance graphical convolutional deep learning model, this paper considers the integration of data fusion technology in the model decision-making layer and proposes a single-model decision-making fusion neural network (S_DFNN) model. Through experiments involving the frame model and the self-designed cable-stayed bridge model, it is concluded that this method has a better performance of damage recognition for different structures, and the accuracy is improved based on a single model and has good damage recognition performance. The method has better damage identification performance in different structures, and the accuracy rate is improved based on the single model, which has a very good damage identification effect. It proves that the structural damage diagnosis method proposed in this paper with data fusion technology combined with deep learning has a strong generalization ability and has great potential in structural damage diagnosis. © 2023 by the authors."
73,72,56,72_galaxies_cosmology_cosmological_galaxy,"galaxies,cosmology,cosmological,galaxy,astrophysical,astrophysics,astronomical,cosmic,universe,stellar","The relationship between galaxies and haloes is central to the description of galaxy formation and a fundamental step towards extracting precise cosmological information from galaxy maps. However, this connection involves several complex processes that are interconnected. Machine Learning methods are flexible tools that can learn complex correlations between a large number of features, but are traditionally designed as deterministic estimators. In this work, we use the IllustrisTNG300-1 simulation and apply neural networks in a binning classification scheme to predict probability distributions of central galaxy properties, namely stellar mass, colour, specific star formation rate, and radius, using as input features the halo mass, concentration, spin, age, and the overdensity on a scale of 3 h?1 Mpc. The model captures the intrinsic scatter in the relation between halo and galaxy properties, and can thus be used to quantify the uncertainties related to the stochasticity of the galaxy properties with respect to the halo properties. In particular, with our proposed method, one can define and accurately reproduce the properties of the different galaxy populations in great detail. We demonstrate the power of this tool by directly comparing traditional single-point estimators and the predicted joint probability distributions, and also by computing the power spectrum of a large number of tracers defined on the basis of the predicted colour-stellar mass diagram. We show that the neural networks reproduce clustering statistics of the individual galaxy populations with excellent precision and accuracy. © 2023 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.,Extracting information from the total matter power spectrum with the precision needed for upcoming cosmological surveys requires unraveling the complex effects of galaxy formation processes on the distribution of matter. We investigate the impact of baryonic physics on matter clustering at z = 0 using a library of power spectra from the Cosmology and Astrophysics with MachinE Learning Simulations project, containing thousands of (25 h?1 Mpc)3 volume realizations with varying cosmology, initial random field, stellar and active galactic nucleus (AGN) feedback strength and subgrid model implementation methods. We show that baryonic physics affects matter clustering on scales k ? 0.4 hMpc?1 and the magnitude of this effect is dependent on the details of the galaxy formation implementation and variations of cosmological and astrophysical parameters. Increasing AGN feedback strength decreases halo baryon fractions and yields stronger suppression of power relative to N-body simulations, while stronger stellar feedback often results in weaker effects by suppressing black hole growth and therefore the impact of AGN feedback. We find a broad correlation between mean baryon fraction of massive haloes (M200c > 1013.5 M) and suppression of matter clustering but with significant scatter compared to previous work owing to wider exploration of feedback parameters and cosmic variance effects. We show that a random forest regressor trained on the baryon content and abundance of haloes across the full mass range 1010 ? Mhalo/M<1015 can predict the effect of galaxy formation on the matter power spectrum on scales k = 1.0–20.0 hMpc?1 © 2023 The Author(s),The properties of the matter density field in the initial conditions have a decisive impact on the features of the large-scale structure of the Universe as observed today. These need to be studied via N-body simulations, which are imperative to analyze high density collapsed regions into dark matter halos. In this paper, we train machine learning algorithms with information from N-body simulations to infer two properties: dark matter particle halo classification that leads to halo formation prediction with the characteristics of the matter density field traced back to the initial conditions, and dark matter halo formation by calculating the halo mass function, which offers the number density of dark matter halos with a given threshold. We map the initial conditions of the density field into classification labels of dark matter halo structures. The halo mass function of the simulations is calculated and reconstructed with theoretical methods as well as our trained algorithms. We test several machine learning techniques where we could find that the random forest and neural networks proved to be the better performing tools to classify dark matter particles in cosmological simulations. We also show that, by using only a few data points, we can effectively train the algorithms to reconstruct the halo mass function in a model-independent way, giving us a highly accurate fitting function that aligns well with both simulation and theoretical results.  © 2023 American Physical Society."
74,73,55,73_gans_generative_artistic_gan,"gans,generative,artistic,gan,adversarial,drawing,artists,styles,painting,paintings","This work presents Unified Contrastive Arbitrary Style Transfer (UCAST), a novel style representation learning and transfer framework, that can fit in most existing arbitrary image style transfer models, such as CNN-based, ViT-based, and flow-based methods. As the key component in image style transfer tasks, a suitable style representation is essential to achieve satisfactory results. Existing approaches based on deep neural networks typically use second-order statistics to generate the output. However, these hand-crafted features computed from a single image cannot leverage style information sufficiently, which leads to artifacts such as local distortions and style inconsistency. To address these issues, we learn style representation directly from a large number of images based on contrastive learning by considering the relationships between specific styles and the holistic style distribution. Specifically, we present an adaptive contrastive learning scheme for style transfer by introducing an input-dependent temperature. Our framework consists of three key components: a parallel contrastive learning scheme for style representation and transfer, a domain enhancement (DE) module for effective learning of style distribution, and a generative network for style transfer. Qualitative and quantitative evaluations show the results of our approach are superior to those obtained via state-of-the-art methods. The code is available at https://github.com/zyxElsa/CAST_pytorch. © 2023 Copyright held by the owner/author(s).,Designing and generating novels fonts manually is a laborious and time-consuming process owing to the large number and complexity of characters in the majority of language systems. Recent advancements in generative adversarial networks (GANs) have significantly improved font generation. These GAN-based approaches either handle the font generation as a vanilla GAN problem (that is, by synthesizing characters from a uniform latent vector) or an image-to-image translation problem. While the former approach has no limitation in generating diverse font styles, the generated fonts contain artifacts and can operate only on low-resolution images, thus impairing their usability. The latter approach generates high-quality font images for previously observed fonts, but the quality degrades during the inference phase while designing novel fonts. Furthermore, additional fine-tuning steps are required to achieve photorealistic results, which is computationally expensive and time-consuming. To address the shortcomings of these approaches, we propose a font generation method that employs the vanilla GAN approach to generate an infinite number of font styles but focuses on the real-time generation of photo-realistic font images. Additionally, we strive to create high-resolution images that can be used in practical applications. To accomplish this, we propose a conditional font GAN (CFGAN) with a sophisticated network architecture that is designed to generate novel style-consistent diverse font character sets. We control the generated characters in the proposed network using a non-trainable fixed character vector, while the style variation sampled from a Gaussian distribution is fused at all blocks of the generator through adaptive instance normalization (AdaIN) operation. Thus, the generator architecture can simultaneously generate an infinite number of font styles with style consistency and diversity during inference. We conducted various quantitative and qualitative experiments to demonstrate the effectiveness of the proposed model in terms of both image quality and computational cost. © 2022 Elsevier Ltd,Creating a novel font set requires domain expertise and is a laborious and time-consuming process, particularly for languages with a large number of characters and complicated structures. Existing deep learning based methods consider font generation (FG) as an image-to-image translation problem, mostly in a supervised setting, either in the form of pair images (paired data) or font labels (character or style labels), which requires extensive effort and is expensive to collect. Additionally, these supervised counter parts lack generalization for extending to other text image-related tasks, such as word image generation and font attribute control at inference time. We found that these drawbacks are mainly due to the supervised setting adopted by these existing methods for font generation. In this paper, we tackle the FG problem in a truly unsupervised fashion, where a complete font set can be generated by training the generator such that adjacent styles are not correlated and projecting the input glyph image into its corresponding font style latent space. To accomplish this, we propose the Font Mixing Generative Adversarial Network (FM-GAN), which employs mixing regularization to supervise the generator to localize the font styles, and a projection encoder to project an arbitrary glyph image into its corresponding semantic space that is compatible with the generator. In the experiments, we demonstrated that our unsupervised model synthesizes font images that are comparable to supervised state-of-the-art FG baselines. Furthermore, FM-GAN can be directly applied to other text image related tasks, such as multi-lingual font style transfer, word image generation, and font attribute control. © 2023 Elsevier B.V."
75,74,55,74_imaging_knee_mri_bone,"imaging,knee,mri,bone,radiographic,radiologists,cartilage,osteoarthritis,knees,fractures","BackgroundOccult scaphoid fractures on initial radiographs of an injury are a diagnostic challenge to physicians. Although artificial intelligence models based on the principles of deep convolutional neural networks (CNN) offer a potential method of detection, it is unknown how such models perform in the clinical setting.Questions/purposes(1) Does CNN-assisted image interpretation improve interobserver agreement for scaphoid fractures? (2) What is the sensitivity and specificity of image interpretation performed with and without CNN assistance (as stratified by type: normal scaphoid, occult fracture, and apparent fracture)? (3) Does CNN assistance improve time to diagnosis and physician confidence level?MethodsThis survey-based experiment presented 15 scaphoid radiographs (five normal, five apparent fractures, and five occult fractures) with and without CNN assistance to physicians in a variety of practice settings across the United States and Taiwan. Occult fractures were identified by follow-up CT scans or MRI. Participants met the following criteria: Postgraduate Year 3 or above resident physician in plastic surgery, orthopaedic surgery, or emergency medicine; hand fellows; and attending physicians. Among the 176 invited participants, 120 completed the survey and met the inclusion criteria. Of the participants, 31% (37 of 120) were fellowship-trained hand surgeons, 43% (52 of 120) were plastic surgeons, and 69% (83 of 120) were attending physicians. Most participants (73% [88 of 120]) worked in academic centers, whereas the remainder worked in large, urban private practice hospitals. Recruitment occurred between February 2022 and March 2022. Radiographs with CNN assistance were accompanied by predictions of fracture presence and gradient-weighted class activation mapping of the predicted fracture site. Sensitivity and specificity of the CNN-assisted physician diagnoses were calculated to assess diagnostic performance. We calculated interobserver agreement with the Gwet agreement coefficient (AC1). Physician diagnostic confidence was estimated using a self-assessment Likert scale, and the time to arrive at a diagnosis for each case was measured.ResultsInterobserver agreement among physicians for occult scaphoid radiographs was higher with CNN assistance than without (AC1 0.42 [95% CI 0.17 to 0.68] versus 0.06 [95% CI 0.00 to 0.17], respectively). No clinically relevant differences were observed in time to arrive at a diagnosis (18 ± 12 seconds versus 30 ± 27 seconds, mean difference 12 seconds [95% CI 6 to 17]; p < 0.001) or diagnostic confidence levels (7.2 ± 1.7 seconds versus 6.2 ± 1.6 seconds; mean difference 1 second [95% CI 0.5 to 1.3]; p < 0.001) for occult fractures.ConclusionCNN assistance improves physician diagnostic sensitivity and specificity as well as interobserver agreement for the diagnosis of occult scaphoid fractures. The differences observed in diagnostic speed and confidence is likely not clinically relevant. Despite these improvements in clinical diagnoses of scaphoid fractures with the CNN, it is unknown whether development and implementation of such models is cost effective.Level of EvidenceLevel II, diagnostic study.  © 2023 by the Association of Bone and Joint Surgeons.,Objective: In this proof-of-concept study, we aimed to develop deep-learning-based classifiers to identify rib fractures on frontal chest radiographs in children under 2 years of age. Methods: This retrospective study included 1311 frontal chest radiographs (radiographs with rib fractures, n = 653) from 1231 unique patients (median age: 4 m). Patients with more than one radiograph were included only in the training set. A binary classification was performed to identify the presence or absence of rib fractures using transfer learning and Resnet-50 and DenseNet-121 architectures. The area under the receiver operating characteristic curve (AUC-ROC) was reported. Gradient-weighted class activation mapping was used to highlight the region most relevant to the deep learning models’ predictions. Results: On the validation set, the ResNet-50 and DenseNet-121 models obtained an AUC-ROC of 0.89 and 0.88, respectively. On the test set, the ResNet-50 model demonstrated an AUC-ROC of 0.84 with a sensitivity of 81% and specificity of 70%. The DenseNet-50 model obtained an AUC of 0.82 with 72% sensitivity and 79% specificity. Conclusion: In this proof-of-concept study, a deep learning-based approach enabled the automatic detection of rib fractures in chest radiographs of young children with performances comparable to pediatric radiologists. Further evaluation of this approach on large multi-institutional data sets is needed to assess the generalizability of our results. Advances in knowledge: In this proof-of-concept study, a deep learning-based approach performed well in identifying chest radiographs with rib fractures. These findings provide further impetus to develop deep learning algorithms for identifying rib fractures in children, especially those with suspected physical abuse or non-accidental trauma. © 2023 The Authors. Published by the British Institute of Radiology.,Background: Osteoarthritis (OA) is a global healthcare problem. The increasing population of OA patients demands a greater bandwidth of imaging and diagnostics. It is important to provide automatic and objective diagnostic techniques to address this challenge. This study demonstrates the utility of unsupervised domain adaptation (UDA) for automated OA phenotype classification. Methods: We collected 318 and 960 three-dimensional double-echo steady-state magnetic resonance images from the Osteoarthritis Initiative (OAI) dataset as the source dataset for phenotype cartilage/ meniscus and subchondral bone, respectively. Fifty three-dimensional turbo spin echo (TSE)/fast spin echo (FSE) MR images from our institute were collected as the target datasets. For each patient, the degree of knee OA was initially graded according to the MRI Knee Osteoarthritis Knee Score before being converted to binary OA phenotype labels. The proposed four-step UDA pipeline included (I) pre-processing, which involved automatic segmentation and region-of-interest cropping; (II) source classifier training, which involved pre-training a convolutional neural network (CNN) encoder for phenotype classification using the source dataset; (III) target encoder adaptation, which involved unsupervised adjustment of the source encoder to the target encoder using both the source and target datasets; and (IV) target classifier validation, which involved statistical analysis of the classification performance evaluated by the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity and accuracy. We compared our model on the target data with the source pre-trained model and the model trained with the target data from scratch. Results: For phenotype cartilage/meniscus, our model has the best performance out of the three models, giving 0.90 [95% confidence interval (CI): 0.79–1.02] of the AUROC score, while the other two model show 0.52 (95% CI: 0.13–0.90) and 0.76 (95% CI: 0.53–0.98). For phenotype subchondral bone, our model gave 0.75 (95% CI: 0.56–0.94) at AUROC, which has a close performance of the source pre-trained model (0.76, 95% CI: 0.55–0.98), and better than the model trained from scratch on the target dataset only (0.53, 95% CI: 0.33–0.73). Conclusions: By utilising a large, high-quality source dataset for training, the proposed UDA approach enhances the performance of automated OA phenotype classification for small target datasets. As a result, our technique enables improved downstream analysis of locally collected datasets with a small sample size. © Quantitative Imaging in Medicine and Surgery. All rights reserved."
76,75,55,75_corpus_speech_phonetic_voice,"corpus,speech,phonetic,voice,audio,neural,pronunciation,speaker,learning,dataset","With the popularization of computers, artificial intelligence technology has become more and more mature, among which speech recognition technology in artificial intelligence is favored by people. In the past few years, the acoustic model combined with the Gaussian mixture model and the hidden Markov model has always been in the leading position. In the field of speech recognition technology, because the development of speech data has gradually expanded, and the complexity of the data has also increased. The larger the data, the traditional data network model is slowly showing inadequacy. However, the deep neural network model is easy to deal with large and complex data modeling. This article combines the advantages of basic learning theory and speech recognition technology, and launches in-depth research on embedding learning theoretical knowledge into the field of speech recognition. Nowadays, large-scale text information databases relying on computers are becoming more and more important in linguistic research, and a large-scale corpus that fully reflects language facts and contains rich language information has been constructed. The establishment of the text information database system is long, from word segmentation, part-of-speech tagging to syntactic tagging to semantic tagging. Therefore, the characteristic of information system processing is that the systematic description depends on the application environment of understanding vocabulary and reasoning. According to different scenarios, the realization methods of semantic description roles are also different, and the description of semantic roles in correct scenarios is clearer, more detailed and systematic. Therefore, this article is of great significance for solving the semantic problem of using Chinese frame network for Chinese information processing in the context of speech recognition. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Significant advances in end-to-end (E2E) automatic speech recognition (ASR) have primarily been concentrated on languages rich in annotated data. Nevertheless, a large proportion of languages worldwide, which are typically low-resource, continue to pose significant challenges. To address this issue, this study presents a novel speech synthesis framework based on data splicing that leverages self-supervised learning (SSL) units from Hidden Unit BERT (HuBERT) as universal phonetic units. In our framework, the SSL phonetic units serve as crucial bridges between speech and text across different languages. By leveraging these units, we successfully splice speech fragments from high-resource languages into synthesized speech that maintains acoustic coherence with text from low-resource languages. To further enhance the practicality of the framework, we introduce a sampling strategy based on confidence scores assigned to the speech segments used in data splicing. The application of this confidence sampling strategy in data splicing significantly accelerates ASR model convergence and enhances overall ASR performance. Experimental results on the CommonVoice dataset show 25-35% relative improvement for four Indo-European languages and about 20% for Turkish using a 4-gram language model for rescoring, under a 10-hour low-resource setup. Furthermore, we showcase the scalability of our framework by incorporating a larger unsupervised speech corpus for generating speech fragments in data splicing, resulting in an additional 10% relative improvement.  © 2014 IEEE.,Recently, more and more personalized speech enhancement (PSE) systems with excellent performance have been proposed. Compared with traditional speech enhancement systems, PSE systems have a wider range of application scenarios, which can simultaneously remove background noise and interfering speaker's speech. However, two issues still limit the performance and generalization ability of the model: 1) Acoustic environment mismatch between the test noisy speech and clean enrollment speech of target speaker, which limits the performance of personalized speech enhancement system; 2) Hard sample mining and learning. How to improve the performance of hard samples determines the practicality of a personalized speech enhancement system in complex real-world scenarios. In this paper, a dynamic acoustic compensation (DAC) is proposed to alleviate the environment mismatch, by intercepting the acoustic segments from noisy speech and mixing it with enrollment speech. To well exploit the hard samples, we propose an adaptive focal training (AFT) strategy by assigning adaptive loss weights to hard and non-hard samples during training. Both the DAC and AFT are proposed to improve and generalize our previous work, a densely-connected pyramid complex convolutional network with speaker encoder (sDPCCN) for personalized speech enhancement. In addition, a time-frequency multi-loss training is further introduced to enhance the improved sDPCCN. To examine the effectiveness of the proposed methods, we generate the noisy-reverb training and test data by utilizing non-overlapping segments of the 4th Deep Noise Suppression (DNS4) Challenge Dataset. Results show that, DAC effectively alleviates the acoustic environment mismatch and brings large improvements in terms of multiple evaluation metrics, and AFT reduces the hard sample rate significantly. When all proposed methods are applied, the perceptual evaluation of speech quality (PESQ) score is improved from 3.21 to 3.36, and the scale invariant signal-to-noise ratio (SISNR) is improved from 15.11 to 15.89 on the test set, which fully verify their effectiveness and practicality. © 2023 Elsevier Ltd"
77,76,53,76_hashing_hash_retrieval_features,"hashing,hash,retrieval,features,binary,storage,feature,supervised,similarity,datasets","Online hashing is a valid storage and online retrieval scheme, which is meeting the rapid increase in data in the optical-sensor network and the real-time processing needs of users in the era of big data. Existing online-hashing algorithms rely on data tags excessively to construct the hash function, and ignore the mining of the structural features of the data itself, resulting in a serious loss of the image-streaming features and the reduction in retrieval accuracy. In this paper, an online hashing model that fuses global and local dual semantics is proposed. First, to preserve the local features of the streaming data, an anchor hash model, which is based on the idea of manifold learning, is constructed. Second, a global similarity matrix, which is used to constrain hash codes is built by the balanced similarity between the newly arrived data and previous data, which makes hash codes retain global data features as much as possible. Then, under a unified framework, an online hash model that integrates global and local dual semantics is learned, and an effective discrete binary-optimization solution is proposed. A large number of experiments on three datasets, including CIFAR10, MNIST and Places205, show that our proposed algorithm improves the efficiency of image retrieval effectively, compared with several existing advanced online-hashing algorithms. © 2023 by the authors.,Hashing can facilitate efficient retrieval and storage for large-scale images due to the binary representation. In the real applications, the trade-off between retrieval accuracy and speed is essential for designing a hashing framework, which is reflected by variable hash code lengths. In light of this, the existing hashing methods need to train different models for different lengths of hash codes, leading to considerable training time cost and hashing flexibility reduction. Given that a sample can be represented by various hash codes with different lengths, there are some helpful relationships that can boost the performance of hashing methods. However, the existing hashing methods do not fully utilize these relationships. To address the aforementioned issues, we propose a new model, known as supervised discrete multiple-length hashing (SDMLH), to simultaneously learn hash codes with multiple lengths. In this proposed SDMLH method, three types of information are respectively derived, from the hash codes with different lengths. The original features of the samples, and the label, are applied for hash learning. Unlike the existing hashing methods, SDMLH can fully employ the assistance among hash codes with different lengths and learn them in one step. Furthermore, given a hash length meeting the demand of users, we propose a hash fusion strategy to obtain the hash code with this desirable length by fusing the multiple-length hash codes. This obtained hash code outperforms the one learned directly. In addition, SDMLH can generate the hash code of any length that is shorter than the sum length of given multiple hash codes with the fusion strategy. To the best of our knowledge, SDMLH is one of the first attempts for learning multiple-length hash codes simultaneously. We conduct extensive experiments based on three benchmark datasets, demonstrating the superiority of this proposed method.  © 2022 IEEE.,Deep hashing methods utilize an end-to-end framework to mutually learn feature representations and hash codes, thereby achieving a better retrieval performance. Traditional supervised hashing methods adopt handcrafted features for hashing function learning and then generate hash codes through classification and quantization. The lack of adaptability and independence of the quantization procedure leads to low retrieval accuracy of supervised hashing methods with handcrafted features in image retrieval. In this study, a non-relaxation deep hashing method for fast image retrieval is proposed. In this method, a differentiable host thresholding function is used to encourage hash-like codes to approach -1 or 1 non-linearly at the output of the convolutional neural, instead of the symbol function for quantization used in the traditional method. The output of the host thresholding function is directly used to compute the network training error, and a loss function is elaborately designed with the norm to constrain each bit of the hash-like code to be as binary as possible. Finally, a symbol function is added outside the trained network model to generate binary hash codes for image storage and retrieval in a low-dimensional binary space. Extensive experiments on two large-scale public datasets show that our method can effectively learn image features, generate accurate binary hash codes, and outperform state-of-the-art methods in terms of the mean average precision. © 2013 IEEE."
78,77,53,77_proteins_protein_proteinprotein_amino,"proteins,protein,proteinprotein,amino,embeddings,peptide,representations,peptides,embedding,models","Pre-trained natural language processing models on a large natural language corpus can naturally transfer learned knowledge to protein domains by fine-tuning specific in-domain tasks. However, few studies focused on enriching such protein language models by jointly learning protein properties from strongly-correlated protein tasks. Here we elaborately designed a multi-task learning (MTL) architecture, aiming to decipher implicit structural and evolutionary information from three sequence-level classification tasks for protein family, superfamily and fold. Considering the co-existing contextual relevance between human words and protein language, we employed BERT, pre-trained on a large natural language corpus, as our backbone to handle protein sequences. More importantly, the encoded knowledge obtained in the MTL stage can be well transferred to more fine-grained downstream tasks of TAPE. Experiments on structure- or evolution-related applications demonstrate that our approach outperforms many state-of-the-art Transformer-based protein models, especially in remote homology detection. © 2022, The Author(s).,In living organisms, proteins are considered as the executants of biological functions. Owing to its pivotal role played in protein folding patterns, comprehension of protein structure is a challenging issue. Moreover, owing to numerous protein sequence exploration in protein data banks and complication of protein structures, experimental methods are found to be inadequate for protein structural class prediction. Hence, it is very much advantageous to design a reliable computational method to predict protein structural classes from protein sequences. In the recent few years there has been an elevated interest in using deep learning to assist protein structure prediction as protein structure prediction models can be utilized to screen a large number of novel sequences. In this regard, we propose a model employing Energy Profile for atom pairs in conjunction with the Legion-Class Bayes function called Energy Profile Legion-Class Bayes Protein Structure Identification model. Followed by this, we use a Thompson Optimized convolutional neural network to extract features between amino acids and then the Thompson Optimized SoftMax function is employed to extract associations between protein sequences for predicting secondary protein structure. The proposed Energy Profile Bayes and Thompson Optimized Convolutional Neural Network (EPB-OCNN) method tested distinct unique protein data and was compared to the state-of-the-art methods, the Template-Based Modeling, Protein Design using Deep Graph Neural Networks, a deep learning-based S-glutathionylation sites prediction tool called a Computational Framework, the Deep Learning and a distance-based protein structure prediction using deep learning. The results obtained when applied with the Biopython tool with respect to protein structure prediction time, protein structure prediction accuracy, specificity, recall, F-measure, and precision, respectively, are measured. The proposed EPB-OCNN method outperformed the state-of-the-art methods, thereby corroborating the objective. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Well understanding protein function and structure in computational biology helps in the understanding of human beings. To face the limited proteins that are annotated structurally and functionally, the scientific community embraces the self-supervised pretraining methods from large amounts of unlabeled protein sequences for protein embedding learning. However, the protein is usually represented by individual amino acids with limited vocabulary size (e.g. 20 type proteins), without considering the strong local semantics existing in protein sequences. In this work, we propose a novel pre-training modeling approach SPRoBERTa. We first present an unsupervised protein tokenizer to learn protein representations with local fragment pattern. Then, a novel framework for deep pretraining model is introduced to learn protein embeddings. After pre-training, our method can be easily fine-tuned for different protein tasks, including amino acid-level prediction task (e.g. secondary structure prediction), amino acid pair-level prediction task (e.g. contact prediction) and also protein-level prediction task (remote homology prediction, protein function prediction). Experiments show that our approach achieves significant improvements in all tasks and outperforms the previous methods. We also provide detailed ablation studies and analysis for our protein tokenizer and training framework. © The Author(s) 2022. Published by Oxford University Press. All rights reserved."
79,78,52,78_lhc_higgs_quarks_quark,"lhc,higgs,quarks,quark,collider,colliders,bosons,lepton,hadrons,mesons","The QCD-like dark sector with GeV-scale dark hadrons has the potential to generate new signatures at the Large Hadron Collider (LHC). In this paper, we consider a singlet scalar mediator in the tens of GeV-scale that connects the dark sector and the Standard Model (SM) sector via the Higgs portal. We focus on the Higgs-strahlung process, qq¯ ? ? W * ? WH, to produce a highly boosted Higgs boson. Our scenario predicts two different processes that can generate dark mesons: (1) the cascade decay from the Higgs boson to two light scalar mediators and then to four dark mesons; (2) the Higgs boson decaying to two dark quarks, which then undergo a QCD-like shower and hadronization to produce dark mesons. We apply machine learning techniques, such as Convolutional Neural Network (CNN) and Energy Flow Network (EFN), to the fat-jet structure to distinguish these signal processes from large SM backgrounds. We find that the branching ratio of the Higgs boson to two light scalar mediators can be constrained to be less than about 10% at 14 TeV LHC with L = 3000 fb ?1. © 2023, The Author(s).,In this article we probe resonant associated production of a Standard Model Higgs boson with new heavy scalar resonance in proton-proton collisions at a center-of-mass energy s=13 TeV. The Higgs boson and new scalar resonant are required to decay into a pair of bottom quarks and a pair of top quarks, respectively. Semileptonic decay of top quarks is considered. The searches are projected into operation conditions of the Large Hadron Collider during Run II data taking period at a center-of-mass energy of 13 TeV using Monte Carlo generated events, realistic detector response simulation and available Open Data samples. Analysis strategies are presented and machine learning approach using Deep Neural Network is proposed to resolve ambiguous in jets assignment and improve kinematic reconstruction of signal events. Sensitivity of the CMS detector is estimated as 95% expected upper limits on the product of the production cross section and the branching fractions of the searched particles. © 2023 The Author(s),A data sample containing top quark pairs ( t t ¯ ) produced in association with a Lorentz-boosted Z or Higgs boson is used to search for signs of new physics using effective field theory. The data correspond to an integrated luminosity of 138 fb - 1 of proton-proton collisions produced at a center-of-mass energy of 13 TeV at the LHC and collected by the CMS experiment. Selected events contain a single lepton and hadronic jets, including two identified with the decay of bottom quarks, plus an additional large-radius jet with high transverse momentum identified as a Z or Higgs boson decaying to a bottom quark pair. Machine learning techniques are employed to discriminate between t t ¯ Z or t t ¯ H events and events from background processes, which are dominated by t t ¯ + jets production. No indications of new physics are observed. The signal strengths of boosted t t ¯ Z and t t ¯ H production are measured, and upper limits are placed on the t t ¯ Z and t t ¯ H differential cross sections as functions of the Z or Higgs boson transverse momentum. The effects of new physics are probed using a framework in which the standard model is considered to be the low-energy effective field theory of a higher energy scale theory. Eight possible dimension-six operators are added to the standard model Lagrangian, and their corresponding coefficients are constrained via fits to the data. © 2023 CERN, for the CMS Collaboration."
80,79,52,79_wildlife_habitat_ecology_habitats,"wildlife,habitat,ecology,habitats,biodiversity,ecological,ecosystem,ecosystems,wetlands,wetland","Seagrass systems are in decline, mainly due to anthropogenic pressures and ongoing climate change. Implementing seagrass protection and restoration measures requires accurate assessment of suitable habitats. Commonly, such assessments have been performed using single-algorithm habitat suitability models, nearly always based on low environmental resolution information and short-term species data series. Here we address eelgrass (Zoostera marina) meadows’ large-scale decline (>80%) in Shandong province (Yellow Sea, China) by developing an ensemble habitat model (EHM) to inform eelgrass conservation and restoration strategies in the Swan Lake (SL). For this, we applied a weighted EHM derived from ten single-algorithm models including profile, regression, classification, and machine learning methods to generate a high-resolution habitat suitability map. The EHM was constructed based on the predictive performances of each model, by combining a series of present-absent eelgrass datasets from recent years coupled with oceanographic and sediment data. The model was cross-validated with independent historical datasets, and a final habitat suitability map for conservation and restoration was generated. Our EHM scheme outperformed all single models in terms of habitat suitability, scoring ?0.95 for both true statistic skill (TSS) and area under the curve (AUC) performance criteria. Machine learning methods outperformed profile, regression and classification methods. Regarding model explanatory variables, overall, topographic characteristics such as depth (DEP) and seafloor slope (SSL) are the most significant factors determining the distribution of eelgrass. The EHM predicted that the overlapping area was almost 90% of the current eelgrass habitat. Using results from our EHM, a LOESS regression model for the relationship of the habitat suitability to both the biomass and density of Z. marina outperformed better than the classic Ordinary Least Squares regression model. The EHM is a promising tool for supporting eelgrass protection and restoration areas in temperate lagoons as data availability improves. © 2022,Biodiversity loss in river ecosystems is much faster and more severe than in terrestrial systems, and spatial conservation and restoration plans are needed to halt this erosion. Reliable and highly resolved data on the state of and change in biodiversity and species distributions are critical for effective measures. However, high-resolution maps of fish distribution remain limited for large riverine systems. Coupling data from global satellite sensors with broad-scale environmental DNA (eDNA) and machine learning could enable rapid and precise mapping of the distribution of river organisms. Here, we investigated the potential for combining these methods using a fish eDNA dataset from 110 sites sampled along the full length of the Rhone River in Switzerland and France. Using Sentinel 2 and Landsat 8 images, we generated a set of ecological variables describing both the aquatic and the terrestrial habitats surrounding the river corridor. We combined these variables with eDNA-based presence and absence data on 29 fish species and used three machine-learning models to assess environmental suitability for these species. Most models showed good performance, indicating that ecological variables derived from remote sensing can approximate the ecological determinants of fish species distributions, but water-derived variables had stronger associations than the terrestrial variables surrounding the river. The species range mapping indicated a significant transition in the species occupancy along the Rhone, from its source in the Swiss Alps to outlet into the Mediterranean Sea in southern France. Our study demonstrates the feasibility of combining remote sensing and eDNA to map species distributions in a large river. This method can be expanded to any large river to support conservation schemes. © 2023 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London.,Aim: The increasing availability of animal tracking datasets collected across many sites provides new opportunities to move beyond local assessments to enable detailed and consistent habitat mapping at biogeographical scales. However, integrating wildlife datasets across large areas and study sites is challenging, as species' varying responses to different environmental contexts must be reconciled. Here, we compare approaches for large-area habitat mapping and assess available habitat for a recolonizing large carnivore, the Eurasian lynx (Lynx lynx). Location: Europe. Methods: We use a continental-scale animal tracking database (450 individuals from 14 study sites) to systematically assess modelling approaches, comparing (1) global strategies that pool all data for training versus building local, site-specific models and combining them, (2) different approaches for incorporating regional variation in habitat selection and (3) different modelling algorithms, testing nonlinear mixed effects models as well as machine-learning algorithms. Results: Testing models on training sites and simulating model transfers, global and local modelling strategies achieved overall similar predictive performance. Model performance was the highest using flexible machine-learning algorithms and when incorporating variation in habitat selection as a function of environmental variation. Our best-performing model used a weighted combination of local, site-specific habitat models. Our habitat maps identified large areas of suitable, but currently unoccupied lynx habitat, with many of the most suitable unoccupied areas located in regions that could foster connectivity between currently isolated populations. Main Conclusions: We demonstrate that global and local modelling strategies can achieve robust habitat models at the continental scale and that considering regional variation in habitat selection improves broad-scale habitat mapping. More generally, we highlight the promise of large wildlife tracking databases for large-area habitat mapping. Our maps provide the first high-resolution, yet continental assessment of lynx habitat across Europe, providing a consistent basis for conservation planning for restoring the species within its former range. © 2023 The Authors. Diversity and Distributions published by John Wiley & Sons Ltd."
81,80,51,80_crops_agriculture_agricultural_crop,"crops,agriculture,agricultural,crop,cropland,yield,prediction,predict,wheat,farmer","Food security has become a real challenge for some organizations in charge of the food program and for the majority of countries, especially African countries. The United Nations Organizations’ has recently defined the end of hunger and the improvement of food security in 2030 as its primary goal. Improving food security could also pass through the handling of agricultural yield. Agricultural yield is affected by climate changes since this latest decade. Climate change is considered one of the major threats to agricultural development in Africa. Decision-making level and farmers need efficient analytical tools to help them in decision making. Machine learning has become an impressive predictive analytical tool for large volume of data. It has been used in many domains such as medicine, finance, sport, and recently in agriculture. In this work, we propose three crop prediction models : Crop Random Forest, Crop Gradient Boosting Machine and Crop Support Vector Machine. We combine climate data, crop production data, and pesticides data to develop a decision system based on advanced machine learning models. Despite the poor availability of data related to agriculture in Africa, we were able to propose a decision system able to predict the crop yield at the country level in fourteen East African countries. Our experimental results show that the three proposed machine learning models fit well the crop data with a high accuracy R2. The Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE) associated to our models are very minimal because the agricultural prediction values are very close to reality. Our proposed models are reliable and generalize well the agricultural predictions in East Africa. © 2022,Simulations of crop yield due to climate change vary widely between models, locations, species, management strategies, and Representative Concentration Pathways (RCPs). To understand how climate and adaptation affects yield change, we developed a meta-model based on 8703 site-level process-model simulations of yield with different future adaptation strategies and climate scenarios for maize, rice, wheat and soybean. We tested 10 statistical models, including some machine learning models, to predict the percentage change in projected future yield relative to the baseline period (2000–2010) as a function of explanatory variables related to adaptation strategy and climate change. We used the best model to produce global maps of yield change for the RCP4.5 scenario and identify the most influential variables affecting yield change using Shapley additive explanations. For most locations, adaptation was the most influential factor determining the projected yield change for maize, rice and wheat. Without adaptation under RCP4.5, all crops are expected to experience average global yield losses of 6%–21%. Adaptation alleviates this average projected loss by 1–13 percentage points. Maize was most responsive to adaptive practices with a projected mean yield loss of ?21% [range across locations: ?63%, +3.7%] without adaptation and ?7.5% [range: ?46%, +13%] with adaptation. For maize and rice, irrigation method and cultivar choice were the adaptation types predicted to most prevent large yield losses, respectively. When adaptation practices are applied, some areas are predicted to experience yield gains, especially at northern high latitudes. These results reveal the critical importance of implementing adequate adaptation strategies to mitigate the impact of climate change on crop yields. © 2023 The Authors. Earth's Future published by Wiley Periodicals LLC on behalf of American Geophysical Union.,A farmer can use machine learning to make decisions about what crops to sow, how to care for those crops throughout the growing season, and how to predict crop yields. According to the World Health Organization, agriculture is essential to the nation’s quick economic development. Food security, access, and adoption are the three cornerstones of the organization. Without a doubt, the main priority is to ensure that there is enough food for everyone. Increasing agricultural yield can help ensure a sufficient supply. The country-wide variation in crop yields is substantial. As a result, this will be the foundation for research into whether cluster analysis can be used to identify crop yield patterns in a field. Previous study investigations were only marginally successful in accomplishing their primary intended objectives because of unstable conditions and imprecise methodology. The vast majority of farmers base their predictions of crop yield on prior observations of crop growth in their farms, which can be deceptive. Standard preprocessing methods and random cluster value selection are not always reliable, according to the literature. The proposed study overcomes the shortcomings of conventional methodology by highlighting the significance of machine learning-based classification/partitioning and hierarchical approaches in offering a trained analysis of yield prediction in the state of Karnataka. The dataset used for the study was collected from the ICAR-Taralabalu Krishi Vigyan Kendra, Davangere, Karnataka. In the two dataset analysis techniques employed in the study to find anomalies, crop area, and crop production are significant variables. Crop area and crop yield are important variables in the two dataset analysis methods used in the study to detect anomalies. The study emphasizes the importance of a mathematical model and algorithm for identifying yield trends, which can assist farmers in selecting crops that have a large seasonal impact on yield productivity. © 2023 by the authors."
82,81,51,81_distillation_knowledge_training_distill,"distillation,knowledge,training,distill,imagenet,distilled,teacher,learn,students,pretrained","Deep neural models have achieved remarkable performance on various supervised and unsupervised learning tasks, but it is a challenge to deploy these large-size networks on resource-limited devices. As a representative type of model compression and acceleration methods, knowledge distillation (KD) solves this problem by transferring knowledge from heavy teachers to lightweight students. However, most distillation methods focus on imitating the responses of teacher networks but ignore the information redundancy of student networks. In this article, we propose a novel distillation framework difference-based channel contrastive distillation (DCCD), which introduces channel contrastive knowledge and dynamic difference knowledge into student networks for redundancy reduction. At the feature level, we construct an efficient contrastive objective that broadens student networks&#x2019; feature expression space and preserves richer information in the feature extraction stage. At the final output level, more detailed knowledge is extracted from teacher networks by making a difference between multiview augmented responses of the same instance. We enhance student networks to be more sensitive to minor dynamic changes. With the improvement of two aspects of DCCD, the student network gains contrastive and difference knowledge and reduces its overfitting and redundancy. Finally, we achieve surprising results that the student approaches and even outperforms the teacher in test accuracy on CIFAR-100. We reduce the top-1 error to 28.16% on ImageNet classification and 24.15% for cross-model transfer with ResNet-18. Empirical experiments and ablation studies on popular datasets show that our proposed method can achieve state-of-the-art accuracy compared with other distillation methods. IEEE,Knowledge distillation is a simple yet effective technique for deep model compression, which aims to transfer the knowledge learned by a large teacher model to a small student model. To mimic how the teacher teaches the student, existing knowledge distillation methods mainly adapt an unidirectional knowledge transfer, where the knowledge extracted from different intermedicate layers of the teacher model is used to guide the student model. However, it turns out that the students can learn more effectively through multi-stage learning with a self-reflection in the real-world education scenario, which is nevertheless ignored by current knowledge distillation methods. Inspired by this, we devise a new knowledge distillation framework entitled multi-target knowledge distillation via student self-reflection or MTKD-SSR, which can not only enhance the teacher’s ability in unfolding the knowledge to be distilled, but also improve the student’s capacity of digesting the knowledge. Specifically, the proposed framework consists of three target knowledge distillation mechanisms: a stage-wise channel distillation (SCD), a stage-wise response distillation (SRD), and a cross-stage review distillation (CRD), where SCD and SRD transfer feature-based knowledge (i.e., channel features) and response-based knowledge (i.e., logits) at different stages, respectively; and CRD encourages the student model to conduct self-reflective learning after each stage by a self-distillation of the response-based knowledge. Experimental results on five popular visual recognition datasets, CIFAR-100, Market-1501, CUB200-2011, ImageNet, and Pascal VOC, demonstrate that the proposed framework significantly outperforms recent state-of-the-art knowledge distillation methods. © 2023, The Author(s).,Knowledge distillation is the technique of compressing a larger neural network, known as the teacher, into a smaller neural network, known as the student, while still trying to maintain the performance of the larger neural network as much as possible. Existing methods of knowledge distillation are mostly applicable for classification tasks. Many of them also require access to the data used to train the teacher model. To address the problem of knowledge distillation for regression tasks in the absence of original training data, the existing method uses a generator model trained adversarially against the student model to generate synthetic data to train the student model. In this study, we propose a new synthetic data generation strategy that directly optimizes for a large but bounded difference between the student and teacher model. Our results on benchmark experiments demonstrate that the proposed strategy allows the student model to learn better and emulate the performance of the teacher model more closely. © 2023 Elsevier Ltd"
83,82,51,82_spiking_neuron_neuromorphic_neurons,"spiking,neuron,neuromorphic,neurons,neuronal,neural,neuroscience,spiketimingdependent,synapses,spikebased","Neuromorphic Computing, a concept pioneered in the late 1980s, is receiving a lot of attention lately due to its promise of reducing the computational energy, latency, as well as learning complexity in artificial neural networks. Taking inspiration from neuroscience, this interdisciplinary field performs a multi-stack optimization across devices, circuits, and algorithms by providing an end-to-end approach to achieving brain-like efficiency in machine intelligence. On one side, neuromorphic computing introduces a new algorithmic paradigm, known as Spiking Neural Networks (SNNs), which is a significant shift from standard deep learning and transmits information as spikes (""1""or ""0"") rather than analog values. This has opened up novel algorithmic research directions to formulate methods to represent data in spike-trains, develop neuron models that can process information over time, design learning algorithms for event-driven dynamical systems, and engineer network architectures amenable to sparse, asynchronous, event-driven computing to achieve lower power consumption. On the other side, a parallel research thrust focuses on development of efficient computing platforms for new algorithms. Standard accelerators that are amenable to deep learning workloads are not particularly suitable to handle processing across multiple timesteps efficiently. To that effect, researchers have designed neuromorphic hardware that rely on event-driven sparse computations as well as efficient matrix operations. While most large-scale neuromorphic systems have been explored based on CMOS technology, recently, Non-Volatile Memory (NVM) technologies show promise toward implementing bio-mimetic functionalities on single devices. In this article, we outline several strides that neuromorphic computing based on spiking neural networks (SNNs) has taken over the recent past, and we present our outlook on the challenges that this field needs to overcome to make the bio-plausibility route a successful one.  © 2023 Copyright held by the owner/author(s).,Investigations in the field of spiking neural networks (SNNs) encompass diverse, yet overlapping, scientific disciplines. Examples range from purely neuroscientific investigations, researches on computational aspects of neuroscience, or applicative-oriented studies aiming to improve SNNs performance or to develop artificial hardware counterparts. However, the simulation of SNNs is a complex task that can not be adequately addressed with a single platform applicable to all scenarios. The optimization of a simulation environment to meet specific metrics often entails compromises in other aspects. This computational challenge has led to an apparent dichotomy of approaches, with model-driven algorithms dedicated to the detailed simulation of biological networks, and data-driven algorithms designed for efficient processing of large input datasets. Nevertheless, material scientists, device physicists, and neuromorphic engineers who develop new technologies for spiking neuromorphic hardware solutions would find benefit in a simulation environment that borrows aspects from both approaches, thus facilitating modeling, analysis, and training of prospective SNN systems. This manuscript explores the numerical challenges deriving from the simulation of spiking neural networks, and introduces SHIP, Spiking (neural network) Hardware In PyTorch, a numerical tool that supports the investigation and/or validation of materials, devices, small circuit blocks within SNN architectures. SHIP facilitates the algorithmic definition of the models for the components of a network, the monitoring of states and output of the modeled systems, and the training of the synaptic weights of the network, by way of user-defined unsupervised learning rules or supervised training techniques derived from conventional machine learning. SHIP offers a valuable tool for researchers and developers in the field of hardware-based spiking neural networks, enabling efficient simulation and validation of novel technologies. Copyright © 2024 Gemo, Spiga and Brivio.,With the help of Artificial Neural Networks?ANNs?? deep reinforcement learning algorithms have achieved great success in complex tasks? such as playing games and robotic control?etc. However?compared with the mechanism of reward-modulated learning in the brain?deep reinforcement learning still has a huge gap in cognitive ability and computational efficiency. Inspired by the spike-driven communication in the brain?Spiking Neural Networks?SNNs?adopt the spiking neuronal models for calculation and exchange information through discrete action potentials?i. e.?spikes?which greatly fit the mechanism of biological neurons. It is demonstrated by much research that SNNs have distinctive properties?such as complex time-series information processing capability? extremely low energy consumption? and strong robustness. In addition?SNNs also show the potential for continual learning. In the field of neuromorphic engineering and brain-inspired computing? SNNs are widely concerned and known as a new generation of neural networks. By combining SNNs with reinforcement learning? spiking reinforcement learning algorithms are considered as a feasible way to develop the artificial brain? and can effectively explain the discovery in the biological brain. As a cross-discipline research area of neuroscience and artificial intelligence? spiking reinforcement learning algorithms cover a large number of outstanding research works. According to the emphasis on different fields? these research works can be divided into two categories?one is to better understand the mechanism of reward-modulated learning in the brain? which is used to explain the findings in animal experiments and simulate the learning process of the brain? such as R-STDP learning rules?The other is to improve the performance? energy efficiency and other specific indicators of various control tasks requiring reinforcement learning algorithms to solve. With the unique advantages of SNNs? this kind of algorithm acts as a robust and energy-efficient solution for artificial intelligence?and shows great application potential in the fields of robotics and autonomous control. In this paper?the first part presents the cornerstone of spiking reinforcement learning algorithms?that is?spiking neural networks and reinforcement learning? and then analyzes the research characteristics and progress of these two kinds of spiking reinforcement learning algorithms. For the first kind of algorithms?this paper focuses on analyzing the reinforcement learning algorithms using the three-factor learning rules? and introduces their physiological background and specific implementation methods. Based on whether to use ANNs during training? this paper further divides the second kind of algorithms into spiking reinforcement learning algorithms using ANNs and spike-based direct reinforcement learning algorithms. As far as we know? this paper takes the lead in systematically sorting out and analyzing the latest progress of spiking reinforcement learning algorithms? and comprehensively shows the different ways of applying SNNs in deep reinforcement learning algorithms. Finally?this paper makes an in-depth discussion of the current challenges and follow-up research directions in this field. We systematically summarize the advantages and disadvantages of the current research?and look forward to its future impact on the field of neuroscience and artificial intelligence? so as to attract more researchers to participate in communication and cooperation in this new direction. © 2023 Science Press. All rights reserved."
84,83,49,83_porosity_permeability_porescale_multiscale,"porosity,permeability,porescale,multiscale,pores,flow,reservoirs,pore,porous,hydraulic","This study presents a workflow to predict the upscaled absolute permeability of the rock core direct from CT images whose resolution is not sufficient to allow direct pore-scale permeability computation. This workflow exploits the deep learning technique with the data of raw CT images of rocks and their corresponding permeability value obtained by performing flow simulation on high-resolution CT images. The permeability map of a much larger region in the rock core is predicted by the trained neural network. Finally, the upscaled permeability of the entire rock core is calculated by the Darcy flow solver, and the results showed a good agreement with the experiment data. This proposed deep learning based upscaling method allows estimating the permeability of large-scale core samples while preserving the effects of fine-scale pore structure variations due to the local heterogeneity. © 2023. American Geophysical Union. All Rights Reserved.,Data-driven deep learning models are emerging as a promising method for characterizing pore-scale flow through complex porous media while requiring minimal computational power. However, previous models often require extensive computation to simulate flow through synthetic porous media for use as training data. We propose a convolutional neural network trained solely on periodic unit cells to predict pore-scale velocity fields of complex heterogeneous porous media from binary images without the need for further image processing. Our model is trained using a range of simple and complex unit cells that can be obtained analytically or numerically at a low computational cost. Our results show that the model accurately predicts the permeability and pore-scale flow characteristics of synthetic porous media and real reticulated foams. We significantly improve the convergence of numerical simulations by using the predictions from our model as initial guesses. Our approach addresses the limitations of previous models and improves computational efficiency, enabling the rigorous characterization of large batches of complex heterogeneous porous media for a variety of engineering applications. © 2023 Author(s).,Predicting permeability over a wide range of thermodynamic and geometric conditions is necessary to understand the transport of fluid through porous materials. Permeability is typically calculated over a representative elementary volume (REV), which is the minimum volume that captures all of the inherent features of the porous material. However, there are many instances where the representative volume is so large that permeability must be computed on a volume smaller than an REV. Furthermore, when the flow in the pores occurs in the non-continuum regime, in addition to the dependence on geometric configuration, permeability is also influenced by the temperature and pressure of the gases flowing through the pores. In these cases, permeability must be computed over a multidimensional nonlinear space of length scales, temperature, and pressure. The current state-of-the-art approach is to compute permeability for a limited set of conditions and interpolate across the multidimensional space, which leads to errors because of the nonlinear dependency of permeability on geometric and flow features of the porous material. Alternatively, permeability could be numerically computed in real time for a given set of thermodynamic and geometric parameters, but it would be computationally prohibitive. To overcome these issues, a supervised learning model capable of capturing the nonlinear relationship of permeability with geometric and fluid flow parameters has been developed. The supervised learning model maps the complex features of the multidimensional geometric and flow parameters to an analytical function that can be used to predict the permeability of the porous material under investigation. Additionally, since the function is both continuous and differentiable, it can be easily incorporated into tools that are used for design and analysis of porous systems. The performance of the newly developed model is evaluated by computing the root-mean-square train and test errors indicating no over-fit or under-fit of the model to the dataset. The permeability model is further validated against experimental data to justify the use of the model as an accurate, cost-effective alternative for determining permeability of porous materials. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
85,84,48,84_forecast_forecasting_forecasts_hydrological,"forecast,forecasting,forecasts,hydrological,rainfall,runoff,precipitation,streamflow,rainfallrunoff,catchment","Rainfall-runoff modeling is a complex nonlinear spatiotemporal prediction problem. However, few studies have considered the spatial characteristics of rainfall-runoff relationship in runoff forecasts based on machine learning. With the emergence of high-resolution Satellite-based Precipitation Products (SPPs) and the continuous improvement of rainfall estimation accuracy, the shortcoming of sparse spatial information for in-situ rainfall monitoring has been made up. Therefore, this study developed a large scale spatiotemporal deep learning rainfall-runoff (SDLRR) forecasting model for hydrological stations in the upper Yangtze River, and evaluated the positive impact of utilizing spatial information of three SPPs on reducing errors of runoff forecasts. The adopted remote sensing precipitation products are bias-corrected Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS), Integrated Multi-satellite Retrievals for Global Precipitation Measurement data (IMERG) and Tropical Rainfall Measuring Mission Multi-satellite Precipitation Analysis data (TMPA). For runoff forecasting at the Luoduxi (LDX) hydrological station, compared to regular Long Short Term Memory Network (LSTM) model, the proposed SDLRR model that utilizing IMERG data as precipitation input (IMERG_SDLRR) improved 15% in terms of Coefficient of Determination (R2) and improved 25% in terms of Root Mean Squared Error (RMSE). Compared to the best performance model among models using area-averaged precipitation as input, IMERG_SDLRR improved 5% in terms of R2 and 11% in terms of RMSE. Good performance was also acquired in the other hydrological stations. For extreme flood forecasts, IMERG_SDLRR decreased Mean Relative Error (MRE) by 0.29 and increased Qualified Rate (QR) by 53% compared to LSTM, and decreased MRE by 0.08 and increased QR by 6% compared to the best performance model using area-averaged precipitation as input. The utilization of IMERG or TMPA spatial information improved the accuracy of runoff forecasting. The accuracy evaluation of SPPs based on the results of spatiotemporal rainfall-runoff forecasts method was also demonstrated. The research is of great significance for developing runoff forecasting methods and optimizing water resources management. © 2022,Deep learning has been widely used in hydrological prediction such as monthly streamflow and its performance is usually dependent on the abundance of training data. Even though the interest in using predictors from multiple data sources (e.g., streamflow observations, local meteorological data, and large-scale climate indexes) to train deep learning models for monthly streamflow prediction is growing, these predictors are usually selected from historical periods. Such approaches have limitations that the non-stationary future climate information is not included in the deep learning models. Climate models can provide non-stationary climate information for the future period, which may be useful for monthly streamflow prediction. Therefore, the objectives of this study are to (1) investigate the added value of using predictors derived from global climate models (GCMs) for monthly streamflow prediction based on a state-of-the-art deep learning model, and (2) propose a framework for integrating heterogeneous data sources for monthly streamflow prediction. The framework consists of five integral components: data collection, predictor combination, predictor selection, model construction, and results evaluation. In this study, a hybrid deep learning model combining Convolutional Neural Network and Gated Recurrent Unit was applied for six hydrological stations from mainstream and six stations from tributaries of the Yangtze River. Historical hydroclimate data and GCM hindcasts from 1982 to 2010 are used in monthly streamflow forecasts. Hindcasts are retrospective forecasts for many variables in the past, ideally conducted using the same model used for real-time forecasts. The results show that GCM hindcasts are useful predictors to improve the prediction accuracy for monthly streamflow predictions, especially for the 1- and 3-month lead times. Combining GCM hindcasts with either historical meteorological data or historical streamflow observations and meteorological data as predictors generally provides the best predictive performance. In addition, using large-scale climate indexes as ancillary information is able to improve the predictive performance at a lead time of 6 months. For lead times of 1, 3, and 6 months, the Kling?Gupta efficiency (KGE) and the mean relative error (MRE)) metrics calculated based on the best-performing predictor combinations are satisfactory for hydrological stations in both mainstream and tributaries, with the median KGE being higher than 0.85 and 0.62, and the median MRE being approximately 20 % and 40 %, respectively, suggesting the monthly streamflow predictions are better for mainstream than for tributaries. Overall results show that (1) the inclusion of predicted information from GCMs can improve the performance for monthly streamflow prediction, and (2) the way of combining various heterogeneous data sources is crucial. © 2022,Medium-range streamflow forecasts largely depend on the accuracy of meteorological forecasts. Due to large errors in precipitation forecasts, most streamflow forecasts based on deep learning rely only on historical data. Here, we apply a cascade Long Short-Term Memory (LSTM) model to forecast daily streamflow over 49 watersheds in the Yangtze River basin for up to 15 days. The first layer of the cascade LSTM model uses atmospheric circulation factors to predict future precipitation, and the second layer uses forecast precipitation to predict streamflow. The results show that the default LSTM model provides skillful streamflow forecasts over most watersheds. At the lead times of 1, 7, and 15 days, the streamflow Kling–Gupta efficiency (KGE) of 78%, 30%, and 20% watersheds are greater than 0.5, respectively. Its performance improves with the increase in drainage area. After implementing the cascade LSTM model, 61–88% of the watersheds show increased KGE at different leads, and the increase is more obvious at longer leads. Using cascade LSTM with perfect future precipitation shows further improvement, especially over small watersheds. In general, cascade LSTM modeling is a good attempt for streamflow forecasts over the Yangtze River, and it has a potential to connect with dynamical meteorological forecasts. © 2023 by the authors."
86,85,48,85_landslide_landslides_deeplabv3_terrain,"landslide,landslides,deeplabv3,terrain,landforms,convolutional,sensing,deep,flood,floods","Loess landslides are one of the geological hazards prevalent in mountainous areas of Loess Plateau, seriously threatening people’s lives and property safety. Accurate identification of landslides is a prerequisite for reducing the risk of landslide hazards. Traditional landslide interpretation methods often have the disadvantage of being laborious and difficult to use on a large scale compared with the recently developed deep learning-based landslide detection methods. In this study, we propose an improved deep learning model, landslide detection- you only look once (LD-YOLO), based on the existing you only look once (YOLO) model for the intelligent identification of old and new landslides in loess areas. Specifically, remote sensing images of landslides in Baoji City, Shaanxi Province, China are acquired from the Google Earth Engine platform. The landslide images of Baoji City (excluding Qianyang County) are used to establish a loess landslide dataset for training the model. The landslide data of Qianyang County is used to verify the detection performance of the model. The focal and efficient IoU (Focal-EIoU) loss function and efficient channel attention (ECA) mechanism are incorporated into the 7th version of YOLO (YOLOv7) model to construct the LD-YOLO model, which makes it more suitable for the landslide detection task. The experiments yielded an improved LD-YOLO model with average precision of 92.05%, precision of 92.31%, recall of 90.28%, and F1-score of 91.28% for loess landslide detection. The landslides in Qianyang County were divided into two test sets, new landslides and old landslides, which were used to test the detection performance of LD-YOLO for both types of landslides. The results show that LD-YOLO detects old landslides with a detection precision of 82.75% and a recall of 80%. When detecting new landslides, the detection precision is 94.29% and the recall is 91.67%. It indicates that our proposed LD-YOLO model has strong detection performance for both new and old landslides in loess areas. Through a proposed solution that can realize the accurate detection of landslides in loess areas, this paper provides a valuable reference for the application of deep learning methods in landslide identification. © 2023, Science Press, Institute of Mountain Hazards and Environment, CAS and Springer-Verlag GmbH Germany, part of Springer Nature.,Loess landslides pose a severe threat of destruction, and detecting them is crucial for minimizing their impact on society. They typically consist of wind-deposited clay and silt, which makes them challenging to detect using conventional methods. Techniques like visual interpretation and field surveys are the most useful, yet these methods can be laborious, expensive, and require a certain level of prior knowledge. Furthermore, remote sensing approaches face the challenge of distinguishing between natural erosion and landslides. Recently, deep learning for landslide detection has the potential to speed up and improve detection accuracy. Nonetheless, deep learning models require large amounts of labeled data and the development of robust algorithms capable of extracting meaningful features from remote sensing data. In this article, a novel approach is introduced to improve the Mask Regional Convolutional Neural Network (Mask-RCNN) algorithm for accurately detecting landslides when the number of available segmentation mask samples is limited. Specifically, A novel loess landslides dataset is established using high-resolution remote sensing images in Gansu Province. In the context of partially supervised learning, a neural network branch containing a weight transfer function is developed to capture mask information from bounding boxes. Additionally, a mask-scoring network block is used to learn the quality of predicted instance masks. Our modified algorithm achieves an average precision improvement of 20.7% compared to the original Mask R-CNN algorithm in small landslide detection. The mask IoU threshold value of 0.5 is used to estimate the average accuracy higher than 0.75. The average precision of the segmentation mask is improved by 16.7% in test set. By proposing a solution that can achieve accurate landslide detection while using limited labeled samples, this study makes a valuable contribution to the application of deep learning in the domains of remote sensing and landslide detection. © 2023 Elsevier B.V.,Accurate and timely landslide mapping plays a critical role in emergency response and long-term land use planning. Deep learning–based methods represented by convolutional neural networks have been widely exploited in automatic landslide detection for their outstanding capability of feature representation and end-to-end learning mode. Most of the recent deep learning–based studies used toll-access high-resolution imagery for landslide detection. Considering demands for the future large-scale landslide mapping, this study aims to develop a new deep learning–based method to detect landslides using medium-resolution imagery and digital elevation model (DEM) data which are free-access and covered globally. Firstly, a workflow for constructing the landslide dataset is developed. Then, we design a semantic segmentation model to learn deep features and generate per-pixel landslide predictions. Specifically, the proposed network has a dual-encoder architecture with feature fusion to hierarchically represent deep features from the optical bands and DEM data. We also employ a self-attention module in the decoder of the proposed network to improve the performance. Experiments on two regions demonstrate that our method achieves the best F1 score of 79.24%, outperforming SegNet, U-Net, and Attention U-Net, the models popularly used in the semantic segmentation–based landslide detection. The proposed method may have an application potential in disaster risk assessment and post-disaster reconstruction and provide a technical reference for the large-scale landslide mapping in the future. © 2023, Springer-Verlag GmbH Germany, part of Springer Nature."
87,86,47,86_tweets_twitter_sentiment_outbreak,"tweets,twitter,sentiment,outbreak,tweet,pandemic,sentiments,vaccine,vaccination,bots","When public health emergencies occur, a large amount of low-credibility information is widely disseminated by social bots, and public sentiment is easily manipulated by social bots, which may pose a potential threat to the public opinion ecology of social media. Therefore, exploring how social bots affect the mechanism of information diffusion in social networks is a key strategy for network governance. This study combines machine learning methods and causal regression methods to explore how social bots influence information diffusion in social networks with theoretical support. Specifically, combining stakeholder perspective and emotional contagion theory, we proposed several questions and hypotheses to investigate the influence of social bots. Then, the study obtained 144,314 pieces of public opinion data related to COVID-19 in J city from March 1, 2022, to April 18, 2022, on Weibo, and selected 185,782 pieces of data related to the outbreak of COVID-19 in X city from December 9, 2021, to January 10, 2022, as supplement and verification. A comparative analysis of different data sets revealed the following findings. Firstly, through the STM topic model, it is found that some topics posted by social bots are significantly different from those posted by humans, and social bots play an important role in certain topics. Secondly, based on regression analysis, the study found that social bots tend to transmit information with negative sentiments more than positive sentiments. Thirdly, the study verifies the specific distribution of social bots in sentimental transmission through network analysis and finds that social bots are weaker than human users in the ability to spread negative sentiments. Finally, the Granger causality test is used to confirm that the sentiments of humans and bots can predict each other in time series. The results provide practical suggestions for emergency management under sudden public opinion and provide a useful reference for the identification and analysis of social bots, which is conducive to the maintenance of network security and the stability of social order. © 2022,Since the spread of the coronavirus flu in 2019 (hereafter referred to as COVID-19), millions of people worldwide have been affected by the pandemic, which has significantly impacted our habits in various ways. In order to eradicate the disease, a great help came from unprecedentedly fast vaccines development along with strict preventive measures adoption like lockdown. Thus, world wide provisioning of vaccines was crucial in order to achieve the maximum immunization of population. However, the fast development of vaccines, driven by the urge of limiting the pandemic caused skeptical reactions by a vast amount of population. More specifically, the people’s hesitancy in getting vaccinated was an additional obstacle in fighting COVID-19. To ameliorate this scenario, it is important to understand people’s sentiments about vaccines in order to take proper actions to better inform the population. As a matter of fact, people continuously update their feelings and sentiments on social media, thus a proper analysis of those opinions is an important challenge for providing proper information to avoid misinformation. More in detail, sentiment analysis (Wankhade et al. in Artif Intell Rev 55(7):5731–5780, 2022. https://doi.org/10.1007/s10462-022-10144-1) is a powerful technique in natural language processing that enables the identification and classification of people feelings (mainly) in text data. It involves the use of machine learning algorithms and other computational techniques to analyze large volumes of text and determine whether they express positive, negative or neutral sentiment. Sentiment analysis is widely used in industries such as marketing, customer service, and healthcare, among others, to gain actionable insights from customer feedback, social media posts, and other forms of unstructured textual data. In this paper, Sentiment Analysis will be used to elaborate on people reaction to COVID-19 vaccines in order to provide useful insights to improve the correct understanding of their correct usage and possible advantages. In this paper, a framework that leverages artificial intelligence (AI) methods is proposed for classifying tweets based on their polarity values. We analyzed Twitter data related to COVID-19 vaccines after the most appropriate pre-processing on them. More specifically, we identified the word-cloud of negative, positive, and neutral words using an artificial intelligence tool to determine the sentiment of tweets. After this pre-processing step, we performed classification using the BERT + NBSVM model to classify people’s sentiments about vaccines. The reason for choosing to combine bidirectional encoder representations from transformers (BERT) and Naive Bayes and support vector machine (NBSVM) can be understood by considering the limitation of BERT-based approaches, which only leverage encoder layers, resulting in lower performance on short texts like the ones used in our analysis. Such a limitation can be ameliorated by using Naive Bayes and Support Vector Machine approaches that are able to achieve higher performance in short text sentiment analysis. Thus, we took advantage of both BERT features and NBSVM features to define a flexible framework for our sentiment analysis goal related to vaccine sentiment identification. Moreover, we enrich our results with spatial analysis of the data by using geo-coding, visualization, and spatial correlation analysis to suggest the most suitable vaccination centers to users based on the sentiment analysis outcomes. In principle, we do not need to implement a distributed architecture to run our experiments as the available public data are not massive. However, we discuss a high-performance architecture that will be used if the collected data scales up dramatically. We compared our approach with the state-of-art methods by comparing most widely used metrics like Accuracy, Precision, Recall and F-measure. The proposed BERT + NBSVM outperformed alternative models by achieving 73% accuracy, 71% precision, 88% recall and 73% F-measure for classification of positive sentiments while 73% accuracy, 71% precision, 74% recall and 73% F-measure for classification of negative sentiments respectively. These promising results will be properly discussed in next sections. The use of artificial intelligence methods and social media analysis can lead to a better understanding of people’s reactions and opinions about any trending topic. However, in the case of health-related topics like COVID-19 vaccines, proper sentiment identification could be crucial for implementing public health policies. More in detail, the availability of useful findings on user opinions about vaccines can help policymakers design proper strategies and implement ad-hoc vaccination protocols according to people’s feelings, in order to provide better public service. To this end, we leveraged geospatial information to support effective recommendations for vaccination centers. © 2023, The Author(s).,Introduction: The development of COVID-19 vaccines has been a great relief in many countries that have been affected by the pandemic. As a result, many governments have made significant efforts to purchase and administer vaccines to their populations. However, accommodating such vaccines is typically confronted with people’s reluctance and fear. Like any other important event, COVID-19 vaccines have attracted people’s discussions on social media and impacted their opinions about vaccination. Objective: The goal of this study is twofold: First, it conducts a sentiment analysis around COVID-19 vaccines by automatically analyzing Arabic users’ tweets. This analysis has been spread over time to better capture the changes in vaccine perceptions. This will provide us with some insights into the most popular and accepted vaccine(s) in the Arab countries, as well as the reasons behind people’s reluctance to take the vaccine. Second, it develops models to detect any vaccine-related tweets, to help with gathering all information related to people’s perception of the virus, and potentially detecting vaccine-related tweets that are not necessarily tagged with the virus’s main hashtags. Methods: Arabic Tweets were collected by the authors, starting from January 1st, 2021, until April 20th, 2021. We deployed various Natural Language Processing (NLP) to distill our selected tweets. The curated dataset included in the analysis consisted of 1,098,376 unique tweets. To achieve the first goal, we designed state-of-the-art sentiment analysis techniques to extract knowledge related to the degree of acceptance of all existing vaccines and what are the main obstacles preventing the wide audience from accepting them. To achieve the second goal, we tackle the detection of vaccine-related tweets as a binary classification problem, where various Machine Learning (ML) models were designed to identify such tweets regardless of whether they use the vaccine hashtags or not. Results: Generally, we found that the highest positive sentiments were registered for Pfizer-BioNTech, followed by Sinopharm-BIBP and Oxford-AstraZeneca. In addition, we found that 38% of the overall tweets showed negative sentiment, and only 12% had a positive sentiment. It is important to note that the majority of the sentiments vary between neutral and negative, showing the lack of conviction of the importance of vaccination among the large majority of tweeters. This paper extracts the top concerns raised by the tweets and advocates for taking them into account when advertising for the vaccination. Regarding the identification of vaccine-related tweets, the Logistic Regression model scored the highest accuracy of 0.82. Our findings are concluded with implications for public health authorities and the scholarly community to take into account to improve the vaccine’s acceptance. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature."
88,87,47,87_multispectral_lakes_lake_aquatic,"multispectral,lakes,lake,aquatic,catchment,waters,pollution,basin,landsat,ecosystems","Total phosphorus (TP) and total nitrogen (TN) reflect the state of eutrophication. However, traditional point-based water quality monitoring methods are time-consuming and labor-intensive, and insufficient to estimate and assess water quality at a large scale. In this paper, we constructed machine learning models for TP and TN inversion using measured data and satellite imagery band reflectance, and verified it by in situ data. Atmospheric correction was performed on the Landsat Top of Atmosphere (TOP) data by removing the effect of the adjacency effect and correcting differences between Landsat sensors. Then, using the established model, the TP and TN patterns in Dongting Lake with a spatial resolution of 30 m from 1996 to 2021 were derived for the first time. The annual and monthly spatio-temporal variation characteristics of TP and TN in Dongting Lake were investigated in details, and the influences of hydrometeorological elements on water quality variations were analyzed. The results show that the established empirical model can accurately estimate TP with coefficient (R2) ? 0.70, root mean square error (RMSE) ? 0.057 mg/L, mean relative error (MRE) ? 0.23 and TN with R2 ? 0.73, RMSE ? 0.48 mg/L and MRE ? 0.20. From 1996 to 2021, TP in Dongting Lake showed a downward trend and TN showed an upward trend, while the summer value was much higher than the other seasons. Furthermore, the influencing factors on TP and TN variations were investigated and discussed. Between 1996 and 2003, the main contributors to the change of water quality in Dongting Lake were external inputs such as water level and flow. The significant changes in water quantity and sediment characteristics following the operation of the Three Gorges Dam (TGD) in 2003 also had an impact on the water quality in Dongting Lake. © 2022 by the authors.,Dissolved oxygen (DO) concentration is a widely used and effective indicator for assessing water quality and pollution in aquatic environments. Continuous and large-scale inversion of water environments using remote sensing imagery has become a hot topic in water environmental research. Remote sensing technology has been extensively applied in water quality monitoring, but its limited sampling frequency necessitates the development of a high-frequency dynamic water quality monitoring model. In this study, we utilized Lake Chaohu as a case study. Firstly, we constructed a dynamic water quality inversion model for monitoring DO concentrations using machine learning methods, with Himawari-8 (H8) satellite imagery as input data and DO concentrations in Lake Chaohu as output data. Secondly, the developed DO concentration inversion model was employed to estimate the overall grid-based DO concentration in the Lake Chaohu region for the years 2019 to 2021. Lastly, Pearson correlation analysis and significance tests were performed to examine the correlation and significance between the estimated grid-based DO concentration and the ERA5 reanalysis dataset. The results demonstrate that the Random Forest (RF) model performs best in DO concentration inversion, with a high R2 score of 0.84, and low RMSE and MAE values of 0.69 and 0.54, respectively. Compared to other models, the RF model improves average performance with a 38% increase in R2, 13% decrease in RMSE, and 33% decrease in MAE. The model accurately predicts DO concentrations. Furthermore, the inversion results reveal seasonal differences in DO concentrations in Lake Chaohu from 2019 to 2021, with higher concentrations in spring and winter, and lower concentrations in summer and autumn. The average DO concentrations in the northwest, central-south, and northeast regions of Lake Chaohu are 10.12 mg/L, 9.98 mg/L, and 9.96 mg/L, respectively, with higher concentrations in the northwest region. Pearson correlation analysis indicates a significant correlation (p < 0.01) between DO concentrations and temperature, surface pressure, latent heat flux from the atmosphere to the surface, and latent heat flux from the surface to the atmosphere, with correlation coefficients of ?0.615, 0.583, ?0.480, and 0.444, respectively. The results verify the feasibility of using synchronous satellites for real-time inversion of DO concentrations, providing a more efficient, economical, and accurate means for real-time monitoring of DO concentrations. This study has practical value in improving the efficiency and accuracy of water environmental monitoring. © 2023 by the authors.,Water quality monitoring of medium-sized inland water is important for water environment protection given the large number of small-to-medium size water bodies in China. A case study was conducted on Yuandang Lake in the Yangtze Delta region, with a surface area of 13 km2. This study proposed utilising a multispectral uncrewed aerial vehicle (UAV) to collect large-scale data and retrieve multiple water quality parameters using machine learning algorithms. An alternate processing method is proposed to process large and repetitive lake surface images for mapping the water quality data to the image. Machine learning regression methods (Random Forest, Gradient Boosting, Backpropagation Neural Network, and Convolutional Neural Network) were used to construct separate water quality inversion models for ten water parameters. The results showed that several water quality parameters (CODMn, temperature, pH, DO, and NC) can be retrieved with reasonable accuracy (R2 = 0.77, 0.75, 0.73, 0.67, and 0.64, respectively), although others (NH3-N, BGA, TP, Turbidity, and Chl-a) have a determination coefficient (R2) less than 0.6. This work demonstrated the tremendous potential of employing multispectral data in conjunction with machine learning algorithms to retrieve multiple water quality parameters for monitoring medium-sized bodies of water. © 2023 by the authors."
89,88,47,88_fish_fisheries_aquatic_recognition,"fish,fisheries,aquatic,recognition,aquaculture,classify,plankton,classification,waters,dataset","Fish assessment and monitoring are important for the development of a modern aquatic ecosystem. Fish are a vital part of the marine and freshwater environments. Morphological and computational details of fish, such as size, shape, and position, are important in fish observation and fisheries. Typically, manual, or low-efficient techniques are used to acquire fish details. However, existing typical methods are usually time-consuming, less accurate, and resource-intensive. Computer-aided methods are crucial for intelligent and automatic fish assessment. Two novel networks, namely parallel feature fusion-based segmentation network (PFFS-Net) and progressive information fusion-based segmentation network (PIFS-Net), were developed for pixel-wise fish segmentation. PFFS-Net is a base network that uses parallel feature fusion to achieve a better segmentation performance. PIFS-Net is the final model of this work and uses a progressive spatial feature fusion (SFF) mechanism to enhance segmentation accuracy. PIFS-Net also employs rapid feature reduction and pre-prediction low-level information fusion blocks to further boost performance. The proposed models were evaluated using the following three publicly available databases: semantic segmentation of underwater imagery (SUIM), DeepFish, and Large-scale fish. The proposed networks outperformed the state-of-the-art methods in challenging underwater conditions with superior computational efficiency. PIFS-Net needs only 2.02 million trainable parameters for its complete training. Automatic and accurate fish segmentation can be a major step towards an intelligent aquatic ecosystem. The codes of our algorithms and trained models are available on Github. © 2023 The Author(s),The classification of underwater fish species holds significant importance for fisheries management. Nevertheless, existing deep fish classification models require high computational resources, which hamper their deployment on underwater devices. Additionally, the complex underwater environment, the camouflaged appearance of fish, and the similarity among fish species pose challenges to the accuracy of lightweight fish classification models. To address the above issues, this paper proposes a novel two-tier knowledge distillation (T-KD) method to improve the accuracy and reduce the parameters of the underwater fish species classification models. Specifically, the T-KD involves the following key steps. Firstly, a new fish species dataset, Fish37, is constructed to augment the diversity of fish species present in existing datasets. Subsequently, we introduce a novel interlayer mapping similarity-preserving (IMSP), to facilitate the learning of richer discriminative features by capturing the mapping relationships between teacher and student network layers. Moreover, a new layer tail response (LTR) is proposed to mimic the predictions of the teacher network, efficiently improving classification performance and generalization capability. The proposed T-KD approach demonstrates remarkable performance in fish species classification, surpassing that of well-known lightweight models. The effectiveness of T-KD is extensively validated across various network depths, including ResNet and EfficientNet, and compared to other knowledge distillation methods like KD, PKT, RKD, and SP. Notably, T-KD outperforms MobileNetv3-large and obtains an impressive Top-1 accuracy of 97.20% on the Fish37 dataset only using about 1/15 model size of Vision Transformer. Furthermore, detailed generalization experiments are conducted to assess T-KD’s performance on popular benchmark datasets, such as A_Large_Scale_Fish_Dataset, Fish4knowledge, WildFish and WildFish++. In conclusion, the results indicate the potential of the T-KD to facilitate underwater fish species classification with limited computational resources on underwater devices. This research also opens up promising avenues for the practical implementation of lightweight fish classification models. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.,Accurate fish individual recognition is one of the critical technologies for large-scale fishery farming when trying to achieve accurate, green farming and sustainable development. It is an essential link for aquaculture to move toward automation and intelligence. However, existing fish individual data collection methods cannot cope with the interference of light, blur, and pose in the natural underwater environment, which makes the captured fish individual images of poor quality. These low-quality images can cause significant interference with the training of recognition networks. In order to solve the above problems, this paper proposes an underwater fish individual recognition method (FishFace) that combines data quality assessment and loss weighting. First, we introduce the Gem pooing and quality evaluation module, which is based on EfficientNet. This module is an improved fish recognition network that can evaluate the quality of fish images well, and it does not need additional labels; second, we propose a new loss function, FishFace Loss, which will weigh the loss according to the quality of the image so that the model focuses more on recognizable fish images, and less on images that are difficult to recognize. Finally, we collect a dataset for fish individual recognition (WideFish), which contains and annotates 5000 images of 300 fish. The experimental results show that, compared with the state-of-the-art individual recognition methods, Rank1 accuracy is improved by 2.60% and 3.12% on the public dataset DlouFish and the proposed WideFish dataset, respectively. © 2023 by the authors."
90,89,47,89_activity_cnn_activities_recognition,"activity,cnn,activities,recognition,recognizing,wearable,features,deepconvlstm,classifier,classification","Recently, Human Activity Recognition (HAR) is becoming one of the prevalent study fields. HAR is a powerful tool for monitoring a person's dynamism, and it can be accomplished through machine learning (ML) techniques. HAR is a technique of automatically analysing and recognizing human activities depending on information from several wearable devices and smartphone sensors, like location, accelerometer, gyroscope, duration, and other environmental sensors. This study introduces a new Robust Human Activity Recognition using Equilibrium Optimizer with Deep Extreme Learning Machine (RHAR-EODELM) model. The presented RHAR-EODELM technique mainly identifies different classes of human activities. It follows a three-stage process. Initially, the RHAR-EODELM technique employs a min-max normalization process for scaling the activity data. Next, the RHAR-EODELM technique exploits a deep extreme learning machine with a radial basis function (DELM-RBF) model for the prediction process. Finally, the EO approach is enforced to adjust the parameters associated with the DELM-RBF method. A large-scale simulating process highlights the improved HAR results of the RHAR-EODELM method. The experimental values signify that the RHAR-EODELM method reaches improved predictive outcomes over other models. © 2023 Seventh Sense Research Group.,Human Activity Recognition (HAR) has been proven to be effective in various healthcare and telemonitoring applications. Current HAR methods, especially deep learning, are extensively employed owing to their exceptional recognition capabilities. However, in pursuit of enhancing feature expression abilities, deep learning often introduces a trade-off by increasing Time complexity. Moreover, the intricate nature of human activity data poses a challenge as it can lead to a notable decrease in recognition accuracy when affected by additional noise. These aspects will significantly impair recognition performance. To advance this field further, we present a HAR method based on an edge-computing-assisted and GRU deep-learning network. We initially proposed a model for edge computing to optimize the energy consumption and processing time of wearable devices. This model transmits HAR data to edge-computable nodes, deploys analytical models on edge servers for remote training, and returns results to wearable devices for processing. Then, we introduced an initial convolution method to preprocess large amounts of training data more effectively. To this end, an attention mechanism was integrated into the network structure to enhance the analysis of confusing data and improve the accuracy of action classification. Our results demonstrated that the proposed approach achieved an average accuracy of 85.4% on the 200 difficult-to-identify HAR data, which outperforms the Recurrent Neural Network (RNN) method’s accuracy of 77.1%. The experimental results showcase the efficacy of the proposed method and offer valuable insights for the future application of HAR. © 2023 by the authors.,Healthcare is an area of concern where the application of human-centred design practices and principles can enormously affect well-being and patient care. The provision of high-quality healthcare services requires a deep understanding of patients&#x2019; needs, experiences, and preferences. Human activity recognition (HAR) is paramount in healthcare monitoring by using machine learning (ML), sensor data, and artificial intelligence (AI) to track and discern individuals&#x2019; behaviours and physical movements. This technology allows healthcare professionals to remotely monitor patients, thereby ensuring they adhere to prescribed rehabilitation or exercise routines, and identify falls or anomalies, improving overall care and safety of the patient. HAR for healthcare monitoring, driven by deep learning (DL) algorithms, leverages neural networks and large quantities of sensor information to autonomously and accurately detect and track patients&#x2019; behaviors and physical activities. DL-based HAR provides a cutting-edge solution for healthcare professionals to provide precise and more proactive interventions, reducing the burden on healthcare systems and improving patient well-being while increasing the overall quality of care. Therefore, the study presents an improved coyote optimization algorithm with a deep learning-assisted HAR (ICOADL-HAR) approach for healthcare monitoring. The purpose of the ICOADL-HAR technique is to analyze the sensor information of the patients to determine the different kinds of activities. In the primary stage, the ICOADL-HAR model allows a data normalization process using the Z-score approach. For activity recognition, the ICOADL-HAR technique employs an attention-based long short-term memory (ALSTM) model. Finally, the hyperparameter tuning of the ALSTM model can be performed by using ICOA. The stimulation validation of the ICOADL-HAR model takes place using benchmark HAR datasets. The wide-ranging comparison analysis highlighted the improved recognition rate of the ICOADL-HAR method compared to other existing HAR approaches in terms of various measures. Authors"
91,90,46,90_glioblastoma_glioma_gliomas_mri,"glioblastoma,glioma,gliomas,mri,neural,tumors,tumour,tumor,tumours,cnn","A brain tumor can have an impact on the symmetry of a person’s face or head, depending on its location and size. If a brain tumor is located in an area that affects the muscles responsible for facial symmetry, it can cause asymmetry. However, not all brain tumors cause asymmetry. Some tumors may be located in areas that do not affect facial symmetry or head shape. Additionally, the asymmetry caused by a brain tumor may be subtle and not easily noticeable, especially in the early stages of the condition. Brain tumor classification using deep learning involves using artificial neural networks to analyze medical images of the brain and classify them as either benign (not cancerous) or malignant (cancerous). In the field of medical imaging, Convolutional Neural Networks (CNN) have been used for tasks such as the classification of brain tumors. These models can then be used to assist in the diagnosis of brain tumors in new cases. Brain tissues can be analyzed using magnetic resonance imaging (MRI). By misdiagnosing forms of brain tumors, patients’ chances of survival will be significantly lowered. Checking the patient’s MRI scans is a common way to detect existing brain tumors. This approach takes a long time and is prone to human mistakes when dealing with large amounts of data and various kinds of brain tumors. In our proposed research, Convolutional Neural Network (CNN) models were trained to detect the three most prevalent forms of brain tumors, i.e., Glioma, Meningioma, and Pituitary; they were optimized using Aquila Optimizer (AQO), which was used for the initial population generation and modification for the selected dataset, dividing it into 80% for the training set and 20% for the testing set. We used the VGG-16, VGG-19, and Inception-V3 architectures with AQO optimizer for the training and validation of the brain tumor dataset and to obtain the best accuracy of 98.95% for the VGG-19 model. © 2023 by the authors.,An abnormal growth of cells in the brain, often known as a brain tumor, has the potential to develop into cancer. Carcinogenesis of glial cells in the brain and spinal cord is the root cause of gliomas, which are the most prevalent type of primary brain tumor. After receiving a diagnosis of glioblastoma, it is anticipated that the average patient will have a survival time of less than 14 months. Magnetic resonance imaging (MRI) is a well-known non-invasive imaging technology that can detect brain tumors and gives a variety of tissue contrasts in each imaging modality. Until recently, only neuroradiologists were capable of performing the tedious and time-consuming task of manually segmenting and analyzing structural MRI scans of brain tumors. This was because neuroradiologists have specialized training in this area. The development of comprehensive and automatic segmentation methods for brain tumors will have a significant impact on both the diagnosis and treatment of brain tumors. It is now possible to recognize tumors in photographs because of developments in computer-aided design (CAD), machine learning (ML), and deep learning (DL) approaches. The purpose of this study is to develop, through the application of MRI data, an automated model for the detection and classification of brain tumors based on deep learning (DLBTDC-MRI). Using the DLBTDC-MRI method, brain tumors can be detected and characterized at various stages of their progression. Preprocessing, segmentation, feature extraction, and classification are all included in the DLBTDC-MRI methodology that is supplied. The use of adaptive fuzzy filtering, often known as AFF, as a preprocessing technique for photos, results in less noise and higher-quality MRI scans. A method referred to as “chicken swarm optimization” (CSO) was used to segment MRI images. This method utilizes Tsallis entropy-based image segmentation to locate parts of the brain that have been injured. In addition to this, a Residual Network (ResNet) that combines handcrafted features with deep features was used to produce a meaningful collection of feature vectors. A classifier developed by combining DLBTDC-MRI and CSO can finally be used to diagnose brain tumors. To assess the enhanced performance of brain tumor categorization, a large number of simulations were run on the BRATS 2015 dataset. It would appear, based on the findings of these trials, that the DLBTDC-MRI method is superior to other contemporary procedures in many respects. © 2022 by the authors.,Accurate diagnosis of the brain tumor type at an earlier stage is crucial for the treatment process and helps to save the lives of a large number of people worldwide. Because they are non-invasive and spare patients from having an unpleasant biopsy, magnetic resonance imaging (MRI) scans are frequently employed to identify tumors. The manual identification of tumors is difficult and requires considerable time due to the large number of three-dimensional images that an MRI scan of one patient’s brain produces from various angles. Moreover, the variations in location, size, and shape of the brain tumor also make it challenging to detect and classify different types of tumors. Thus, computer-aided diagnostics (CAD) systems have been proposed for the detection of brain tumors. In this paper, we proposed a novel unified end-to-end deep learning model named TumorDetNet for brain tumor detection and classification. Our TumorDetNet framework employs 48 convolution layers with leaky ReLU (LReLU) and ReLU activation functions to compute the most distinctive deep feature maps. Moreover, average pooling and a dropout layer are also used to learn distinctive patterns and reduce overfitting. Finally, one fully connected and a softmax layer are employed to detect and classify the brain tumor into multiple types. We assessed the performance of our method on six standard Kaggle brain tumor MRI datasets for brain tumor detection and classification into (malignant and benign), and (glioma, pituitary, and meningioma). Our model successfully identified brain tumors with remarkable accuracy of 99.83%, classified benign and malignant brain tumors with an ideal accuracy of 100%, and meningiomas, pituitary, and gliomas tumors with an accuracy of 99.27%. These outcomes demonstrate the potency of the suggested methodology for the reliable identification and categorization of brain tumors. Copyright: © 2023 Ullah et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
92,91,46,91_cancers_transcriptomic_cancer_biomarkers,"cancers,transcriptomic,cancer,biomarkers,transcriptome,tumors,tumor,genome,tumoronly,genes","Abstract: The heterogeneous nature of breast cancer necessitates exploring its molecular subtypes for the early prognosis and treatment of cancer patients. Recent advances in genomics have enabled the investigation of gene expression data in breast cancer research as an alternative to traditional methods. In this regard, a project like The Cancer Genome Atlas (TCGA) provided easy access to the vast high-throughput sequencing gene expression data, including Breast cancer. However, finding evidence of the involvement of a set of genes in a particular breast cancer subtype from this large bulk of gene expression dataset is a demanding task. Here, we propose to develop a classification model based on machine learning to uncover the significant genes associated with different breast cancer subtypes like Basal, human epidermal growth factor receptor 2, luminal A, and luminal B. The RNA-Sequence gene expression data from The Cancer Genome Atlas is used for the tumor and normal sample classification and breast cancer subtype-specific optimal set of gene identification for this experiment. Experimental results show that the average classification accuracy value for different gene subsets varies from 75.36–77.74% depending upon the breast cancer subtype and feature selection method. Additionally, the feature scoring mechanism introduced in our model ranks the Feature Importance genes as three*, four*, five*, and six*. Besides this, Kaplan–Meier survival analysis, Composite network analysis, and Gene Ontology analysis are conducted to highlight the biological significance of the Feature Importancegenes. Given the classification results and the biological insight, we may conclude that the proposed model extracts a set of informative genes involved in breast cancer development, particularly the Basal, human epidermal growth factor receptor 2, luminal A, and luminal B subtypes. © 2023, Pleiades Publishing, Inc.,Background: Cancer is a set of diseases characterized by unchecked cell proliferation and invasion of surrounding tissues. The many genes that have been genetically associated with cancer or shown to directly contribute to oncogenesis vary widely between tumor types, but common gene signatures that relate to core cancer pathways have also been identified. It is not clear, however, whether there exist additional sets of genes or transcriptomic features that are less well known in cancer biology but that are also commonly deregulated across several cancer types. Results: Here, we agnostically identify transcriptomic features that are commonly shared between cancer types using 13,461 RNA-seq samples from 19 normal tissue types and 18 solid tumor types to train three feed-forward neural networks, based either on protein-coding gene expression, lncRNA expression, or splice junction use, to distinguish between normal and tumor samples. All three models recognize transcriptome signatures that are consistent across tumors. Analysis of attribution values extracted from our models reveals that genes that are commonly altered in cancer by expression or splicing variations are under strong evolutionary and selective constraints. Importantly, we find that genes composing our cancer transcriptome signatures are not frequently affected by mutations or genomic alterations and that their functions differ widely from the genes genetically associated with cancer. Conclusions: Our results highlighted that deregulation of RNA-processing genes and aberrant splicing are pervasive features on which core cancer pathways might converge across a large array of solid tumor types. © 2022, The Author(s).,Introduction: Monitoring the response after treatment of liver cancer and timely adjusting the treatment strategy are crucial to improve the survival rate of liver cancer. At present, the clinical monitoring of liver cancer after treatment is mainly based on serum markers and imaging. Morphological evaluation has limitations, such as the inability to measure small tumors and the poor repeatability of measurement, which is not applicable to cancer evaluation after immunotherapy or targeted treatment. The determination of serum markers is greatly affected by the environment and cannot accurately evaluate the prognosis. With the development of single cell sequencing technology, a large number of immune cell-specific genes have been identified. Immune cells and microenvironment play an important role in the process of prognosis. We speculate that the expression changes of immune cell-specific genes can indicate the process of prognosis. Method: Therefore, this paper first screened out the immune cell-specific genes related to liver cancer, and then built a deep learning model based on the expression of these genes to predict metastasis and the survival time of liver cancer patients. We verified and compared the model on the data set of 372 patients with liver cancer. Result: The experiments found that our model is significantly superior to other methods, and can accurately identify whether liver cancer patients have metastasis and predict the survival time of liver cancer patients according to the expression of immune cell-specific genes. Discussion: We found these immune cell-specific genes participant multiple cancer-related pathways. We fully explored the function of these genes, which would support the development of immunotherapy for liver cancer. Copyright © 2023 Liu, Qu, Xu, Qiao, Shao, Liu, He and Zhang."
93,92,46,92_wildlife_camera_cameras_animals,"wildlife,camera,cameras,animals,recognition,animal,birds,detection,images,classification","Computer vision has found many applications in automatic wildlife data analytics and biodiversity monitoring. Automating tasks like animal recognition or animal detection usually require machine learning models (e.g., deep neural networks) trained on annotated datasets. However, image datasets built for general purposes fail to capture realistic conditions of ecological studies, and existing datasets collected with camera-traps mainly focus on medium to large-sized animals. There is a lack of annotated small-sized animal datasets in the field. Small-sized animals (e.g., small mammals, frogs, lizards, arthropods) play an important role in ecosystems but are difficult to capture on camera-traps. They also present additional challenges: small animals can be more difficult to identify and blend more easily with their surroundings. To fill this gap, we introduce in this paper a new dataset dedicated to ecological studies of small-sized animals, and provide benchmark results of computer vision-based wildlife monitoring. The novelty of our work lies on SAWIT (small-sized animal wild image dataset), the first real-world dataset of small-sized animals, collected from camera traps and in realistic conditions. Our dataset consists of 34,434 images and is annotated by experts in the field with object-level annotations (bounding boxes) providing 34,820 annotated animals for seven animal categories. The dataset encompasses a wide range of challenging scenarios, such as occlusions, blurriness, and instances where animals blend into the dense vegetation. Based on the dataset, we benchmark two prevailing object detection algorithms: Faster RCNN and YOLO, and their variants. Experimental results show that all the variants of YOLO (version 5) perform similarly, ranging from 59.3% to 62.6% for the overall mean Average Precision (mAP) across all the animal categories. Faster RCNN with ResNet50 and HRNet backbone achieve 61.7% mAP and 58.5% mAP respectively. Through experiments, we indicate challenges and suggest research directions for computer vision-based wildlife monitoring. We provide both the dataset and the animal detection code at https://github.com/dtnguyen0304/sawit . © 2023, The Author(s).,As the capacity to collect and store large amounts of data expands, identifying and evaluating strategies to efficiently convert raw data into meaningful information is increasingly necessary. Across disciplines, this data processing task has become a significant challenge, delaying progress and actionable insights. In ecology, the growing use of camera traps (i.e., remotely triggered cameras) to collect information on wildlife has led to an enormous volume of raw data (i.e., images) in need of review and annotation. To expedite camera trap image processing, many have turned to the field of artificial intelligence (AI) and use machine learning models to automate tasks such as detecting and classifying wildlife in images. To contribute understanding of the utility of AI tools for processing wildlife camera trap images, we evaluated the performance of a state-of-the-art computer vision model developed by Microsoft AI for Earth named MegaDetector using data from an ongoing camera trap study in Arctic Alaska, USA. Compared to image labels determined by manual human review, we found MegaDetector reliably determined the presence or absence of wildlife in images generated by motion detection camera settings (?94.6% accuracy), however, performance was substantially poorer for images collected with time-lapse camera settings (?61.6% accuracy). By examining time-lapse images where MegaDetector failed to detect wildlife, we gained practical insights into animal size and distance detection limits and discuss how those may impact the performance of MegaDetector in other systems. We anticipate our findings will stimulate critical thinking about the tradeoffs of using automated AI tools or manual human review to process camera trap images and help to inform effective implementation of study designs. © 2022,As human activities in natural areas increase, understanding human–wildlife interactions is crucial. Big data approaches, like large-scale camera trap studies, are becoming more relevant for studying these interactions. In addition, open-source object detection models are rapidly improving and have great potential to enhance the image processing of camera trap data from human and wildlife activities. In this study, we evaluate the performance of the open-source object detection model MegaDetector in cross-regional monitoring using camera traps. The performance at detecting and counting humans, animals and vehicles is evaluated by comparing the detection results with manual classifications of more than 300 000 camera trap images from three study regions. Moreover, we investigate structural patterns of misclassification and evaluate the results of the detection model for typical temporal analyses conducted in ecological research. Overall, the accuracy of the detection model was very high with 96.0% accuracy for animals, 93.8% for persons and 99.3% for vehicles. Results reveal systematic patterns in misclassifications that can be automatically identified and removed. In addition, we show that the detection model can be readily used to count people and animals on images with underestimating persons by ?0.05, vehicles by ?0.01 and animals by ?0.01 counts per image. Most importantly, the temporal pattern in a long-term time series of manually classified human and wildlife activities was highly correlated with classification results of the detection model (Pearson's r = 0.996, p < 0.001) and diurnal kernel densities of activities were almost equivalent for manual and automated classification. The results thus prove the overall applicability of the detection model in the image classification process of cross-regional camera trap studies without further manual intervention. Besides the great acceleration in processing speed, the model is also suitable for long-term monitoring and allows reproducibility in scientific studies while complying with privacy regulations. © 2023 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London."
94,93,45,93_malware_malwares_cnn_ransomware,"malware,malwares,cnn,ransomware,antivirus,malicious,classifier,classify,apps,detection","The popularity of the Android platform has led to an explosion in malware. The current research on Android malware mainly focuses on malware detection or malware family classification. These studies need to extract a large number of features, which consumes a lot of manpower and material resources. Moreover, some malware use obfuscation to evade decompiler tools extracting features. To address these problems, we propose ImageDroid, a method based on the image format of Android applications that can not only detect and classify malware without prior knowledge but also detect the obfuscated malware. Furthermore, we utilize the Grad-CAM interpretable mechanism of the deep learning model to automatically label the image that play a key role in determining maliciousness in a visual way. We evaluate ImageDroid over 10,000 Android applications. Experimental results show that the accuracy of malicious detection and multifamily classification achieve 97.2% and 95.1%, respectively, and the detection accuracy of obfuscated malware achieves 94.6%.  © 2023 Pengfei Liu et al.,To accurately find malware in a large number of mobile APPs, and determine which family it belongs to is one of the most important challenges in Android malware detection. Existed research focuses on using the extracted features to distinguish Android malicious APPs, and less attention is paid to the category and family classification of Android malware. Meanwhile, feature selection has always been a choose-difficult issue in malware detection with machine learning methods. In this paper, SelAttConvLstm was designed to classify android malware by category and family without manually selecting features. To identify Android malware, we first convert all the network traffic flows into grayscale images according to chronological order through data preprocessing. Second, we design SelAttConvLstm, a deep learning model to detect malicious Android APPs with network flows images. This model can consider both the spatial and temporal features of network flow at the same time. In addition, to improve the performance of the model, self-attention weights are added to focus on different features of the input. Finally, comprehensive experiments are conducted to verify the effectiveness of the detection model. Experimental results showed that our method can not only effectively detect malware, but also classify malware in detail and accurately by category and family. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The number of complex and novel malware attacks is increasing exponentially in the cyberworld. Malware detection systems are facing new challenges due to the volume, velocity, and complexity of malware. The current malware detection system relies on a time-consuming, resource-intensive, and knowledge-intensive classification approach. Most of the existing malware detection system is ineffective in detecting novel malware attacks. A deep learning approach can be used to build a malware detection system that can effectively detect novel malware attacks without much human intervention. The current circumstance necessitates not just a malware system with excellent accuracy, but also one that can serve a large volume of demand in near real-time. A scalable malware detection system capable of detecting complex attacks is the need of time. This article discusses a scalable and distributed deep learning approach for malware detection using convolutional neural network and bidirectional long short-term memory (CNN-BiLSTM). The deep learning approach has been used to make the system learn and make predictive decisions without human intervention. The performance of the deep learning approach depends on various parameters and training data sets. Hence, different combinations of deep learning algorithms have been used to design and test the models to achieve the desired result. The experimental results show that the double layer of CNN and BiLSTM has better performance than single-layer CNN. © 2022 Taylor & Francis Group, LLC."
95,94,45,94_credit_predicting_prediction_lending,"credit,predicting,prediction,lending,banking,predictive,predict,loans,loan,banks","Commercial banks are required to explain the credit evaluation results to their customers. Therefore, banks attempt to improve the performance of their credit scoring models while ensuring the interpretability of the results. However, there is a tradeoff between the logistic regression model and machine learning-based techniques regarding interpretability and model performance because machine learning-based models are a black box. To deal with the tradeoff, in this study, we present a two-stage logistic regression method based on the Bayesian approach. In the first stage, we generate the derivative variables by linearly combining the original features with their explanatory powers based on the Bayesian inference. The second stage involves developing a credit scoring model through logistic regression using these derivative variables. Through this process, the explanatory power of a large number of original features can be utilized for default prediction, and the use of logistic regression maintains the model's interpretability. In the empirical analysis, the independent sample t-test reveals that our proposed approach significantly improves the model’s performance compared to that based on the conventional single-stage approach, i.e., the baseline model. The Kolmogorov–Smirnov statistics show a 3.42 percentage points (%p) increase, and the area under the receiver operating characteristic shows a 2.61%p increase. Given that our two-stage modeling approach has the advantages of interpretability and enhanced performance of the credit scoring model, our proposed method is essential for those in charge of banking who must explain credit evaluation results and find ways to improve the performance of credit scoring models. © 2022, The Author(s).,Corporate credit ratings are comprehensive indicators of a company's management performance, earnings quality, and future prospects; they represent its market evaluation and status in the industry and are relevant to the financing and investment decision-making process. Financial institutions determine corporate credit ratings using corporate financial and governance indicators. With advancements in the Internet and the popularity of social media, the appeal of enterprises on social media has become a relevant research topic. Social media represents an alternative method for financial institutions to determine corporate credit ratings. Therefore, the large amounts of data from social media have been used to effectively analyze and predict corporate credit ratings in risk management departments of financial institutions in the field of financial technology (FinTech). This study develops an approach to forecasting corporate credit ratings by analyzing public opinion toward corporations on social media to assist financial institutions in effectively evaluating and controlling corporate risk. This objective is achieved through the following steps: (i) designing a corporate credit rating forecasting process based on big data from social media, (ii) developing techniques for corporate credit rating forecasting, and (iii) implementing and evaluating the corporate credit rating forecasting mechanism. The experimental results of this research show that the accuracy of corporate credit rating prediction based on social media big data is higher than that of traditional financial report, corporate governance and macroeconomic indicators. Moreover, the adopted forecasting model, K-Nearest Neighbor (KNN), is superior to the other machine learning models in terms of accuracy. © 2022 Elsevier Ltd,Financial institutions and regulators increasingly rely on large-scale data analysis, particularly machine learning, for credit decisions. This paper assesses ten machine learning algorithms using a dataset of over 2.5 million observations from a financial institution. We also summarize key statistical and machine learning models in credit scoring and review current research findings. Our results indicate that ensemble models, particularly XGBoost, outperform traditional algorithms such as logistic regression in credit classification. Researchers and experts in the subject of credit risk can use this work as a practical reference as it covers crucial phases of data processing, exploratory data analysis, modeling, and evaluation metrics. © 2023 by the authors."
96,95,45,95_relations_relation_relational_entities,"relations,relation,relational,entities,entity,semantic,corpus,sentencelevel,sentences,extracting","Sentence-level relation classification is a technique for classifying the relation between the head entity and the tail entity in a sentence. Currently, it is popularly used to realize relation classification based on deep learning methods. However, these methods rely heavily on large-scale annotated data, and the role of head and tail entities’ information is not fully explored. In response to the above problems, we propose a prototypical networks model based on entity convolution for relation classification, which deforms the head entity and tail entity vectors encoded by BERT into multiple different convolution kernels and then performs convolution operations on the original sentence. Thus we can extract the features related to the entities in the sentence and classify the extracted features by using prototypical networks to realize relation classification. Experimental results strongly demonstrate that our model achieves state-of-the-art results compared with baseline models. © 2022 Elsevier Ltd,Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information. We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements (4.1 percent F1 on average). Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes and data can be found in https://github.com/CrisJk/PA-TRP. © 1989-2012 IEEE.,Spatial relation extraction is the process of identifying geographic entities from text and determining their corresponding spatial relations. Traditional spatial relation extraction mainly uses rule-based pattern matching, supervised learning-based or unsupervised learning-based methods. However, these methods suffer from poor time-sensitive, high labor cost and high dependence on large-scale data. With the development of pre-trained language models greatly alleviating the shortcomings of traditional methods, supervised learning methods incorporating pre-trained language models have become the mainstream relation extraction methods. Pipeline extraction and joint extraction, as the two most dominant ideas of relation extraction, both have obtained good performance on different datasets, and whether to share the contextual information of entities and relations is the main differences between the two ideas. In this paper, we compare the performance of two ideas oriented to spatial relation extraction based on Chinese corpus data in the field of geography and verify which method based on pre-trained language models is more suitable for Chinese spatial relation extraction. We fine-tuned the hyperparameters of the two models to optimize the extraction accuracy before the comparison experiments. The results of the comparison experiments show that pipeline extraction performs better than joint extraction of spatial relation extraction for Chinese text data with sentence granularity, because different tasks have different focus on contextual information, and it is difficult to take account into the needs of both tasks by sharing contextual information. In addition, we further compare the performance of the two models with the rule-based template approach in extracting topological, directional and distance relations, summarize the shortcomings of this experiment and provide an outlook for future work. © 2022 Wuhan University. Published by Informa UK Limited, trading as Taylor & Francis Group."
97,96,44,96_adaptation_learning_domaininvariant_adversarial,"adaptation,learning,domaininvariant,adversarial,transferability,domains,learn,discriminative,transfer,transferable","In this work, we explore the usage of the frequency transformation for reducing the domain shift between the source and target domain (e.g., synthetic image and real image respectively) towards solving the domain adaptation task. Most of the unsupervised domain adaptation (UDA) algorithms focus on reducing the global domain shift between labelled source and unlabelled target domains by matching the marginal distributions under a small domain gap assumption. UDA performance degrades for the cases where the domain gap between source and target distribution is large. In order to bring the source and the target domains closer, we propose a novel approach based on traditional image processing technique Class Aware Frequency Transformation (CAFT) that utilizes pseudo label based class consistent low-frequency swapping for improving the overall performance of the existing UDA algorithms. The proposed approach, when compared with the state-of-the-art deep learning based methods, is computationally more efficient and can easily be plugged into any existing UDA algorithm to improve its performance. Additionally, we introduce a novel approach based on absolute difference of top-2 class prediction probabilities for filtering target pseudo labels into clean and noisy sets. Samples with clean pseudo labels can be used to improve the performance of unsupervised learning algorithms. We name the overall framework as CAFT++. We evaluate the same on the top of different UDA algorithms across many public domain adaptation datasets. Our extensive experiments indicate that CAFT++ is able to achieve significant performance gains across all the popular benchmarks. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,By leveraging the labeled data samples of the source domain to learn the unlabeled data samples of the target domain, unsupervised domain adaptation (DA) has achieved promising performance. However, it is still a vital problem for unsupervised domain adaptation to deal with cross-domain distribution mismatch. Therefore, we present a new model framework for cross-domain image classification in the paper, which is termed manifold transfer subspace learning based on double relaxed discriminative regression (MTSL-DRDR). First, the global geometry information of the samples from the source and target domain can be preserved by utilizing the low-rank constraint. Second, the two transformation projections are employed to project both domains to a unified subspace, in which each data sample of the target domain can be represented by some samples from the source domain with the sparse and low-rank coefficient matrix. Third, the local structure information of the data points with the same semantics from the different domains is preserved by means of the adaptive weight graph based on the low-rank coefficient matrix. Last, for fully use the discriminative information of data from the source domain, the discriminant information of the source domain based on intra-class and inter-class graphs is encoded to the target domain. Our MTSL-DRDR algorithm is evaluated on challenging benchmark datasets, and a large number of experiment results show the superiority of the proposed method. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.,Unsupervised domain adaptation (UDA) has successfully addressed the domain shift problem for visual applications. Yet, these approaches may have limited performance for time series data due to the following reasons. First, they mainly rely on the large-scale dataset (i.e., ImageNet) for source pretraining, which is not applicable for time series data. Second, they ignore the temporal dimension on the feature space of the source and target domains during the domain alignment step. Finally, most of the prior UDA methods can only align the global features without considering the fine-grained class distribution of the target domain. To address these limitations, we propose a SeLf-supervised AutoRegressive Domain Adaptation (SLARDA) framework. In particular, we first design a self-supervised (SL) learning module that uses forecasting as an auxiliary task to improve the transferability of source features. Second, we propose a novel autoregressive domain adaptation technique that incorporates temporal dependence of both source and target features during domain alignment. Finally, we develop an ensemble teacher model to align class-wise distribution in the target domain via a confident pseudo labeling approach. Extensive experiments have been conducted on three real-world time series applications with 30 cross-domain scenarios. The results demonstrate that our proposed SLARDA method significantly outperforms the state-of-the-art approaches for time series domain adaptation. Our source code is available at: https://github.com/mohamedr002/SLARDA.  © 2012 IEEE."
98,97,44,97_drilling_prediction_predicting_geotechnical,"drilling,prediction,predicting,geotechnical,excavation,tunnels,predict,optimization,tunnel,offshore","Drilling optimization is essential in constructing an enhanced geothermal system (EGS) and can be facilitated through predicting the rate of penetration (ROP). The ROP evolution along the depth was forecasted by considering the current and previous ROP values as input to a gated recurrent unit (GRU)-based deep learning model. Drilling data was obtained from two geothermal wells in Pohang, South Korea. Multiple data configurations for training and testing were designed from both wells. The proximity of the training section to the target results in improved accuracy in prediction (MAPE smaller than ~ 3%). Furthermore, larger depth spans of ROPs used for training resulted in better prediction outcomes. The model trained with the entire dataset from an adjacent well exhibited well-predicted ROP values for a new drilling hole (MAPE smaller than 5–10%). From the multiple-step forecasting analysis, the error tended to sharply increase as the number of predicted ROP values increased despite a large number of the input sequence (MAPE larger than 20%). Incorporating other drilling data besides ROP evolution did not improve the prediction. By predicting ROP evolution along the depth, the GRU-based model can assist operators in optimizing drilling processes and preparing for upcoming scenarios. The model can serve as a valuable tool for enhancing drilling efficiency and effectively managing operational uncertainties. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Large-scale cluster extended reach wells (ERWs) are usually drilled from one platform to target distant multiple reservoirs in offshore oil & gas development. Due to the complicated downhole environments and complex geological conditions in various azimuths, the accurate prediction of the rate of penetration (ROP) in offshore large-scale cluster ERWs drilling is a challenging task. In this work, a ROP prediction method for offshore large-scale cluster ERWs drilling based on machine learning and big-data techniques is presented. 27 input parameters affecting ROP are collected from block N of China Bohai Oilfield, with a total of 9,193,408 data points. The internal relation between various parameters and ROP at the data level is revealed, which verifies the feasibility of the collected data set training. Based on various machine learning algorithms, 12 kinds of ROP prediction models based on support vector regression (SVR), random forest regression (RFR), back propagation neural network (BPNN), and recurrent neural network (RNN) are established respectively. Meanwhile, the prediction accuracy of the ROP prediction models is evaluated based on six performance indicators. It shows that the prediction results of the RNN model with LSTM as neuron structure are the best (all of the 6 indicators are optimal) among the 12 kinds of ROP prediction models, and the mean absolute error is only 6.12 m/h. The RNN model with LSTM as neuron structure is the best machine learning model in the prediction of ROP in offshore large-scale cluster ERWs drilling. It provides a practical method for enhancing cleaner development of offshore oil & gas. © 2023 Elsevier Ltd,Drilling rate of penetration (ROP) prediction has long been a part of any drilling activity. By accurate prediction of ROP, optimization can be done that maximizes ROP and reduces drilling costs. ROP prediction relies on good quality data from nearby wells and is mostly performed through multiple-regression models. Recently, machine learning and artificial intelligence tools have been suggested as an alternative. On the other hand, the lack of knowledge about the uncertainty in these data-driven models can limit their applicability. A trained machine learning model can perform satisfactorily on the data on which it was trained, yet fail when applied to new data. In this work, we explore the application of Bayesian neural networks and the notion of uncertainty in ROP prediction. We compare the prediction of several deterministic and probabilistic models on a real drilling dataset. Despite accurately predicting the ROP in a 5000ft section of a horizontal well, the models are shown to have large uncertainties, potentially caused by the size of the training dataset. The results and analysis show how the adoption of a probabilistic framework can be leveraged to pinpoint the root cause of an ROP model's lack of accuracy. © 2022 Elsevier B.V."
99,98,44,98_raman_spectroscopy_spectra_classification,"raman,spectroscopy,spectra,classification,spectrometer,spectral,cnn,learning,spectroscopic,spectrum","Deep learning techniques provide powerful solutions to several pattern-recognition problems, including Raman spectral classification. However, these networks require large amounts of labeled data to perform well. Labeled data, which are typically obtained in a laboratory, can potentially be alleviated by data augmentation. This study investigated various data augmentation techniques and applied multiple deep learning methods to Raman spectral classification. Raman spectra yield fingerprint-like information about chemical compositions, but are prone to noise when the particles of the material are small. Five augmentation models were investigated to build robust deep learning classifiers: weighted sums of spectral signals, imitated chemical backgrounds, extended multiplicative signal augmentation, and generated Gaussian and Poisson-distributed noise. We compared the performance of nine state-of-the-art convolutional neural networks with all the augmentation techniques. The LeNet5 models with background noise augmentation yielded the highest accuracy when tested on real-world Raman spectral classification at 88.33% accuracy. A class activation map of the model was generated to provide a qualitative observation of the results. © The Korea Institute of Information and Communication Engineering,The overuse of plastics releases large amounts of microplastics. These tiny and complex pollutants may cause immeasurable damage to human social life. Raman spectroscopy detection technology is widely used in the detection, identification and analysis of microplastics due to its advantages of fast speed, high sensitivity and non-destructive. In this work, we first recorded the Raman spectra of eight common plastics in daily life. By adjusting parameters such as laser wavelength, laser power, and acquisition time, the Raman data under different acquisition conditions were diversified, and the corresponding Raman spectra were obtained, and a database of eight household plastics was established. Combined with deep learning algorithms, an accurate, fast and simple classification and identification method for 8 types of plastics is established. Firstly, the acquired spectral data were preprocessed for baseline correction and noise reduction, Then, four machine learning algorithms, linear discriminant analysis (LDA), decision tree, support vector machine (SVM) and one-dimensional convolutional neural network (1D-CNN), are used to classify and identify the preprocessed data. The results showed that the classification accuracy of the three machine learning models for the Raman spectra of standard plastic samples were 84%, 93% and 93% respectively. The 1D-CNN model has an accuracy rate of up to 97% for Raman spectroscopy. Our study shows that the combination of Raman spectroscopy detection techniques and deep learning algorithms is a very valuable approach for microplastic classification and identification. © 2024 Elsevier B.V.,Rapid and early identification of pathogens is critical to guide antibiotic therapy. Raman spectroscopy as a noninvasive diagnostic technique provides rapid and accurate detection of pathogens. Raman spectrum of single cells serves as the “fingerprint” of the cell, revealing its metabolic characteristics. Rapid identification of pathogens can be achieved by combining Raman spectroscopy and deep learning. Traditional classification techniques frequently require lots of data for training, which is time costing to collect Raman spectra. For trace samples and strains that are difficult to culture, it is difficult to provide an accurate classification model. In order to reduce the number of samples collected and improve the accuracy of the classification model, a new pathogen detection method integrating Raman spectroscopy, variational auto-encoder (VAE), and long short-term memory network (LSTM) is proposed in this paper. We collect the Raman signals of pathogens and input them to VAE for training. VAE will generate a large number of Raman spectral data that cannot be distinguished from the real spectrum, and the signal-to-noise ratio is higher than that of the real spectrum. These spectra are input into the LSTM together with the real spectrum for training, and a good classification model is obtained. The results of the experiments reveal that this method not only improves the average accuracy of pathogen classification to 96.9% but also reduces the number of Raman spectra collected from 1000 to 200. With this technology, the number of Raman spectra collected can be greatly reduced, so that strains that are difficult to culture or trace can be rapidly identified. © 2022 Wiley-VCH GmbH."
100,99,44,99_gestures_gesture_recognition_recognizing,"gestures,gesture,recognition,recognizing,hands,hand,sensing,classification,features,humancomputer","The development of hand gesture recognition systems has gained more attention in recent days, due to its support of modern human-computer interfaces. Moreover, sign language recognition is mainly developed for enabling communication between deaf and dumb people. In conventional works, various image processing techniques like segmentation, optimization, and classification are deployed for hand gesture recognition. Still, it limits the major problems of inefficient handling of large dimensional datasets and requires more time consumption, increased false positives, error rate, and misclassification outputs. Hence, this research work intends to develop an efficient hand gesture image recognition system by using advanced image processing techniques. During image segmentation, skin color detection and morphological operations are performed for accurately segmenting the hand gesture portion. Then, the Heuristic Manta-ray Foraging Optimization (HMFO) technique is employed for optimally selecting the features by computing the best fitness value. Moreover, the reduced dimensionality of features helps to increase the accuracy of classification with a reduced error rate. Finally, an Adaptive Extreme Learning Machine (AELM) based classification technique is employed for predicting the recognition output. During results validation, various evaluation measures have been used to compare the proposed model's performance with other classification approaches.  © 2018 Tsinghua University Press.,In recent years, hand gesture recognition in human-computer interfaces is usually based on surface electromyography because the signals are non-intrusive and are not affected by the variations of light, position, and orientation of the hand. Deep learning algorithms have become increasingly more prominent in gesture recognition for the ability to automatically learn features from large amounts of data. However, delicate and complicated network structures brought by deep learning, which are elaborately designed for cross session tasks, need more computing time to be trained and tested, which can hardly be applied to the online system. In this study, an online electromyographic hand gesture recognition method using deep learning and transfer learning is proposed. The deep learning model includes a feature extractor, a label classifier, and a gesture predictor. The feature extractor is based on the temporal convolutional network, which is designed to learn high-level discriminant features from the input signals. The label classifier includes three fully connected layers, designed to classify hand gesture labels using the feature vector which is produced by the feature extractor. The gesture predictor uses a threshold voting algorithm to predict the gesture, used at the stage of testing to perform the online recognition. Transfer learning technique is used to transfer model parameters from one pre-trained model, which costs less time and can be applied for online applications. The proposed model is verified on both the Myo dataset and the public NinaPro database. The proposed transfer learning scheme is shown to systematically and significantly enhance the performance of the proposed model on the two datasets, only using no more than three sessions to retrain the label predictor can achieve the accuracy of more than 90% of that obtained though the normal training of the whole parts of the model using full training sessions. © 2023 Elsevier Ltd,Developments in radio detection and ranging (radar) technology have made hand gesture recognition feasible. In heat map-based gesture recognition, feature images have a large size and require complex neural networks to extract information. Machine learning methods typically require large amounts of data and collecting hand gestures with radar is time- and energy-consuming. Therefore, a low computational complexity algorithm for hand gesture recognition based on a frequency-modulated continuous-wave (FMCW) radar and a synthetic hand gesture feature generator are proposed. In the low computational complexity algorithm, two-dimensional Fast Fourier Transform is implemented on the radar raw data to generate a range-Doppler matrix. After that, background modelling is applied to separate the dynamic object and the static background. Then a bin with the highest magnitude in the range-Doppler matrix is selected to locate the target and obtain its range and velocity. The bins at this location along the dimension of the antenna can be utilised to calculate the angle of the target using Fourier beam steering. In the synthetic generator, the Blender software is used to generate different hand gestures and trajectories and then the range, velocity and angle of targets are extracted directly from the trajectory. The experimental results demonstrate that the average recognition accuracy of the model on the test set can reach 89.13% when the synthetic data are used as the training set and the real data are used as the test set. This indicates that the generation of synthetic data can make a meaningful contribution in the pre-training phase. © 2022 by the authors."
101,100,43,100_recognition_yolov5_detecting_detection,"recognition,yolov5,detecting,detection,vehicles,detector,vehicle,driving,detectors,roadside","A traffic sign recognition system is crucial for safely operating an autonomous driving car and efficiently managing road facilities. Recent studies on traffic sign recognition tasks show significant advances in terms of accuracy on several benchmarks. However, they lack performance evaluation in driving cars in diverse road environments. In this study, we develop a traffic sign recognition framework for a vehicle to evaluate and compare deep learning-based object detection and tracking models for practical validation. We collect a large-scale highway image set using a camera-installed vehicle for training models, and evaluate the model inference during a test drive in terms of accuracy and processing time. In addition, we propose a novel categorization method for urban road scenes with possible scenarios. The experimental results show that the YOLOv5 detector and strongSORT tracking model result in better performance than other models in terms of accuracy and processing time. Furthermore, we provide an extensive discussion on possible obstacles in traffic sign recognition tasks to facilitate future research through numerous experiments for each road condition. © 2023 by the authors.,As one of the most indispensable means of transportation in modern society, vehicles guarantee our daily commuting and logistics transportation. However, with the increasing number of vehicles, vehicles have also caused increasingly serious traffic safety problems while providing convenience to our lives. One of the most common of these is traffic accidents caused by vehicle yaw due to driver distraction. As a potential solution to this problem, lane departure warning systems (LDWS) focus on detecting and determining whether the vehicle is deviating from the driveway, considered an essential part of autonomous driving technology, and have received significant attention in recent years. A large number of different types of LDWS systems have been developed, especially in recent years, with the development of artificial intelligence technology, many methods based on deep learning and machine vision have been proposed. However, it is well known that due to the complexity of the network structure in deep learning-based object detection algorithms, the operation of such methods relies on a large amount of computing power support. However, due to the limitation of the overall energy supply of the vehicle, it is usually unable to support computing power similar to the laboratory level. Therefore, how to realize efficient lane departure warnings under the condition of limited computing power is a critical problem to be solved. Accordingly, in this paper, we propose a novel lightweight LDWS model. Different from deep learning methods of LDWS, our LDWS model LEHA can achieve high accuracy and efficiency by relying only on simple hardware. The proposed LEHA consists of three modules: the image processing module, the lane detection module, and the lane departure recognition module. The image pre-processing module is applied to pre-process the original road image, which can improve the accuracy and efficiency of the following lane detection module. After obtaining the processed image, lane detection begins to detect and label the lanes. Finally, lane departure recognition is used to calculate the deviation distance and direction to determine whether the warning should be initiated. To evaluate the performance of LEHA, we compare our method with other state-of-the-art LDWS models in terms of detection accuracy and processing time under ideal and non-ideal lane conditions based on the KITTI dataset. The experimental results demonstrate that our LEHA outperforms state-of-the-art techniques. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,With the rapid advancements in the field of autonomous driving, the need for faster and more accurate object detection frameworks has become a necessity. Many recent deep learning-based object detectors have shown compelling performance for the detection of large objects in a variety of real-time driving applications. However, the detection of small objects such as traffic signs and traffic lights is a challenging task owing to the complex nature of such objects. Additionally, the complexity present in a few images due to the existence of foreground/background imbalance and perspective distortion caused by adverse weather and low-lighting conditions further makes it difficult to detect small objects accurately. In this letter, we investigate how an existing object detector can be adjusted to address specific tasks and how these modifications can impact the detection of small objects. To achieve this, we explore and introduce architectural changes to the popular YOLOv5 model to improve its performance in the detection of small objects without sacrificing the detection accuracy of large objects, particularly in autonomous driving. We will show that our modifications barely increase the computational complexity but significantly improve the detection accuracy and speed. Compared to the conventional YOLOv5, the proposed iS-YOLOv5 model increases the mean Average Precision (mAP) by 3.35% on the BDD100K dataset. Nevertheless, our proposed model improves the detection speed by 2.57 frames per second (FPS) compared to the YOLOv5 model. © 2023 Elsevier B.V."
102,101,43,101_livestock_cows_pigs_cow,"livestock,cows,pigs,cow,pig,cattle,recognition,animal,animals,farm","Motion and aggressive behaviors in pigs provide important information for the study of social hierarchies in pigs and can be used as a selection indicator for pig health and aggression parameters. However, relying only on visual observation or surveillance video to record the number of aggressive acts is time-consuming, labor-intensive, and lasts for only a short period of time. Manual observation is too short compared to the growth cycle of pigs, and complete recording is impractical in large farms. In addition, due to the complex process of assessing the intensity of pig aggression, manual recording is highly influenced by human subjective vision. In order to efficiently record pig motion and aggressive behaviors as parameters for breeding selection and behavioral studies, the videos and pictures were collected from typical commercial farms, with each unit including 8~20 pigs in 7~25 m2 space; they were bred in stable social groups and a video was set up to record the whole day’s activities. We proposed a deep learning-based recognition method for detecting and recognizing the movement and aggressive behaviors of pigs by recording and annotating head-to-head tapping, head-to-body tapping, neck biting, body biting, and ear biting during fighting. The method uses an improved EMA-YOLOv8 model and a target tracking algorithm to assign a unique digital identity code to each pig, while efficiently recognizing and recording pig motion and aggressive behaviors and tracking them, thus providing statistics on the speed and duration of pig motion. On the test dataset, the average precision of the model was 96.4%, indicating that the model has high accuracy in detecting a pig’s identity and its fighting behaviors. The model detection results were highly correlated with the manual recording results (R2 of 0.9804 and 0.9856, respectively), indicating that the method has high accuracy and effectiveness. In summary, the method realized the detection and identification of motion duration and aggressive behavior of pigs under natural conditions, and provided reliable data and technical support for the study of the social hierarchy of pigs and the selection of pig health and aggression phenotypes. © 2023 by the authors.,Behavior is one of the important factors reflecting the health status of dairy cows, and when dairy cows encounter health problems, they exhibit different behavioral characteristics. Therefore, identifying dairy cow behavior not only helps in assessing their physiological health and disease treatment but also improves cow welfare, which is very important for the development of animal husbandry. The method of relying on human eyes to observe the behavior of dairy cows has problems such as high labor costs, high labor intensity, and high fatigue rates. Therefore, it is necessary to explore more effective technical means to identify cow behaviors more quickly and accurately and improve the intelligence level of dairy cow farming. Automatic recognition of dairy cow behavior has become a key technology for diagnosing dairy cow diseases, improving farm economic benefits and reducing animal elimination rates. Recently, deep learning for automated dairy cow behavior identification has become a research focus. However, in complex farming environments, dairy cow behaviors are characterized by multiscale features due to large scenes and long data collection distances. Traditional behavior recognition models cannot accurately recognize similar behavior features of dairy cows, such as those with similar visual characteristics, i.e., standing and walking. The behavior recognition method based on 3D convolution solves the problem of small visual feature differences in behavior recognition. However, due to the large number of model parameters, long inference time, and simple data background, it cannot meet the demand for real-time recognition of dairy cow behaviors in complex breeding environments. To address this, we developed an effective yet lightweight model for fast and accurate dairy cow behavior feature learning from video data. We focused on four common behaviors: standing, walking, lying, and mounting. We recorded videos of dairy cow behaviors at a dairy farm containing over one hundred cows using surveillance cameras. A robust model was built using a complex background dataset. We proposed a two-pathway X3DFast model based on spatiotemporal behavior features. The X3D and fast pathways were laterally connected to integrate spatial and temporal features. The X3D pathway extracted spatial features. The fast pathway with R(2 + 1)D convolution decomposed spatiotemporal features and transferred effective spatial features to the X3D pathway. An action model further enhanced X3D spatial modeling. Experiments showed that X3DFast achieved 98.49% top-1 accuracy, outperforming similar methods in identifying the four behaviors. The method we proposed can effectively identify similar dairy cow behaviors while improving inference speed, providing technical support for subsequent dairy cow behavior recognition and daily behavior statistics. © 2023, The Author(s).,In modern dairy farms, accurate and reliable identification of each individual cow is of great significance for precision livestock farming. Individual cow identification is the basis for applications such as disease detection, automatic behaviour analysis, intelligent milking, and individual counting and is crucial for improving the welfare and breeding efficiency of dairy cows. Computer vision-based method is a low-cost, non-contact, automatic, and efficient way. To improve the accuracy and efficiency of cow recognition in different large-scale dairy farms, we proposed a BottleNet Transformer (BoTNet) model based on Graph Sampling and Counterfactual Attention Learning for cow surveillance videos. First, we replace the 3 × 3 spatial convolution with Multi-Head Attention in the final three bottleneck blocks of the ResNet. The BoT block module combines attention mechanisms and residual connection to enhance the global representation of cow images, which in turn better captures the features of the cow's back pattern region and ignores the influence of irrelevant information, such as the background of the dairy barn. Subsequently, counterfactual learning measures the quality of attention by comparing the difference between the generated output and the true label. The difference can be used to enhance the causal relationship between prediction results and cow feature attention, allowing the model to obtain more comprehensive cow appearance features. Finally, we added a Graph Sampling module before the feature extraction phase to produce small batches of samples for training. The GS sampler improves the learning efficiency while reducing the memory and computation consumption compared with the usual adopted PK sampling. We conducted comparison experiments on the public dataset Dataset1, and the experimental results reveal that the Rank-1, Rank-5, and mAP values of this study's method are 4%, 3.2%, and 5.3% higher than the optimal results, respectively, when compared with the existing state-of-the-art methods for animal individual recognition. In particular, we construct a challenging dataset by intercepting individual cow images from videos in the public dataset of farms. Experimental results indicate that the proposed method has better generalization performance. © 2024 Elsevier B.V."
103,102,43,102_uav_uavs_features_yolov7,"uav,uavs,features,yolov7,yolov4,detector,detection,drones,unmanned,detecting","In recent years, unmanned aerial vehicles (UAVs) have developed rapidly. Because of their small size, low cost, and strong maneuverability, they have been widely used in several fields such as aerial photography, rescue, transportation, and agriculture. Object recognition requires a large amount of data, but in real application scenarios, due to factors such as privacy and high data labeling costs, it is impossible to obtain sufficient label training samples. This paper proposes an unmanned aerial vehicle (UAV) image object recognition model based on small sample learning (IORS). Based on data enhancement and improved feature fusion capabilities, the YOLOv4_Tiny model is improved to make it more applicable to UAV images. This solves the problem of identifying dense small targets in UAV images when dealing with a small number of samples. The experimental results showed that in UAV images, the proposed method has a good target recognition effect without reducing the speed, while the overall accuracy is increased by approximately 4.5%. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Target detection for aerial images has been the focus of research. However, there are several difficulties in aerial image detection, such as complex backgrounds, high resolution, and a large number of small targets in UAV (Unmanned Aerial Vehicles) aerial images. In order to achieve high-precision detection of ground targets, a lightweight aerial image target detection algorithm LATD-YOLO (Lightweight Aerial Image Target Detector) based on YOLOv7 is proposed in this paper. Firstly, a new network structure dedicated to small target detection is proposed, and the feature extraction network and feature fusion network architectures are both lightweight. The fusion relationship between shallow and deep features is reconstructed. Then, the ELAN-OD module is proposed to reduce the model computation and strengthen the feature extraction ability of the network. In addition, the hybrid attention mechanism is added to the structure of the feature extraction network, where the effective information is extracted to enhance the learning ability. Finally, a new anchor frame position metric is introduced to improve the model’s ability to handle small targets. The experimental results show that LATD-YOLO can effectively improve the detection effect. The detection accuracy is improved by 359.6% on the ITCVD and 3.6% on the VisDrone2019 datasets respectively; the volume of the model is decreased by 67.74%; the amount of parameters is reduced by 67.91%; the amount of computation is decreased by 30.92%. Therefore, LATD-YOLO can achieve excellent detection performance in high-precision and lightweight computation scenarios. 2023 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.,The targets of UAV target detection are usually small targets, and the backgrounds are complex. In this work, aiming at the problem that small targets are easy to be missed or misdetected during the UAV detection, an improved YOLOv5s_MSES target detection algorithm based on YOLOv5s is proposed. First of all, to solve the problem of UAV's difficulty in detecting small targets, the detection layer is ameliorated into the small target detection layer STD, which makes the model more easily detect the small targets. Then, the multi-scale feature fusion module is added to improve the detection accuracy of the small targets. Furthermore, by combining multi-scale module and attention module, a new connection method is proposed to retain the large scale of feature information. Finally, in contrast with some existent methods, the experimental results of VisDrone2019 UAV target detection dataset show that our proposed YOLOv5s_MSES can achieve the better detection effect, and more effectively complete the small target detection task for UAV aerial photography images. © 2023 Elsevier Inc."
104,103,43,103_earthquakes_earthquake_quakeflow_seismic,"earthquakes,earthquake,quakeflow,seismic,seismicity,microseismic,foreshock,geological,alert,ggmms","Earthquake early warning (EEW) systems aim to forecast the shaking intensity rapidly after an earthquake occurs and send warnings to affected areas before the onset of strong shaking. The system relies on rapid and accurate estimation of earthquake source parameters. However, it is known that source estimation for large ruptures in real-time is challenging, and it often leads to magnitude underestimation. In a previous study, we showed that machine learning, HR-GNSS, and realistic rupture synthetics can be used to reliably predict earthquake magnitude. This model, called Machine-Learning Assessed Rapid Geodetic Earthquake model (M-LARGE), can rapidly forecast large earthquake magnitudes with an accuracy of 99%. Here, we expand M-LARGE to predict centroid location and fault size, enabling the construction of the fault rupture extent for forecasting shaking intensity using existing ground motion models. We test our model in the Chilean Subduction Zone with thousands of simulated and five real large earthquakes. The result achieves an average warning time of 40.5 s for shaking intensity MMI4+, surpassing the 34 s obtained by a similar GNSS EEW model. Our approach addresses a critical gap in existing EEW systems for large earthquakes by demonstrating real-time fault tracking feasibility without saturation issues. This capability leads to timely and accurate ground motion forecasts and can support other methods, enhancing the overall effectiveness of EEW systems. Additionally, the ability to predict source parameters for real Chilean earthquakes implies that synthetic data, governed by our understanding of earthquake scaling, is consistent with the actual rupture processes. © 2023. American Geophysical Union. All Rights Reserved.,In this study, magnitude estimation in earthquake early warning (EEW) systems is seen as a classification problem: the single-channel waveform, starting from the P-wave onset and lasting 4 s, is given in the input, and earthquake severity (medium and large earthquakes: local magnitude (ML) ? 5; small earthquakes: ML < 5) is the classification result. The convolutional neural network (CNN) is proposed to estimate the severity of the earthquake, which is composed of several blocks that can extract the latent representation of the input from different receptive fields automatically. We train and test the proposed CNN model using two data sets. One is recorded by the China Earthquake Networks Center (CENC), and the other is the Stanford Earthquake Dataset (STEAD). Accordingly, the proposed CNN model achieves a test accuracy of 97.90 per cent. The proposed CNN model is applied to estimate two real-world earthquake swarms in China (the Changning earthquake and the Tangshan earthquake swarms) and the INSTANCE data set, and demonstrated the promising performance of generalization. In addition, the proposed CNN model has been connected to the CENC for further testing using real-world real-time seismic data.  © 2023 The Author(s).,Earthquake prediction is a long-standing problem in seismology that has garnered attention from the scientific community and the public. Despite ongoing efforts to understand the physical mechanisms of earthquake occurrence, there is no convincing physical or statistical model for predicting large earthquakes. Machine learning methods, such as random forest and long short-term memory (LSTM) neural networks, excel at identifying patterns in large-scale databases and offer a potential means to improve earthquake prediction performance. Differing from physical and statistical approaches to earthquake prediction, we explore whether small earthquakes can be used to predict large earthquakes within the framework of machine learning. Specifically, we attempt to answer two questions for a given region: (1) Is there a likelihood of a large earthquake (e.g., M ? 6.0) occurring within the next year? (2) What is the maximum magnitude of an earthquake expected to occur within the next year? Our results show that the random forest method performs best in classifying large earthquake occurrences, while the LSTM method provides a rough estimation of earthquake magnitude. We conclude that small earthquakes contain information relevant to predicting future large earthquakes and that machine learning provides a promising avenue for improving the prediction of earthquake occurrences. © 2023 by the authors."
105,104,42,104_relational_embeddings_embeddingbased_entities,"relational,embeddings,embeddingbased,entities,relations,rdf,embedding,entity,knowledge,semantic","Most knowledge graphs(KGs) are large and incomplete graph-structure database, which can be completed by predicting miss links according to the existing knowledge. The mainstream method is knowledge graph embedding (KGE) which is designed to learn low dimensional embedding of entities and relations. However, knowledge graph embedding still faces two major issues: (1) How to generate more expressive embeddings? (2) How to solve semantic polysemy of entities in different relations? In this paper, we propose a novel KG embedding model, RIECN (Relation-based Interactive Embedding Convolutional Network), which achieves high-quality performance and shows some advancements in modeling complex relations. In RIECN, FIR (Feature Interaction Reshaping) method is introduced to increase the feature interactions between entity and relation embeddings to generate more expressive feature maps. In addition, a new method of generating relation-based dynamic convolution filters, RDCF, is proposed. RDCF generates specific relation and hybird-size convolution filters, which enriches the feature maps of each entity improving the accuracy of link prediction task especially in complex relations scenario. We tested the performance of our model on five benchmark datasets. The experimental results show that the RIECN model significantly outperforms recent state-of-the-art models by 0.1–3.2% and 1.1–3.7%, in terms of MMR metric and Hit@1 metric, respectively. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Due to the heterogeneous structure of the knowledge graph (KG), relationships between entities remain missing. However, optimal use of KG requires inference of missing fact triplet (entity-relation-entity). The fact inference predicts a missing relationship using an embedding approach in a supervised learning setup, representing entities and relationships in a low-dimensional vector space. Recent work uses attention-aware embeddings, but when applied directly to entire KG, attention mechanisms can be computationally expensive, especially for large graphs. The attention-based KG embedding model uses negative sampling, which can cause a gradient vanishing problem during learning. This paper proposes a novel triplet subgraph attention embedding (TSAE) model that combines a simplified graph attention mechanism with a neural network to learn embedding without negative sampling requirements. The attention layer processes the triplet-level subgraph entities to learn the central entity features by aggregating the neighbor’s features. A neural network processes attention-aware triplet entity features through hidden layers to compute the likelihood of relationship types between triplet entities. TSAE generates more fine-grained entity embeddings using simplified attention mechanism, reduces computational complexity, and offers interpretable embeddings. Experimental results on the benchmark data sets exhibit TSAE superiority over the baselines. The case study shows the efficacy of the model for the KG completion task. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Reasoning over knowledge graphs (KGs) has received increasing attention recently due to its promising applications in many areas, such as semantic search and recommendation systems. Subsequently, most reasoning models are inherently transductive and ignore uncertainties of KGs, making it difficult to generalize to unseen entities. Moreover, existing approaches usually require each entity in the KG to have sufficient training samples, which leads to the overfitting of the entity having few instances. In fact, long-tail distributions are quite widespread in KGs, and newly emerging entities will tend to have only a few related triples. In this work, we aim at studying knowledge graph reasoning under a challenging setting where only limited training samples are available. Specifically, we propose a Bayesian inductive reasoning method and incorporate meta-learning techniques in few-shot learning to solve data deficiency and uncertainties. We design a Bayesian graph neural network as a meta-learner to achieve Bayesian inference, which can extrapolate meta-knowledge from observed KG to emerging entities. We conduct extensive experiments on two large-scale benchmark datasets, and the results demonstrate considerable performance improvement with the proposed approach over other baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM."
106,105,41,105_biomass_biogas_fermentation_regression,"biomass,biogas,fermentation,regression,prediction,boosting,pyrolysis,biochar,predict,models","Biochar is a high-carbon-content organic compound that has potential applications in the field of energy storage and conversion. It can be produced from a variety of biomass feedstocks such as plant-based, animal-based, and municipal waste at different pyrolysis conditions. However, it is difficult to produce biochar on a large scale if the relationship between the type of biomass, operating conditions, and biochar properties is not understood well. Hence, the use of machine learning-based data analysis is necessary to find the relationship between biochar production parameters and feedstock properties with biochar energy properties. In this work, a rough set-based machine learning (RSML) approach has been applied to generate decision rules and classify biochar properties. The conditional attributes were biomass properties (volatile matter, fixed carbon, ash content, carbon, hydrogen, nitrogen, and oxygen) and pyrolysis conditions (operating temperature, heating rate residence time), while the decision attributes considered were yield, carbon content, and higher heating values. The rules generated were tested against a set of validation data and evaluated for their scientific coherency. Based on the decision rules generated, biomass with ash content of 11–14 wt%, volatile matter of 60–62 wt% and carbon content of 42–45.3 wt% can generate biochar with promising yield, carbon content and higher heating value via a pyrolysis process at an operating temperature of 425°C–475°C. This work provided the optimal biomass feedstock properties and pyrolysis conditions for biochar production with high mass and energy yield. © 2023 Informa UK Limited, trading as Taylor & Francis Group.,This work aims to implement and use machine learning algorithms to predict the yield of bio-oil during the pyrolysis of lignocellulosic biomass based on the physicochemical properties and composition of the biomass feed and pyrolysis conditions. The biomass pyrolysis process is influenced by different process parameters, such as pyrolysis temperature, heating rate, composition of biomass, and purge gas flow rate. The inter-relation between the yield of different pyrolysis products and process parameters can be well predicted by using different machine learning algorithms. In this study, different machine learning algorithms, namely, multi-linear regression, gradient boosting, random forest, and decision tree, have been trained on the dataset and the models are compared to identify the optimum method for the determination of bio-oil yield prediction model. Analysis of the results showed the gradient boosting method to possess a regression score of 0.97 and 0.89 for the training and testing sets with root-mean-squared error (RMSE) values of 1.19 and 2.39, respectively, and overcome the problem of overfitting. Therefore, the present study provides an approach to train a generalized machine learning model, which can be employed on large datasets while avoiding the error of overfitting. © 2022 Canadian Society for Chemical Engineering.,Co-pyrolysis of biomass and coal presents a promising opportunity for large-scale biomass utilization while reducing fossil fuel consumption. However, this process is highly complex and influenced by various factors, including the physicochemical properties of biomass and coal, their blending ratio, and operating parameters. To effectively comprehend and optimize the biomass-coal co-pyrolysis process, many experiments are required to achieve the desired product quantity and quality. In addressing this challenge, machine learning technology emerges as a valuable solution, as it can learn the relationships between input and output variables from available examples without explicit knowledge of the underlying mechanisms. This study conducts a comprehensive literature review to establish an extensive database encompassing biomass-coal compositions and pyrolysis reaction conditions. Using the collected data, robust statistical analyses are applied to understand better the underlying mechanisms governing the biomass-coal co-pyrolysis process. Four machine learning methods, namely support vector regression, artificial neural network, random forest regression, and gradient boosting regression, are employed to model the process. The best-performing model is subjected to an extensive feature importance analysis to identify the essential input features associated with each output response. The gradient boosting regression technique demonstrates superior performance with excellent results, characterized by higher coefficients of determination (R2 > 0.96) and lower errors (RMSE < 3.01 and MAE < 2.27). The temperature range of 450 to 550 °C, biomass blending ratios between 30 and 60%, and heating rates of 30 to 50 °C/min were identified as the conditions that maximize pyrolysis oil yield. Furthermore, the feature importance analysis reveals that the operating temperature and the biomass blending ratio are the most significant descriptors in the biomass-coal co-pyrolysis process. © 2023 Elsevier Ltd"
107,106,41,106_radiomics_radiomicsbased_cancer_prognosis,"radiomics,radiomicsbased,cancer,prognosis,prognostic,lymphoma,tumor,tomography,survivalcnn,radiomic","This study aimed to develop an analytic approach based on [18F]FDG PET radiomics using stacking ensemble learning to improve the outcome prediction in diffuse large B-cell lymphoma (DLBCL). Methods: In total, 240 DLBCL patients from 2 medical centers were divided into the training set (n 5 141), internal testing set (n 5 61), and external testing set (n 5 38). Radiomics features were extracted from pretreatment [18F]FDG PET scans at the patient level using 4 semiautomatic segmentation methods (SUV threshold of 2.5, SUV threshold of 4.0 [SUV4.0], 41% of SUVmax, and SUV threshold of mean liver uptake [PERCIST]). All extracted features were harmonized with the ComBat method. The intraclass correlation coefficient was used to evaluate the reliability of radiomics features extracted by different segmentation methods. Features from the most reliable segmentation method were selected by Pearson correlation coefficient analysis and the LASSO (least absolute shrinkage and selection operator) algorithm. A stacking ensemble learning approach was applied to build radiomics-only and combined clinical–radiomics models for prediction of 2-y progression-free survival and overall survival based on 4 machine learning classifiers (support vector machine, random forests, gradient boosting decision tree, and adaptive boosting). Confusion matrix, receiver-operating-characteristic curve analysis, and survival analysis were used to evaluate the model performance. Results: Among 4 semiautomatic segmentation methods, SUV4.0 segmentation yielded the highest interobserver reliability, with 830 (66.7%) selected radiomics features. The combined model constructed by the stacking method achieved the best discrimination performance. For progression-free survival prediction in the external testing set, the areas under the receiver-operating-characteristic curve and accuracy of the stacking-based combined model were 0.771 and 0.789, respectively. For overall survival prediction, the stacking-based combined model achieved an area under the curve of 0.725 and an accuracy of 0.763 in the external testing set. The combined model also demonstrated a more distinct risk stratification than the International Prognostic Index in all sets (log-rank test, all P, 0.05). Conclusion: The combined model that incorporates [18F]FDG PET radiomics and clinical characteristics based on stacking ensemble learning could enable improved risk stratification in DLBCL. COPYRIGHT ß 2023 by the Society of Nuclear Medicine and Molecular Imaging.,Objectives Optimizing a machine learning (ML) pipeline for radiomics analysis involves numerous choices in data set composition, preprocessing, and model selection. Objective identification of the optimal setup is complicated by correlated features, interdependency structures, and a multitude of available ML algorithms. Therefore, we present a radiomics-based benchmarking framework to optimize a comprehensive ML pipeline for the prediction of overall survival. This study is conducted on an image set of patients with hepatic metastases of colorectal cancer, for which radiomics features of the whole liver and of metastases from computed tomography images were calculated. A mixed model approach was used to find the optimal pipeline configuration and to identify the added prognostic value of radiomics features. Materials and Methods In this study, a large-scale ML benchmark pipeline consisting of preprocessing, feature selection, dimensionality reduction, hyperparameter optimization, and training of different models was developed for radiomics-based survival analysis. Portal-venous computed tomography imaging data from a previous prospective randomized trial evaluating radioembolization of liver metastases of colorectal cancer were quantitatively accessible through a radiomics approach. One thousand two hundred eighteen radiomics features of hepatic metastases and the whole liver were calculated, and 19 clinical parameters (age, sex, laboratory values, and treatment) were available for each patient. Three ML algorithms - a regression model with elastic net regularization (glmnet), a random survival forest (RSF), and a gradient tree-boosting technique (xgboost) - were evaluated for 5 combinations of clinical data, tumor radiomics, and whole-liver features. Hyperparameter optimization and model evaluation were optimized toward the performance metric integrated Brier score via nested cross-validation. To address dependency structures in the benchmark setup, a mixed-model approach was developed to compare ML and data configurations and to identify the best-performing model. Results Within our radiomics-based benchmark experiment, 60 ML pipeline variations were evaluated on clinical data and radiomics features from 491 patients. Descriptive analysis of the benchmark results showed a preference for RSF-based pipelines, especially for the combination of clinical data with radiomics features. This observation was supported by the quantitative analysis via a linear mixed model approach, computed to differentiate the effect of data sets and pipeline configurations on the resulting performance. This revealed the RSF pipelines to consistently perform similar or better than glmnet and xgboost. Further, for the RSF, there was no significantly better-performing pipeline composition regarding the sort of preprocessing or hyperparameter optimization. Conclusions Our study introduces a benchmark framework for radiomics-based survival analysis, aimed at identifying the optimal settings with respect to different radiomics data sources and various ML pipeline variations, including preprocessing techniques and learning algorithms. A suitable analysis tool for the benchmark results is provided via a mixed model approach, which showed for our study on patients with intrahepatic liver metastases, that radiomics features captured the patients' clinical situation in a manner comparable to the provided information solely from clinical parameters. However, we did not observe a relevant additional prognostic value obtained by these radiomics features.  © Wolters Kluwer Health, Inc. All rights reserved.,Background: Diffuse large B-cell lymphoma (DLBCL) is the most common subtype of non-Hodgkin lymphoma in adults. Standard treatment includes chemoimmunotherapy with R-CHOP or similar regimens. Despite treatment advancements, many patients with DLBCL experience refractory disease or relapse. While baseline 18F-fluorodeoxyglucose positron emission tomography (18F-FDG PET) parameters have shown promise in predicting survival, they may not fully capture lesion heterogeneity. This study aimed to assess the prognostic value of baseline 18F-FDG PET radiomics features in comparison with clinical factors and metabolic parameters for assessing 2-year progression-free survival (PFS) and 5-year overall survival (OS) in patients with DLBCL. Results: A total of 201 patients with DLBCL were enrolled in this study, and 1328 radiomics features were extracted. The radiomics signatures, clinical factors, and metabolic parameters showed significant prognostic value for individualized prognosis prediction in patients with DLBCL. Radiomics signatures showed the lowest Akaike information criterion (AIC) value and highest Harrell’s concordance index (C-index) value in comparison with clinical factors and metabolic parameters for both PFS (AIC: 571.688 vs. 596.040 vs. 576.481; C-index: 0.732 vs. 0.658 vs. 0.702, respectively) and OS (AIC: 339.843 vs. 363.671 vs. 358.412; C-index: 0.759 vs. 0.667 vs. 0.659, respectively). Statistically significant differences were observed in the area under the curve (AUC) values between the radiomics signatures and clinical factors for both PFS (AUC: 0.768 vs. 0.681, P = 0.017) and OS (AUC: 0.767 vs. 0.667, P = 0.023). For OS, the AUC of the radiomics signatures were significantly higher than those of metabolic parameters (AUC: 0.767 vs. 0.688, P = 0.007). However, for PFS, no significant difference was observed between the radiomics signatures and metabolic parameters (AUC: 0.768 vs. 0.756, P = 0.654). The combined model and the best-performing individual model (radiomics signatures) alone showed no significant difference for both PFS (AUC: 0.784 vs. 0.768, P = 0.163) or OS (AUC: 0.772 vs. 0.767, P = 0.403). Conclusions: Radiomics signatures derived from PET images showed the high predictive power for progression in patients with DLBCL. The combination of radiomics signatures, clinical factors, and metabolic parameters may not significantly improve predictive value beyond that of radiomics signatures alone. © 2023, The Author(s)."
108,107,41,107_handwritten_handwriting_recognition_ocr,"handwritten,handwriting,recognition,ocr,cnn,neural,recognize,scanned,rnn,characters","Optical Character Recognition (OCR), helps to convert different types of scanned documents, such as images into searchable and editable content. OCR is language dependant and very limited research has been carried out in this field for Urdu and Urdu like scriptures (E.g. Farsi, Arabic, and Urdu) unlike other languages like English, Hindi, etc. The lack of research work is attributed to a lack of publically available benchmark databases and inherent complexities involved in these languages like cursive nature and change in the shape of a character depending upon its position in a ligature. Each character has 2–4 different shapes depending upon its position in the word; initial, medial, or final. In this article, the we have proposed a methodology to automate the data collection process and collected a large handwritten dataset of 110,785 Urdu characters and laid out the comaparative analysis of two deep learning models SimpleRNN and LSTM to showcase the potential of RNN models for chararacter recognition. Data was collected from 250 authors on the A4 size sheet. Each sheet contains 132 shapes for Urdu characters and 10 numerals. As far as the authors know, this is the first time that such a large dataset has been proposed which contains all the possible shapes of Urdu character numerals as well. Experimentation has been done for the numeral, full characters, and for whole data set separately to lay a comparative analysis of classification capabilities of RNN and LSTM models. Despite of such inherit complexities in Urdu script, the RNN and LSTM models proved to be more effective in achieving a high accuracy rates. Respective accuracy for RNN achieved for each category are: 96.96% for numerals, 85.22% for full characters and 73.62% for whole data and LSTM outperforms the prior one with max accuracy for each category of data as 97.80% for numerals, 97.43% for full characters and 91.30% for whole data. Besides, the proposed dataset opens a new window for future research, showcasing the huge potential of this dataset for data analysis not only for Urdu language but for other languages like Arabic, Persian,etc. which uses similar kind of character sets. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,With the development of society, the intangible cultural heritage of Chinese Nüshu is in danger of extinction. To promote the research and popularization of traditional Chinese culture, we use deep learning to automatically detect and recognize handwritten Nüshu characters. To address difficulties such as the creation of a Nüshu character dataset, uneven samples, and difficulties in character recognition, we first build a large-scale handwritten Nüshu character dataset, HWNS2023, by using various data augmentation methods. This dataset contains 5500 Nüshu images and 1364 labeled character samples. Second, in this paper, we propose a two-stage scheme model combining GoogLeNet-tiny and YOLOv5-CBAM (SGooTY) for Nüshu recognition. In the first stage, five basic deep learning models including AlexNet, VGGNet16, GoogLeNet, MobileNetV3, and ResNet are trained and tested on the dataset, and the model structure is improved to enhance the accuracy of recognising handwritten Nüshu characters. In the second stage, we combine an object detection model to re-recognize misidentified handwritten Nüshu characters to ensure the accuracy of the overall system. Experimental results show that in the first stage, the improved model achieves the highest accuracy of 99.3% in recognising Nüshu characters, which significantly improves the recognition rate of handwritten Nüshu characters. After integrating the object recognition model, the overall recognition accuracy of the model reached 99.9%. © 2023 by the authors.,Any optical character recognition (OCR) system recognizes each and every character present in any document image. But, the task of performing OCR in ancient handwritten document images is challenging due to the existence of faded text and dark spots in the ancient document images. The presence of intrinsic patterns of characters and large number of character classes in most of the Indian scripts make this task even more challenging. This research article proposes a novel hybrid deep learning based OCR method to recognize each character present in ancient handwritten document image written in two different Indian scripts, Devanagari and Maithili. Various discriminating features have been extracted from each character present in the document using several convolutional layers and each extracted feature vector has been classified into a proper character class using hybrid deep learning model. The hybrid deep learning model consists of one dense layer and several recurrently connected hidden layers. Both long-short-term-memory (LSTM) and Bidirectional-long-short-term-memory (Bi-LSTM) variants of recurrent neural network (RNN) have been employed in the portion of recurrently connected hidden layers of hybrid deep learning model. The performance of the proposed OCR method has been evaluated on two self-generated datasets of ancient handwritten document images in Devanagari and Maithili scripts. The proposed method has achieved the character recognition accuracy of 96.97 percent and 95.83 percent in Devanagari and Maithili scripts respectively. The experimental results demonstrate that the proposed OCR method outperforms the state-of-the-art methods in this regard. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
109,108,41,108_preoperative_perioperative_postoperative_transfusion,"preoperative,perioperative,postoperative,transfusion,prediction,predicting,prognostic,arthroplasty,hospital,surgicalsite","Objective: Postoperative red blood cell (RBC) transfusion is widely used during the perioperative period but is often associated with a high risk of infection and complications. However, prediction models for RBC transfusion in patients with orthopedic surgery have not yet been developed. We aimed to identify predictors and constructed prediction models for RBC transfusion after orthopedic surgery using interpretable machine learning algorithms. Methods: This retrospective cohort study reviewed a total of 59,605 patients undergoing orthopedic surgery from June 2013 to January 2019 across 7 tertiary hospitals in China. Patients were randomly split into training (80%) and test subsets (20%). The feature selection method of recursive feature elimination (RFE) was used to identify an optimal feature subset from thirty preoperative variables, and six machine learning algorithms were applied to develop prediction models. The Shapley Additive exPlanations (SHAP) value was employed to evaluate the contribution of each predictor towards the prediction of postoperative RBC transfusion. For simplicity of the clinical utility, a risk score system was further established using the top risk factors identified by machine learning models. Results: Of the 59,605 patients with orthopedic surgery, 19,921 (33.40%) underwent postoperative RBC transfusion. The CatBoost model exhibited an AUC of 0.831 (95% CI: 0.824–0.836) on the test subset, which significantly outperformed five other prediction models. The risk of RBC transfusion was associated with old age (>60 years) and low RBC count (<4.0 × 1012/L) with clear threshold effects. Extremes of BMI, low albumin, prolonged activated partial thromboplastin time, repair and plastic operations on joint structures were additional top predictors for RBC transfusion. The risk score system derived from six risk factors performed well with an AUC of 0.801 (95% CI: 0.794–0.807) on the test subset. Conclusion: By applying an interpretable machine learning framework in a large-scale multicenter retrospective cohort, we identified novel modifiable risk factors and developed prediction models with good performance for postoperative RBC transfusion in patients undergoing orthopedic surgery. Our findings may allow more precise identification of high-risk patients for optimal control of risk factors and achieve personalized RBC transfusion for orthopedic patients. 2023 Chen, Cai, Cao, Lin, Huang, Zhuang, Xiao, Guan, Wang, Xia, Jiao, Du, Jiang and Wang.,Background: The rates of blood transfusion following primary and revision total hip arthroplasty (THA) remain as high as 9% and 18%, respectively, contributing to patient morbidity and healthcare costs. Existing predictive tools are limited to specific populations, thereby diminishing their clinical applicability. This study aimed to externally validate our previous institutionally developed machine learning (ML) algorithms to predict the risk of postoperative blood transfusion following primary and revision THA using national inpatient data. Methods: Five ML algorithms were trained and validated using data from 101,266 primary THA and 8,594 revision THA patients from a large national database to predict postoperative transfusion risk after primary and revision THA. Models were assessed and compared based on discrimination, calibration, and decision curve analysis. Results: The most important predictors of transfusion following primary and revision THA were preoperative hematocrit (<39.4%) and operation time (>157 minutes), respectively. All ML models demonstrated excellent discrimination (area under the curve (AUC) >0.8) in primary and revision THA patients, with artificial neural network (AUC = 0.84, slope = 1.11, intercept = ?0.04, Brier score = 0.04), and elastic-net-penalized logistic regression (AUC = 0.85, slope = 1.08, intercept = ?0.01, and Brier score = 0.12) performing best, respectively. On decision curve analysis, all 5 models demonstrated a higher net benefit than the conventional strategy of intervening for all or no patients in both patient cohorts. Conclusions: This study successfully validated our previous institutionally developed ML algorithms for the prediction of blood transfusion following primary and revision THA. Our findings highlight the potential generalizability of predictive ML tools developed using nationally representative data in THA patients. © 2023 Elsevier Inc.,BackgroundAseptic revision THA and TKA are associated with an increased risk of adverse outcomes compared with primary THA and TKA. Understanding the risk profiles for patients undergoing aseptic revision THA or TKA may provide an opportunity to decrease the risk of postsurgical complications. There are risk stratification tools for postoperative complications after aseptic revision TKA or THA; however, current tools only include nonmodifiable risk factors, such as medical comorbidities, and do not include modifiable risk factors.Questions/purposes(1) Can machine learning predict 30-day mortality and complications for patients undergoing aseptic revision THA or TKA using a cohort from the American College of Surgeons National Surgical Quality Improvement Program database? (2) Which patient variables are the most relevant in predicting complications?MethodsThis was a temporally validated, retrospective study analyzing the 2014 to 2019 National Surgical Quality Improvement Program database, as this database captures a large cohort of aseptic revision THA and TKA patients across a broad range of clinical settings and includes preoperative laboratory values. The training data set was 2014 to 2018, and 2019 was the validation data set. Given that predictive models learn expected prevalence of outcomes, this split allows assessment of model performance in contemporary patients. Between 2014 and 2019, a total of 24,682 patients underwent aseptic revision TKA and 17,871 patients underwent aseptic revision THA. Of those, patients with CPT codes corresponding to aseptic revision TKA or THA were considered as potentially eligible. Based on excluding procedures involving unclean wounds, 78% (19,345 of 24,682) of aseptic revision TKA procedures and 82% (14,711 of 17,871) of aseptic revision THA procedures were eligible. Ten percent of patients in each of the training and validation cohorts had missing predictor variables. Most of these missing data were preoperative sodium or hematocrit (8% in both the training and validation cohorts). No patients had missing outcome data. No patients were excluded due to missing data. The mean patient was age 66 ± 12 years, the mean BMI was 32 ± 7 kg/m2, and the mean American Society of Anesthesiologists (ASA) Physical Score was 3 (56%). XGBoost was then used to create a scoring tool for 30-day adverse outcomes. XGBoost was chosen because it can handle missing data, it is nonlinear, it can assess nuanced relationships between variables, it incorporates techniques to reduce model complexity, and it has a demonstrated record of producing highly accurate machine-learning models. Performance metrics included discrimination and calibration. Discrimination was assessed by c-statistics, which describe the area under the receiver operating characteristic curve. This quantifies how well a predictive model discriminates between patients who have the outcome of interest versus those who do not. Relevant ranges for c-statistics include good (0.70 to 0.79), excellent (0.80 to 0.89), and outstanding (> 0.90). We estimated 95% confidence intervals (CIs) for c-statistics by 500-sample bootstrapping. Calibration curves quantify reliability of model predictions. Reliable models produce prediction probabilities for outcomes that are similar to observed probabilities of those outcomes, so a well-calibrated model should demonstrate a calibration curve that does not deviate substantially from a line of slope 1 and intercept 0. Calibration curves were generated on the 2019 validation data. Shapley Additive Explanations (SHAP) visualizations were used to investigate feature importance to gain insight into how models made predictions. The models were built into an online calculator for ongoing testing and validation. The risk calculator, which is freely available (http://nb-group.org/rev2/), allows a user to input patient data to calculate postoperative risk of 30-day mortality, cardiac, and respiratory complications after aseptic revision TKA or THA. A post hoc analysis was performed to assess whether using data from 2020 would improve calibration on 2019 data.ResultsThe model accurately predicted mortality, cardiac complications, and respiratory complications after aseptic revision THA or TKA, with c-statistics of 0.88 (95% CI 0.83 to 0.93), 0.80 (95% CI 0.75 to 0.84), and 0.78 (95% CI 0.74 to 0.82), respectively, on internal validation and 0.87 (95% CI 0.77 to 0.96), 0.70 (95% CI 0.61 to 0.78), and 0.82 (95% CI 0.75 to 0.88), respectively, on temporal validation. Calibration curves demonstrated slight over-confidence in predictions (most predicted probabilities were higher than observed probabilities). Post hoc analysis of 2020 data did not yield improved calibration on the 2019 validation set. Important risk factors for all models included increased age and higher ASA, BMI, hematocrit level, and sodium level. Hematocrit and ASA were in the top three most important features for all models. The factor with the strongest association for mortality and cardiac complication models was age, and for the respiratory model, chronic obstructive pulmonary disease. Risk related to sodium followed a U-shaped curve. Preoperative hyponatremia and hypernatremia predicted an increased risk of mortality and respiratory complications, with a nadir of 138 mmol/L; hyponatremia was more strongly associated with mortality than hypernatremia. A hematocrit level less than 36% predicted an increased risk of all three adverse outcomes. A BMI less than 24 kg/m2 - and especially less than 20 kg/m2 - predicted an increased risk of all three adverse outcomes, with little to no effect for higher BMI.ConclusionThis temporally validated model predicted 30-day mortality, cardiac complications, and respiratory complications after aseptic revision THA or TKA with c-statistics ranging from 0.78 to 0.88. This freely available risk calculator can be used preoperatively by surgeons to educate patients on their individual postoperative risk of these specific adverse outcomes. Unanswered questions that remain include whether altering the studied preoperative patient variables, such as sodium or hematocrit, would affect postoperative risk of adverse outcomes; however, a prospective cohort study is needed to answer this question.  Copyright © 2022 by the Association of Bone and Joint Surgeons."
110,109,40,109_docking_ligand_ligands_proteins,"docking,ligand,ligands,proteins,protein,docked,geodock,inhibitors,proteinprotein,molecular","The expertise accumulated in deep neural network-based structure prediction has been widely transferred to the field of protein-ligand binding pose prediction, thus leading to the emergence of a variety of deep learning-guided docking models for predicting protein-ligand binding poses without relying on heavy sampling. However, their prediction accuracy and applicability are still far from satisfactory, partially due to the lack of protein-ligand binding complex data. To this end, we create a large-scale complex dataset containing ?9 M protein-ligand docking complexes for pre-training, and propose CarsiDock, the first deep learning-guided docking approach that leverages pre-training of millions of predicted protein-ligand complexes. CarsiDock contains two main stages, i.e., a deep learning model for the prediction of protein-ligand atomic distance matrices, and a translation, rotation and torsion-guided geometry optimization procedure to reconstruct the matrices into a credible binding pose. The pre-training and multiple innovative architectural designs facilitate the dramatically improved docking accuracy of our approach over the baselines in terms of multiple docking scenarios, thereby contributing to its outstanding early recognition performance in several retrospective virtual screening campaigns. Further explorations demonstrate that CarsiDock can not only guarantee the topological reliability of the binding poses but also successfully reproduce the crucial interactions in crystalized structures, highlighting its superior applicability. © 2024 The Royal Society of Chemistry.,Background: Molecular docking-based virtual screening (VS) aims to choose ligands with potential pharmacological activities from millions or even billions of molecules. This process could significantly cut down the number of compounds that need to be experimentally tested. However, during the docking calculation, many molecules have low affinity for a particular protein target, which waste a lot of computational resources. Methods: We implemented a fast and practical molecular screening approach called DL-DockVS (deep learning dock virtual screening) by using deep learning models (regression and classification models) to learn the outcomes of pipelined docking programs step-by-step. Results: In this study, we showed that this approach could successfully weed out compounds with poor docking scores while keeping compounds with potentially high docking scores against 10 DUD-E protein targets. A self-built dataset of about 1.9 million molecules was used to further verify DL-DockVS, yielding good results in terms of recall rate, active compounds enrichment factor and runtime speed. Conclusions: We comprehensively evaluate the practicality and effectiveness of DL-DockVS against 10 protein targets. Due to the improvements of runtime and maintained success rate, it would be a useful and promising approach to screen ultra-large compound libraries in the age of big data. It is also very convenient for researchers to make a well-trained model of one specific target for predicting other chemical libraries and high docking-score molecules without docking computation again. © The Author(s) 2023. Published by Higher Education Press.,Protein-ligand docking is an essential tool in structure-based drug design with applications ranging from virtual high-throughput screening to pose prediction for lead optimization. Most docking programs for pose prediction are optimized for redocking to an existing cocrystallized protein structure, ignoring protein flexibility. In real-world drug design applications, however, protein flexibility is an essential feature of the ligand-binding process. Flexible protein-ligand docking still remains a significant challenge to computational drug design. To target this challenge, we present a deep learning (DL) model for flexible protein-ligand docking based on the prediction of an intermolecular Euclidean distance matrix (EDM), making the typical use of iterative search algorithms obsolete. The model was trained on a large-scale data set of protein-ligand complexes and evaluated on independent test sets. Our model generates high quality poses for a diverse set of protein and ligand structures and outperforms comparable docking methods. © 2023 American Chemical Society."
111,110,40,110_gait_walking_recognition_features,"gait,walking,recognition,features,movements,kinematic,posture,motion,pose,kinematics","Walking ability of broilers can be improved by selective breeding, but large-scale phenotypic records are required. Currently, gait of individual broilers is scored by trained experts, however, precision phenotyping tools could offer a more objective and high-throughput alternative. We studied whether specific walking characteristics determined through pose estimation are linked to gait in broilers. We filmed male broilers from behind, walking through a 3 m × 0.4 m (length × width) corridor one by one, at 3 time points during their lifetime (at 14, 21, and 33 d of age). We used a deep learning model, developed in DeepLabCut, to detect and track 8 keypoints (head, neck, left and right knees, hocks, and feet) of broilers in the recorded videos. Using the keypoints of the legs, 6 pose features were quantified during the double support phase of walking, and 1 pose feature was quantified during steps, at maximum leg lift. Gait was scored on a scale from 0 to 5 by 4 experts, using the videos recorded on d 33, and the broilers were further classified as having either good gait (mean gait score ?2) or suboptimal gait (mean gait score >2). The relationship of pose features on d 33 with gait was analyzed using the data of 84 broilers (good gait: 57.1%, suboptimal gait: 42.9%). Birds with suboptimal gait had sharper hock joint lateral angles and lower hock-feet distance ratios during double support on d 33, on average. During steps, relative step height was lower in birds with suboptimal gait. Step height and hock-feet distance ratio showed the largest mean deviations in broilers with suboptimal gait compared to those with good gait. We demonstrate that pose estimation can be used to assess walking characteristics during a large part of the productive life of broilers, and to phenotype and monitor broiler gait. These insights can be used to understand differences in the walking patterns of lame broilers, and to build more sophisticated gait prediction models. © 2023 The Authors,A quantitative gait assessment system is crucial for clinical analysis and decision-making. Such rigorous evaluation involves costly clinical setups and domain experts for observation and analysis. To circumvent such constraints, the proposed work is conducted in a markerless environment and divided into three stages: First, we prepared a markerless gait database using videos from MNIT RAMAN LABORATORY in Jaipur. Second, we adapt the skeletal landmark data to generate kinematic gait characteristics comparable to gold-standard marker-based techniques. We provide a novel set of parameters based on video sequences and spatiotemporal and sagittal kinematic parameters to optimize accuracy and reliability. Third, we develop multi-feature based gait analysis, an ensemble model based on Convolutional Neural Networks + LSTM (Long-Short Term Memory), for gait classification. In addition, we deployed transfer learning models to correlate with our ensemble model. The findings indicate that gait analysis can be used successfully in a low-cost clinical gait monitoring system in a constraint-free environment. While considering the multiple gait variables, our proposed model attained an accuracy of 95.3%. Our model for quantifying gait analysis will improve access to quantitative gait analysis in clinics and rehabilitation centers and enable researchers to conduct large-scale studies for gait-related disorders. © 2023 John Wiley & Sons Ltd.,Walking speed provides a good proxy for gait abnormalities as individuals with medical morbidities tend to walk slower than healthy subjects. The walking speed assessment can be utilized as a powerful predictor of health events, which are related to musculoskeletal disorder and mental disease. The expanding need to distinguish gait pattern of individual according to health status has driven various analytical methods such as observational and instrumented gait analysis methods in capturing the human movement. Significant advances in 3D-gait analysis system have enabled a myriad of studies that advance our understanding of gait biomechanics. However, the data samples obtained from this system are large, with high degrees of variability. Hence, developing a reliable approach to distinguish gait patterns specific to the underlying pathologies is of paramount importance. Through this study, we have proposed the use of a deep learning framework with recurrent neural network (RNN) to interpret human walking speed based on kinematic data, whereby RNN is capable for time series data processing. Nevertheless, this model can hardly learn long-range dependencies across time steps in a sequence due to vanishing gradient. In this study, an improved RNN integrated with NVIDIA CUDA® Deep Neural Network Library Long Short-Term Memory (cuDNN LSTM) is introduced. This model is capable to classify the gait patterns of different walking speeds from seventeen healthy subjects, with a total of 453 gait cycles. Gait kinematic parameters were employed as the input layer of the deep learning architecture based on RNN is integrated with cuDNN LSTM. Our proposed framework has achieved an accuracy of 97% to classify different speeds (slow, normal and fast). This study therefore presents a method towards establishing a powerful tool to translate machine learning for gait analysis into clinical practice, whereby automated classifications of gait pattern could now improve acuity of clinical diagnoses. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
112,111,40,111_anomaly_outliers_outlier_anomalies,"anomaly,outliers,outlier,anomalies,anomalous,timeseries,monitoring,detection,detect,dataset","Detecting anomalies in large complex systems is a critical and challenging task. The difficulties arise from several aspects. First, collecting ground truth labels or prior knowledge for anomalies is hard in real-world systems, which often lead to limited or no anomaly labels in the dataset. Second, anomalies in large systems usually occur in a collective manner due to the underlying dependency structure among devices or sensors. Lastly, real-time anomaly detection for high-dimensional data requires efficient algorithms that are capable of handling different types of data (i.e. continuous and discrete). We propose a correlation structure-based collective anomaly detection (CSCAD) model for high-dimensional anomaly detection problem in large systems, which is also generalizable to semi-supervised or supervised settings. Our framework utilize graph convolutional network combining a variational autoencoder to jointly exploit the feature space correlation and reconstruction deficiency of samples to perform anomaly detection. We propose an extended mutual information (EMI) metric to mine the internal correlation structure among different data features, which enhances the data reconstruction capability of CSCAD. The reconstruction loss and latent standard deviation vector of a sample obtained from reconstruction network can be perceived as two natural anomalous degree measures. An anomaly discriminating network can then be trained using low anomalous degree samples as positive samples, and high anomalous degree samples as negative samples. Experimental results on five public datasets demonstrate that our approach consistently outperforms all the competing baselines. © 1989-2012 IEEE.,Large-scale sewage treatment plants are one of the typical Industrial Internet of Things systems, where the presence of a large number of sensors generates massive dynamic time series data, and such multivariate time series data are usually time-dependent and random. Therefore, there is a certain risk when fitting the potential anomalies of real-world data, which will bring great challenges to anomaly detection. In this article, we propose a time-series mutual adversarial network (TMAN), a novel reconstruction model for anomaly detection on multivariate time series. It is based on the idea of adversarial learning and consists of two identical subnetworks. During the training process, two subnetworks can independently complete the learning of the time distribution of normal samples of industrial time series data for mutual adversarial. In the process of detecting, we obtain the residual values of TMAN reconstructed for different time series samples to discriminate anomalies. We combine TMAN and anomaly determination mechanisms to build a new industrial time series anomaly detection framework named TMANomaly. In addition, we select the dataset features with a grey correlation algorithm to achieve very high performance with a small number of features. Experimental results show that our proposed TMANomaly outperforms five popular anomaly detection methods and effectively improves the accuracy of industrial multivariate time series anomaly detection.  © 2005-2012 IEEE.,In a large-scale cloud environment, many key performance indicators (KPIs) of entities are monitored in real time. These multivariate time series consist of high-dimensional, high-noise, random and time-dependent data. As a common method implemented in artificial intelligence for IT operations (AIOps), time series anomaly detection has been widely studied and applied. However, the existing detection methods cannot fully consider the influence of multiple factors and cannot quickly and accurately detect anomalies in multivariate KPIs of entities. Concurrently, fine-grained root cause locations cannot be determined for detected anomalies and often require abundant normal data that are difficult to obtain for model training. To solve these problems, we propose a long short-term memory (LSTM)-based semisupervised variational autoencoder (VAE) anomaly detection strategy called LR-SemiVAE. First, LR-SemiVAE uses VAE to perform feature dimension reduction and reconstruction of multivariate time series data and judges whether the entity is abnormal by calculating the reconstruction probability score. Second, by introducing an LSTM network into the VAE encoder and decoder, the model can fully learn the time dependence of multivariate time series. Then, LR-SemiVAE predicts the data labels by introducing a classifier to reduce the dependence on the original labeled data during model training. Finally, by proposing a new evidence lower bound (ELBO) loss function calculation method, LR-SemiVAE pays attention to the normal pattern and ignores the abnormal pattern during training to reduce the time cost of removing random anomaly and noise data. However, due to the limitations of LSTM in learning the long-term dependence of time series data, based on LR-SemiVAE, we propose a transformer-based semisupervised VAE anomaly detection and location strategy called RT-SemiVAE for cluster systems with complex service dependencies. This method learns the long-term dependence of multivariate time series by introducing a parallel multihead attention mechanism transformer, while LSTM is used to capture short-term dependence, and the introduction of parallel computing also markedly reduces model training time. After RT-SemiVAE detects entity anomalies, it traces the root entities according to the obtained service dependence graph and locates the root causes at the indicator level. We verify the strategies by using public data sets and constructing a system prototype. Experimental results show that compared with existing baseline methods, the LR-SemiVAE and RT-SemiVAE strategies can detect anomalies more quickly and accurately and perform fine-grained and accurate localization of the root causes of anomalies. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
113,112,40,112_deepspeed_distributed_parallel_parallelism,"deepspeed,distributed,parallel,parallelism,networks,labnet,straggler,cluster,speedup,network","Foundation models are in the process of becoming the dominant deep learning technology. Pretraining a foundation model is always time-consuming due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the pretraining process is extremely memory-and communication-intensive. These challenges make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism, and tensor model parallelism, to achieve high training efficiency. However, current 3D parallelism frameworks still encounter two issues: i) they are not transparent to model developers, requiring manual model modification to parallelize training, and ii) their utilization of computation resources, GPU memory, and network bandwidth is insufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automatically deploys 3D parallelism with an automatic model partitioner, which includes a graph-sharding algorithm and proxy node-based model graph. Merak also offers a non-intrusive API to scale out foundation model training with minimal code modification. In addition, we design a high-performance 3D parallel runtime engine that employs several techniques to exploit available training resources, including a shifted critical path pipeline schedule that increases computation utilization, stage-aware recomputation that makes use of idle worker memory, and sub-pipelined tensor model parallelism that overlaps communication and computation. Experiments on 64 GPUs demonstrate Merak's capability to speed up training performance over state-of-the-art 3D parallelism frameworks of models with 1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42, 1.39, 1.43, and 1.61×, respectively.  © 1990-2012 IEEE.,Deep learning is a growing technique used to solve complex artificial intelligence (AI) problems. Large-scale deep learning has become a significant issue as a result of the expansion of datasets and the complexity of deep learning models. For training large-scale models, the cloud can be used as a distributed HPC (high-performance computing) tool with benefits in cost and flexibility. However, one of the major performance barriers in distributed deep learning in a distributed HPC environment is the network. The performance is often limited by heavy traffic like many stochastic gradient descent transfers for distributed communication. There are many network studies in distributed deep learning to solve these problems, but most research only focuses on improving communication performance and applying new methods or algorithms like overlapping parameter synchronization to minimize communication delay rather than considering the actual network. In this paper, we are focusing on the actual network, especially in a distributed HPC environment. In such an environment, if cluster nodes are assigned to different zones/regions which means a set of an appropriate number of distributed HPC nodes when performing distributed deep learning tasks, performance degradation due to network delay may occur. The proposed network optimization algorithm ensures that distributed work is placed in the same zone as much as possible to reduce network delay. Furthermore, scoring using network monitoring tools like loss, delay, and throughput is applied to select the optimal node within the zone. Our proposal has been validated on the Kubernetes platform, an open source orchestrator for the automatic management and deployment of micro-services. The performance of distributed deep learning is improved through the proposed scheduler. © 2023 by the authors.,With the increasing scale of data sets and neural network models, distributed training of deep neural networks has become a trend. The main distributed parallel technology is based on expert experience, it is low efficient and hard to optimize as it needs lots of domain knowledge. There are some researchers have proposed auto-parallel technology to implement model distributed training which focused on specific models and parallel optimization factors. These methods have the problems of single factor of performance optimization, complex and low efficiency, etc. In this paper, we propose an adaptive distributed parallel training method (MP-DPS), based on the node merging of heterogeneous computing power-aware and path prediction, to search optimal parallel strategy automatically in large-scale networks. Firstly, we construct a multidimensional performance cost model to guide the design and implementation of the distributed parallel strategy. Secondly, we propose a node merging method with heterogeneous computing power awareness to reduce the search space and improve search efficiency. Finally, a graph search algorithm based on path prediction is proposed, it finds the optimal distributed parallel strategy by optimizing critical path execution time, which is based on predicting the optimal placement of critical operator node on the path. The experiments show that the deep learning model (such as ResNet, NasNet, etc.) can effectively be trained on 4 GPU and 8 GPU (P100) with the distributed parallel strategy searched by MP-DPS method, and the search time of optimal distributed parallel strategy can be reduced efficiently, compared with the FastT method. © 2022, China Computer Federation (CCF)."
114,113,40,113_alzheimer_neural_cnn_neuroimaging,"alzheimer,neural,cnn,neuroimaging,alzheimers,dementia,convolutional,neurodegenerative,mri,imaging","Alzheimer’s disease (AD) is a neurodegenerative disorder characterized by cognitive impairment and aberrant protein deposition in the brain. Therefore, the early detection of AD is crucial for the development of effective treatments and interventions, as the disease is more responsive to treatment in its early stages. It is worth mentioning that deep learning techniques have been successfully applied in recent years to a wide range of medical imaging tasks, including the detection of AD. These techniques have the ability to automatically learn and extract features from large datasets, making them well suited for the analysis of complex medical images. In this paper, we propose an improved lightweight deep learning model for the accurate detection of AD from magnetic resonance imaging (MRI) images. Our proposed model achieves high detection performance without the need for deeper layers and eliminates the use of traditional methods such as feature extraction and classification by combining them all into one stage. Furthermore, our proposed method consists of only seven layers, making the system less complex than other previous deep models and less time-consuming to process. We evaluate our proposed model using a publicly available Kaggle dataset, which contains a large number of records in a small dataset size of only 36 Megabytes. Our model achieved an overall accuracy of 99.22% for binary classification and 95.93% for multi-classification tasks, which outperformed other previous models. Our study is the first to combine all methods used in the publicly available Kaggle dataset for AD detection, enabling researchers to work on a dataset with new challenges. Our findings show the effectiveness of our lightweight deep learning framework to achieve high accuracy in the classification of AD. © 2023 by the authors.,Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that affects millions of people worldwide. Early and accurate prediction of AD progression is crucial for early intervention and personalized treatment planning. Although AD does not yet have a reliable therapy, several medications help slow down the disease’s progression. However, more study is still needed to develop reliable methods for detecting AD and its phases. In the recent past, biomarkers associated with AD have been identified using neuroimaging methods. To uncover biomarkers, deep learning techniques have quickly emerged as a crucial methodology. A functional molecular imaging technique known as fluorodeoxyglucose positron emission tomography (18F-FDG-PET) has been shown to be effective in assisting researchers in understanding the morphological and neurological alterations to the brain associated with AD. Convolutional neural networks (CNNs) have also long dominated the field of AD progression and have been the subject of substantial research, while more recent approaches like vision transformers (ViT) have not yet been fully investigated. In this paper, we present a self-supervised learning (SSL) method to automatically acquire meaningful AD characteristics using the ViT architecture by pretraining the feature extractor using the self-distillation with no labels (DINO) and extreme learning machine (ELM) as classifier models. In this work, we examined a technique for predicting mild cognitive impairment (MCI) to AD utilizing an SSL model which learns powerful representations from unlabeled 18F-FDG PET images, thus reducing the need for large-labeled datasets. In comparison to several earlier approaches, our strategy showed state-of-the-art classification performance in terms of accuracy (92.31%), specificity (90.21%), and sensitivity (95.50%). Then, to make the suggested model easier to understand, we highlighted the brain regions that significantly influence the prediction of MCI development. Our methods offer a precise and efficient strategy for predicting the transition from MCI to AD. In conclusion, this research presents a novel Explainable SSL-ViT model that can accurately predict AD progress based on 18F-FDG PET scans. SSL, attention, and ELM mechanisms are integrated into the model to make it more predictive and interpretable. Future research will enable the development of viable treatments for neurodegenerative disorders by combining brain areas contributing to projection with observed anatomical traits. © 2023 by the authors.,Alzheimer’s disease (AD) is a chronic and common form of dementia that mainly affects elderly individuals. The disease is dangerous because it causes damage to brain cells and tissues before the symptoms appear, and there is no medicinal or surgical treatment available yet for AD. AD causes loss of memory and functionality control in multiple degrees according to AD’s progression level. However, early diagnosis of AD can hinder its progression. Brain imaging tools such as magnetic resonance imaging (MRI), computed tomography (CT) scans, positron emission tomography (PET), etc. can help in medical diagnosis of AD. Recently, computer-aided diagnosis (CAD) such as deep learning applied to brain images obtained with these tools, has been an established strategic methodology that is widely used for clinical assistance in prognosis of AD. In this study, we proposed an intelligent methodology for building a convolutional neural network (CNN) from scratch to detect AD stages from the brain MRI images dataset and to improve patient care. It is worth mentioning that training a deep-learning model requires a large amount of data to produce accurate results and prevent the model from overfitting problems. Therefore, for better understanding of classifiers and to overcome the model overfitting problem, we applied data augmentation to the minority classes in order to increase the number of MRI images in the dataset. All experiments were conducted using Alzheimer’s MRI dataset consisting of brain MRI scanned images. The performance of the proposed model determines detection of the four stages of AD. Experimental results show high performance of the proposed model in that the model achieved a 99.38% accuracy rate, which is the highest so far. Moreover, the proposed model performance in terms of accuracy, precision, sensitivity, specificity, and f-measures is promising when compared to the very recent state-of-the-art domain-specific models existing in the literature. © 2023 Tech Science Press. All rights reserved."
115,114,40,114_stroke_ischemic_cerebral_artery,"stroke,ischemic,cerebral,artery,thrombolysis,neurologists,hemorrhage,vessel,prognosis,outcomes","Background: Despite evolving treatments, functional recovery in patients with large vessel occlusion stroke remains variable and outcome prediction challenging. Can we improve estimation of functional outcome with interpretable deep learning models using clinical and magnetic resonance imaging data? Methods: In this observational study, we collected data of 222 patients with middle cerebral artery M1 segment occlusion who received mechanical thrombectomy. In a 5-fold cross validation, we evaluated interpretable deep learning models for predicting functional outcome in terms of modified Rankin scale at 3 months using clinical variables, diffusion weighted imaging and perfusion weighted imaging, and a combination thereof. Based on 50 test patients, we compared model performances to those of 5 experienced stroke neurologists. Prediction performance for ordinal (modified Rankin scale score, 0-6) and binary (modified Rankin scale score, 0-2 versus 3-6) functional outcome was assessed using discrimination and calibration measures like area under the receiver operating characteristic curve and accuracy (percentage of correctly classified patients). Results: In the cross validation, the model based on clinical variables and diffusion weighted imaging achieved the highest binary prediction performance (area under the receiver operating characteristic curve, 0.766 [0.727-0.803]). Performance of models using clinical variables or diffusion weighted imaging only was lower. Adding perfusion weighted imaging did not improve outcome prediction. On the test set of 50 patients, binary prediction performance between model (accuracy, 60% [55.4%-64.4%]) and neurologists (accuracy, 60% [55.8%-64.21%]) was similar when using clinical data. However, models significantly outperformed neurologists when imaging data were provided, alone or in combination with clinical variables (accuracy, 72% [67.8%-76%] versus 64% [59.8%-68.4%] with clinical and imaging data). Prediction performance of neurologists with comparable experience varied strongly. Conclusions: We hypothesize that early prediction of functional outcome in large vessel occlusion stroke patients may be significantly improved if neurologists are supported by interpretable deep learning models.  © 2023 American Heart Association, Inc.,At present, clinicians are expected to manage a large volume of complex clinical, laboratory, and imaging data, necessitating sophisticated analytic approaches. Machine learning-based models can use this vast amount of data to create forecasting models. We aimed to predict short- and medium-term functional outcomes in acute ischemic stroke (AIS) patients with proximal middle cerebral artery (MCA) occlusions using machine learning models with clinical, laboratory, and quantitative imaging data as inputs. Included were consecutive AIS patients with MCA M1 and proximal M2 occlusions. The XGBoost, LightGBM, CatBoost, and Random Forest were used to predict the outcome. Minimum redundancy maximum relevancy was used for selecting features. The primary outcomes were the National Institutes of Health Stroke Scale (NIHSS) shift and the modified Rankin Score (mRS) at 90 days. The algorithm with the highest area under the receiver operating characteristic curve (AUROC) for predicting the favorable and unfavorable outcome groups at 90 days was LightGBM. Random Forest had the highest AUROC when predicting the favorable and unfavorable groups based on the NIHSS shift. Using clinical, laboratory, and imaging parameters in conjunction with machine learning, we accurately predicted the functional outcome of AIS patients with proximal MCA occlusions. © 2023 by the authors.,Background There is high variability in the clinical outcomes of patients with acute ischemic stroke (AIS) after mechanical thrombectomy (MT). Methods 217 consecutive patients with anterior circulation large vessel occlusion who underwent MT between August 2018 and January 2022 were analysed. The primary outcome was functional independence defined as a modified Rankin Scale score of 0-2 at 3 months. In the derivation cohort (August 2018 to December 2020), 7 ensemble ML models were trained on 70% of patients and tested on the remaining 30%. The model's performance was further validated on the temporal validation cohort (January 2021 to January 2022). The SHapley Additive exPlanations (SHAP) framework was applied to interpret the prediction model. Results Derivation analyses generated a 9-item score (PFCML-MT) comprising age, National Institutes of Health Stroke Scale score, collateral status, and postoperative laboratory indices (albumin-to-globulin ratio, estimated glomerular filtration rate, blood neutrophil count, C-reactive protein, albumin and serum glucose levels). The area under the curve was 0.87 for the test set and 0.84 for the temporal validation cohort. SHAP analysis further determined the thresholds for the top continuous features. This model has been translated into an online calculator that is freely available to the public (https://zhelvyao-123-60-sial5s.streamlitapp.com). Conclusions Using ML and readily available features, we developed an ML model that can potentially be used in clinical practice to generate real-time, accurate predictions of the outcome of patients with AIS treated with MT.  © 2023 BMJ Publishing Group. All rights reserved."
116,115,39,115_reservoirs_porosity_shale_basin,"reservoirs,porosity,shale,basin,reservoir,geological,boreholes,sandstone,borehole,drilling","The accurate estimation of reservoir porosity plays a vital role in estimating the amount of hydrocarbon reserves and evaluating the economic potential of a reservoir. It also aids decision making during the exploration and development phases of oil and gas fields. This study evaluates the integration of artificial intelligence techniques, conventional well logs, and core analysis for the accurate prediction of porosity in carbonate reservoirs. In general, carbonate reservoirs are characterized by their complex pore systems, with the wide spatial variation and highly nonlinear nature of their petrophysical properties. Therefore, they require detailed well-log interpretations to accurately estimate their properties, making them good candidates for the application of machine learning techniques. Accordingly, a large database of (2100) well-log records and core-porosity measurements were integrated with four state-of-the-art machine learning techniques (multilayer perceptron artificial neural network, MLP-ANN; Gaussian process regression, GPR; least squares gradient boosting ensemble, LS-Boost; and radial basis function neural network, RBF-NN) for the prediction of reservoir porosity. The well-log data used in this study include sonic acoustic travel time, Gamma-ray, and bulk density log records, which were carefully collected from five wells in a carbonate reservoir. This study revealed that all the artificial intelligence models achieved high accuracy, with R-squared values exceeding 90% during both the training and blind-testing phases. Among the AI models examined, the GPR model outperformed the others in terms of the R-squared values, root-mean-square error (RMSE), and coefficient of variation of the root-mean-square error (CVRMSE). Furthermore, this study introduces an artificially intelligent AI-based correlation for the estimation of reservoir porosity from well-log data; this correlation was developed using an in-house, Fortran-coded MLP-ANN model presented herein. This AI-based correlation gave a promising level of accuracy, with R-squared values of 92% and 90% for the training and blind-testing datasets, respectively. This correlation can serve as an accurate and easy-to-use tool for porosity prediction without any prior experience in utilizing or implementing machine learning models. © 2023 by the authors.,Recently, the petroleum industry has focused on deeply buried reservoir discoveries and exploring potential CO2 storage sites close to existing infrastructure to increase the life span of already operating installations to save time and cost. It is therefore essential for the petroleum industry to find an innovative approach that exploits the existing core- and well log data to be successful in their endeavor of effectively characterizing and predicting reservoir quality. Continuous data sources (e.g. wireline logs) have a huge potential compared with expensive, time inefficient and sporadic data from cores in determining reservoir quality for use in a regional context. However, whereas core analysis offers in-depth knowledge about rock properties and diagenetic processes, continuous data sources can be difficult to interpret without a formation-specific framework. Here, we demonstrated how the pre-existing core data could be effectively used by integrating petrographic- and facies data with a pure predictive machine learning (ML) based porosity predictor. The inclusion of detailed core analysis is important for determining which reservoir parameter(s) that should be modeled and for the interpretation of model outputs. By applying this methodology, a framework for deducing lithological and diagenetic attributes can be established to aid reservoir quality delineation from wireline logs that can be used in frontier areas. With the ML porosity model, a Random Forest Regressor, the square of the correlation was 0.84 between predicted- and helium porosity test data over a large dataset consisting of 38 wells within the Stø Formation across the SW Barents Sea. By integrating the continuous ML porosity logs and core data, it was possible to differentiate three distinct bed types on wireline log responses within the Stø Formation. Particularly, the relationship between Gamma ray (GR) and porosity was effective in separating high porosity clean sand-, low porosity cemented clean sand and more clay and silt rich intervals. Additionally, in the P-wave velocity (VP) - density domain, separation of high porosity clean sand- and heavily cemented low porosity clean sand intervals were possible. The results also show that the ML derived porosity curves coincide with previously published and independent facies data from a selection of the wells included in the study. This demonstrates the applicability of the model in the region, because the Stø Formation has been described to exhibit similar lithological- and mineralogical properties over large parts of the Western Barents Sea area. Even though, continuous porosity data could be estimated from other sources like VP, neutron or density logs, this would generally require matrix and fluid information. This study demonstrated the effectiveness of the ML model in generating continuous porosity logs that are useful for characterizing and predicting reservoir properties in new wells. This methodology offers a workflow for exploiting already acquired core and well log data for frontier exploration that can be adapted to other formations and exploration scenarios worldwide. © 2022 The Authors,The target formation in the study area of the Pearl River Mouth Basin is characterized by complex lithology and thin interbedded layers, with a large pore-permeability distribution range and strongly heterogeneous characteristics, which makes the reservoir pore structure and production capacity significantly different and brings research difficulties for reservoir logging evaluation and desert identification. The conventional reservoir classification method is mainly based on physical research, which requires developing extremely accurate formulas for calculating porosity and permeability; the calculation accuracy of pore permeability of low-porosity and low-permeability reservoirs is difficult to guarantee; and the conventional logging data cannot be comprehensively applied in reservoir classification. In this paper, taking Zhujiang and Zhuhai Formation reservoirs in the Huizhou M oilfield as an example, we integrated core analysis data such as core cast thin section, pore permeability data, rock electrical parameters, grain size, and relative permeability curves and combined with petrophysical parameters and pore structure characteristics to classify the reservoirs. The artificial neural network is used to predict the resistivity of saturated pure water (R0) to remove the influence of oil and gas on reservoir resistivity. The natural gamma ray (GR) “fluctuation” is used to calculate the variance root of variation (GS) to reflect the lithological variability and sedimentary heterogeneity of the reservoir, and then the conventional logging preferences, R0 and Gs (based on GR), are classified based on the automatic clustering MRGC algorithm to classify the logging facies. To classify the petrophysical phase reservoirs under the constraint of pore structure classification, we proposed a petrophysical classification logging model based on the natural gamma curve “fluctuation” intensity for strongly heterogeneous reservoirs. The learning model is extended to the whole area for training and prediction of desert identification, and the prediction results of the model are in good agreement with the actual results, which is important for determining favorable reservoirs in the area and the adjustment of oilfield development measures. Copyright © 2023 Zhao, Miao, Zhao, Liang, Wang and Tian."
117,116,39,116_microbiome_microbiotas_microbiota_microbiomebased,"microbiome,microbiotas,microbiota,microbiomebased,microbial,microbes,metagenomics,gut,bacteria,bacterial","Background: The gut-lung axis is generally recognized, but there are few large studies of the gut microbiome and incident respiratory disease in adults. Objective: We sought to investigate the association and predictive capacity of the gut microbiome for incident asthma and chronic obstructive pulmonary disease (COPD). Methods: Shallow metagenomic sequencing was performed for stool samples from a prospective, population-based cohort (FINRISK02; N = 7115 adults) with linked national administrative health register–derived classifications for incident asthma and COPD up to 15 years after baseline. Generalized linear models and Cox regressions were used to assess associations of microbial taxa and diversity with disease occurrence. Predictive models were constructed using machine learning with extreme gradient boosting. Models considered taxa abundances individually and in combination with other risk factors, including sex, age, body mass index, and smoking status. Results: A total of 695 and 392 statistically significant associations were found between baseline taxonomic groups and incident asthma and COPD, respectively. Gradient boosting decision trees of baseline gut microbiome abundance predicted incident asthma and COPD in the validation data sets with mean area under the curves of 0.608 and 0.780, respectively. Cox analysis showed that the baseline gut microbiome achieved higher predictive performance than individual conventional risk factors, with C-indices of 0.623 for asthma and 0.817 for COPD. The integration of the gut microbiome and conventional risk factors further improved prediction capacities. Conclusions: The gut microbiome is a significant risk factor for incident asthma and incident COPD and is largely independent of conventional risk factors. © 2023 The Authors,Background: A growing body of evidence suggests that the gut microbiota is strongly linked to general human health. Microbiome-directed interventions, such as diet and exercise, are acknowledged as a viable and achievable strategy for preventing disorders and improving human health. However, due to the significant inter-individual diversity of the gut microbiota between subjects, lifestyle recommendations are expected to have distinct and highly variable impacts to the microbiome structure. Results: Here, through a large-scale meta-analysis including 1448 shotgun metagenomics samples obtained longitudinally from 396 individuals during lifestyle studies, we revealed Bacteroides stercoris, Prevotella copri, and Bacteroides vulgatus as biomarkers of microbiota’s resistance to structural changes, and aromatic and non-aromatic amino acid biosynthesis as important regulator of microbiome dynamics. We established criteria for distinguishing between significant compositional changes from normal microbiota fluctuation and classified individuals based on their level of response. We further developed a machine learning model for predicting “responders” and “non-responders” independently of the type of intervention with an area under the curve of up to 0.86 in external validation cohorts of different ethnicities. Conclusions: We propose here that microbiome-based stratification is possible for identifying individuals with highly plastic or highly resistant microbial structures. Identifying subjects that will not respond to generalized lifestyle therapeutic interventions targeting the restructuring of gut microbiota is important to ensure that primary end-points of clinical studies are reached. [MediaObject not available: see fulltext.]. © 2023, BioMed Central Ltd., part of Springer Nature.,Research on microecology has been carried out with broad perspectives in recent decades, which has enabled a better understanding of the gut microbiota and its roles in human health and disease. It is of great significance to routinely acquire the status of the human gut microbiota; however, there is no method to evaluate the gut microbiome through small amounts of fecal microbes. In this study, we found ten predominant groups of gut bacteria that characterized the whole microbiome in the human gut from a large-sample Chinese cohort, constructed a real-time quantitative polymerase chain reaction (qPCR) method and developed a set of analytical approaches to detect these ten groups of predominant gut bacterial species with great maneuverability, efficiency, and quantitative features. Reference ranges for the ten predominant gut bacterial groups were established, and we found that the concentration and pairwise ratios of the ten predominant gut bacterial groups varied with age, indicating gut microbial dysbiosis. By comparing the detection results of liver cirrhosis (LC) patients with those of healthy control subjects, differences were then analyzed, and a classification model for the two groups was built by machine learning. Among the six established classification models, the model established by using the random forest algorithm achieved the highest area under the curve (AUC) value and sensitivity for predicting LC. This research enables easy, rapid, stable, and reliable testing and evaluation of the balance of the gut microbiota in the human body, which may contribute to clinical work. © 2023 Chinese Academy of Engineering"
118,117,38,117_optimization_hyperparameters_optimisation_hyperparameter,"optimization,hyperparameters,optimisation,hyperparameter,algorithms,metaheuristics,learning,algorithm,search,selection","There have been many applications for machine learning algorithms in different fields. The importance of hyperparameters for machine learning algorithms is their control over the behaviors of training algorithms and their crucial impact on the performance of machine learning models. Tuning hyperparameters crucially affects the performance of machine learning algorithms, and future advances in this area mainly depend on well-tuned hyperparameters. Nevertheless, the high computational cost involved in evaluating the algorithms in large datasets or complicated models is a significant limitation that causes inefficiency of the tuning process. Besides, increased online applications of machine learning approaches have led to the requirement of producing good answers in less time. The present study first presents a novel classification of hyperparameter types based on their types to create high-quality solutions quickly. Then, based on this classification and using the hypergradient technique, some hyperparameters of deep learning algorithms are adjusted during the training process to decrease the search space and discover the optimal values of the hyperparameters. This method just needs only the parameters of the previous two steps and the gradient of the previous step. Finally, the proposed method is combined with other techniques in hyperparameter optimization, and the results are reviewed in two case studies. As confirmed by experimental results, the performance of the algorithms with the proposed method have been increased 36.62% and 23.16% (based on the best average accuracy) for Cifar10 and Cifar100 dataset respectively in early stages while the final produced answers with this method are equal to or better than the algorithms without it. Therefore, this method can be combined with hyperparameter optimization algorithms in order to improve their performance and make them more appropriate for online use by just using the parameters of the previous two steps and the gradient of the previous step. © 2023 by the authors; licensee Growing Science, Canada.,Most existing evolutionary search strategies are not so efficient when directly handling the decision space of large-scale multiobjective optimization problems (LMOPs). To enhance the efficiency of tackling LMOPs, this article proposes an accelerated evolutionary search (AES) strategy. Its main idea is to learn a gradient-descent-like direction vector (GDV) for each solution via the specially trained feedforward neural network, which may be the learnt possibly fastest convergent direction to reproduce new solutions efficiently. To be specific, a multilayer perceptron (MLP) with only one hidden layer is constructed, in which the number of neurons in the input and output layers is equal to the dimension of the decision space. Then, to get appropriate training data for the model, the current population is divided into two subsets based on the nondominated sorting, and each poor solution in one subset with worse convergence will be paired to an elitist solution in another subset with the minimum angle to it, which is considered most likely to guide it with rapid convergence. Next, this MLP is updated via backpropagation with gradient descent by using the above elaborately prepared dataset. Finally, an accelerated large-scale multiobjective evolutionary algorithm (ALMOEA) is designed by using AES as a reproduction operator. Experimental studies validate the effectiveness of the proposed AES when handling the search space of LMOPs with dimensionality ranging from 1000 to 10000. When compared with six state-of-the-art evolutionary algorithms, the experimental results also show the better efficiency and performance of the proposed optimizer in solving various LMOPs.  © 1997-2012 IEEE.,For machine learning algorithms, fine-tuning hyperparameters is a computational challenge due to the large size of the problem space. An efficient strategy for adjusting hyperparameters can be established with the use of the greedy search and Swarm intelligence algorithms. The Random Search and Grid Search optimization techniques show promise and efficiency for this task. The small population of solutions used at the outset, and the costly goal functions used by these searches, can lead to slow convergence or execution time in some cases. In this research, we propose using the machine learning model known as Support Vector Machine and optimizing it using four distinct algorithms—the Ant Bee Colony Algorithm, the Genetic Algorithm, the Whale Optimization, and the Particle Swarm Optimization—to evaluate the computational cost of SVM after hyper-tuning. Computational complexity comparisons of these optimization algorithms were performed to determine the most effective strategies for hyperparameter tuning. It was found that the Genetic Algorithm had a lower temporal complexity than other algorithms. © 2023 by the authors."
119,118,38,118_genomics_genotypephenotype_genomic_genetic,"genomics,genotypephenotype,genomic,genetic,genomewide,phenotype,phenotypes,genotypes,genotype,genome","Whole-genome sequencing (WGS) is the gold standard for fully characterizing genetic variation but is still prohibitively expensive for large samples. To reduce costs, many studies sequence only a subset of individuals or genomic regions, and genotype imputation is used to infer genotypes for the remaining individuals or regions without sequencing data. However, not all variants can be well imputed, and the current state-of-the-art imputation quality metric, denoted as standard Rsq, is poorly calibrated for lower-frequency variants. Here, we propose MagicalRsq, a machine-learning-based method that integrates variant-level imputation and population genetics statistics, to provide a better calibrated imputation quality metric. Leveraging WGS data from the Cystic Fibrosis Genome Project (CFGP), and whole-exome sequence data from UK BioBank (UKB), we performed comprehensive experiments to evaluate the performance of MagicalRsq compared to standard Rsq for partially sequenced studies. We found that MagicalRsq aligns better with true R2 than standard Rsq in almost every situation evaluated, for both European and African ancestry samples. For example, when applying models trained from 1,992 CFGP sequenced samples to an independent 3,103 samples with no sequencing but TOPMed imputation from array genotypes, MagicalRsq, compared to standard Rsq, achieved net gains of 1.4 million rare, 117k low-frequency, and 18k common variants, where net gains were gained numbers of correctly distinguished variants by MagicalRsq over standard Rsq. MagicalRsq can serve as an improved post-imputation quality metric and will benefit downstream analysis by better distinguishing well-imputed variants from those poorly imputed. MagicalRsq is freely available on GitHub. © 2022 The Authors,Background: Genomic variants of the disease are often discovered nowadays through population-based genome-wide association studies (GWAS). Identifying genomic variations potentially underlying a phenotype, such as hypertension, in an individual is important for designing personalized treatment; however, population-level models, such as GWAS, may not capture all the important, individualized factors well. In addition, GWAS typically requires a large sample size to detect the association of low-frequency genomic variants with sufficient power. Here, we report an individualized Bayesian inference (IBI) algorithm for estimating the genomic variants that influence complex traits, such as hypertension, at the level of an individual (e.g., a patient). By modeling at the level of the individual, IBI seeks to find genomic variants observed in the individual’s genome that provide a strong explanation of the phenotype observed in this individual. Results: We applied the IBI algorithm to the data from the Framingham Heart Study to explore the genomic influences of hypertension. Among the top-ranking variants identified by IBI and GWAS, there is a significant number of shared variants (intersection); the unique variants identified only by IBI tend to have relatively lower minor allele frequency than those identified by GWAS. In addition, IBI discovered more individualized and diverse variants that explain hypertension patients better than GWAS. Furthermore, IBI found several well-known low-frequency variants as well as genes related to blood pressure that GWAS missed in the same cohort. Finally, IBI identified top-ranked variants that predicted hypertension better than GWAS, according to the area under the ROC curve. Conclusions: The results support IBI as a promising approach for complementing GWAS, especially in detecting low-frequency genomic variants as well as learning personalized genomic variants of clinical traits and disease, such as the complex trait of hypertension, to help advance precision medicine. © 2023, The Author(s).,Purpose: The analysis of exome and genome sequencing data for the diagnosis of rare diseases is challenging and time-consuming. In this study, we evaluated an artificial intelligence model, based on machine learning for automating variant prioritization for diagnosing rare genetic diseases in the Baylor Genetics clinical laboratory. Methods: The automated analysis model was developed using a supervised learning approach based on thousands of manually curated variants. The model was evaluated on 2 cohorts. The model accuracy was determined using a retrospective cohort comprising 180 randomly selected exome cases (57 singletons, 123 trios); all of which were previously diagnosed and solved through manual interpretation. Diagnostic yield with the modified workflow was estimated using a prospective “production” cohort of 334 consecutive clinical cases. Results: The model accurately pinpointed all manually reported variants as candidates. The reported variants were ranked in top 10 candidate variants in 98.4% (121/123) of trio cases, in 93.0% (53/57) of single proband cases, and 96.7% (174/180) of all cases. The accuracy of the model was reduced in some cases because of incomplete variant calling (eg, copy number variants) or incomplete phenotypic description. Conclusion: The automated model for case analysis assists clinical genetic laboratories in prioritizing candidate variants effectively. The use of such technology may facilitate the interpretation of genomic data for a large number of patients in the era of precision medicine. © 2023 American College of Medical Genetics and Genomics"
120,119,37,119_ionic_electrochemical_ions_graphene,"ionic,electrochemical,ions,graphene,conductivity,sodium,metals,batteries,electrolytes,ion","Sodium-ion batteries (SIBs) provide a feasible solution for large-scale energy storage applications of sustainable energy resources like wind and solar energy. Layered sodium nickel titanates (Na2x[NixTi1-x]O2) (SNTL) are considered a promising electrode material for SIBs since they can function as either a positive or negative electrode due to the coexistence of high redox potential element Ni (V = 3.7 V vs. Na/Na+) and low redox potential element Ti (V = 0.7 V vs. Na/Na+). There have been both experimental and computational studies on the family of Na2x[NixTi1-x]O2 materials, investigating their structures, electrochemical performance, electronic and ionic conductivity, etc. However, the current understanding of the Na migration mechanism is still very limited at an atomic level because of the restrictions of simulation methods: density-functional theory (DFT) simulation is restricted to a small length and time scale, whereas the conventional empirical interatomic potential (IP) is restricted by a limited accuracy. To solve this efficiency-versus-accuracy dilemma, herein we developed a neural network (NN)-based machine-learning interatomic potential (MLIP), which is trained using DFT data. Our MLIP demonstrates a DFT-approaching accuracy in terms of atomic forces. Moreover, the extracted Na self-diffusivity, ionic conductivity, and activation energy values from MLIP-MD simulation agree well with experimental data. In general, P2-SNTL demonstrates a higher Na self-diffusivity and ionic conductivity than O3-SNTL, mainly due to the larger bottleneck size through which Na ions migrate. We also employed the incoherent density correlation function to study the Na migration mechanism. The Singwi-Sjölander (SS) model and Chudley-Elliot (CE) model were used to fit the jump-diffusion behavior in P2- and O3-SNTL, respectively. The obtained jump distance values match well with the typical distance between the nearest Na sites in P2- and O3-SNTL, confirming that the hopping events happen mostly between neighboring Na sites in SNTL. The decrease of residence time and jump distance values with temperature is a result of higher ion mobility and level of delocalization, as validated by the nuclear density maps. In addition, our study also confirmed that both the “straight” and “curved” Na migration paths exist in O3-SNTL, in contrast to previous studies which only mention the “curved” migration path. In the end, our calculated Haven ratios have a value <1, indicating that it is more likely for the Na hopping events to occur via a concerted migration of several atoms other than the jump of an isolated atom. © 2023,The rise in demand for lithium-ion batteries has led to a large-scale search for electrode materials and intercalating ion species to meet the demands of next-generation energy technologies. Recent efforts largely focus on searching for cathodes that can accommodate large amounts of intercalating ions, but similar work on anodes is relatively limited. This study utilizes machine learning methods to find alternative two-dimensional (2D) materials and intercalating ions beyond Li for metal-ion batteries with high-power efficiencies. The approach first uses density functional theory (DFT) calculations to estimate the theoretical capacities and voltages of various metal ions on 2D materials. The DFT-generated data also provide insights into the local structural accommodation upon ion intercalation on various 2D materials. Significant changes to the lattice can result in irreversible changes to the bonding environments in the anode material, resulting in poor cycling stability. Next, this study develops a binding energy and structural accommodation-based classification model to screen anode materials for next-generation batteries. The classification model selects intercalating ions and 2D material pairs suitable for batteries based on the calculated voltage and volumetric changes in the 2D material upon intercalation. Finally, this study builds a regression model to accurately predict the binding energies of the various intercalating ions on 2D materials. The approach highlights the importance of different elemental and structural features for classification and regression tasks. The insights gained from this study on the role of involved features, such as electronegativities of the constituent ions and the presence of unfilled electronic levels, will help to streamline further studies towards the search for future layered battery materials. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Na-ion batteries are considered a promising alternative to the analogous Li-ion batteries because of their low manufacturing cost, large abundance, and similar chemical/electrochemical properties. In particular, research on Na-ion solid electrolytes, which resolve the flammability issues associated with liquid electrolytes and increase the energy density obtained using a particular metal anode, is rapidly growing. However, the ionic conductivities of these materials are lower than those of liquids. We present a novel classification approach based on machine learning for identifying Na superionic conductor (NASICON) materials with outstanding ionic conductivities. We obtained new features based on chemical descriptors such as Na content, elemental radii, and electronegativity. We then classified 3573 NASICON structures by implementing the ensemble model of gradient boosting algorithms, with an average prediction accuracy of 84.2%. We further validated the thermodynamic stability and ionic conductivity values of the materials classified as superionic materials by employing density functional theory calculations and ab initio molecular dynamics simulations. Na3YTaSi2PO12, Na3HfZrSi2PO12, Na3LaTaSi2PO12, and Na3ScTaSi2PO12 were confirmed as promising NASICON structures that fulfill the requirements of solid-state electrolytes. © 2023 American Chemical Society"
121,120,37,120_gps_locationbased_fingerprinting_fingerprint,"gps,locationbased,fingerprinting,fingerprint,geopositioning,waypoint,tracking,wifi,landmarks,location","Seamless positioning ability has become an essential requirement in large-scale smart city scenes with the development of Artificial Intelligence of Things technology. The performance of seamless positioning is limited by the inaccurate crowdsourced navigation database, cumulative error of built-in sensors, and changeable measurement errors of different location sources. In order to solve these problems, this paper presents the CrowdLOC-S framework, which provides a concrete and accurate indoor/outdoor localization performance using the combination of crowdsourced Wi-Fi fingerprinting, Global Navigation Satellite System (GNSS), and low-cost sensors. A data and model dual-driven based trajectory estimator is developed for improving the long-term positioning performance of built-in sensors, and a hybrid one-dimensional convolutional neural network (1D-CNN), Bi-directional Long Short-Term Memory (Bi-LSTM), and Multilayer Perceptron (MLP) enhanced quality indicator is proposed for quality evaluation of crowdsourced trajectories and further Wi-Fi fingerprinting database construction. Besides, the transfer learning approach is applied in the quality indicator for autonomously predicting the location errors towards different indoor and outdoor location sources and realizing seamless scenes switching. Finally, a unified extended Kalman filter is developed to realize multi-source integration-based seamless localization using the positioning information provided by indoor and outdoor location sources and corresponding quality indicator results. Comprehensive experiments demonstrate that the presented CrowdLOC-S system is proven to realize precise and efficient indoor and outdoor positioning performance in complex and large-scale urban environments. © 2023 Elsevier Ltd,Indoor 3D positioning is useful in multistory buildings, such as shopping malls, libraries, and airports. This study focuses on indoor 3D positioning using wireless access points (AP) in an environment without adding additional hardware facilities in large-scale complex places. The integration of a deep learning algorithm into indoor 3D positioning is studied, and a 3D dynamic positioning model based on temporal fingerprints is proposed. In contrast to the traditional positioning models with a single input, the proposed method uses a sliding time window to build a temporal fingerprint chip as the input of the positioning model to provide abundant information for positioning. Temporal information can be used to distinguish locations with similar fingerprint vectors and to improve the accuracy and robustness of positioning. Moreover, deep learning has been applied for the automatic extraction of spatiotemporal features. A temporal convolutional network (TCN) feature extractor is proposed in this paper, which adopts a causal convolution mechanism, dilated convolution mechanism, and residual connection mechanism and is not limited by the size of the convolution kernel. It is capable of learning hidden information and spatiotemporal relationships from the input features and the extracted spatiotemporal features are connected with a deep neural network (DNN) regressor to fit the complex nonlinear mapping relationship between the features and position coordinates to estimate the 3D position coordinates of the target. Finally, an open-source public dataset was used to verify the performance of the localization algorithm. Experimental results demonstrated the effectiveness of the proposed positioning model and a comparison between the proposed model and existing models proved that the proposed model can provide more accurate three-dimensional position coordinates. © 2022 by the authors.,Accurate indoor positioning has become an indispensable technology for location-based service in indoor environments. Geomagnetic field fingerprinting for indoor positioning is an attractive alternative to Wi-Fi and Bluetooth because of the omnipresent signals and the infrastructure-free mode. However, heterogeneous devices and users with fluctuant geomagnetism and massive crowdsourced data covering large-scale indoor scenes may result in a low degree of discernibility and degrade the rerecognizable performance for pedestrian positioning. To overcome the problem, a deep-learning-based indoor geomagnetic positioning method with direction-aware multiscale recurrent neural networks (DM-RNNs) is presented. First, the direction information is taken into the fingerprint construction and localization process to increase spatial identification, which is necessary for indoor pedestrian navigation. Second, instead of using a single holistic feature from sequence fingerprints directly, we employ the different scale-based feature extraction units for variational anomalies of the signal by using multiscale RNNs, increasing the model adaptability and generality for random pedestrian motion and device heterogeneity. Third, the ensemble learning mechanism is adopted in the model to obtain robust localization results. Furthermore, we employ the convenient visual-inertial odometry (VIO) to collect the geomagnetic signal dataset and utilize the sequence augmentation approach to generate synthesized trajectories from multiple single-point magnetic values for training and testing the model. Extensive experiments in three different indoor scenes demonstrate that the proposed approach outperforms state-of-the-art competing schemes by a wide margin, reducing mean localization error by more than 30% and obtaining over 79% direction estimation precision in different indoor scenarios.  © 2001-2012 IEEE."
122,121,36,121_cnn_pathology_bowel_colorectal,"cnn,pathology,bowel,colorectal,classification,pathologist,segmentation,colitis,gastrointestinal,endoscopy","Endoscopic image analysis has played a pivotal function in the diagnosis and management of gastrointestinal (GI) tract diseases. Gastrointestinal endoscopy is a medical procedure where a flexible tube with an endoscope (camera) is inserted into the GI tract to visualize the inner lining of the colon, esophagus, stomach, and small intestine. The videos and images attained during endoscopy provide valuable data for detecting and monitoring a large number of GI diseases. Computer-assisted automated diagnosis technique helps to achieve accurate diagnoses and provide the patient the relevant medical care. Machine learning (ML) and deep learning (DL) methods have been exploited to endoscopic images for classifying diseases and providing diagnostic support. Convolutional Neural Networks (CNN) and other DL algorithms can learn to discriminate between various kinds of GI lesions based on visual properties. This study presents an Endoscopic Image Analysis for Gastrointestinal Tract Disease Diagnosis using an inspired Algorithm with Deep Learning (EIAGTD-NIADL) technique. The EIAGTD-NIADL technique intends to examine the endoscopic images using nature nature-inspired algorithm with a DL model for gastrointestinal tract disease detection and classification. To pre-process the input endoscopic images, the EIAGTD-NIADL technique uses a bilateral filtering (BF) approach. For feature extraction, the EIAGTD-NIADL technique applies an improved ShuffleNet model. To improve the efficacy of the improved ShuffleNet model, the EIAGTD-NIADL technique uses an improved spotted hyena optimizer (ISHO) algorithm. Finally, the classification process is performed by the use of the stacked long short-term memory (SLSTM) method. The experimental outcomes of the EIAGTD-NIADL system can be confirmed on benchmark medical image datasets. The obtained outcomes demonstrate the promising results of the EIAGTD-NIADL approach over other models.  © 2023 The Authors.,Colorectal cancer is one of the most common cancers worldwide, accounting for an annual estimated 1.8 million incident cases. With the increasing number of colonoscopies being performed, colorectal biopsies make up a large proportion of any histopathology laboratory workload. We trained and validated a unique artificial intelligence (AI) deep learning model as an assistive tool to screen for colonic malignancies in colorectal specimens, in order to improve cancer detection and classification; enabling busy pathologists to focus on higher order decision-making tasks. The study cohort consists of Whole Slide Images (WSI) obtained from 294 colorectal specimens. Qritive’s unique composite algorithm comprises both a deep learning model based on a Faster Region Based Convolutional Neural Network (Faster-RCNN) architecture for instance segmentation with a ResNet-101 feature extraction backbone that provides glandular segmentation, and a classical machine learning classifier. The initial training used pathologists’ annotations on a cohort of 66,191 image tiles extracted from 39 WSIs. A subsequent application of a classical machine learning-based slide classifier sorted the WSIs into ‘low risk’ (benign, inflammation) and ‘high risk’ (dysplasia, malignancy) categories. We further trained the composite AI-model’s performance on a larger cohort of 105 resections WSIs and then validated our findings on a cohort of 150 biopsies WSIs against the classifications of two independently blinded pathologists. We evaluated the area under the receiver-operator characteristic curve (AUC) and other performance metrics. The AI model achieved an AUC of 0.917 in the validation cohort, with excellent sensitivity (97.4%) in detection of high risk features of dysplasia and malignancy. We demonstrate an unique composite AI-model incorporating both a glandular segmentation deep learning model and a classical machine learning classifier, with excellent sensitivity in picking up high risk colorectal features. As such, AI plays a role as a potential screening tool in assisting busy pathologists by outlining the dysplastic and malignant glands. © 2022, The Author(s).,Background: Histopathological examination is a crucial step in the diagnosis and treatment of many major diseases. Aiming to facilitate diagnostic decision making and improve the workload of pathologists, we developed an artificial intelligence (AI)-based prescreening tool that analyses whole-slide images (WSIs) of large-bowel biopsies to identify typical, non-neoplastic, and neoplastic biopsies. Methods: This retrospective cohort study was conducted with an internal development cohort of slides acquired from a hospital in the UK and three external validation cohorts of WSIs acquired from two hospitals in the UK and one clinical laboratory in Portugal. To learn the differential histological patterns from digitised WSIs of large-bowel biopsy slides, our proposed weakly supervised deep-learning model (Colorectal AI Model for Abnormality Detection [CAIMAN]) used slide-level diagnostic labels and no detailed cell or region-level annotations. The method was developed with an internal development cohort of 5054 biopsy slides from 2080 patients that were labelled with corresponding diagnostic categories assigned by pathologists. The three external validation cohorts, with a total of 1536 slides, were used for independent validation of CAIMAN. Each WSI was classified into one of three classes (ie, typical, atypical non-neoplastic, and atypical neoplastic). Prediction scores of image tiles were aggregated into three prediction scores for the whole slide, one for its likelihood of being typical, one for its likelihood of being non-neoplastic, and one for its likelihood of being neoplastic. The assessment of the external validation cohorts was conducted by the trained and frozen CAIMAN model. To evaluate model performance, we calculated area under the convex hull of the receiver operating characteristic curve (AUROC), area under the precision-recall curve, and specificity compared with our previously published iterative draw and rank sampling (IDaRS) algorithm. We also generated heat maps and saliency maps to analyse and visualise the relationship between the WSI diagnostic labels and spatial features of the tissue microenvironment. The main outcome of this study was the ability of CAIMAN to accurately identify typical and atypical WSIs of colon biopsies, which could potentially facilitate automatic removing of typical biopsies from the diagnostic workload in clinics. Findings: A randomly selected subset of all large bowel biopsies was obtained between Jan 1, 2012, and Dec 31, 2017. The AI training, validation, and assessments were done between Jan 1, 2021, and Sept 30, 2022. WSIs with diagnostic labels were collected between Jan 1 and Sept 30, 2022. Our analysis showed no statistically significant differences across prediction scores from CAIMAN for typical and atypical classes based on anatomical sites of the biopsy. At 0·99 sensitivity, CAIMAN (specificity 0·5592) was more accurate than an IDaRS-based weakly supervised WSI-classification pipeline (0·4629) in identifying typical and atypical biopsies on cross-validation in the internal development cohort (p<0·0001). At 0·99 sensitivity, CAIMAN was also more accurate than IDaRS for two external validation cohorts (p<0·0001), but not for a third external validation cohort (p=0·10). CAIMAN provided higher specificity than IDaRS at some high-sensitivity thresholds (0·7763 vs 0·6222 for 0·95 sensitivity, 0·7126 vs 0·5407 for 0·97 sensitivity, and 0·5615 vs 0·3970 for 0·99 sensitivity on one of the external validation cohorts) and showed high classification performance in distinguishing between neoplastic biopsies (AUROC 0·9928, 95% CI 0·9927–0·9929), inflammatory biopsies (0·9658, 0·9655–0·9661), and atypical biopsies (0·9789, 0·9786–0·9792). On the three external validation cohorts, CAIMAN had AUROC values of 0·9431 (95% CI 0·9165–0·9697), 0·9576 (0·9568–0·9584), and 0·9636 (0·9615–0·9657) for the detection of atypical biopsies. Saliency maps supported the representation of disease heterogeneity in model predictions and its association with relevant histological features. Interpretation: CAIMAN, with its high sensitivity in detecting atypical large-bowel biopsies, might be a promising improvement in clinical workflow efficiency and diagnostic decision making in prescreening of typical colorectal biopsies. Funding: The Pathology Image Data Lake for Analytics, Knowledge and Education Centre of Excellence; the UK Government's Industrial Strategy Challenge Fund; and Innovate UK on behalf of UK Research and Innovation. © 2023 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license"
123,122,36,122_transcriptomics_transcriptomic_transcriptome_transcriptomes,"transcriptomics,transcriptomic,transcriptome,transcriptomes,celltype,singlecell,rna,rnaseq,cell,cells","Single-cell RNA-seq analysis has become a powerful tool to analyse the transcriptomes of individual cells. In turn, it has fostered the possibility of screening thousands of single cells in parallel. Thus, contrary to the traditional bulk measurements that only paint a macroscopic picture, gene measurements at the cell level aid researchers in studying different tissues and organs at various stages. However, accurate clustering methods for such high-dimensional data remain exiguous and a persistent challenge in this domain. Of late, several methods and techniques have been promulgated to address this issue. In this article, we propose a novel framework for clustering large-scale single-cell data and subsequently identifying the rare-cell sub-populations. To handle such sparse, high-dimensional data, we leverage PaCMAP (Pairwise Controlled Manifold Approximation), a feature extraction algorithm that preserves both the local and the global structures of the data and Gaussian Mixture Model to cluster single-cell data. Subsequently, we exploit Edited Nearest Neighbours sampling and Isolation Forest/One-class Support Vector Machine to identify rare-cell sub-populations. The performance of the proposed method is validated using the publicly available datasets with varying degrees of cell types and rare-cell sub-populations. On several benchmark datasets, the proposed method outperforms the existing state-of-the-art methods. The proposed method successfully identifies cell types that constitute populations ranging from 0.1 to 8% with F1-scores of 0.91 0.09. © The Author(s) 2023. Published by Oxford University Press. All rights reserved.,Single-cell RNA sequencing (RNA-seq) has been demonstrated to be a proven method for quantifying gene-expression heterogeneity and providing insight into the transcriptome at the single-cell level. When combining multiple single-cell transcriptome datasets for analysis, it is common to first correct the batch effect. Most of the state-of-the-art processing methods are unsupervised, i.e., they do not utilize single-cell cluster labeling information, which could improve the performance of batch correction methods, especially in the case of multiple cell types. To better utilize known labels for complex dataset scenarios, we propose a novel deep learning model named IMAAE (i.e., integrating multiple single-cell datasets via an adversarial autoencoder) to correct the batch effects. After conducting experiments with various dataset scenarios, the results show that IMAAE outperforms existing methods for both qualitative measures and quantitative evaluation. In addition, IMAAE is able to retain both corrected dimension reduction data and corrected gene expression data. These features make it a potential new option for large-scale single-cell gene expression data analysis. © 2023 by the authors.,Cell type identification from single-cell transcriptomic data is a common goal of single-cell RNA sequencing (scRNAseq) data analysis. Deep neural networks have been employed to identify cell types from scRNAseq data with high performance. However, it requires a large mount of individual cells with accurate and unbiased annotated types to train the identification models. Unfortunately, labeling the scRNAseq data is cumbersome and time-consuming as it involves manual inspection of marker genes. To overcome this challenge, we propose a semi-supervised learning model 'SemiRNet' to use unlabeled scRNAseq cells and a limited amount of labeled scRNAseq cells to implement cell identification. The proposed model is based on recurrent convolutional neural networks (RCNN), which includes a shared network, a supervised network and an unsupervised network. The proposed model is evaluated on two large scale single-cell transcriptomic datasets. It is observed that the proposed model is able to achieve encouraging performance by learning on the very limited amount of labeled scRNAseq cells together with a large number of unlabeled scRNAseq cells.  © 2004-2012 IEEE."
124,123,35,123_tomography_radiotherapy_contouring_segmentations,"tomography,radiotherapy,contouring,segmentations,segmentation,autosegmentations,contours,ct,autosegmentation,mri","Background: The high-dose rate (HDR) brachytherapy treatment planning workflow for cervical cancer is a labor-intensive, time-consuming, and expertise-driven process. These issues are amplified in low/middle-income countries with large deficits in experienced healthcare professionals. Automation has the ability to substantially reduce bottlenecks in the planning process but often require a high level of expertise to develop. Purpose: To implement the out of the box self-configuring nnU-Net package for the auto-segmentation of the organs at risk (OARs) and high-risk CTV (HR CTV) for Ring-Tandem (R-T) HDR cervical brachytherapy treatment planning. Methods: The computed tomography (CT) scans of 100 previously treated patients were used to train and test three different nnU-Net configurations (2D, 3DFR, and 3DCasc). The performance of the models was evaluated by calculating the Sørensen-dice similarity coefficient, Hausdorff distance (HD), 95th percentile Hausdorff distance, mean surface distance (MSD), and precision score for 20 test patients. The dosimetric accuracy between the manual and predicted contours was assessed by looking at the various dose volume histogram (DVH) parameters and volume differences. Three different radiation oncologists (ROs) scored the predicted bladder, rectum, and HR CTV contours generated by the best performing model. The manual contouring, prediction, and editing times were recorded. Results: The mean DSC, HD, HD95, MSD and precision scores for our best performing model (3DFR) were 0.92/7.5 mm/3.0 mm/ 0.8 mm/0.91 for the bladder, 0.84/13.8 mm/5.3 mm/1.4 mm/0.84 for the rectum, and 0.81/8.5 mm/6.0 mm/2.2 mm/0.80 for the HR CTV. Mean dose differences (D2cc/90%) and volume differences were 0.08 Gy/1.3 cm3 for the bladder, 0.02 Gy/0.7 cm3 for the rectum, and 0.33 Gy/1.5 cm3 for the HR CTV. On average, 65% of the generated contours were clinically acceptable, 33% requiring minor edits, 2% required major edits, and no contours were rejected. Average manual contouring time was 14.0 min, while the average prediction and editing times were 1.6 and 2.1 min, respectively. Conclusion: Our best performing model (3DFR) provided fast accurate auto generated OARs and HR CTV contours with a large clinical acceptance rate. © 2023 The Authors. Journal of Applied Clinical Medical Physics published by Wiley Periodicals, LLC on behalf of The American Association of Physicists in Medicine.,Purpose: The large variability in tumor appearance and shape makes manual delineation of the clinical target volume (CTV) time-consuming, and the results depend on the oncologists’ experience. Whereas deep learning techniques have allowed oncologists to automate the CTV delineation, multi-site tumor analysis is often lacking in the literature. This study aimed to evaluate the deep learning models that automatically contour CTVs of tumors at various sites on computed tomography (CT) images from objective and subjective perspectives. Methods and Materials: 577 patients were selected for the present study, including nasopharyngeal (n = 34), esophageal (n = 40), breast-conserving surgery (BCS) (left-sided, n = 71; right-sided, n = 71), breast-radical mastectomy (BRM) (left-sided, n = 43; right-sided, n = 37), cervical (radical radiotherapy, n = 45; postoperative, n = 85), prostate (n = 42), and rectal (n = 109) carcinomas. Manually delineated CTV contours by radiation oncologists are served as ground truth. Four models were evaluated: Flexnet, Unet, Vnet, and Segresnet, which are commercially available in the medical product “AccuLearning AI model training platform”. The data were divided into the training, validation, and testing set at a ratio of 5:1:4. The geometric metrics, including Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), were calculated for objective evaluation. For subjective assessment, oncologists rated the segmentation contours of the testing set visually. Results: High correlations were observed between automatic and manual contours. Based on the results of the independent test group, most of the patients achieved satisfactory quantitative results (DSC > 0.8), except for patients with esophageal carcinoma (DSC: 0.62–0.64). The subjective review indicated that 82.65% of predicted CTVs scored either as clinically accepting (8.68%) or requiring minor revision (73.97%), and no patients were scored as rejected. Conclusion: This experimental work demonstrated that auto-generated contours could serve as an initial template to help oncologists save time in CTV delineation. The deep learning-based auto-segmentations achieve acceptable accuracy and show the potential to improve clinical efficiency for radiotherapy of a variety of cancer. © 2023, Italian Society of Medical Radiology.,Purpose: Segmenting organs in cone-beam CT (CBCT) images would allow to adapt the radiotherapy based on the organ deformations that may occur between treatment fractions. However, this is a difficult task because of the relative lack of contrast in CBCT images, leading to high inter-observer variability. Deformable image registration (DIR) and deep-learning based automatic segmentation approaches have shown interesting results for this task in the past years. However, they are either sensitive to large organ deformations, or require to train a convolutional neural network (CNN) from a database of delineated CBCT images, which is difficult to do without improvement of image quality. In this work, we propose an alternative approach: to train a CNN (using a deep learning-based segmentation tool called nnU-Net) from a database of artificial CBCT images simulated from planning CT, for which it is easier to obtain the organ contours. Methods: Pseudo-CBCT (pCBCT) images were simulated from readily available segmented planning CT images, using the GATE Monte Carlo simulation. CT reference delineations were copied onto the pCBCT, resulting in a database of segmented images used to train the neural network. The studied segmentation contours were: bladder, rectum, and prostate contours. We trained multiple nnU-Net models using different training: (1) segmented real CBCT, (2) pCBCT, (3) segmented real CT and tested on pseudo-CT (pCT) generated from CBCT with cycleGAN, and (4) a combination of (2) and (3). The evaluation was performed on different datasets of segmented CBCT or pCT by comparing predicted segmentations with reference ones thanks to Dice similarity score and Hausdorff distance. A qualitative evaluation was also performed to compare DIR-based and nnU-Net-based segmentations. Results: Training with pCBCT was found to lead to comparable results to using real CBCT images. When evaluated on CBCT obtained from the same hospital as the CT images used in the simulation of the pCBCT, the model trained with pCBCT scored mean DSCs of 0.92 ± 0.05, 0.87 ± 0.02, and 0.85 ± 0.04 and mean Hausdorff distance 4.67 ± 3.01, 3.91 ± 0.98, and 5.00 ± 1.32 for the bladder, rectum, and prostate contours respectively, while the model trained with real CBCT scored mean DSCs of 0.91 ± 0.06, 0.83 ± 0.07, and 0.81 ± 0.05 and mean Hausdorff distance 5.62 ± 3.24, 6.43 ± 5.11, and 6.19 ± 1.14 for the bladder, rectum, and prostate contours, respectively. It was also found to outperform models using pCT or a combination of both, except for the prostate contour when tested on a dataset from a different hospital. Moreover, the resulting segmentations demonstrated a clinical acceptability, where 78% of bladder segmentations, 98% of rectum segmentations, and 93% of prostate segmentations required minor or no corrections, and for 76% of the patients, all structures of the patient required minor or no corrections. Conclusion: We proposed to use simulated CBCT images to train a nnU-Net segmentation model, avoiding the need to gather complex and time-consuming reference delineations on CBCT images. © 2022 American Association of Physicists in Medicine."
125,124,35,124_melanoma_dermatology_dermatologist_skin,"melanoma,dermatology,dermatologist,skin,dermatologists,classification,dermatological,dataset,datasets,ai","Skin cancer is a risky ailment that can be effectively managed if detected promptly. However, timely recognition of skin cancer remains a challenge. In this study, a robotic computer-assisted tactic is proposed for the early detection of skin cancer. The motivation behind this research is to improve the accuracy and efficiency of skin cancer recognition. The proposed methodology involves a series of steps. Initially, the input images undergo preprocessing to enhance their quality and extract relevant features. These preprocessed images are then fed into a Gated Recurrent Unit (GRU) Network, a type of deep learning model known for its ability to capture sequential information. To optimize the performance of the GRU Network, we employ an enhanced variant of the Orca Predation Algorithm (OPA). This algorithm helps fine-tune the network parameters, improving its diagnostic capabilities. To validate and evaluate the effectiveness of our skin cancer diagnosis algorithm, we conducted experiments using the HAM10000 dataset, which contains a large collection of skin lesion images. We compared the results obtained from our proposed method, named GRU/IOPA, with eight existing techniques commonly used for skin cancer diagnosis. Five execution indices, namely sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy, were used to assess the performance of these methods. Our findings demonstrate that the GRU/IOPA system outperforms other existing methods in terms of sensitivity (0.95), specificity (0.97), PPV (0.95), NPV (0.96), and accuracy. These results indicate the effectiveness of our proposed method in diagnosing skin cancer compared to traditional approaches. The superior performance of GRU/IOPA highlights its potential impact on improving skin cancer diagnosis and reinforces its promise as an advanced tool in the field. In conclusion, our study presents a novel approach for timely skin cancer recognition through a robotic computer-assisted tactic. By utilizing the GRU/IOPA system, we achieve superior accuracy and efficiency in diagnosing skin cancer compared to existing techniques. This research offers significant contributions to the field of skin cancer diagnosis and opens up new avenues for future advancements in this area. © 2023 Elsevier Ltd,Background: The field of dermatological image analysis using deep neural networks includes the semantic segmentation of skin lesions, pivotal for lesion analysis, pathology inference, and diagnoses. While biases in neural network-based dermatoscopic image classification against darker skin tones due to dataset imbalance and contrast disparities are acknowledged, a comprehensive exploration of skin color bias in lesion segmentation models is lacking. It is imperative to address and understand the biases in these models. Methods: Our study comprehensively evaluates skin tone bias within prevalent neural networks for skin lesion segmentation. Since no information about skin color exists in widely used datasets, to quantify the bias we use three distinct skin color estimation methods: Fitzpatrick skin type estimation, Individual Typology Angle estimation as well as manual grouping of images by skin color. We assess bias across common models by training a variety of U-Net-based models on three widely-used datasets with 1758 different dermoscopic and clinical images. We also evaluate commonly suggested methods to mitigate bias. Results: Our findings expose a significant and large correlation between segmentation performance and skin color, revealing consistent challenges in segmenting lesions for darker skin tones across diverse datasets. Using various methods of skin color quantification, we have found significant bias in skin lesion segmentation against darker-skinned individuals when evaluated both in and out-of-sample. We also find that commonly used methods for bias mitigation do not result in any significant reduction in bias. Conclusions: Our findings suggest a pervasive bias in most published lesion segmentation methods, given our use of commonly employed neural network architectures and publicly available datasets. In light of our findings, we propose recommendations for unbiased dataset collection, labeling, and model development. This presents the first comprehensive evaluation of fairness in skin lesion segmentation. © 2024 The Author(s),Skin is a most essential and extraordinary part of the human structure. Exposure to chemicals such as nitrates, sunlight, arsenic, and UV rays due to pollution and depletion of the ozone layer is causing various skin diseases to spread rapidly. Digital healthcare offers many opportunities to reduce time, and human error, and improve clinical outcomes. However, the automatic recognition of skin disease is a major challenge due to high visual similarity between different skin diseases, low contrast, and large inter variation. Early detection of skin cancer can prevent death. Thus, Artificial intelligence (AI) and Machine Learning (ML) helps the physicians to improve clinical judgment or change manual perception. For skin cancer diagnostics, the ML/AI algorithm can outperform or match professional dermatologists in multiple studies. Different pre-trained architectures such as ResNet152, AlexNet, VGGNet, etc. are used for fusing different skin disease features such as texture, color, etc. and they are also utilized for conducting segmentation tasks. The variations in reflection, lesion size, shape, illumination, etc. often make automatic skin disease classification a complex task. ISIC 2019 and HAM 10000 are the widely used public datasets for skin disease prediction. More technical paper on skin cancer diagnosis is compared in this study. This report examines the majority of technical papers published between 2018 and October 2022 in order to appreciate current trends in the disciplines of skin cancer prediction. A study that combined clinical patient data with deep learning models (DL) increased the accuracy of predicting skin cancer. This article presents a visually attractive and well-organized summary of the current study findings. © 2024, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
126,125,35,125_badlabel_supervised_labeling_lowlabel,"badlabel,supervised,labeling,lowlabel,labeled,labels,label,classification,unlabeled,labelled","The success of deep learning is mainly dependent on large-scale and accurately labeled datasets. However, real-world datasets are marked with much noise. Directly training on datasets with label noise may lead to the overfitting. Recent research is under the spotlight on how to design algorithms that can learn robust models from noisy datasets, via designing the loss function and integrating the idea of Semi-supervised learning (SSL). This paper proposes a robust algorithm for learning with label noise that does not require additional clean data and an auxiliary model. Specifically, on the one hand, Jensen–Shannon (JS) divergence is introduced as a component of the loss function, which measures the distance between the predicted distribution and the noisy label distribution. It can alleviate the overfitting problem caused by the traditional cross entropy loss theoretically and experimentally. On the other hand, a dynamic sample selection mechanism is also proposed. The dataset is divided into the pseudo-clean labeled subset and the pseudo-noisy labeled subset. Two subsets are treated differently to exploit prior information about the data, and then learned by SSL. The dynamic sample selection is performed with the iteration between two subsets and the model parameters, which are different from the conventional training. Considering the label of the pseudo-clean labeled subset is not correct entirely, they are also refined by linear interpolation. Furthermore, we experimentally show that the integration of SSL helps the model divide two subsets more precise and build decision boundaries more explicit. Extensive experimental results on corrupted data from benchmark datasets and the real-world dataset, including CIFAR-10, CIFAR-100, and Clothing1M, demonstrate that our method is superior to many state-of-the-art approaches for learning with label noise. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Deep neural networks have achieved significant success in the artificial intelligence community and various downstream tasks. They encode images or texts into dense feature representations and are supervised by a large amount of labeled data. Due to the expensiveness of high-quality labeled data, a huge number of easy-to-access instances are collected to conduct supervised learning. However, they have not been annotated by experts and thus can contain numerous noisy instances, which will degrade the performance. To learn robust feature representations despite misleading noisy labels, we employ supervised contrastive learning to directly perform supervision in the hidden space, rather than in the prediction space like the prevalent cross-entropy loss function. However, cutting-edge noisy label learning methods with supervised contrastive learning always discard the data considered to be noisy, and thus cannot tolerate high-ratio noisy datasets. Therefore, we propose a novel training strategy named Supervised Contrastive Learning with Corrected Labels (Scl 2) to defend against the attack of noisy labels. Scl 2 corrects the noisy labels with an empirical small-loss assumption and conducts supervised contrastive learning using these corrected data. Specifically, we employ the generated soft labels as supervisory information to facilitate our implementation of supervised contrastive learning. This expansion of contrastive learning ensures the integrity of the supervisory information while effectively enhancing the learning process. In addition, samples sharing the same soft labels are treated as positive sample pairs, while those with different soft labels are considered to be negative sample pairs. With this strategy, the representations from neural networks keep the local discrimination in one mini-batch. Besides, we also employ a prototype contrastive learning technique to ensure global discrimination. Our Scl 2 has demonstrated excellent performance on numerous benchmark datasets, showcasing its effectiveness in various standardized evaluation scenarios. Additionally, our model has proven to be highly valuable when applied to real-world noisy datasets. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Deep neural networks (DNNs) require large amounts of labeled data for model training. However, label noise is a common problem in datasets due to the difficulty of classification and high cost of labeling processes. Introducing the concepts of curriculum learning and progressive learning, this paper presents a novel solution that is able to handle massive noisy labels and improve model generalization ability. It proposes a new network model training strategy that considers mislabeled samples directly in the network training process. The new learning curriculum is designed to measures the complexity of the data with their distribution density in a feature space. The sample data in each category are then divided into easy-to-classify (clean samples), relatively easy-to-classify, and hard-to-classify (noisy samples) subsets according to the smallest intra-class local density with each cluster. On this basis, DNNs are trained progressively in three stages, from easy to hard, i.e., from clean to noisy samples. The experimental results demonstrate that the accuracy of image classification can be improved through data augmentation, and the classification accuracy of the proposed method is clearly higher than that of standard Inception_v2 for the NEU dataset after data augmentation, when the proportion of noisy labels in the training set does not exceed 60%. With 50% noisy labels in the training set, the classification accuracy of the proposed method outperformed recent state-of-the-art label noise learning methods, CleanNet and MentorNet. The proposed method also performed well in practical applications, where the number of noisy labels was uncertain and unevenly distributed. In this case, the proposed method not only can alleviate the adverse effects of noisy labels, but it can also improve the generalization ability of standard deep networks and their overall capability. © 2022 by the authors."
127,126,35,126_genome_genomes_genes_gene,"genome,genomes,genes,gene,genomic,transcriptional,epigenomic,chromatin,transcription,geneformer","Natural and experimental genetic variants can modify DNA loops and insulating boundaries to tune transcription, but it is unknown how sequence perturbations affect chromatin organization genome wide. We developed a deep-learning strategy to quantify the effect of any insertion, deletion, or substitution on chromatin contacts and systematically scored millions of synthetic variants. While most genetic manipulations have little impact, regions with CTCF motifs and active transcription are highly sensitive, as expected. Our unbiased screen and subsequent targeted experiments also point to noncoding RNA genes and several families of repetitive elements as CTCF-motif-free DNA sequences with particularly large effects on nearby chromatin interactions, sometimes exceeding the effects of CTCF sites and explaining interactions that lack CTCF. We anticipate that our disruption tracks may be of broad interest and utility as a measure of 3D genome sensitivity, and our computational strategies may serve as a template for biological inquiry with deep learning. © 2023 The Authors,Utilizing large-scale epigenomics data, deep learning tools can predict the regulatory activity of genomic sequences, annotate non-coding genetic variants, and uncover mechanisms behind complex traits. However, these tools primarily rely on human or mouse data for training, limiting their performance when applied to other species. Furthermore, the limited exploration of many species, particularly in the case of livestock, has led to a scarcity of comprehensive and high-quality epigenetic data, posing challenges in developing reliable deep learning models for decoding their non-coding genomes. The cross-species prediction of the regulatory genome can be achieved by leveraging publicly available data from extensively studied organisms and making use of the conserved DNA binding preferences of transcription factors within the same tissue. In this study, we introduced DeepSATA, a novel deep learning-based sequence analyzer that incorporates the transcription factor binding affinity for the cross-species prediction of chromatin accessibility. By applying DeepSATA to analyze the genomes of pigs, chickens, cattle, humans, and mice, we demonstrated its ability to improve the prediction accuracy of chromatin accessibility and achieve reliable cross-species predictions in animals. Additionally, we showcased its effectiveness in analyzing pig genetic variants associated with economic traits and in increasing the accuracy of genomic predictions. Overall, our study presents a valuable tool to explore the epigenomic landscape of various species and pinpoint regulatory deoxyribonucleic acid (DNA) variants associated with complex traits. © 2023 by the authors.,Background: Evolutionary conservation is an invaluable tool for inferring functional significance in the genome, including regions that are crucial across many species and those that have undergone convergent evolution. Computational methods to test for sequence conservation are dominated by algorithms that examine the ability of one or more nucleotides to align across large evolutionary distances. While these nucleotide alignment-based approaches have proven powerful for protein-coding genes and some non-coding elements, they fail to capture conservation of many enhancers, distal regulatory elements that control spatial and temporal patterns of gene expression. The function of enhancers is governed by a complex, often tissue- and cell type-specific code that links combinations of transcription factor binding sites and other regulation-related sequence patterns to regulatory activity. Thus, function of orthologous enhancer regions can be conserved across large evolutionary distances, even when nucleotide turnover is high. Results: We present a new machine learning-based approach for evaluating enhancer conservation that leverages the combinatorial sequence code of enhancer activity rather than relying on the alignment of individual nucleotides. We first train a convolutional neural network model that can predict tissue-specific open chromatin, a proxy for enhancer activity, across mammals. Next, we apply that model to distinguish instances where the genome sequence would predict conserved function versus a loss of regulatory activity in that tissue. We present criteria for systematically evaluating model performance for this task and use them to demonstrate that our models accurately predict tissue-specific conservation and divergence in open chromatin between primate and rodent species, vastly out-performing leading nucleotide alignment-based approaches. We then apply our models to predict open chromatin at orthologs of brain and liver open chromatin regions across hundreds of mammals and find that brain enhancers associated with neuron activity have a stronger tendency than the general population to have predicted lineage-specific open chromatin. Conclusion: The framework presented here provides a mechanism to annotate tissue-specific regulatory function across hundreds of genomes and to study enhancer evolution using predicted regulatory differences rather than nucleotide-level conservation measurements. © 2022, The Author(s)."
128,127,34,127_feature_features_classification_classifier,"feature,features,classification,classifier,selection,datasets,lasso,selects,algorithms,sparse","Handling high-dimensional big data presents substantial challenges for Machine Learning (ML) algorithms, mainly due to the curse of dimensionality that leads to computational inefficiencies and increased risk of overfitting. Various dimensionality reduction and Feature Selection (FS) techniques have been developed to alleviate these challenges. Random Forest (RF), a widely-used Ensemble Learning Method (ELM), is recognized for its high accuracy and robustness, including its lesser-known capability for effective FS. While specialized RF models are designed for FS, they often struggle with computational efficiency on large datasets. Addressing these challenges, this study proposes a novel Feature Selection Model (FSM) integrated with data reduction techniques, termed Dynamic Correlated Regularized Random Forest (DCRRF). The architecture operates in four phases: Preprocessing, Feature Reduction (FR) using Best-First Search with Rough Set Theory (BFS-RST), FS through DCRRF, and feature efficacy assessment using a Support Vector Machine (SVM) classifier. Benchmarked against four gene expression datasets, the proposed model outperforms existing RF-based methods in computational efficiency and classification accuracy. This study introduces a robust and efficient approach to feature selection in highdimensional big-data scenarios. © 2023, Science and Information Organization. All Rights Reserved.,Companies have an increasing access to very large datasets within their domain. Analysing these datasets often requires the application of feature selection techniques in order to reduce the dimensionality of the data and prioritize features for downstream knowledge generation tasks. Effective feature selection is a key part of clustering, regression and classification. It presents a myriad of opportunities to improve the machine learning pipeline: eliminating redundant and irrelevant features, reducing model over-fitting, faster model training times and more explainable models. By contrast, and despite the widespread availability and use of categorical data in practice, feature selection for categorical and/or mixed data has received relatively little attention in comparison to numerical data. Furthermore, existing feature selection methods for mixed data are sensitive to number of objects by having nonlinear time complexities with respect to number of objects. In this work, we propose a generic multiple association measure for mixed datasets and a novel feature selection algorithm that uses multiple association across features. Our algorithm is based upon the belief that the most representative chosen set of features should be as diverse and minimally dependent on each other as possible. The proposed algorithm formulates the problem of feature selection as an optimization problem, searching for the set of features that have minimum association amongst them. We present a generic multiple association measure and two associated feature selection algorithms: Naive and Greedy Feature Selection Algorithms called NFSA and GFSA, respectively. Our proposed GFSA algorithm is evaluated on 15 benchmark datasets, and compared to four existing state of the art feature selection techniques. We demonstrate that our approach provides comparable downstream classification performance outperforming other leading techniques on several datasets. Both time complexity analysis and experimental results show that our proposed algorithm significantly reduces the processing time required for unsupervised feature selection algorithms especially for long datasets which have a huge number of objects, whilst also yielding comparable clustering and classification performance. On the other hand, we do not recommend our approach for wide datasets where the number of features is huge with respect to the number of objects e.g., image, text and genome datasets. © 2022 Elsevier Ltd,Feature Selection (FS) is an important preprocessing step in data analytics. It is used to select a subset of the original feature set such that the selected subset does not affect the classification performance significantly. Its objective is to remove irrelevant and redundant features from the original dataset. FS can be done either in offline mode or in online mode. The basic assumption in the former mode is that the entire dataset has been available for the FS algorithm; and the FS algorithm takes multiple epochs to select optimal feature subset that gives good accuracy. In contrast, the FS algorithms in online mode take input data one instance at a time and accumulate knowledge by learning each one of them. In online mode each instance of the original dataset is considered as training and testing sample as well. The offline FS algorithms require long time periods, if the data to be processed is large such as Big data. Whereas online FS algorithms will take only one epoch to learn the entire data and can produce the results swiftly which is highly desirable in the case of Big data. This paper deals with the online FS problem and provides a novel Feature Selection algorithm which uses the Sparse Gradient method to build a sparse classifier. In this proposed method, an online classifier is built and maintained throughout the learning process and feature weights, which are limited to a particular boundary limit, are reduced in a step by step decrement process. This method creates sparsity in the classifier. Effectively, the built classifier is used to select optimal feature subset from the incoming data. As this method reduces the weights in the classifier in step by step manner, only those important features which have value higher than the boundary survive from this repeated decrement process. The resultant optimal feature subset is formed using these non-zero weighted features. Most significantly, this particular method can be used with any learning algorithm. To show its applicability with different learning algorithms, various online feature selection models have been built using Learning Vector Quantization, Radial Basis Function Networks and Adaptive Resonance Theory MAP. In all these models, the proposed Sparse Gradient method is used. The encouraging results shows the effectiveness of the proposed method with different learning algorithms in medium and large sized benchmark datasets.  © 2022 World Scientific Publishing Company."
129,128,34,128_wildfires_fires_wildfire_fire,"wildfires,fires,wildfire,fire,forest,climate,peatlands,vegetation,climatic,ecosystems","Analysing wildfire initiation patterns and identifying their primary drivers is essential for the development of more efficient fire prevention strategies. However, such analyses have traditionally been conducted at local or national scales, hindering cross-border comparisons and the formulation of broad-scale policy initiatives. In this study, we present an analysis of the spatial variability of wildfire initiations across Europe, focusing specifically on moderate to large fires (> 100 ha), and examining the influence of both human and climatic factors on initiation areas. We estimated drivers of fire initiation using machine learning algorithms, specifically Random Forest (RF), covering the majority of the European territory (referred to as the “ET scale”). The models were trained using data on fire initiations extracted from a satellite burned area product, comprising fires occurring from 2001 to 2019. We developed six RF models: three considering all fires larger than 100 ha, and three focused solely on the largest events (> 1000 ha). Models were developed using climatic and human predictors separately, as well as both types of predictors mixed together. We found that both climatic and mixed models demonstrated moderate predictive capacity, with AUC values ranging from 79 % to 81 %; while models based only on human variables have had poor predictive capacity (AUC of 60 %). Feature importance analysis, using Shapley Additive Explanations (SHAP), allowed us to assess the primary drivers of wildfire initiations across the European Territory. Aridity and evapotranspiration had the strongest effect on fire initiation. Among human variables, population density and aging had considerable effects on fire initiation, the former with a strong effect in mixed models estimating large fires, while the latter had a more important role in the prediction of very large fires. Distance to roads and forest-agriculture interfaces were also relevant in some initiation models. A better understanding of drivers of main fire events should help designing European forest fire management strategies, particularly in the light of growing importance of climate change, as it would affect both fire severity and areas at risk. Factors of fire initiation should also be part of a comprehensive approach for fire risk assessment, reduction and adaption, contributing to more effective wildfire management and mitigation across the continent. © 2024 The Authors,Wildfires are a growing management concern in western US rangelands, where invasive annual grasses have altered fire regimes and contributed to an increased incidence of catastrophic large wildfires. Fire activity in arid, nonforested ecosystems is thought to be largely controlled by interannual variation in fuel amount, which in turn is controlled by antecedent weather. Thus, long-range forecasting of fire activity in rangelands should be feasible given annual estimates of fuel quantity. Using a 32-yr time series of spatial data, we employed machine learning algorithms to predict the relative probability of large (> 405 ha) wildfire in the Great Basin based on fine-scale annual and 16-d estimates of cover and production of vegetation functional groups, weather, and multitemporal scale drought indices. We evaluated the predictive utility of these models with a leave-1-yr-out cross-validation, building spatial hindcasts of fire probability for each year that we compared against actual footprints of large wildfires. Herbaceous aboveground biomass production, bare ground cover, and long-term drought indices were the most important predictors of burning. Across 32 fire seasons, 88% of the area burned in large wildfires coincided with the upper 3 deciles of predicted fire probabilities. At the scale of the Great Basin, several metrics of fire activity were moderately to strongly correlated with average fire probability, including total area burned in large wildfires, number of large wildfires, and maximum fire size. Our findings show that recent years of exceptional fire activity in the Great Basin were predictable based on antecedent weather-driven growth of fine fuels and reveal a significant increasing trend in fire probability over the past 3 decades driven by widespread changes in fine fuel characteristics. © 2022 The Author(s),Considerable economic losses and ecological damage can be caused by forest fires, and compared to suppression, prevention is a much smarter strategy. Accordingly, this study focuses on developing a novel framework to assess forest fire risks and policy decisions on forest fire management in China. This framework integrated deep learning algorithms, geographic information, and multisource data. Compared to conventional approaches, our framework featured timesaving, easy implementation, and importantly, the use of deep learning that vividly integrates various factors from the environment and human activities. Information on 96,594 forest fire points from 2001 to 2019 was collected on Moderate Resolution Imaging Spectroradiometer (MODIS) fire hotspots from 2001 to 2019 from NASA's Fire Information Resource Management System. The information was classified into factors such as topography, climate, vegetation, and society. The prediction of forest fire risk was generated using a fully connected network model, and spatial autocorrelation used to analyze the spatial aggregation correlation of active fire hotspots in the whole area of China. The results show that high accuracy prediction of fire risks was achieved (accuracy 87.4%, positive predictive value 87.1%, sensitivity 88.9%, area under curve (AUC) 94.1%). Based on this, it was found that Chinese forest fire risk shows significant autocorrelation and agglomeration both in seasons and regions. For example, forest fire risk usually raises dramatically in spring and winter, and decreases in autumn and summer. Compared to the national average, Yunnan Province, Guangdong Province, and the Greater Hinggan Mountains region of Heilongjiang Province have higher fire risks. In contrast, a large region in central China has been recognized as having a long-term, low risk of forest fires. All forest risks in each region were recorded into the database and could contribute to the forest fire prevention. The successful assessment of forest fire risks in this study provides a comprehensive knowledge of fire risks in China over the last 20 years. Deep learning showed its advantage in integrating multiple factors in predicting forest fire risks. This technical framework is expected to be a feasible evaluation tool for the occurrence of forest fires in China. © 2022, Northeast Forestry University."
130,129,34,129_catalysis_chemistry_synthesis_molecular,"catalysis,chemistry,synthesis,molecular,learning,chemist,chemists,predicting,models,chemical","Data-driven synthesis planning with machine learning is a key step in the design and discovery of novel inorganic compounds with desirable properties. Inorganic materials synthesis is often guided by heuristics and chemists’ prior knowledge and experience, built upon experimental trial-and-error that can be both time and resource consuming. Recent developments in natural language processing have enabled large-scale text mining of scientific literature, providing open-source databases of synthesis information on realized compounds, material precursors, and reaction conditions (temperatures, times). We employ supervised classification machine learning (ML) models to distinguish between solid-state, sol-gel, and solution (hydrothermal, precipitation) synthesis routes based on specified reaction target material and/or precursor materials. We demonstrate regression ML models that are able to predict suitable temperatures and times for the crucial inorganic synthesis steps of calcination and sintering given the reaction target and precursor materials. We contrast this regression-based condition modeling with a conditional variational autoencoder neural network that can generate appropriate distributions for the synthesis conditions of interest. We evaluate model interpretability using the Shapley additive explanations approach to gain insight into factors influencing suitability of synthesis route and reaction conditions. We find that the aforementioned models are capable of learning subtle differences in target material composition, precursor compound identities, and choice of synthesis route that are present in the inorganic synthesis space. Moreover, they generalize well to unseen chemical entities, outperform common heuristics in the field, and show promise for predicting appropriate reaction routes and conditions for previously unsynthesized compounds of interest. © 2023 American Chemical Society.,The molecular structures synthesizable by organic chemists dictate the molecular functions they can create. The invention and development of chemical reactions are thus critical for chemists to access new and desirable functional molecules in all disciplines of organic chemistry. This work seeks to expedite the exploration of emerging areas of organic chemistry by devising a machine-learning-guided workflow for reaction discovery. Specifically, this study uses machine learning to predict competent electrochemical reactions. To this end, we first develop a molecular representation that enables the production of general models with limited training data. Next, we employ automated experimentation to test a large number of electrochemical reactions. These reactions are categorized as competent or incompetent mixtures, and a classification model was trained to predict reaction competency. This model is used to screen 38,865 potential reactions in silico, and the predictions are used to identify a number of reactions of synthetic or mechanistic interest, 80% of which are found to be competent. Additionally, we provide the predictions for the 38,865-member set in the hope of accelerating the development of this field. We envision that adopting a workflow such as this could enable the rapid development of many fields of chemistry. © 2022 American Chemical Society. All rights reserved.,Conspectus In the domain of reaction development, one aims to obtain higher efficacies as measured in terms of yield and/or selectivities. During the empirical cycles, an admixture of outcomes from low to high yields/selectivities is expected. While it is not easy to identify all of the factors that might impact the reaction efficiency, complex and nonlinear dependence on the nature of reactants, catalysts, solvents, etc. is quite likely. Developmental stages of newer reactions would typically offer a few hundreds of samples with variations in participating molecules and/or reaction conditions. These “observations” and their “output” can be harnessed as valuable labeled data for developing molecular machine learning (ML) models. Once a robust ML model is built for a specific reaction under development, it can predict the reaction outcome for any new choice of substrates/catalyst in a few seconds/minutes and thus can expedite the identification of promising candidates for experimental validation. Recent years have witnessed impressive applications of ML in the molecular world, most of them aimed at predicting important chemical or biological properties. We believe that an integration of effective ML workflows can be made richly beneficial to reaction discovery. As with any new technology, direct adaptation of ML as used in well-developed domains, such as natural language processing (NLP) and image recognition, is unlikely to succeed in reaction discovery. Some of the challenges stem from ineffective featurization of the molecular space, unavailability of quality data and its distribution, in making the right choice of ML model and its technically robust deployment. It shall be noted that there is no universal ML model suitable for an inherently high-dimensional problem such as chemical reactions. Given these backgrounds, rendering ML tools conducive for reactions is an exciting as well as challenging endeavor at the same time. With the increased availability of efficient ML algorithms, we focused on tapping their potential for small-data reaction discovery (a few hundreds to thousands of samples). In this Account, we describe both feature engineering and feature learning approaches for molecular ML as applied to diverse reactions of high contemporary interest. Among these, catalytic asymmetric hydrogenation of imines/alkenes, ?-C(sp3)-H bond functionalization, and relay Heck reaction employed a feature engineering approach using the quantum-chemically derived physical organic descriptors as the molecular features?all designed to predict the enantioselectivity. The selection of molecular features to customize it for a reaction of interest is described, along with emphasizing the chemical insights that could be gathered through the use of such features. Feature learning methods for predicting the yield of Buchwald-Hartwig cross-coupling, deoxyfluorination of alcohols, and enantioselectivity of N,S-acetal formation are found to offer excellent predictions. We propose a transfer learning protocol, wherein an ML model such as a language model is trained on a large number of molecules (105-106) and fine-tuned on a focused library of target task reactions, as an effective alternative for small-data reaction discovery (102-103 reactions). The exploitation of deep neural network latent space as a method for generative tasks to identify useful substrates for a reaction is demonstrated as a promising strategy. © 2023 American Chemical Society."
131,130,34,130_innovation_enterprises_organizational_capabilities,"innovation,enterprises,organizational,capabilities,strategic,enterprise,firms,industry,technological,organizations","The servitization transformation of manufacturers to solution providers is a hot topic of both the scholarship and the practice. Existing studies argue that manufacturers should extend their services based on the availability and capacity of their products. However, only a few studies examined the leapfrog servitization transformation of manufacturers without product basis. For a large number of latedeveloping manufacturing enterprises, it is difficult to rely on their own products to extend their services due to their low manufacturing production capacity and their lack of key core technologies to effectively promote the transformation. Therefore, these manufacturers consider the leapfrog servitization transformation strategy as a realistic choice to promote the transformation. However, leapfrog servitization transformation is not always easy. A number of enterprises want to become solution providers, but only a few of them succeed because they fail to consider the huge gap that exists between manufacturing products and providing solutions, especially if the products are not the core in the solution. At this point, the challenge lies in knowing how to effectively and efficiently absorb and incorporate new external knowledge into their current internal knowledge to counter this limitation. This paper focuses on the rapid development of digital technology, which is driving the whole link reconstruction of industry, and the transition period of the paradigm brings new possibility for the manufacturer to make the leapfrog servitization transformation. Therefore, this paper aims to examine how manufacturers can promote the leapfrog servitization transformation by using digital technology. This paper uses the case study as a research method and theoretically take Noblelift Intelligent Equipment Co., Ltd. as the research sample. By introducing the perspective of absorptive capacity, this study analyzes the leap-forward servitization process of Noblelift that used to be a manufacturer of unmanned vehicles to now an internal intelligent logistics solutions company with the use of digital technology. A group was established to collect data and build a database, and structured coding methods were used to encode and analyze the original data. Firstly, the original data is briefly refined by first-order coding to form the initial concept. Then, in dialogue with the theory, first-order codes are assigned to second-order themes that describe and explain phenomena. Finally, the second order topics are summarized to form three aggregation dimensions in this paper. Based on this, a model of the leapfrog servitization process based on digital technology is constructed. Additionally, the data analysis is composed of multiple rounds of case discussion, analysis, and revision of ideas. The case data and theoretical model were repeatedly examined to ensure the validity of the theory and story. By examining the association between digitalization and servitization, the results show that manufacturers can promote the formation and utilization of absorptive capacity of enterprises while using digital technology to improve the original business efficiency by considering these three sub-processes: knowledge bricolage identification, knowledge scene transformation, and knowledge graph exploitation. In the knowledge bricolage identification process, the knowledge resources at hand can be digitally reconstructed using "" data visualization precipitation” and “service knowledge recognition” to narrow down the knowledge distance. In the knowledge scene transformation process, “business interconnection recombination” is used to accelerate the organization process from “service knowledge acquisition” to “ internal and external knowledge integration. ” In the iterative process of “ technology affordance enhancement” and “ modular utilization of knowledge”, it promotes the knowledge graph exploitation to realize its own leapfrog growth. The results of this study conclude that, (1) the transition strategy of manufacturers using digital technology to promote leapfrog servitization transformation is an interesting “ anomaly” of the theoretical hypothesis of continuous service growth process. This study responds to the call to use digital technology to promote servitization transformation as a priority of servitization research, and further answers the question of how manufacturers should use digital technology to leapfrog servitization transformation to achieve innovation catch-up when they do not completely rely on their own products. ( 2) Theoretical studies on the cultivation of absorptive capacity emphasize that existing internal knowledge is the basis of formulating absorptive capacity. How manufacturers cultivate absorptive capacity when a huge gap exists between the new external knowledge and existing internal knowledge still remains to be studied. In the absence of a product base provided by intelligent solutions, this paper provides an in-depth analysis of the recursive process of developing and using digital technologies to connect and accelerate manufacturer organizational ( inter-organizational) learning and to cultivate its absorptive capacity related to service knowledge. This study responds to the concerns on methods and processes of enterprise absorptive capacity cultivation under the condition of insufficient absorptive capacity, enrichs the research on absorptive capacity cultivation process, and expands the application boundary of absorptive capacity theory © 2023,Journal of Industrial Engineering and Engineering Management. All Rights Reserved.,Purpose: Embracing a large set of innovation objectives and collaborating with diverse partners have been promoted as a means to improve innovation performance. However, empirical evidence on the relationships between breadth of objectives, breadth of cooperation and innovation performance is limited, particularly in the context of emerging economies. A larger number of objectives and cooperation partners inevitably increases the complexity in organizational alignment, and cooperation eventually leads to diminishing returns. This study adds to the debate on the costs and benefits of cooperation for innovation. Understanding the optimal levels of the breadth of objectives and cooperation supports managerial decision-making and productivity in the practice of cooperation for innovation. Design/methodology/approach: Operationalizing breadth of innovation objectives and cooperation via the Turkish Community Innovation Survey data, self-reports reflecting 5,863 firm-level responses between 2006 and 2008 are analysed using tobit and probit models. The maximum likelihood estimator is used to find the optimal levels for breadth of objectives and cooperation. Findings: Firms with greater breadth of innovation objectives experience higher innovation performance; those with greater breadth of cooperation also experience higher innovation performance, but our results indicate the existence of optimal levels of breadth for both innovation objectives and cooperation. Research limitations/implications: The authors extend the logic that there is no safety in numbers in cooperation for innovation. If the aim is to enhance innovation performance, managers and policymakers need to pay attention to the number of innovation objectives and the amount of cooperation pursued by firms. However, innovation success may be closely associated with a firm's dynamic capabilities and ability to mobilize its resources. Drawing on organizational learning theories, future research could explore why a lower than maximum level of cooperation may be more conducive to reaching levels of enhanced innovation performance and whether this level is influenced by cognitive processes. Originality/value: The authors draw attention to the ideal number of innovation objectives and number of cooperating partners required to enhance innovation performance, thus contributing to the debate on the complex relationships between innovation, performance and cooperation in the unique setting of a large developing economy. © 2021, Emerald Publishing Limited.,Purpose. This research aims to assess the motivational factors in creating an innovation culture in agribusiness of micro, small, and medium-sized enterprises (MSMEs) by examining the organizational perspective, including organizational culture, organizational learning, market orientation, and attitude to innovation culture. Methodology/approach. A random cluster sampling method was used to select 100 agribusiness MSMEs in Sukoharjo Regency. This research considered exogenous latent variables, such as organizational culture, organizational learning, market orientation, and attitudes toward intention to innovate. The endogenous latent variables used were innovation intentions and innovation culture. Data were collected through an online survey and analyzed using Structural Equation Modeling (SEM) and Partial Least Square (PLS) analysis tools. Results. The results showed that organizational culture, market orientation, and attitudes influenced the intention to innovate, while the level of organizational learning did not. The intention to innovate, in turn, had a significant effect on innovative behavior, as shown in the performance of agribusiness companies. This innovative behavior was reflected in the emergence of new ideas to enhance the business, including the development of new product variants, the utilization of e-commerce for sales, and the improvement of production technology to increase cost efficiency. Therefore, there was a relationship between cultural innovation and company performance. Originality/scientific novelty. This research model was built based on behavioral theory and organizational culture, taking into account the difficulty and time required to develop an innovative culture within agribusiness MSMEs. These enterprises have organizational features different from medium and large companies. The objective of this research was to change the mindset of agribusiness MSMEs towards innovation, where innovation was no longer viewed as an option but rather a condition for success. Innovation became a value in fostering organizational culture. Practical value/implications. The practical implications of the findings were that agribusiness MSMEs should focus on efforts such as adapting to external changes, involving all employees, consistency, and a clear and long-term business vision to promote innovation. This positive attitude towards innovation could create a culture of innovation within the company, enabling MSME agribusiness to compete and develop into a better company. © 2023, Institute of Eastern European Research and Consulting. All rights reserved."
132,131,34,131_dehazing_haze_deblurring_singleimage,"dehazing,haze,deblurring,singleimage,hazefree,hazy,vision,defogging,dehazed,images","Traditional dehazing approaches that rely on prior knowledge exhibit limited efficacy when confronted with the intricacies of real-world hazy environments. While learning-based dehazing techniques necessitate large-scale datasets for effective model training, the acquisition of these datasets is time-consuming and laborious, and the resulting models may encounter a domain shift when processing real-world hazy images. To overcome the limitations of prior-based and learning-based dehazing methods, we propose a self-supervised remote sensing (RS) image-dehazing network based on zero-shot learning, where the self-supervised process avoids dense dataset requirements and the learning-based structures refine the artifacts in extracted image priors caused by complex real-world environments. The proposed method has three stages. The first stage involves pre-processing the input hazy image by utilizing a prior-based dehazing module; in this study, we employed the widely recognized dark channel prior (DCP) to obtain atmospheric light, a transmission map, and the preliminary dehazed image. In the second stage, we devised two convolutional neural networks, known as RefineNets, dedicated to enhancing the transmission map and the initial dehazed image. In the final stage, we generated a hazy image using the atmospheric light, the refined transmission map, and the refined dehazed image by following the haze imaging model. The meticulously crafted loss function encourages cycle-consistency between the regenerated hazy image and the input hazy image, thereby facilitating a self-supervised dehazing model. During the inference phase, the model undergoes training in a zero-shot manner to yield the haze-free image. These thorough experiments validate the substantial improvement of our method over the prior-based dehazing module and the zero-shot training efficiency. Furthermore, assessments conducted on both uniform and non-uniform RS hazy images demonstrate the superiority of our proposed dehazing technique. © 2023 by the authors.,Single-image rain streaks&#x2019; removal has attracted great attention in recent years. However, due to the highly visual similarity between the rain streaks and the line pattern image edges, the over-smoothing of image edges or residual rain streaks&#x2019; phenomenon may unexpectedly occur in the deraining results. To overcome this problem, we propose a direction and residual awareness network within the curriculum learning paradigm for the rain streaks&#x2019; removal. Specifically, we present a statistical analysis of the rain streaks on large-scale real rainy images and figure out that rain streaks in local patches possess principal directionality. This motivates us to design a direction-aware network for rain streaks&#x2019; modeling, in which the principal directionality property endows us with the discriminative representation ability of better differing rain streaks from image edges. On the other hand, for image modeling, we are motivated by the iterative regularization in classical image processing and unfold it into a novel residual-aware block (RAB) to explicitly model the relationship between the image and the residual. The RAB adaptively learns balance parameters to selectively emphasize informative image features and better suppress the rain streaks. Finally, we formulate the rain streaks&#x2019; removal problem into the curriculum learning paradigm which progressively learns the directionality of the rain streaks, rain streaks&#x2019; appearance, and the image layer in a coarse-to-fine, easy-to-hard guidance manner. Solid experiments on extensive simulated and real benchmarks demonstrate the visual and quantitative improvement of the proposed method over the state-of-the-art methods. IEEE,Single image dehazing is a critical problem in computer vision. However, most recently proposed learning-based dehazing methods achieve unsatisfactory quality with dehazed images due to inaccurate parametric estimation. The size of these models is also large to be applied with mobile devices' limited resources. Last, most models are tailored to image dehazing, achieving poor migration. Thus, we propose a compact multiscale attention feature fusion network with a model size of 2 MB called MSAFF-Net to perform end-to-end single image dehazing. In the proposed model, we design a simple and powerful feature extraction module to extract complex features from hazy images. We use a channel attention module and a multiscale spatial attention module to consider the regions with haze-relevant features. To our knowledge, this study is the first to directly apply the attention mechanism rather than to embed it into certain modules for single image dehazing. We compare MSAFF-Net with other approaches on the NTIRE18, RESIDE, and Middlebury Stereo datasets. We show that MSAFF-Net achieves comparable or better performance than other models. We also extend MSAFF-Net to single image deraining, and various experiments demonstrate its effectiveness. Results suggest that MSAFF-Net can directly restore clear images using channels with the most useful haze- or rain-relevant features and spatial locations.  © 1999-2012 IEEE."
133,132,33,132_resolution_resolutions_superresolution_srgan,"resolution,resolutions,superresolution,srgan,downscaling,imagespecific,images,jpegls,lowresolution,deep","Machine learning-based image super-resolution (SR) has garnered increasing research interest in recent years. However, there are two issues that have not been adequately addressed. The first issue is that existing SR methods often overlook the importance of improving the quality of the training dataset, which is a crucial factor in determining SR performance, regardless of the training method employed. The second issue is that while some studies report high numerical metrics, the visual results remain unsatisfactory. To address the first problem, we propose a new image down-sampling method to obtain higher-quality training datasets. To tackle the second problem, we present a new image super-resolution model based on a large-size convolution kernel and a multi-path algorithm. Specifically, we use an adaptive large-size convolutional kernel to extract features from the image based on the size of the input image, and a residual network to generate a deeper model to retain more details of the original input image. Experimental results demonstrate that the proposed multilayer downsampling method (MDM) can significantly improve the visual quality compared to traditional downsampling methods. Moreover, our proposed method achieves the best peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) values compared to several typical SR algorithms. Furthermore, subjective evaluation by human observers reveals that our method retains more details of the original image and produces smoother high-resolution images. Our proposed method effectively addresses the two aforementioned issues, which leads to improved SR performance in terms of both quantitative and qualitative measures. © 2024, The Author(s).,In recent years, deep learning-based methods have emerged as dominant players in the field of super-resolution (SR), owing to their exceptional reconstruction performance. The primary driver of their effectiveness lies in their utilization of extensive sets of paired low-resolution and high-resolution images for training deep learning models. This training enables the models to effectively replicate the intricate mapping relationship between low-resolution and high-resolution images. Nevertheless, at present, acquiring a sufficient quantity of such image pairs that satisfy the requirements remains a formidable obstacle. Therefore, in order to break the restriction of limited training sets, self-supervised learning has been introduced to train a model for each low-quality image, without requiring pairwise ground-truths. However, they generally presuppose the generation of low-resolution (LR) images from their high-resolution (HR) counterparts using a pre-defined kernel, such as Bicubic downscaling. Such an assumption is seldom valid for real-world LR images, where degradation processes in practical applications are diverse, intricate, and often undisclosed. Therefore, when the presumed downscaling kernel does not match the actual one, the outcomes of state-of-the-art approaches degrade substantially. In this paper, we introduce KGSR, a kernel-guided network for addressing real-world blind SR, effectively avoiding requiring large training image pairs and transforming the blind image super-resolution problem into a supervised learning and non-blind scenario. Specifically, KGSR trains two networks, namely Upscaling and Downscaling, utilizing only patches extracted from the input test image. On one hand, owing to the cross-scale recurrence property of the SR kernel within a single image, the Downscaling network acquires knowledge of the image-specific degradation process through a generative adversarial network. Consequently, the Downscaling network is capable of generating a downsampled version of the LR test image even when the acquisition process is unknown or less than ideal. Additionally, we employ a dedicated discriminator to compel the Downscaling network to prioritize the characterization of kernel orientations. Conversely, a precise blur kernel has the potential to yield superior performance. Guided by the accurate image-specific SR kernel acquired from the Downscaling network and the downsampled LR input, the Upscaling network is capable of producing a high-quality HR image from the LR input. Within the Upscaling network, we additionally introduce an effective module for harnessing the acquired image-specific SR kernel. KGSR operates as a fully unsupervised approach, yet it can concurrently produce both the image-specific SR kernel and high-quality HR images. Comprehensive experiments conducted on standard benchmarks validate the efficacy of the proposed approach compared to state-of-the-art methodologies. Moreover, the suggested method can deliver visually appealing SR outcomes while exhibiting shorter processing times when applied to real-world LR images. © 2023 Elsevier Ltd,In single image super-resolution (SISR), deep neural networks learn mainly nonlinear functions to obtain promising high-resolution (HR) images. However, there are usually two undesired limitations to recovered images of existing SR methods. First, since this task is typically an ill-posed problem, recovering the structural information for the SR process is usually sharp edges but distorted. Second, since this task generally has an extremely large space for the mapping function, learning this mapping from low-resolution (LR) to HR images is typically difficult. For the most part, SR models usually suffer from the lack of structural information for the objects in images and poor performance. To address the above issues, we propose a dual gradient regression scheme in the first stage of the network to recover the structure information of objects in images and the corresponding loss function to learn the difference between structural maps. In the second stage of the network, we restore HR gradient maps by the first stage to provide additional structural information for the second stage, further constraining the structure of the SR image and reducing the space of the possible functions. Extensive experiments demonstrate our results outperform the state-of-the-art SR methods in the SSIM, and achieve better visual effects than the state-of-the-art SR methods. © 2022"
134,133,33,133_accidents_prediction_fatalities_traffic,"accidents,prediction,fatalities,traffic,svm,crash,driving,safety,accident,models","One of the main objectives of an urban traffic control system is to reduce the crash frequency and the loss caused by these crashes on urban expressways. Real-time crash risk prediction (RTCRP) is an essential technique to identify crash precursors so as to take proactive measures to smooth traffic fluctuations. In addition, automatic incident detection (AID) is another important approach to timely detect an incident so as to design countermeasures that reduce any negative impacts on traffic dynamics. With the introduction of disruptive technologies in transport, highly disaggregated large datasets have started to emerge for modelling while existing modelling techniques utilized in RTCRP and AID may not be able to accurately predict traffic crashes in real-time. Therefore, this paper proposes a state-of-the-art reinforcement learning tree (RLT) approach to develop RTCRP model and automatic crash detection (ACD) model similar to AID, and further utilizes it to build a real-time traffic safety management framework for urban expressways with the input of online traffic data streaming. Recorded traffic flow data and historical crash data were extracted and integrated to develop and implement both RTCRP models and ACD models. The prediction results were compared with the frequently used logistic regression (LR), support vector machine (SVM) and deep neural network (DNN) and a sensitivity analysis for variable effects was conducted. The results confirm that RLT outperforms LR, SVM and DNN in developing RTCRP and ACD models. At the cost of 10% false-alarm rate, about 96% of the crashes were predicted or detected correctly by the proposed framework. The results also indicate that: i) collecting more data is helpful to improve the predictive performance and approximatively a minimum sample size of 20 observations per variable is reasonable for training RLT models; and ii) obtaining more factors is beneficial to improve the predictive performance. With the RLT approach, it was demonstrated that selected important variables also have the capability to provide reasonable predictive performance. © 2022 Elsevier Ltd,A large number of fatalities and severe injuries are caused by motorcycle crashes worldwide, particularly in developing countries. More than 50% of crashes in Pakistan involve motorcycles. To analyze motorcycle crash severity, various models, including both statistical and machine learning methods, have been applied. Researchers have widely acknowledged that machine learning methods provide superior prediction performance but have weaker interpretability power. However, no study has investigated the consistency of risk factors identified by the two streams of models. The consistency of the findings between these two kinds of methods is vital to improve the interpretability power of machine learning methods for policymaking in an era with more and more applications in the area of traffic safety. This study aims to narrow this research gap by comparing the consistency of crash severity risk factors identified by statistical models and machine learning methods. The study analyzes motorcycle crashes in Rawalpindi city of Pakistan. Multinomial logit model (MNL) and three machine learning models, i.e., the random forest (RF), naive Bayes, and gradient-boosted trees methods, are used to analyze the prediction performance and identify risk factors. The results show that the RF model, with an overall accuracy of 86.7%, outperformed other models. The SHapley Additive exPlanations (SHAP) method was adopted to explore the interpretability of machine learning methods. It was found that the contributing factors to crash injury severity identified by the RF method, such as distracted driving, collisions involving pedestrians, collisions involving a truck, and female riders, are consistent with those determined by the MNL model. These results have clear implications for developing cost-effective safety countermeasures to improve motorcycle safety in Pakistan. © 2023 Elsevier Ltd,The rapid increase in traffic volume on urban roads, over time, has altered the global traffic scenario. Additionally, it has increased the number of road crashes, some of which are severe and fatal in nature. The identification of hazardous roadway sections using the spatial pattern analysis of crashes and recognition of the primary and contributing factors may assist in reducing the severity of road traffic crashes (R.T.C.s). For crash severity prediction, along with spatial patterns, various machine learning models are used, and the spatial relations of R.T.C.s with neighboring areas are evaluated. In this study, tree-based ensemble models (gradient boosting and random forest) and a logistic regression model are compared for the prediction of R.T.C. severity. Sample data of road crashes in Al-Ahsa, the eastern province of Saudi Arabia, were obtained from 2016 to 2018. Random forest (R.F.) identifies significant features strongly correlated with the severity of the R.T.C.s. The analysis findings showed that the cause of the crash and the type of collision are the most crucial elements affecting the severity of injuries in traffic crashes. Furthermore, the target-specific model interpretation results showed that distracted driving, speeding, and sudden lane changes significantly contributed to severe crashes. The random forest (R.F.) method surpassed other models in terms of injury severity, individual class accuracies, and collective prediction accuracy when using k-fold (k = 10) based on various performance metrics. In addition to taking into account the machine learning approach, this study also included spatial autocorrelation analysis based on G.I.S. for identifying crash hotspots, and Getis Ord (Formula presented.) statistics were devised to locate cluster zones with high- and low-severity crashes. The results demonstrated that the research area’s spatial dependence was very strong, and the spatial patterns were clustered with a distance threshold of 500 m. The analysis’s approaches, which included Getis Ord (Formula presented.), the crash severity index, and the spatial autocorrelation of accident incidents according to Moran’s I, were found to be a successful way of locating and rating crash hotspots and crash severity. The techniques used in this study could be applied to large-scale crash data analysis while providing a useful tool for policymakers looking to improve roadway safety. © 2022 by the authors."
135,134,33,134_faces_facebased_face_facial,"faces,facebased,face,facial,recognition,mask,masks,depthnet,masked,pose","During COVID-19 coronavirus epidemic, almost everyone wears a mask to prevent the spread of virus. It raises a problem that the traditional face recognition model basically fails in the scene of face-based identity verification, such as security check, community visit check-in, etc. Therefore, it is imminent to boost the performance of masked face recognition. Most recent advanced face recognition methods are based on deep learning, which heavily depends on a large number of training samples. However, there are presently no publicly available masked face recognition datasets, especially real ones. To this end, this work proposes three types of masked face datasets, including Masked Face Detection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD) and Synthetic Masked Face Recognition Dataset (SMFRD). Besides, we conduct benchmark experiments on these three datasets for reference. As far as we know, we are the first to publicly release large-scale masked face recognition datasets that can be downloaded for free at https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset.  © 2019 IEEE.,Deep learning approaches achieve highly accurate face recognition by training the models with huge face image datasets. Unlike 2D face image datasets, there is a lack of large 3D face datasets available to the public. Existing public 3D face datasets were usually collected with few subjects, leading to the over-fitting problem. This paper proposes two CNN models to improve the RGB-D face recognition task. The first is a segmentation-aware depth estimation network, called DepthNet, which estimates depth maps from RGB face images by exploiting semantic segmentation for more accurate face region localization. The other is a novel segmentation-guided RGB-D face recognition model that contains an RGB recognition branch, a depth map recognition branch, and an auxiliary segmentation mask branch. In our multi-modality face recognition model, a feature disentanglement scheme is employed to factorize the feature representation into identity-related and style-related components. DepthNet is applied to augment a large 2D face image dataset to a large RGB-D face dataset, which is used for training our RGB-D face recognition model. Our experimental results show that DepthNet can produce more reliable depth maps from face images with the segmentation mask. Our multi-modality face recognition model fully exploits the depth map and outperforms state-of-the-art methods on several public 3D face datasets with challenging variations. © 2019 IEEE.,With the continuous development of deep learning, the face recognition field has also developed rapidly. However, with the massive popularity of COVID-19, face recognition with masks is a problem that is now about to be tackled in practice. In recognizing a face wearing a mask, the mask obscures most of the facial features of the face, resulting in the general face recognition model only capturing part of the facial information. Therefore, existing face recognition models are usually ineffective in recognizing faces wearing masks. This article addresses this problem in the existing face recognition model and proposes an improvement of Facenet. We use ConvNeXt-T as the backbone of the network model and add the ECA (Efficient Channel Attention) mechanism. This enhances the feature extraction of the unobscured part of the face to obtain more useful information, while avoiding dimensionality reduction and not increasing the model complexity. We design new face recognition models by investigating the effects of different attention mechanisms on face mask recognition models and the effects of different data set ratios on experimental results. In addition, we construct a large set of faces wearing masks so that we can efficiently and quickly train the model. Through experiments, our model proved to be 99.76% accurate for real faces wearing masks. A combined accuracy of 99.48% for extreme environments such as too high or lousy contrast and brightness. © 2023 by the authors."
136,135,33,135_imaging_microscopy_microscope_aperture,"imaging,microscopy,microscope,aperture,reconstructed,microscopes,fov,hologram,bioimaging,fovs","Digital holographic microscopy is an imaging process that encodes the 3D information of a sample into a single 2D hologram. The holographic reconstruction that decodes the hologram is conventionally based on the diffraction formula and involves various iterative steps in order to recover the lost phase information of the hologram. In the past few years, the deep-learning-based model has shown great potential to perform holographic reconstruction directly on a single hologram. However, preparing a large and high-quality dataset to train the models remains a challenge, especially when the holographic reconstruction images that serve as ground truth are difficult to obtain and can have a deteriorated quality due to various interferences of the imaging device. A cycle generative adversarial network is first trained with unpaired brightfield microscope images to restore the visual quality of the holographic reconstructions. The enhanced holographic reconstructions then serve as ground truth for the supervised learning of a U-Net that performs the holographic reconstruction on a single hologram. The proposed method was evaluated on plankton images and could also be applied to achieve super-resolution or colorization of the holographic reconstructions. © 2022 Optica Publishing Group.,The millimeter-wave frequency-diverse imaging regime has recently received considerable attention in both the security screening and synthetic aperture radar imaging literature. Considering that the minor systematic errors and alignment errors could still produce heavily corrupted images, these complex-based imaging reconstructions rely heavily on the precise measurement of both phase and amplitude of radiation field patterns and echo signals. In the literature, it is shown that by leveraging phase-retrieval techniques, salient reconstruction images can still be acquired, even in the presence of significant phase errors, which could ease the phase error calibration pressure to a large extent in practical imaging applications. In this paper, in the regime of phaseless frequency-diverse imaging, with the powerful feature inference and generation power of unsupervised generative models, an end-to-end deep prior generative neural network is designed to achieve near real-time imaging. The harsh imaging reconstruction with both the high radiation mode correlations and extremely low scene compression sampling ratio, which are extremely troublesome to tackle for generally applied matched-filter and compressed sensing approach in the current frequency-diverse imaging literature, can still be preferably handled with our reconstruction network. The well-trained reconstruction network is constituted by prior inference and deep generative modules with excellent generative capabilities and significant prior inference abilities. Using simulation experiments with radiation field data, we verify that the integration of phase-free frequency-change imaging with deep learning networks can effectively improve reconstruction capabilities and improve robustness to systematic phase errors. Compared with existing imaging methods, our imaging method has high imaging performance and can even reconstruct targets under low compression ratio conditions, which is somewhat competitive with current state-of-the-art algorithms. Moreover, we find that the proposed method has good anti-noise and stability. © 2022 by the authors.,Traditional miniaturized fluorescence microscopes are critical tools for modern biology. Invariably, they struggle to simultaneously image with a high spatial resolution and a large field of view (FOV). Lensless microscopes offer a solution to this limitation. However, real-time visualization of samples is not possible with lensless imaging, as image reconstruction can take minutes to complete. This poses a challenge for usability, as real-time visualization is a crucial feature that assists users in identifying and locating the imaging target. The issue is particularly pronounced in lensless microscopes that operate at close imaging distances. Imaging at close distances requires shift-varying deconvolution to account for the variation of the point spread function (PSF) across the FOV. Here, we present a lensless microscope that achieves real-time image reconstruction by eliminating the use of an iterative reconstruction algorithm. The neural network-based reconstruction method we show here, achieves more than 10000 times increase in reconstruction speed compared to iterative reconstruction. The increased reconstruction speed allows us to visualize the results of our lensless microscope at more than 25 frames per second (fps), while achieving better than 7 µm resolution over a FOV of 10 mm2. This ability to reconstruct and visualize samples in real-time empowers a more user-friendly interaction with lensless microscopes. The users are able to use these microscopes much like they currently do with conventional microscopes. © 2023 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement."
137,136,33,136_adsorption_adsorptionbased_adsorbents_adsorbent,"adsorption,adsorptionbased,adsorbents,adsorbent,adsorptive,nanoporous,co2,wastewater,molecular,co2n2","Nanoporous materials such as metal-organic frameworks (MOFs) have been extensively studied for their potential for adsorption and separation applications. In this respect, grand canonical Monte Carlo (GCMC) simulations have become a well-established tool for computational screenings of the adsorption properties of large sets of MOFs. However, their reliance on empirical force field potentials has limited the accuracy with which this tool can be applied to MOFs with challenging chemical environments such as open-metal sites. On the other hand, density-functional theory (DFT) is too computationally demanding to be routinely employed in GCMC simulations due to the excessive number of required function evaluations. Therefore, we propose in this paper a protocol for training machine learning potentials (MLPs) on a limited set of DFT intermolecular interaction energies (and forces) of CO2 in ZIF-8 and the open-metal site containing Mg-MOF-74, and use the MLPs to derive adsorption isotherms from first principles. We make use of the equivariant NequIP model which has demonstrated excellent data efficiency, and as such an error on the interaction energies below 0.2 kJ mol-1 per adsorbate in ZIF-8 was attained. Its use in GCMC simulations results in highly accurate adsorption isotherms and heats of adsorption. For Mg-MOF-74, a large dependence of the obtained results on the used dispersion correction was observed, where PBE-MBD performs the best. Lastly, to test the transferability of the MLP trained on ZIF-8, it was applied to ZIF-3, ZIF-4, and ZIF-6, which resulted in large deviations in the predicted adsorption isotherms and heats of adsorption. Only when explicitly training on data for all ZIFs, accurate adsorption properties were obtained. As the proposed methodology is widely applicable to guest adsorption in nanoporous materials, it opens up the possibility for training general-purpose MLPs to perform highly accurate investigations of guest adsorption. © 2023 American Chemical Society.,Conspectus Carbon capture, utilization, and storage have been identified as key technologies to decarbonize the energy and industrial sectors. Although postcombustion CO2 capture by absorption in aqueous amines is a mature technology, the required high regeneration energy, losses due to degradation and evaporation, and corrosion carry a high economic cost, precluding this technology to be used today at the scale required to mitigate climate change. Solid adsorbent-based systems with high CO2 capacities, high selectivity, and lower regeneration energy are becoming an attractive alternative for this purpose. Conscious of this opportunity, the search for optimal adsorbents for the capture of CO2 has become an urgent task. To accurately assess the performance of CO2 separation by adsorption at the needed scale, adsorbents should be synthesized and fully characterized under the required operating conditions, and the proper design and simulation of the process should be implemented along with techno-economic and environmental assessments. Several works have examined pure CO2 single-component adsorption or binary mixtures of CO2 with nitrogen for different families of adsorbents, primarily addressing their CO2 adsorption capacity and selectivity; however, very limited data is available under other conditions and/or with impurities, mainly due to the intensive experimental (modeling) efforts required for the large number of adsorbents to be studied, posing a challenge for their assessment under the needed conditions. In this regard, molecular simulations can be employed in synergy with experiments, reliably generating missing adsorption properties of mixtures while providing understanding at the molecular level of the mechanism of the adsorption process. This Account provides an outlook on strategies used for the rational design of materials for CO2 capture from different sources from the understanding of the adsorption mechanism at the molecular level. We illustrate with practical examples from our work and others’ work how molecular simulations can be reliably used to link the molecular knowledge of novel adsorbents for which limited data exist for CO2 capture adsorption processes. Molecular simulation results of different adsorbents, including MOFs, zeolites, and carbon-based and silica-based materials, are discussed, focusing on understanding the role of physical and chemical adsorption obtained from simulations and quantifying the impact of impurities in the performance of the materials. Furthermore, simulation results can be used for screening adsorbents from basic key performance indicators, such as cycling the working capacity, selectivity, and energy requirement, or for feeding detailed dynamic models to assess their performance in swing adsorption processes on the industrial scale, additionally including monetized performance indicators such as operating expenses, equipment sizes, and compression cost. Moreover, we highlight the role of molecular simulations in guiding strategies for improving the performance of these materials by functionalization with amines or creating hybrid solid materials. We show how integrating models at different scales provides a robust and reliable assessment of the performance of the adsorbent materials under the required industrial conditions, rationally guiding the search for best performers. Trends in additional computational resources that can be used, including machine learning, and perspectives on practical requirements for leveraging CO2 capture adsorption technologies on the needed scale are also discussed. © 2023 The Authors. Published by American Chemical Society.,Adsorption-based separations using metal-organic frameworks (MOFs) are promising candidates for replacing common energy-intensive separation processes. The so-called adsorption space formed by the combination of billions of possible molecules and thousands of reported MOFs is vast. It is very challenging to comprehensively evaluate the performance of MOFs for chemical separation through experiments. Molecular simulations and machine learning (ML) have been widely applied to make predictions for adsorption-based separations. Previous ML approaches to these issues were typically limited to smaller molecules and often had poor accuracy in the dilute limit. To enable exploration of a wider adsorption space, we carefully selected a diverse set of 45 molecules and 335 MOFs and generated single-component isotherms of 15,075 MOF-molecule pairs by grand canonical Monte Carlo. Using this database, we successfully developed accurate (r2 > 0.9) machine learning models predicting adsorption isotherms of diverse molecules in large libraries of MOFs. With this approach, we can efficiently make predictions of large collections of MOFs for arbitrary mixture separations. By combining molecular simulation data and ML predictions with Ideal Adsorbed Solution Theory, we tested the ability of these approaches to make predictions of adsorption selectivity and loading for challenging near-azeotropic mixtures. © 2023 The Authors. Published by American Chemical Society."
138,137,32,137_photovoltaic_pv_solar_cnn,"photovoltaic,pv,solar,cnn,renewable,classify,features,feature,panels,dataset","Solar energy production has significantly increased in recent years in the European Union (EU), accounting for 12% of the total in 2022. The growth in solar energy production can be attributed to the increasing adoption of solar photovoltaic (PV) panels, which have become cost-effective and efficient means of energy production, supported by government policies and incentives. The maturity of solar technologies has also led to a decrease in the cost of solar energy, making it more competitive with other energy sources. As a result, there is a growing need for efficient methods for detecting and mapping the locations of PV panels. Automated detection can in fact save time and resources compared to manual inspection. Moreover, the resulting information can also be used by governments, environmental agencies and other companies to track the adoption of renewable sources or to optimize energy distribution across the grid. However, building effective models to support the automated detection and mapping of solar photovoltaic (PV) panels presents several challenges, including the availability of high-resolution aerial imagery and high-quality, manually-verified labels and annotations. In this study, we address these challenges by first constructing a dataset of PV panels using very-high-resolution (VHR) aerial imagery, specifically focusing on the region of Piedmont in Italy. The dataset comprises 105 large-scale images, providing more than 9,000 accurate and detailed manual annotations, including additional attributes such as the PV panel category. We first conduct a comprehensive evaluation benchmark on the newly constructed dataset, adopting various well-established deep-learning techniques. Specifically, we experiment with instance and semantic segmentation approaches, such as Rotated Faster RCNN and Unet, comparing strengths and weaknesses on the task at hand. Second, we apply ad-hoc modifications to address the specific issues of this task, such as the wide range of scales of the installations and the sparsity of the annotations, considerably improving upon the baseline results. Last, we introduce a robust and efficient post-processing polygonization algorithm that is tailored to PV panels. This algorithm converts the rough raster predictions into cleaner and more precise polygons for practical use. Our benchmark evaluation shows that both semantic and instance segmentation techniques can be effective for detecting and mapping PV panels. Instance segmentation techniques are well-suited for estimating the localization of panels, while semantic solutions excel at surface delineation. We also demonstrate the effectiveness of our ad-hoc solutions and post-processing algorithm, which can provide an improvement up to +10% on the final scores, and can accurately convert coarse raster predictions into usable polygons.  © 2013 IEEE.,The widespread adoption of photovoltaic (PV) technology for renewable energy necessitates accurate segmentation of PV panels to estimate installation capacity. However, achieving highly efficient and precise segmentation methods remains a pressing challenge. Recent advancements in artificial intelligence and remote sensing techniques have shown promise in PV segmentation. Nevertheless, real-world scenarios introduce complexities such as diverse sensing platforms, sensors, panel categories, and testing regions. These factors contribute to resolution, size, and foreground-background class imbalances, impeding accurate and generalized PV panel segmentation over large areas. To address these challenges, we propose GenPV, a deep learning model that leverages data distribution analysis and PV panel characteristics to enhance segmentation accuracy and generalization. GenPV employs a multi-scale feature learning approach, utilizing an enhanced feature pyramid network to fuse data features from multiple resolutions, effectively addressing resolution imbalance. Moreover, inductive learning is employed through a multitask approach, facilitating the detection and identification of both small and large-sized PV panels to mitigate size imbalance. To address significant class imbalance in PV panel recognition tasks, we integrate the Focal loss function for effective hard sample mining. Through experimental evaluation conducted in Heilbronn, Germany, our proposed method demonstrates superior performance compared to state-of-the-art approaches in PV panel segmentation. The results exhibit progressively higher accuracy and improved generalization capability. These findings highlight the potential of our method to serve as an advanced and practical tool for PV segmentation in the renewable energy field. © 2023,Photovoltaic (PV) boards are a perfect way to create eco-friendly power from daylight. The defects in the PV panels are caused by various conditions; such defective PV panels need continuous monitoring. The recent development of PV panel monitoring systems provides a modest and viable approach to monitoring and managing the condition of the PV plants. In general, conventional procedures are used to identify the faulty modules earlier and to avoid declines in power generation. The existing deep learning architectures provide the required output to predict the faulty PV panels with less accuracy and a more time-consuming process. To increase the accuracy and to reduce the processing time, a new Convolutional Neural Network (CNN) architecture is required. Hence, in the present work, a new Real-time Multi Variant Deep learning Model (RMVDM) architecture is proposed, and it extracts the image features and classifies the defects in PV panels quickly with high accuracy. The defects that arise in the PV panels are identified by the CNN based RMVDM using RGB images. The biggest difference between CNN and its predecessors is that CNN automatically extracts the image features without any help from a person. The technique is quantitatively assessed and compared with existing faulty PV board identification approaches on the large real-time dataset. The results show that 98% of the accuracy and recall values in the fault detection and classification process. © 2023 Tech Science Press. All rights reserved."
139,138,32,138_gans_gan_dcgan_generative,"gans,gan,dcgan,generative,pggan,adversarial,cyclegan,srdgan,wgan,fedgans","Data augmentation is widely used in image processing and pattern recognition problems in order to increase the richness in diversity of available data. It is commonly used to improve the classification accuracy of images when the available datasets are limited. Deep learning approaches have demonstrated an immense breakthrough in medical diagnostics over the last decade. A significant amount of datasets are needed for the effective training of deep neural networks. The appropriate use of data augmentation techniques prevents the model from over-fitting and thus increases the generalization capability of the network while testing afterward on unseen data. However, it remains a huge challenge to obtain such a large dataset from rare diseases in the medical field. This study presents the synthetic data augmentation technique using Generative Adversarial Networks to evaluate the generalization capability of neural networks using existing data more effectively. In this research, the convolutional neural network (CNN) model is used to classify the X-ray images of the human chest in both normal and pneumonia conditions; then, the synthetic images of the X-ray from the available dataset are generated by using the deep convolutional generative adversarial network (DCGAN) model. Finally, the CNN model is trained again with the original dataset and augmented data generated using the DCGAN model. The classification performance of the CNN model is improved by 3.2% when the augmented data were used along with the originally available dataset.  © 2023 World Scientific Publishing Company.,Lung cancer is the most frequent cancer and the reason for cancer death, with high morbidity and mortality. Computed tomography is one of the efficient medical imaging tools for lung cancer diagnosis, which offers internal lung details. However, as there is limited availability of datasets and requires large number of images for interpretation, it is hard for radiologists to diagnose lung cancer. The generative adversarial network (GAN) is a significant generative model employed for data augmentation, which has the benefit of simulating data distribution without the explicit modeling of potential probability density functions. Despite the benefits of GAN, training process remains challenging due to high convergence time and mode collapse problems. To resolve these issues, in this paper, a Hessian Adaptive Learning (HAL) Optimization technique. The proposed HL technique uses gradient and curvature data to eliminate the mode collapse problem and to improve the dataset size via a generation of diverse images. The experiments were conducted on Vanilla GAN, Wasserstein Generative Adversarial Network (WGAN), Conditional Generative Adversarial Network (CGAN), Wasserstein, and Deep Convolutional Generative Adversarial Network (DCGAN). Each GAN is tested using stochastic Gradient descent (SGD), Gauss–Newton (GN) Second-order learning, and proposed HAL optimization techniques. The experimental outcomes prove that the GANs with HAL optimization technique yields better performance compared to SGD and GN models. The experimental results assured that the GANs converge fast and eliminate mode collapse problems using HAL optimization. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,In the recent past, deep learning-based models have achieved tremendous success in computer vision-related tasks with the help of large-scale annotated datasets. An interesting application of deep learning is synthetic data generation, especially in the domain of medical image analysis. The need for such a task arises due to the scarcity of original data. Class imbalance is another reason for applying data augmentation techniques. Generative Adversarial Networks (GANs) are beneficial for synthetic image generation in various fields. However, stand-alone GANs may only fetch the localized features in the latent representation of an image, whereas combining different GANs might understand the distributed features. To this end, we have proposed AGGrGAN, an aggregation of three base GAN models—two variants of Deep Convolutional Generative Adversarial Network (DCGAN) and a Wasserstein GAN (WGAN) to generate synthetic MRI scans of brain tumors. Further, we have applied the style transfer technique to enhance the image resemblance. Our proposed model efficiently overcomes the limitation of data unavailability and can understand the information variance in multiple representations of the raw images. We have conducted all the experiments on the two publicly available datasets - the brain tumor dataset and the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 dataset. Results show that the proposed model can generate fine-quality images with maximum Structural Similarity Index Measure (SSIM) scores of 0.57 and 0.83 on the said two datasets. © 2022, The Author(s)."
140,139,32,139_confounders_interventions_propensity_bias,"confounders,interventions,propensity,bias,estimating,causal,outcomes,estimates,trials,inference","There is a long-standing debate in the statistical, epidemiological, and econometric fields as to whether nonparametric estimation that uses machine learning in model fitting confers any meaningful advantage over simpler, parametric approaches in finite sample estimation of causal effects. We address the question: when estimating the effect of a treatment on an outcome, how much does the choice of nonparametric vs parametric estimation matter? Instead of answering this question with simulations that reflect a few chosen data scenarios, we propose a novel approach to compare estimators across a large number of datagenerating mechanisms drawn from nonparametric models with semi-informative priors. We apply this proposed approach and compare the performance of two nonparametric estimators (Bayesian adaptive regression tree and a targeted minimum loss-based estimator) to two parametric estimators (a logistic regression- based plug-in estimator and a propensity score estimator) in terms of estimating the average treatment effect across thousands of data-generating mechanisms. We summarize performance in terms of bias, confidence interval coverage, and mean squared error. We find that the two nonparametric estimators can substantially reduce bias as compared to the two parametric estimators in large-sample settings characterized by interactions and nonlinearities while compromising very little in terms of performance even in simple, small-sample settings.  © 2023 the author(s).,Propensity score matching is commonly used in observational studies to control for confounding and estimate the causal effects of a treatment or exposure. Frequently, in observational studies data are clustered, which adds to the complexity of using propensity score techniques. In this article, we give an overview of propensity score matching methods for clustered data, and highlight how propensity score matching can be used to account for not just measured confounders, but also unmeasured cluster level confounders. We also consider using machine learning methods such as generalized boosted models to estimate the propensity score and show that accounting for clustering when using these methods can greatly reduce the performance, particularly when there are a large number of clusters and a small number of subjects per cluster. In order to get around this we highlight scenarios where it may be possible to control for measured covariates using propensity score matching, while using fixed effects regression in the outcome model to control for cluster level covariates. Using simulation studies we compare the performance of different propensity score matching methods for clustered data across a number of different settings. Finally, as an illustrative example we apply propensity score matching methods for clustered data to study the causal effect of aspirin on hearing deterioration using data from the conservation of hearing study. © The Author(s) 2022.,Personalized medicine requires an understanding of treatment effect heterogeneity. Evolving toward causal evidence for scenarios not studied in randomized trials necessitates a methodology using real-world evidence. Herein, we demonstrate a methodology that generates causal effects, assesses the heterogeneity of the effects and adjusts for the clustered nature of the data. This study uses a state-of-the-art machine learning survival model, riAFT-BART, to draw causal inferences about individual survival treatment effects, while accounting for the variability in institutional effects; further, it proposes a data-driven approach to agnostically (as opposed to a priori hypotheses) ascertain which subgroups exhibit an enhanced treatment effect from which intervention, relative to global evidence—average treatment effects measured at the population level. Comprehensive simulations show the advantages of the proposed method in terms of bias, efficiency and precision in estimating heterogeneous causal effects. The empirically validated method was then used to analyze the National Cancer Database. © 2022 by the authors."
141,140,31,140_photonics_photonic_optical_optoelectronic,"photonics,photonic,optical,optoelectronic,silicon,wavelength,wavelengths,networks,neural,hardware","In light of recent achievements in optical computing and machine learning, we consider the conditions under which all-optical computing may surpass electronic and optoelectronic computing in terms of energy efficiency and scalability. When considering the performance of a system as a whole, the cost of memory access and data acquisition is likely to be one of the main efficiency bottlenecks not only for electronic, but also for optoelectronic and all-optical devices. However, we predict that all-optical devices will be at an advantage in the case of inference in large neural network models, and the advantage will be particularly large in the case of generative models. We also consider the limitations of all-optical neural networks, including footprint, strength of nonlinearity, optical signal degradation, limited precision of computations, and quantum noise.  © 2024 American Physical Society.,Traditional computers are limited by the separation of memory and processor units, is difficult to achieve fast, efficient, and low-power computing. While photonic spiking neural networks (SNNs) can overcome these shortcomings, they encounter limitations in large-scale integration. Silicon photonics platform, compatible with mature Complementary Metal Oxide Semiconductor (CMOS) platforms, is a promising candidate for realizing large-scale photonic SNNs. In this work, we proposed an integrated photonic SNN by exploiting the photonic properties of phase-change material (PCM) Ge2Sb2Te5 (GST) and micro-ring resonators (MRR), and demonstrated its integrate-and-fire (IF) behavior. Based on a system-level behavioral model, we adopt an improved Tempotron-like ReSuMe supervised learning algorithm to train the proposed photonic SNNs and complete a pattern recognition task for the clock's 12 clockwise directions. Then the influence of different noise levels is considered, and the accuracy is close to 1 when the noise level is less than 0.2. We propose a photonic implementation of such an SNN system, and use wavelength division multiplexing to achieve a scalable architecture for the pattern recognition task. The collaborative design and optimization of hardware architecture and algorithm are realized, providing a theoretical basis for the realization of photonic SNN based on MRRs and PCM. © 2023 Elsevier B.V.,As Deep Learning (DL) models grow larger and more complex, training jobs are increasingly distributed across multiple Computing Units (CU) such as GPUs and TPUs. Each CU processes a sub-part of the model and synchronizes results with others. Communication among these CUs has emerged as a key bottleneck in the training process. In this work, we present SiPAC, a Silicon Photonic Accelerated Compute cluster. SiPAC accelerates distributed DL training by means of two co-designed components: a photonic physical layer and a novel collective algorithm. The physical layer exploits embedded photonics to bring peta-scale I/O directly to the CUs of a DL optimized cluster and uses resonator-based optical wavelength selectivity to realize hardware multi-casting. The collective algorithm builds on the hardware multi-casting primitive. This combination expedites a variety of collective communications commonly employed in DL training and has the potential to drastically ease the communication bottlenecks. We demonstrate the feasibility of realizing the SiPAC architecture through 1) an optical testbed experiment where an array of comb laser wavelengths are shuffled by a cascaded ring switch, with each ring selecting and forwarding multiple wavelengths to increase the effective communication bandwidth and hence demonstrating the hardware multicasting primitive, and 2) a four-GPU testbed running a realistic DL workload that achieves 22% system-level performance improvement relative to a similarly-sized leaf-spine topology. Large scale simulations show that SiPAC achieves a 1.4× to 5.9× communication time reduction compared to state-of-the-art compute clusters for representative collective communications.  © 1983-2012 IEEE."
142,141,31,141_housing_predictive_neighborhood_house,"housing,predictive,neighborhood,house,attributes,estate,census,regression,data,urban","The Turkish Housing Market has experienced a steep increase in prices. Individual and corporate investors now possess tools to estimate the real estate evaluation while using smaller amounts of data with traditional techniques. Not having an analytical approach to evaluate the price of real estate could cause the investor to lose considerable amounts of money, especially in the case of individual investors. This study aims to determine how different machine learning algorithms with real market data can improve this process. To be able to test this, over 30000 lines of housing market data with over 13 variables is scraped. Data is cleansed, manipulated and visualized, while predictive models such as linear regression, polynomial regression, decision trees, random forests, and XGboost are created and compared according to the CRISP-DM framework. The results show that using complex techniques to create machine learning models could improve the accuracy in predicting the listing prices of houses. This paper aims to: - analyze the effects of using a real and relatively large amount of data, - determine the main variables that contribute to the evaluation of an estate, - compare different machine learning models to find the optimal one for the real estate market, - create an accurate model to predict the value of any house on the Istanbul market.  © 2022 Mert Tekin et al.,House price prediction is an important problem that could benefit home buyers and sellers. Traditional models for house price prediction use numerical attributes such as the number of rooms but disregard the house description text. The recent developments in text processing suggest these can be valuable attributes, which motivated us to use house descriptions. This paper focuses on the house asking/advertising price and studies the impact of using house description texts to predict the final house price. To achieve this, we collected a large and diverse set of attributes on house postings, including the house advertising price. Then, we compare the performance of three scenarios: using only the house description, only numeric attributes, or both. We processed the description text through three word embedding techniques: TF-IDF, Word2Vec, and BERT. Four regression algorithms are trained using only textual data, non-textual data, or both. Our results show that by using exclusively the description data with Word2Vec and a Deep Learning model, we can achieve good performance. However, the best overall performance is obtained when using both textual and non-textual features. An <![CDATA[ $R^2$ ]]> of 0.7904 is achieved by the deep learning model using only description data on the testing data. This clearly indicates that using the house description text alone is a strong predictor for the house price. However, when observing the RMSE on the test data, the best model was gradient boosting using both numeric and description data. Overall, we observe that combining the textual and non-textual features improves the learned model and provides performance benefits when compared against using only one of the feature types. We also provide a freely available application for house price prediction, which is solely based on a house text description and uses our final developed model with Word2Vec and Deep Learning to predict the house price.  © The Author(s), 2023. Published by Cambridge University Press.,We present Luce, the first life-long predictive model for automated property valuation. Luce addresses two critical issues of property valuation: the lack of recent sold prices and the sparsity of house data. It is designed to operate on a limited volume of recent house transaction data. As a departure from prior work, Luce organizes the house data in a heterogeneous information network (HIN) where graph nodes are house entities and attributes that are important for house price valuation. We employ a Graph Convolutional Network (GCN) to extract the spatial information from the HIN for house-related data like geographical locations, and then use a Long Short Term Memory (LSTM) network to model the temporal dependencies for house transaction data over time. Unlike prior work, Luce can make effective use of the limited house transactions data in the past few months to update valuation information for all house entities within the HIN. By providing a complete and up-to-date house valuation dataset, Luce thus massively simplifies the downstream valuation task for the targeting properties. We demonstrate the benefit of Luce by applying it to large, real-life datasets obtained from the Toronto real estate market. Extensive experimental results show that Luce not only significantly outperforms prior property valuation methods but also often reaches and sometimes exceeds the valuation accuracy given by independent experts when using the actual realization price as the ground truth.  © 1989-2012 IEEE."
143,142,31,142_landslide_landslides_geomorphological_svm,"landslide,landslides,geomorphological,svm,geology,hydrological,floods,geological,flooding,slope","Detecting and mapping landslides are crucial for effective risk management and planning. With the great progress achieved in applying optimized and hybrid methods, it is necessary to use them to increase the accuracy of landslide susceptibility maps. Therefore, this research aims to compare the accuracy of the novel evolutionary methods of landslide susceptibility mapping. To achieve this, a unique method that integrates two techniques from Machine Learning and Neural Networks with novel geomorphological indices is used to calculate the landslide susceptibility index (LSI). The study was conducted in western Azerbaijan, Iran, where landslides are frequent. Sixteen geology, environment, and geomorphology factors were evaluated, and 160 landslide events were analyzed, with a 30:70 ratio of testing to training data. Four Support Vector Machine (SVM) algorithms and Artificial Neural Network (ANN)-MLP were tested. The study outcomes reveal that utilizing the algorithms mentioned above results in over 80% of the study area being highly sensitive to large-scale movement events. Our analysis shows that the geological parameters, slope, elevation, and rainfall all play a significant role in the occurrence of landslides in this study area. These factors obtained 100%, 75.7%, 68%, and 66.3%, respectively. The predictive performance accuracy of the models, including SVM, ANN, and ROC algorithms, was evaluated using the test and train data. The AUC for ANN and each machine learning algorithm (Simple, Kernel, Kernel Gaussian, and Kernel Sigmoid) was 0.87% and 1, respectively. The Classification Matrix algorithm and Sensitivity, Accuracy, and Specificity variables were used to assess the models' efficacy for prediction purposes. Results indicate that machine learning algorithms are more effective than other methods for evaluating areas' sensitivity to landslide hazards. The Simple SVM and Kernel Sigmoid algorithms performed well, with a performance score of one, indicating high accuracy in predicting landslide-prone areas. © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Landslide is a serious natural disaster next only to earthquake and flood, which will cause a great threat to people's lives and property safety. The traditional research of landslide disaster based on experience-driven or statistical model and its assessment results are subjective, difficult to quantify, and no pertinence. As a new research method for landslide susceptibility assessment, machine learning can greatly improve the landslide susceptibility model's accuracy by constructing statistical models. Taking Western Henan for example, the study selected 16 landslide influencing factors such as topography, geological environment, hydrological conditions, and human activities, and 11 landslide factors with the most significant influence on the landslide were selected by the recursive feature elimination (RFE) method. Five machine learning methods [Support Vector Machines (SVM), Logistic Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Linear Discriminant Analysis (LDA)] were used to construct the spatial distribution model of landslide susceptibility. The models were evaluated by the receiver operating characteristic curve and statistical index. After analysis and comparison, the XGBoost model (AUC 0.8759) performed the best and was suitable for dealing with regression problems. The model had a high adaptability to landslide data. According to the landslide susceptibility map of the five models, the overall distribution can be observed. The extremely high and high susceptibility areas are distributed in the Funiu Mountain range in the southwest, the Xiaoshan Mountain range in the west, and the Yellow River Basin in the north. These areas have large terrain fluctuations, complicated geological structural environments and frequent human engineering activities. The extremely high and highly prone areas were 12043.3 km2 and 3087.45 km2, accounting for 47.61% and 12.20% of the total area of the study area, respectively. Our study reflects the distribution of landslide susceptibility in western Henan Province, which provides a scientific basis for regional disaster warning, prediction, and resource protection. The study has important practical significance for subsequent landslide disaster management. ©2023 China Geology Editorial Office. © 2023 Editorial Office of China Geology. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd.,This research aimed to assess landslide susceptibility in the Badakhshan province of Afghanistan, an area highly susceptible to landslides due to its complex topography and geological conditions. Three distinct machine learning (ML) models, namely the Generalized Linear Model (GLM), Maximum Entropy (ME), and Random Forest (RF), were employed to identify the key contributing factors to landslide occurrences in the study region. The dataset used in this study consisted of landslide conditioning factors and a landslide inventory map. The conditioning factors encompassed lithology, soil type, plane curvature, profile curvature, elevation, slope, aspect, precipitation, land use/land cover (LULC), distance to fault, river, road, Normalized Difference Vegetation Index (NDVI), Topographic Wetness Index (TWI), Terrain Ruggedness Index (TRI), and Standardized Precipitation Index (SPI). The landslide inventory map contained 177 landslide locations and 65 non-landslide points obtained from Google Earth. Each machine learning (ML) model was trained and implemented independently using 70% of the training data, with the results validated against the remaining 30% of the landslide inventory dataset. Ensemble results from GLM, ME, and RF were obtained using the median approach. All three models exhibited consistent performance and identified similar landslide-prone areas. Among the various factors studied, proximity to rivers emerged as the most influential factor contributing to landslides, followed by the distance to roads and slope gradient. The study revealed that the districts of Argo and Yaftali Sufla, near Faizabad, were identified as particularly susceptible to landslides, especially in the vicinity of large valleys. Out of the total study area of 3086.4 km2, ?2162 km2 were deemed relatively safe from landslides, while 149 km2 (representing 4.8% of the study region) were identified as highly susceptible to landslides. Area Under the Receiver Operating Characteristic Curve (AUC) and Root Mean Square Error (RMSE) statistics were used to evaluate the performance of the machine learning (ML) algorithms. The RF and ME models demonstrated the highest performance levels. This research contributes to our understanding of landslide susceptibility in Badakhshan province and can aid in implementing effective landslide risk management strategies in the region. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group."
144,143,31,143_fires_wildfires_wildfire_fire,"fires,wildfires,wildfire,fire,firefighting,yolov5,convolutional,flamenet,detection,yolov5s","Fire accidents occur in every part of the world and cause a large number of casualties because of the risks involved in manually extinguishing the fire. In most cases, humans cannot detect and extinguish fire manually. Fire extinguishing robots with sophisticated functionalities are being rapidly developed nowadays, and most of these systems use fire sensors and detectors. However, they lack mechanisms for the early detection of fire, in case of casualties. To detect and prevent such fire accidents in its early stages, a deep learning-based automatic fire extinguishing mechanism was introduced in this work. Fire detection and human presence in fire locations were carried out using convolution neural networks (CNNs), configured to operate on the chosen fire dataset. For fire detection, a custom learning network was formed by tweaking the layer parameters of CNN for detecting fires with better accuracy. For human detection, Alex-net architecture was employed to detect the presence of humans in the fire accident zone. We experimented and analyzed the proposed model using various optimizers, activation functions, and learning rates, based on the accuracy and loss metrics generated for the chosen fire dataset. The best combination of neural network parameters was evaluated from the model configured with an Adam optimizer and softmax activation, driven with a learning rate of 0.001, providing better accuracy for the learning model. Finally, the experiments were tested using a mobile robotic system by configuring them in automatic and wireless control modes. In automatic mode, the robot was made to patrol around and monitor for fire casualties and fire accidents. It automatically extinguished the fire using the learned features triggered through the developed model. © 2023 by the authors.,Forest fires occur frequently around the world, causing serious economic losses and human casualties. Deep learning techniques based on convolutional neural networks (CNN) are widely used in the intelligent detection of forest fires. However, CNN-based forest fire target detection models lack global modeling capabilities and cannot fully extract global and contextual information about forest fire targets. CNNs also pay insufficient attention to forest fires and are vulnerable to the interference of invalid features similar to forest fires, resulting in low accuracy of fire detection. In addition, CNN-based forest fire target detection models require a large number of labeled datasets. Manual annotation is often used to annotate the huge amount of forest fire datasets; however, this takes a lot of time. To address these problems, this paper proposes a forest fire detection model, TCA-YOLO, with YOLOv5 as the basic framework. Firstly, we combine the Transformer encoder with its powerful global modeling capability and self-attention mechanism with CNN as a feature extraction network to enhance the extraction of global information on forest fire targets. Secondly, in order to enhance the model’s focus on forest fire targets, we integrate the Coordinate Attention (CA) mechanism. CA not only acquires inter-channel information but also considers direction-related location information, which helps the model to better locate and identify forest fire targets. Integrated adaptively spatial feature fusion (ASFF) technology allows the model to automatically filter out useless information from other layers and efficiently fuse features to suppress the interference of complex backgrounds in the forest area for detection. Finally, semi-supervised learning is used to save a large amount of manual labeling effort. The experimental results show that the average accuracy of TCA-YOLO improves by 5.3 compared with the unimproved YOLOv5. TCA-YOLO also outperformed in detecting forest fire targets in different scenarios. The ability of TCA-YOLO to extract global information on forest fire targets was much improved. Additionally, it could locate forest fire targets more accurately. TCA-YOLO misses fewer forest fire targets and is less likely to be interfered with by forest fire-like targets. TCA-YOLO is also more focused on forest fire targets and better at small-target forest fire detection. FPS reaches 53.7, which means that the detection speed meets the requirements of real-time forest fire detection. © 2023 by the authors.,Intense, large-scale forest fires are damaging and very challenging to control. Locations, where various types of fire behavior occur, vary depending on environmental factors. According to the burning site of forest fires and the degree of damage, this paper considers the classification and identification of surface fires and canopy fires. Deep learning-based forest fire detection uses convolutional neural networks to automatically extract multidimensional features of forest fire images with high detection accuracy. To accurately identify different forest fire types in complex backgrounds, an improved forest fire classification and detection model (FCDM) based on YOLOv5 is presented in this paper, which uses image-based data. By changing the YOLOv5 bounding box loss function to SIoU Loss and introducing directionality in the cost of the loss function to achieve faster convergence, the training and inference of the detection algorithm are greatly improved. The Convolutional Block Attention Module (CBAM) is introduced in the network to fuse channel attention and spatial attention to improve the classification recognition accuracy. The Path Aggregation Network (PANet) layer in the YOLOv5 algorithm is improved into a weighted Bi-directional Feature Pyramid Network (BiFPN) to fuse and filter forest fire features of different dimensions to improve the detection of different types of forest fires. The experimental results show that this improved forest fire classification and identification model outperforms the YOLOv5 algorithm in both detection performances. The mAP@0.5 of fire detection, surface fire detection, and canopy fire detection was improved by 3.9%, 4.0%, and 3.8%, respectively. Among them, the mAP@0.5 of surface fire reached 83.1%, and the canopy fire detection reached 90.6%. This indicates that the performance of our proposed improved model has been effectively improved and has some application prospects in forest fire classification and recognition. © 2022 by the authors."
145,144,30,144_deep3d_depth_3d_disparity,"deep3d,depth,3d,disparity,scenes,rgbdepth,camera,rgb,scene,stereo","Despite significant progress made in the past few years, challenges remain for depth estimation using a single monocular image. First, it is nontrivial to train a metric-depth prediction model that can generalize well to diverse scenes mainly due to limited training data. Thus, researchers have built large-scale relative depth datasets that are much easier to collect. However, existing relative depth estimation models often fail to recover accurate 3D scene shapes due to the unknown depth shift caused by training with the relative depth data. We tackle this problem here and attempt to estimate accurate scene shapes by training on large-scale relative depth data, and estimating the depth shift. To do so, we propose a two-stage framework that first predicts depth up to an unknown scale and shift from a single monocular image, and then exploits 3D point cloud data to predict the depth shift and the camera's focal length that allow us to recover 3D scene shapes. As the two modules are trained separately, we do not need strictly paired training data. In addition, we propose an image-level normalized regression loss and a normal-based geometry loss to improve training with relative depth annotation. We test our depth model on nine unseen datasets and achieve state-of-the-art performance on zero-shot evaluation. Code is available at: https://github.com/aim-uofa/depth/.  © 1979-2012 IEEE.,Recent advances in monocular 3D detection leverage a depth estimation network explicitly as an intermediate stage of the 3D detection network. Depth map approaches yield more accurate depth to objects than other methods thanks to the depth estimation network trained on a large-scale dataset. However, depth map approaches can be limited by the accuracy of the depth map, and sequentially using two separated networks for depth estimation and 3D detection significantly increases computation cost and inference time. In this work, we propose a method to boost the RGB image-based 3D detector by jointly training the detection network with a depth prediction loss analogous to the depth estimation task. In this way, our 3D detection network can be supervised by more depth supervision from raw LiDAR points, which does not require any human annotation cost, to estimate accurate depth without explicitly predicting the depth map. Our novel object-centric depth prediction loss focuses on depth around foreground objects, which is important for 3D object detection, to leverage pixel-wise depth supervision in an object-centric manner. Our depth regression model is further trained to predict the uncertainty of depth to represent the 3D confidence of objects. To effectively train the 3D detector with raw LiDAR points and to enable end-to-end training, we revisit the regression target of 3D objects and design a network architecture. Extensive experiments on KITTI and nuScenes benchmarks show that our method can significantly boost the monocular image-based 3D detector to outperform depth map approaches while maintaining the real-time inference speed. © 2000-2011 IEEE.,With the fast development and wide application of stereo depth estimation, adequate high-quality stereo training data with groundtruth depth information plays an important role, but is not easily acquired in underwater environments. Therefore, satisfactory performance of depth estimation is difficult to achieve in underwater environments. In addition, the domain gap also leads to the failure of directly applying existing models of terrestrial scene to underwater scene. Therefore, this paper proposes a novel underwater depth estimation network which can infer depth maps from real underwater stereo images in an adaptation manner. The proposed learning pipeline mainly contains three different adaptation modules, i.e., style adaptation, semantic adaptation and disparity range adaptation, to progressively adapt a terrestrial depth estimation model to the underwater domain. Specifically, due to the lack of underwater training data, we first propose a depth-aware stereo image translation network to synthesize stylized underwater stereo images from terrestrial dataset, thus benefiting the effective training of depth estimation network. Then, considering the weak generalization to the real underwater data when only trained on the above synthetic data, we present a self-ensembling semantic adaptation for depth estimation network to minimize the semantic domain discrepancy between synthetic and real underwater data. Meanwhile, we design a disparity range adaptation module to address the problem of disparity range miss-match between both data, thus obtaining more accurate depth predictions for large-disparity-span underwater images. Experimental results show that by integrating the proposed adaptation modules into the off-the-shelf depth estimation backbones, our method successfully achieves superior performance of underwater depth estimation compared to other state-of-the-art methods. © 1991-2012 IEEE."
146,145,30,145_seismic_earthquakes_earthquake_buildings,"seismic,earthquakes,earthquake,buildings,structural,prediction,building,concrete,regression,models","Nonlinear response history analysis (NLRHA) is generally considered to be a reliable and robust method to assess the seismic performance of buildings under strong ground motions. While NLRHA is fairly straightforward to evaluate individual structures for a select set of ground motions at a specific building site, it becomes less practical for performing large numbers of analyses to evaluate either (1) multiple models of alternative design realizations with a site-specific set of ground motions, or (2) individual archetype building models at multiple sites with multiple sets of ground motions. In this regard, surrogate models offer an alternative to running repeated NLRHAs for variable design realizations or ground motions. In this paper, a recently developed surrogate modeling technique, called probabilistic learning on manifolds (PLoM), is presented to estimate structural seismic response. Essentially, the PLoM method provides an efficient stochastic model to develop mappings between random variables, which can then be used to efficiently estimate the structural responses for systems with variations in design/modeling parameters or ground motion characteristics. The PLoM algorithm is introduced and then used in two case studies of 12-story buildings for estimating probability distributions of structural responses. The first example focuses on the mapping between variable design parameters of a multidegree-of-freedom analysis model and its peak story drift and acceleration responses. The second example applies the PLoM technique to estimate structural responses for variations in site-specific ground motion characteristics. In both examples, training data sets are generated for orthogonal input parameter grids, and test data sets are developed for input parameters with prescribed statistical distributions. Validation studies are performed to examine the accuracy and efficiency of the PLoM models. Overall, both examples show good agreement between the PLoM model estimates and verification data sets. Moreover, in contrast to other common surrogate modeling techniques, the PLoM model is able to preserve correlation structure between peak responses. Parametric studies are conducted to understand the influence of different PLoM tuning parameters on its prediction accuracy. © 2023 The Authors. Earthquake Engineering & Structural Dynamics published by John Wiley & Sons Ltd.,The process of ground motion selection and scaling is an integral part of hazard- and risk-consistent seismic demand analysis of structures. Due to the lack of ground motion records that naturally possess high amplitude and intensity, the research community generally relies on scaling the records to match a target hazard intensity level. The scaling factors used are frequently as high as 10. Due to the criticism received in previous research studies, the extent of amplitude scaling and its process has become a matter of debate, and various constraints on the scaling factors have been proposed. The primary argument against unrestricted amplitude scaling is the unrealistic nature of the scaled records and the possible biases caused in the engineering demand parameters (EDPs) of structures. This study presents a framework to utilize machine-learning and statistical techniques for the assessment of ground motion amplitude scaling for nonlinear time-history analysis (NTHA) of structures. The framework utilizes Bayesian non-parametric Gaussian process regressions (GPRs) as surrogate models to obtain statistical estimates of EDPs for scaled and unscaled ground motions. The GPR surrogate models are developed based on a large-scale analysis of five steel moment frames (SMFs) using 200 unscaled as-recorded ground motions for ten spectral acceleration levels, ((Formula presented.)) (ranging from 0 g to maximum considered earthquake, MCE) and 2500 scaled ground motions representing 50 scale factors ((Formula presented.)), and the 10 (Formula presented.) levels for each SMF. For each building, two types of EDPs are considered: i) peak inter-story drift ratio (PIDR) and ii) peak floor acceleration (PFA). To provide a better interpretation of the GPR surrogate models, the concept of explainable artificial intelligence (i.e., Shapley additive explanation, SHAP) is used to obtain insights into the decision-making process of the GPR models with respect to the (Formula presented.) and (Formula presented.). Then, for the 10 (Formula presented.) levels, the GPR-based EDP estimates under scaled ground motions corresponding to 50 different SFs are compared with the EDP estimates of unscaled ground motions. The comparison is conducted using Kolmogorov–Smirnov (KS) statistical hypothesis test. Results indicate that the range of allowable (Formula presented.) s depends on two factors: i) intensity level (characterized by (Formula presented.)), and ii) the dynamic properties of the building. In general, it is noticed that allowable (Formula presented.) s range between 0.5 and 3.0 for PIDRs, and from 0.6 to 2.0 for PFAs. Finally, the EDP between the unscaled and scaled ground motions are adhered to various discrepancies observed in different intensity measures representing amplitude-, duration-, energy-, and frequency- content of the two sets of ground motions. © 2023 The Authors. Earthquake Engineering & Structural Dynamics published by John Wiley & Sons Ltd.,The seismic design and assessment of steel moment resisting frames (SMRFs) rely heavily on drifts. It is unsurprising, therefore, that several simplified methods have been proposed to predict lateral deformations in SMRFs, ranging from the purely mechanics-based to the wholly data-driven, which aim to alleviate the structural engineer's burden of conducting detailed nonlinear analyses either as part of preliminary design iterations or during regional seismic assessments. While many of these methods have been incorporated in design codes or are commonly used in research, they all suffer from a lack of consideration of the causal link between the seismic hazard level and the ground-motion suite used for their formulation. In this paper, we propose hybrid data-driven models that preserve this critical relationship of hazard-consistency. To this end, we assemble a large database of non-linear response history analyses (NRHA) on 24 SMRFs of different structural characteristics. These structural models are subjected to 816 ground-motion records whose occurrence rates and spectral shapes are selected to ensure the hazard consistency of our outputs. Two sites with different seismic hazards are examined to enable comparisons under different seismic demands. An initial examination of the resulting drift hazard curves allows us to re-visit the influence of salient structural modelling assumptions such as plastic resistance, geometric configurations and joint deterioration modelling. This is followed by a machine learning (ML)-guided feature selection process that considers structural and seismic parameters as well as key static response features, hence the hybrid nature of our models. New models for inter-storey drift and roof displacements are then developed. A comparison with currently available formulations highlights the significant levels of overestimation associated with previously proposed non-hazard consistent models. © 2023 The Authors. Earthquake Engineering & Structural Dynamics published by John Wiley & Sons Ltd."
147,146,30,146_forgetting_learning_continual_memory,"forgetting,learning,continual,memory,learn,training,classification,classifier,classifiers,incremental","While continual learning has shown its impressive performance in addressing catastrophic forgetting of traditional neural networks and enabling them to learn multiple tasks continuously, it still requires a large amount of input data to train neural networks with satisfactory classification performance. Since collecting a large amount of training data is a time-consuming and expensive procedure, this study attempts to propose a novel data-free contrastive reversion method for continual learning (DFCRCL) to significantly reduce the number of training data for continual learning, while maintaining or even improving the classification performance of continual learning. In order to achieve such a goal, DFCRCL uses contrastive reversion to generate high-semantic pseudo samples from the previous task to guide the training of the current task. DFCRCL has three merits: (1) knowledge distillation from the previous task model to the current task model guarantees both the reduction of training data and the avoidance of catastrophic forgetting, and thus DFCRCL can effectively learn a sequence of tasks continuously (2) contrastive reversion enhances the semantic diversity of pseudo samples by learning the distinguishability between distinct pseudo samples in the feature space (3) contrastive reversion improves the performance of knowledge distillation in DFCRCL by enhancing the semantic diversity of the pseudo samples generated from the previous task model. Compared to six mainstream continual learning methods, the proposed DFCRCL achieves at least comparable or even better classification performance and stability in four benchmarking continual learning scenarios. In addition, the effectiveness of DFCRCL is demonstrated by ablation experiments. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,One notable weakness of current machine learning algorithms is the poor ability of models to solve new problems without forgetting previously acquired knowledge. The Continual Learning paradigm has emerged as a protocol to systematically investigate settings where the model sequentially observes samples generated by a series of tasks. In this work, we take a task-agnostic view of continual learning and develop a hierarchical information-theoretic optimality principle that facilitates a trade-off between learning and forgetting. We derive this principle from a Bayesian perspective and show its connections to previous approaches to continual learning. Based on this principle, we propose a neural network layer, called the Mixture-of-Variational-Experts layer, that alleviates forgetting by creating a set of information processing paths through the network which is governed by a gating policy. Equipped with a diverse and specialized set of parameters, each path can be regarded as a distinct sub-network that learns to solve tasks. To improve expert allocation, we introduce diversity objectives, which we evaluate in additional ablation studies. Importantly, our approach can operate in a task-agnostic way, i.e., it does not require task-specific knowledge, as is the case with many existing continual learning algorithms. Due to the general formulation based on generic utility functions, we can apply this optimality principle to a large variety of learning problems, including supervised learning, reinforcement learning, and generative modeling. We demonstrate the competitive performance of our method on continual reinforcement learning and variants of the MNIST, CIFAR-10, and CIFAR-100 datasets. © 2022, The Author(s).,When incrementally trained on new classes, deep neural networks are subject to catastrophic forgetting which leads to an extreme deterioration of their performance on the old classes while learning the new ones. Using a small memory containing few samples from past classes has shown to be an effective method to mitigate catastrophic forgetting. However, due to the limited size of the replay memory, there is a large imbalance between the number of samples for the new and the old classes in the training dataset resulting in bias in the final model. To address this issue, we propose to use the Balanced Softmax Cross-Entropy and show that it can be seamlessly combined with state-of-the-art approaches for class-incremental learning in order to improve their accuracy while also potentially decreasing the computational cost of the training procedure. We further extend this approach to the more demanding class-incremental learning without memory setting and achieve competitive results with memory-based approaches. Experiments on the challenging ImageNet, ImageNet-Subset, and CIFAR100 benchmarks with various settings demonstrate the benefits of our approach. © 2022 The Author(s)"
148,147,30,147_lungeffnet_lung_cnn_inceptionv3,"lungeffnet,lung,cnn,inceptionv3,carcinoma,tomography,classification,cancer,pulmonary,detection","Recently, lung cancer is observed as the most deadly disease throughout the world with a high mortality rate. The survival rate with lung cancer is minimal due to the difficulty in detection of cancer in early stages. Various screening techniques are available such as X-ray, CT, and Sputum Cytology; here, CT images are considered for identification of the lung tumor. Computed tomography has been widely exploited for various clinical applications. Early detection and treatment of lung tumor can aid in improving the survival rate, and CT scan is the best modality for imaging lung tumor. In many cases, when the nodules are identified, it might be either more advanced or too large to be effectively cured. Physical characteristics of the nodules such as the size, tumor type and type of borders are very significant in the examination of nodules. Lung cancer detection and treatment will be of significant value for early diagnosis. Machine learning classification can benefit greatly from the wealth of research on the use of image processing for detecting lung cancer. In this paper, an effective classification model significant value for early diagnosis is developed. The segmentation in CT images is performed with marker-controlled segmentation with likelihood estimation between the features. The proposed model Markov likelihood grasshopper classification (MLGC) is utilized for the classification of nodules in the CT images. The MLGC model performs the estimation of features and computes the likelihood distance between those features. With the estimated features, grasshopper optimization algorithm (GOA) is employed for the optimization of the features. The optimized features are applied over the Boltzmann machine to derive the classification results. The MLGC model estimates the hyperparameters for the selection of set to derive classification results. The simulation results expressed that the proposed MLGC model achieves the higher accuracy value of 99.5% compared with the existing model accuracy which are AlexNet of 96.35%, GoogleNet as 93.45% and VGG-16 as 92.56%. © 2023, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.,The CT images of Lung illnesses or diseases that damage the lungs and weaken the respiratory system. Lung cancer is one of the topmost causes of death in humans around the world. Humans have a better chance of surviving if they are detected early. The average survival rate of persons with lung cancer increases from 14 to 49 percent if the disease is detected early.While computed tomography (CT) is significantly more effective than X-ray, a complete diagnosis requires a combination of imaging techniques that complement each other. But, because there are multiple phases of cancer that develop into different types of tumors with varying sizes and risks, finding lung cancer does not predict the risk of cancer. A deep neural network is constructed and tested for detecting lung cancer CT images. This research work analyses different types of tumor sizes such as large cell carcinoma, normal, squamous cell carcinoma, and adenocarcinoma. Also, the lung tumors are detected and predicted with the help of computer vision methods such as Residual neural network (ResNet), Convolutional neural network (CNN). Finally, the results of all the methods are compared and various parameters were calculated. Thus, the proposed method (ResNet) gives an optimal solution on comparison with respect to all the parameters. © 2022 Lavoisier. All rights reserved.,Lung cancer (LC) remains a leading cause of death worldwide. Early diagnosis is critical to protect innocent human lives. Computed tomography (CT) scans are one of the primary imaging modalities for lung cancer diagnosis. However, manual CT scan analysis is time-consuming and prone to errors/not accurate. Considering these shortcomings, computational methods especially machine learning and deep learning algorithms are leveraged as an alternative to accelerate the accurate detection of CT scans as cancerous, and non-cancerous. In the present article, we proposed a novel transfer learning-based predictor called, Lung-EffNet for lung cancer classification. Lung-EffNet is built based on the architecture of EfficientNet and further modified by adding top layers in the classification head of the model. Lung-EffNet is evaluated by utilizing five variants of EfficientNet i.e., B0–B4. The experiments are conducted on the benchmark dataset “IQ-OTH/NCCD” for lung cancer patients grouped as benign, malignant, or normal based on the presence or absence of lung cancer. The class imbalance issue was handled through multiple data augmentation methods to overcome the biases. The developed model Lung-EffNet attained 99.10% of accuracy and a score of 0.97 to 0.99 of ROC on the test set. We compared the efficacy of the proposed fine-tuned pre-trained EfficientNet with other pre-trained CNN architectures. The predicted outcomes demonstrate that EfficientNetB1 based Lung-EffNet outperforms other CNNs in terms of both accuracy and efficiency. Moreover, it is faster and requires fewer parameters to train than other CNN based models, making it a good choice for large-scale deployment in clinical settings and a promising tool for automated lung cancer diagnosis from CT scan images. © 2023 The Authors"
149,148,30,148_scheduling_factories_heuristics_algorithms,"scheduling,factories,heuristics,algorithms,heuristic,computational,manufacturing,programming,algorithm,allocation","Inspired by real-life applications, mainly in hand-intensive manufacturing, the incorporation of learning effects into scheduling problems has garnered attention in recent years. This paper deals with the flowshop scheduling problem with a learning effect, when minimising the makespan. Four approaches to model the learning effect, well-known in the literature, are considered. Mathematical models are providing for each case. A solver allows us to find the optimal solution in small problem instances, while a Simulated Annealing algorithm is proposed to deal with large problem instances. In the latter, the initial solution is obtained using the well-known Nawaz-Enscore-Ham algorithm, and two local search operators are evaluated. Computational experiments are carried out using benchmark datasets from the literature. The Simulated Annealing algorithm shows a better result for learning approaches with fast learning effects as compared to slow learning effects. Finally, for industrial decision makers, some insights about how the learning effect model might affect the makespan minimisation flowshop scheduling problem are presented. © 2023 Informa UK Limited, trading as Taylor & Francis Group.,Distributed manufacturing involving heterogeneous factories presents significant challenges to enterprises. Furthermore, the need to prioritize various jobs based on order urgency and customer importance further complicates the scheduling process. Consequently, this study addresses the practical issue by tackling the distributed heterogeneous hybrid flow shop scheduling problem with multiple priorities of jobs (DHHFSP-MPJ). The primary objective is to simultaneously minimize the total weighted tardiness and total energy consumption. To solve DHHFSP-MPJ, a double deep Q-network-based co-evolution (D2QCE) is developed with four features: i) The global and local searches are allocated into two populations to balance computational resources; ii) A hybrid heuristic strategy is proposed to obtain an initialized population with great convergence and diversity; iii) Four knowledge-based neighborhood structures are proposed to accelerate converging. Next, the double deep Q-Network is applied to learn operator selection; and iv) An energy-efficient strategy is presented to save energy. To verify the effectiveness of D2QCE, five state-of-the-art algorithms are compared on 20 instances and a real-world case. The results of numerical experiments indicate that: i) The D2QN can learn fast by only consuming a few computation resources and can select the best operator. ii) Combining D2QN and co-evolution can vastly improve the performance of evolutionary algorithms for solving distributed shop scheduling. iii) The proposed D2QCE has better performance than state-of-the-arts for DHHFSP-MPJ <italic>Note to Practitioners</italic>&#x2014;This paper is inspired by a real-world problem encountered in blanking workshop systems within the manufacturing of large engineering equipment. In this practical scenario, jobs come with varying priorities and distinct due dates. Balancing these priority and due date constraints while efficiently scheduling a considerable volume of jobs to enhance enterprise profitability poses a significant challenge. Thus, this scheduling problem is abstracted to the distributed heterogeneous hybrid flow shop scheduling problem with multiple priorities of jobs. The objectives are minimizing weighted due date delay and total energy consumption. Notably, this model has never been studied before. To address this, we&#x2019;ve formulated a mixed-integer linear programming model and developed a novel co-evolutionary algorithm based on double deep Q-networks (DQN). Our approach introduces several key components. First, we present a co-evolutionary framework to strike a balance between global and local search aspects. Additionally, we&#x2019;ve devised three problem-specific enhancement strategies to expedite convergence, which include hybrid initialization, local search techniques, and energy-saving measures. To accelerate the learning process of selecting the optimal operator with minimal computational resources, we employ the double DQN. Experimental results demonstrate the superior performance of our approach, outperforming state-of-the-art algorithms when applied to a real-world case. In summary, this work proposes an extended DHHFSP and provides a case of designing the deep learning-assisted evolutionary algorithm. However, online deep reinforcement learning (DRL) consumes additional time, and the generalization of online DRL needs to be improved. In future research, we will consider the dynamic events such as new jobs insert and due date change for the blanking workshop. Moreover, the end-to-end model will be considered to save energy and realize sustainable DRL. IEEE,The ever-changing and dynamic market environment requires applying job shop systems based on real-time data. The establishment of physical-virtual systems in the production process has led to the emergence of intelligent factories. Compared with those employing traditional production methods, such factories manufacture products with higher quality, higher production speed, and other economic benefits. Regarding the virtual connections of factories, events such as the arrival of new jobs, and machine breakdowns, are identified by Radio Frequency Identification System between different production units, and related decisions are made quickly and carefully. In this intelligent job shop scheduling, it is assumed that independent factories create virtual production networks in which, each factory focuses on its own interests. Regarding the importance of this issue in today's industry, this research explores the real-time scheduling problem in multi-agent production networks distributed in smart factories. Since this problem is a combination of static scheduling and real-time scheduling, a bi-objective model of mixed-integer linear programming is first developed. An approach is then proposed to solve the dynamic real-time scheduling problem. In addition, a learning-based memetic algorithm for solving large-size bi-objective instances is proposed due to the NP-hardness of the considered problem. Afterward, the results of the proposed algorithm are compared with the hybrid Pareto-based tabu search algorithm. The computational results show that in large-size instances, the proposed algorithm outperforms the competing algorithm. © 2023 Elsevier Ltd"
150,149,29,149_prompts_prompt_corpus_neuralbased,"prompts,prompt,corpus,neuralbased,promptbased,annotation,text,nlp,prompting,learning","Large language models (LLMs) have revolutionized natural language processing, but they require significant data and hardware resources. Prompt learning offers a solution by enabling a single model for multiple downstream tasks. However, current prompt learning methods rely on costly prompt templates for training. This is a challenge for tasks like sentiment classification, where high-quality templates are hard to create and pseudo-token composed templates can be expensive to train. Recent studies on the chain of thought (COT) have shown that enhancing the presentation of certain aspects of the reasoning process can improve the performance of LLMs. With this in mind, this research introduces the auto-generated COT and verbalizer templates (AGCVT-Prompt) technique, which clusters unlabeled texts according to their identified topic and sentiment. Subsequently, it generates dual verbalizers and formulates both topic and sentiment prompt templates, utilizing the categories discerned within the text and verbalizers. This method significantly improves the transparency and interpretability of the model's decision-making processes. The AGCVT-Prompt technique was evaluated against conventional prompt learning and advanced sentiment classification methods, using state-of-the-art LLMs on both Chinese and English datasets. The results showed superior performance in all evaluations. Specifically, the AGCVT-Prompt method outperformed previous prompt learning techniques in few-shot learning scenarios, providing higher zero-shot and few-shot learning capabilities. Additionally, AGCVT-Prompt was utilized to analyze network comments about Corona Virus Disease 2019, providing valuable insights. These findings indicate that AGCVT-Prompt is a promising alternative for sentiment classification tasks, particularly in situations where labeled data is scarce. © 2024,In recent years, large-scale pretrained language models have become widely used in natural language processing tasks. On this basis, prompt learning has achieved excellent performance in specific few-shot classification scenarios. The core idea of prompt learning is to convert a downstream task into a masked language modelling task. However, different prompt templates can greatly affect the results, and finding an appropriate template is difficult and time-consuming. To this end, this study proposes a novel hybrid prompt approach, which combines discrete prompts and continuous prompts, to motivate the model to learn more semantic knowledge from a small number of training samples. By comparing the performance difference between discrete prompts and continuous prompts, we find that hybrid prompts achieve the best results, reaching a 73.82% F1 value in the test set. In addition, we analyze the effect of different virtual token lengths in continuous prompts and hybrid prompts in a few-shot cross-language topic classification scenario. The results demonstrate that there is a threshold for the length of virtual tokens, and too many virtual tokens decrease the performance of the model. It is better not to exceed the average length of the training set corpus. Finally, this paper designs a method based on vector similarity to explore the real meanings represented by virtual tokens. The experimental results show that the prompt automatically learnt from the virtual token has a certain correlation with the input text. © 2023 by the authors.,Insufficiently labeled samples and low-generalization performance have become significant natural language processing problems, drawing significant concern for few-shot text classification (FSTC). Advances in prompt learning have significantly improved the performance of FSTC. However, prompt learning methods typically require the pre-trained language model and tokens of the vocabulary list for model training, while different language models have different token coding structures, making it impractical to build effective Chinese prompt learning methods from previous approaches related to English. In addition, a majority of current prompt learning methods do not make use of existing unlabeled data, thus often leading to unsatisfactory performance in real-world applications. To address the above limitations, we propose a novel Chinese FSTC method called CIPLUD that combines an improved prompt learning method and existing unlabeled data, which are used for the classification of a small amount of Chinese text data. We used the Chinese pre-trained language model to build two modules: the Multiple Masks Optimization-based Prompt Learning (MMOPL) module and the One-Class Support Vector Machine-based Unlabeled Data Leveraging (OCSVM-UDL) module. The former generates prompt prefixes with multiple masks and constructs suitable prompt templates for Chinese labels. It optimizes the random token combination problem during label prediction with joint probability and length constraints. The latter, by establishing an OCSVM model in the trained text vector space, selects reasonable pseudo-label data for each category from a large amount of unlabeled data. After selecting the pseudo-label data, we mixed them with the previous few-shot annotated data to obtain brand new training data and then repeated the steps of the two modules as an iterative semi-supervised optimization process. The experimental results on the four Chinese FSTC benchmark datasets demonstrate that our proposed solution outperformed other prompt learning methods with an average accuracy improvement of 2.3%. © 2023 by the authors."
151,150,29,150_cells_cell_microscopy_subcellular,"cells,cell,microscopy,subcellular,microscope,cellular,segmentation,cnns,organelles,cellacdc","To produce abundant cell culture samples to generate large, standardized image datasets of human induced pluripotent stem (hiPS) cells, we developed an automated workflow on a Hamilton STAR liquid handler system. This was developed specifically for culturing hiPS cell lines expressing fluorescently tagged proteins, which we have used to study the principles by which cells establish and maintain robust dynamic localization of cellular structures. This protocol includes all details for the maintenance, passage and seeding of cells, as well as Matrigel coating of 6-well plastic plates and 96-well optical-grade, glass plates. We also developed an automated image-based hiPS cell colony segmentation and feature extraction pipeline to streamline the process of predicting cell count and selecting wells with consistent morphology for high-resolution three-dimensional (3D) microscopy. The imaging samples produced with this protocol have been used to study the integrated intracellular organization and cell-to-cell variability of hiPS cells to train and develop deep learning-based label-free predictions from transmitted-light microscopy images and to develop deep learning-based generative models of single-cell organization. This protocol requires some experience with robotic equipment. However, we provide details and source code to facilitate implementation by biologists less experienced with robotics. The protocol is completed in less than 10 h with minimal human interaction. Overall, automation of our cell culture procedures increased our imaging samples’ standardization, reproducibility, scalability and consistency. It also reduced the need for stringent culturist training and eliminated culturist-to-culturist variability, both of which were previous pain points of our original manual pipeline workflow. © 2023, Crown.,Massive, parallelized 3D stem cell cultures for engineering in vitro human cell types require imaging methods with high time and spatial resolution to fully exploit technological advances in cell culture technologies. Here, we introduce a large-scale integrated microfluidic chip platform for automated 3D stem cell differentiation. To fully enable dynamic high-content imaging on the chip platform, we developed a label-free deep learning method called Bright2Nuc to predict in silico nuclear staining in 3D from confocal microscopy bright-field images. Bright2Nuc was trained and applied to hundreds of 3D human induced pluripotent stem cell cultures differentiating toward definitive endoderm on a microfluidic platform. Combined with existing image analysis tools, Bright2Nuc segmented individual nuclei from bright-field images, quantified their morphological properties, predicted stem cell differentiation state, and tracked the cells over time. Our methods are available in an open-source pipeline, enabling researchers to upscale image acquisition and phenotyping of 3D cell culture. © 2023 The Author(s),Background: High-throughput live-cell imaging is a powerful tool to study dynamic cellular processes in single cells but creates a bottleneck at the stage of data analysis, due to the large amount of data generated and limitations of analytical pipelines. Recent progress on deep learning dramatically improved cell segmentation and tracking. Nevertheless, manual data validation and correction is typically still required and tools spanning the complete range of image analysis are still needed. Results: We present Cell-ACDC, an open-source user-friendly GUI-based framework written in Python, for segmentation, tracking and cell cycle annotations. We included state-of-the-art deep learning models for single-cell segmentation of mammalian and yeast cells alongside cell tracking methods and an intuitive, semi-automated workflow for cell cycle annotation of single cells. Using Cell-ACDC, we found that mTOR activity in hematopoietic stem cells is largely independent of cell volume. By contrast, smaller cells exhibit higher p38 activity, consistent with a role of p38 in regulation of cell size. Additionally, we show that, in S. cerevisiae, histone Htb1 concentrations decrease with replicative age. Conclusions: Cell-ACDC provides a framework for the application of state-of-the-art deep learning models to the analysis of live cell imaging data without programming knowledge. Furthermore, it allows for visualization and correction of segmentation and tracking errors as well as annotation of cell cycle stages. We embedded several smart algorithms that make the correction and annotation process fast and intuitive. Finally, the open-source and modularized nature of Cell-ACDC will enable simple and fast integration of new deep learning-based and traditional methods for cell segmentation, tracking, and downstream image analysis. Source code: https://github.com/SchmollerLab/Cell_ACDC © 2022, The Author(s)."
152,151,29,151_scheduling_scheduler_schedulers_workloads,"scheduling,scheduler,schedulers,workloads,cloud,workload,clusters,straggler,servers,jobs","With the rapid proliferation of Machine Learning (ML) and Deep learning (DL) applications running on modern platforms, it is crucial to satisfy application performance requirements such as meeting deadline and ensuring accuracy. To this end, researchers have proposed several job schedulers for ML clusters. However, none of the previously proposed schedulers consider ML model parallelism, though it has been proposed as an approach to increase the efficiency of running large-scale ML and DL jobs. Thus, in this paper, we propose an ML job Feature based job Scheduling system (MLFS) for ML clusters running both data parallelism and model parallelism ML jobs. MLFS first uses a heuristic scheduling method that considers an ML job's spatial and temporal features to determine task priority for job queue ordering in order to improve job completion time (JCT) and accuracy performance. It uses the data from the heuristic scheduling method for training a deep reinforcement learning (RL) model. After the RL model is well trained, it then switches to the RL method to automatically make decisions on job scheduling. In addition, MLFS has a system load control method that selects tasks from overloaded servers to move to underloaded servers based on task priority, and also intelligently removes the tasks that generate little or no improvement on the desired accuracy performance when the system is overloaded to improve JCT and accuracy by job deadline. Furthermore, we propose Optimal ML iteration stopping method that determines the proper time to stop training ML model when this model reaches the minimum loss value. Our real experiments and large-scale simulation based on real trace show that MLFS reduces JCT by up to 53% and makespan by up to 52%, and improves accuracy by up to 64% when compared with existing ML job schedulers. We also open sourced our code.  © 1993-2012 IEEE.,Algorithms and frameworks for distributed machine learning have been widely used in numerous artificial intelligence engineering applications. A cloud platform provides a large number of resources at a lower cost and is a more convenient method for such applications. With the rapid development of containerization, native cloud combinations based on Docker and Kubernetes have provided effective resource support for distributed machine learning. However, native Kubernetes does not provide efficient priority or fair resource scheduling strategies for distributed machine learning in computationally intensive and time-consuming jobs, which easily leads to resource deadlock, resource waste, and low job execution efficiency. Therefore, to utilize the execution order between multiple jobs in distributed machine learning as well as the dependencies between multiple tasks for the same job, considering intra- and inter-group scheduling priorities, a combined priority scheduling method is proposed for distributed machine learning based on Kubernetes and Volcano. Considering the user priority, task priority, longest wait time, task parallelism, and affinity and non-affinity between the parameter server and worker nodes, a combined priority scheduling model of inter- and intra-job priority is proposed, which is mapped into a scheduling strategy of inter- and intra-group priorities of pods, enabling the efficient scheduling and training of distributed machine learning. The experiment results show that the proposed method achieves preferential resource allocation for urgent, high parallelism, and high-priority jobs with high-priority users and improves the job execution efficiency. The affinity and anti-affinity settings among pods reduce the time of information interaction between the parameter server and worker nodes to a certain extent, thereby improving the job completion efficiency. This group scheduling strategy alleviates the problems of resource deadlock and waste caused by insufficient resources in cloud computing. © 2023, The Author(s).,Efficient scheduling of distributed deep learning (DL) jobs in large GPU clusters is crucial for resource efficiency and job performance. While server sharing among jobs improves resource utilization, interference among co-located DL jobs occurs due to resource contention. Interference-aware job placement has been studied, with white-box approaches based on explicit interference modeling and black-box schedulers with reinforcement learning. In today's clusters containing thousands of GPU servers, running a single scheduler to manage all arrival jobs in a timely and effective manner is challenging, due to the large workload scale. We adopt multiple schedulers in a large-scale cluster/data center, and propose a multi-agent reinforcement learning (MARL) scheduling framework to cooperatively learn fine-grained job placement policies, towards the objective of minimizing job completion time (JCT). To achieve topology-aware placements, our proposed framework uses hierarchical graph neural networks to encode the data center topology and server architecture. In view of a common lack of precise reward samples corresponding to different placements, a job interference model is further devised to predict interference levels in face of various co-locations, for training of the MARL schedulers. Testbed and trace-driven evaluations show that our scheduler framework outperforms representative scheduling schemes by more than 20% in terms of average JCT, and is adaptive to various machine learning cluster topologies.  © 2004-2012 IEEE."
153,152,29,152_encoder_classification_recognition_signaltonoise,"encoder,classification,recognition,signaltonoise,modulation,signal,signals,wireless,neural,fingerprinting","Automatic modulation classification (AMC), which plays a significant role in wireless communication, can recognize the modulation type of the received signal without large amounts of transmitted data and parameter information. Supported by deep learning, which is a powerful tool for functional expression and feature extraction, the development of AMC can be greatly promoted. In this paper, we propose a deep learning-based modulation classification method with 2D time-frequency signal representation. In our proposed method, signals which have been received are first analyzed by time-frequency based on continuous wavelet transform (CWT). Then, CWT images of received signals are obtained and input to the deep learning model for classifying. We create a new CWT image dataset including 12 modulation types of signals under various signal-to-noise ratio (SNR) environment to verify the effectiveness of the proposed method. The experimental results demonstrate that our proposed method can reach to a high classification accuracy over the SNR of ?11 dB. © 2022 by the authors.,With the advancement of 5G technology, wireless communication resources such as channels and spectrum become scarce. This necessitates ensuring the efficiency and security of signal modulation and demodulation, which imposes higher requirements for wireless communication systems. However, signal modulation has the problems of large amount of data, low recognition accuracy and various types. In this study, a classification network of automatic modulation classification recognition algorithm for signal-to-noise ratio is proposed to solve the problem that traditional noise reduction algorithms will damage signals with high signal-to-noise ratio, consequently reducing the accuracy of signal recognition. In order to solve the problem of high complexity of network model algorithm, in particular, a signal automatic modulation classification and recognition algorithm based on neural network autoencoder is proposed. Experimental results show that the accuracy of signal automatic modulation classification recognition in the algorithm increases as the increase of modulation signals and tends to be stable. When the modulation signal is 0dB, the recognition accuracy gradually converges to the highest, and reaches 81.6% when the modulation signal is 18 dB. In contrast, the DenseNet algorithm has the lowest recognition accuracy, with only 77.5% recognition accuracy when the signal modulation classification is 18dB, a difference of 4.1%. This indicates that the algorithm performs exceptionally well in automatic signal modulation classification, and its complexity is lower than other comparative network models, providing certain advantages.  © 2013 IEEE.,Specific emitter identification (SEI) is a technique of identifying individual emitters via unique characteristics of different emitters. In this paper, we consider a SEI problem with transmitter changing modulations scenario. There have been few previous studies on this type of scenario. To cope with the daunting challenge, a variable-modulation SEI framework with domain adaptation is proposed. The components characteristics of transmitter are analyzed and the distortion models are established for simulation dataset generation. The received in-phase/quadrature (I/Q) signals are demodulated and reconstructed to obtain baseband ideal modulation signals. The received signals and the ideal modulation signals corresponding to demodulation and reconstruction are merged and embedded into the feature extraction network. Domain adversarial neural network (DANN) is added into the SEI framework to generate domain-invariant fingerprint features, thus realizing variable-modulation SEI. To better align the distortion features of emitters with variable modulations, Gaussian Encoder is designed to project fingerprint features into Gaussian distribution space. Numerous experiments show that the proposed SEI framework can improve recognition accuracy of individual emitter for single modulation and variable transfer greatly, and outperform the existing transfer learning methods. The ablation study demonstrates the components of framework are complementary. The complexity of framework is acceptable and it can extend to large-scale use. The robustness of framework is verified through modulation transfer among PSK and QAM.  © 2005-2012 IEEE."
154,153,29,153_grasping_grasp_grasps_gripper,"grasping,grasp,grasps,gripper,robotic,robotics,grip,tactile,sensing,manipulator","As the basis for prehensile manipulation, it is vital to enable robots to grasp as robustly as humans. Our innate grasping system is prompt, accurate, flexible, and continuous across spatial and temporal domains. Few existing methods cover all these properties for robot grasping. In this article, we propose AnyGrasp for grasp perception to enable robots these abilities using a parallel gripper. Specifically, we develop a dense supervision strategy with real perception and analytic labels in the spatial-temporal domain. Additional awareness of objects' center-of-mass is incorporated into the learning process to help improve grasping stability. Utilization of grasp correspondence across observations enables dynamic grasp tracking. Our model can efficiently generate accurate, 7-DoF, dense, and temporally-smooth grasp poses and works robustly against large depth-sensing noise. Using AnyGrasp, we achieve a 93.3% success rate when clearing bins with over 300 unseen objects, which is on par with human subjects under controlled conditions. Over 900 mean-picks-per-hour is reported on a single-arm system. For dynamic grasping, we demonstrate catching swimming robot fish in the water.  © 2004-2012 IEEE.,Slipping detection and avoidance are key issues in dexterous robotic manipulation. The capability of robots to grasp and manipulate objects of common use can be greatly enhanced by endowing these robots with force/tactile sensors on their fingertips. Object slipping can be caused by both tangential and torsional loads when the grip force is too low. Contact force and moment measurements are required to counteract such loads and avoid slippage by controlling the grip force. In this paper, we use the SUNTouch force/tactile sensor, which provides the robotic control system with reliable measurements of both normal and tangential contact force components together with the torsional moment. By exploiting the limit surface concept and the LuGre friction model, we build a model of the object/fingertip planar sliding. This model is the basis of a nonlinear observer that estimates the sliding velocity and the friction state variable from the measured contact force and torsional moment. The slipping control system uses the estimated friction state to detect the slipping event and the estimated sliding velocity to control the grasp force. The control modality is twofold: the first one is aimed at avoiding object slip, while the second one allows the object to perform a controlled pivoting about the grasping axis. Experiments show that the robot is able to safely manipulate objects that require grasping forces in a large range, from (Formula presented.) N to (Formula presented.) N. This level of manipulation autonomy is attained by a suitably identified dynamic model that overcomes the limited generalization capability of existing learning-based approaches in the general roto-translational slip control. © 2023 by the authors.,Damage-free grasping of deformable objects has been a long-standing difficult problem in the field of robotics. Humans can perceive the physical properties of objects and apply accurate force to complete dexterous and non-destructive operations when grasping vulnerable targets. In order to transfer this ability from humans to robots, a special neural network utilizing the improved Transformer structure is proposed to process the complete tactile time sequence during the grasping, considering the fabulous performance and extensive successful application of deep learning on large-scale datasets. Compared with computer vision, there are far from enough grasp datasets in the haptic domain. Tactile datasets for fruit grasping are almost unavailable. So we established a tactile dataset containing 9375 grasp of 15 fruits for experimental research. The proposed network has achieved a fruit recognition accuracy of 97.33% on this dataset, better than the traditional recurrent neural network (RNN) model. Furthermore, the performance of the proposed model is evaluated from various aspects, and the prediction of the subsequent grasp force is explored. Our work contributes to the realization of robotic haptic perception and damage-free grasping in the agricultural field, and can be helpful to fruit picking, handling, sorting and other related areas. Our method also provides a certain degree of technical reference for other researches on grasp tactile data. © 2023 Elsevier B.V."
155,154,28,154_combustion_flame_combustors_kinetic,"combustion,flame,combustors,kinetic,kinetics,combustor,simulations,fuels,computational,simulation","This study proposes a machine learning tabulation (MLT) method that employs deep neural networks (DNNs) to predict ignition delay and knock propensity in spark ignition (SI) engines. The commonly used Arrhenius model and Livengood-Wu integral for fast knock prediction are not accurate enough to account for residual gas species and may require adjustments or modifications to account for specific engine characteristics. Detailed kinetics modeling is computationally expensive, so the MLT approach is introduced to solve these issues. The MLT method uses precalculated thermochemical states of the mixture that are clustered based on a combustion progress variable. Hundreds of DNNs are trained with the stochastic Levenberg-Marquardt (SLM) optimization algorithm, reducing training time and memory requirements for large-scale problems. MLT has high interpolation accuracy, eliminates the need for table storage, and reduces memory requirements by three orders of magnitude. The proposed MLT approach can operate across a wider range of conditions and handle a variety of fuels, including those with complex reaction mechanisms. MLT computational time is independent of the reaction mechanism's size. It demonstrates a remarkable capability to reduce computation time by a factor of approximately 300 when dealing with complex reaction mechanisms comprising 621 species. MLT has the potential to significantly advance our understanding of complex combustion processes and aid in the design of more efficient and environmentally friendly combustion engines. In summary, the MLT approach has acceptable accuracy with less computation cost than detailed kinetics, making it ideal for fast model-based knock detection. This article presents a detailed description of the MLT method, including its workflow, challenges involved in data generation, pre-processing, data classification and regression, and integration into the engine cycle simulation. The results of the study are summarized, which includes validation against kinetics for ignition delay and engine simulation for knock angle prediction. The conclusions are presented along with future work.  © 2024 SAE International.,The development of skeletal mechanisms has become essential for multi-dimensional simulations of plasma-assisted combustion (PAC). However, reduction tools developed for traditional combustion applications are not always applicable to PAC, due to the complex interplay between non-equilibrium plasma and combustion kinetics. Plasma direct relation graph with error propagation (P-DRGEP) is a recent plasma-specific reduction method developed in order to incorporate plasma energy branching in the reduction. In the first part of this work, the applicability of P-DRGEP to large kinetic mechanisms is investigated. A detailed isooctane/air plasma mechanism containing 2805 species and 18457 reactions is reduced to 415 species and 4716 reactions, keeping errors on ignition time within 3% for a wide range of initial conditions: from 750 K to 1200 K, 10 atm and equivalence ratios from 0.75 to 1.50. The second part focuses on isomer lumping, which is another reduction technique widely used in combustion. When applied to PAC, it is shown that the resulting lumped mechanism produces poor results. A novel plasma-specific isomer lumping strategy using machine learning is proposed instead. With the supervised algorithm of gradient boosting, predictive regression models are generated, which describe rate coefficients of lumped reactions adequately. These models are trained with simulation data. Leveraging this newly proposed lumping approach on the reduced mechanism, allows for an additional 28% reduction in the number of species and 19% reduction in the number of reactions. Two different versions are presented: in the first one the models are trained using one input feature (1D), while in the second one, two input features are selected (2D). The resulting lumped mechanism is shown to produce accurate predictions of PAC over the entire parameter space of interest, while significantly decreasing the computational time. Indicatively, with the 1D version the maximum error on ignition time in this range of conditions is 6%. The 2D approach produces even lower errors, which do not exceed 3%. Novelty and significance statement In this work, a novel approach for isomer lumping, in plasma-assisted combustion mechanisms, is demonstrated. This plasma-specific approach, uses predictive machine learning regression models to describe the complex evolution of lumped reaction rate coefficients. Combining it with the plasma direct relation graph with error propagation, a powerful reduction framework is created, which is successfully demonstrated on a detailed isooctane/air plasma kinetic mechanism, via zero-dimensional ignition simulations. This framework constitutes a useful tool towards the creation of highly accurate skeletal mechanisms, which significantly reduce the computational costs of simulations. © 2023 The Combustion Institute,The numerical integration of the differential equations describing chemical kinetics consumes the majority of computational time in combustion simulations that involve direct coupling of chemistry and flow, such as transported probability density function (PDF) methods, direct numerical simulation (DNS), conditional moment closure (CMC), unsteady flamelet, multiple mapping closure (MMC), thickened flame model, linear eddy model (LEM), partially stirred reactor (PaSR) as in OpenFOAM and laminar flame computation. This step can be accelerated by tabulation, and artificial neural networks (ANNs) have recently emerged as a powerful technique in this domain. To be applicable to a wide family of problems, an ANN tabulation approach must be based on data generated by an abstract process, rather than from the turbulent flame to be simulated. In the present work, the hybrid flamelet/random data and multiple multilayer perceptrons (HFRD-MMLP) method (Ding et al., Combust. Flame 231, 111493, 2021) for non-premixed flames is taken as a basis to develop a thermochemistry tabulation method for premixed flames. In the spirit of maintaining an essentially random data set that still originates in meaningful composition states, a set of one-dimensional premixed flame simulations is employed to generate data that are used as starting points for a random data generation process and subsequently discarded. The approach is applied to large eddy simulations (LES) of the Cambridge/Sandia swirl burner in configurations five and six, with the transported PDF method employed to provide closure for the filtered reaction source terms and the stochastic fields method used for numerical solution. Very good agreement in both major and minor species is observed between the LES-PDF simulations using direct integration of the reaction source term and the ones with the ANNs. Furthermore, the average time taken for reaction source term computations is reduced by fourteen times, while memory requirements constitute only 1.4 MB. © 2023 The Author(s)"
156,155,28,155_crop_cropland_croptype_crops,"crop,cropland,croptype,crops,agricultural,classification,imagery,features,irrigation,planting","Crop-type mapping is the foundation of grain security and digital agricultural management. Accuracy, efficiency and large-scale scene consistency are required to perform crop classification from remote sensing images. Many current remote-sensing crop extraction methods based on deep learning cannot account for adaptation effects in large-scale, complex scenes. Therefore, this study proposes a novel adaptive feature-fusion network for crop classification using single-temporal Sentinel-2 images. The selective patch module implemented in the network can adaptively integrate the features of different patch sizes to assess complex scenes better. TabNet was used simultaneously to extract spectral information from the center pixels of the patches. Multitask learning was used to supervise the extraction process to improve the weight of the spectral characteristics while mitigating the negative impact of a small sample size. In the network, superpixel optimization was applied to post-process the classification results to improve the crop edges. By conducting the crop classification of peanut, rice, and corn based on Sentinel-2 images in 2022 in Henan Province, China, the novel method proposed in this paper was more accurate, indicated by an F1 score of 96.53%, than other mainstream methods. This indicates our model’s potential for application in crop classification in large scenes. © 2023 by the authors.,The timely and accurate mapping of crops over large areas is essential for alleviating food crises and formulating agricultural policies. However, most existing classical crop mapping methods usually require the whole-year historical time-series data that cannot respond quickly to the current planting information, let alone for future prediction. To address this issue, we propose a novel spatial–temporal feature and deep integration strategy for crop growth pattern prediction and early mapping (STPM). Specifically, the STPM first learns crop spatial–temporal evolving patterns from historical data to generate future remote sensing images based on the current observations. Then, a robust crop type recognition model is applied by combining the current early data with the predicted images for early crop mapping. Compared to existing spatial–temporal prediction models, our proposed model integrates local, global, and temporal multi-modal features comprehensively. Not only does it achieve the capability to predict longer sequence lengths (exceeding 100 days), but it also demonstrates a significant improvement in prediction accuracy for each time step. In addition, this paper analyses the impact of feature dimensionality and initial data length on prediction and early crop mapping accuracy, demonstrating the necessity of multi-modal feature fusion for spatial–temporal prediction of high-resolution remote sensing data and the benefits of longer initial time-series (i.e., longer crop planting time) for crop identification. In general, our method has the potential to carry out early crop mapping on a large scale and provide information to formulate changes in agricultural conditions promptly. © 2023 by the authors.,Crop mapping provides information on crop types and cropland spatial distribution. Therefore, accurate and timely crop mapping serves as the fundamental step to higher-level agricultural applications, such as crop yield prediction. Recently, deep learning (DL) classification models have been explored for crop mapping. However, most existing methods are still limited in practical applications for large-scale crop mapping due to the great need for representative labeled data that cover diverse geographical conditions. To address this issue, we proposed a novel deep active learning (AL) method, named Bayesian Semi-Supervised Active Learning (BSSAL), using time-series Sentinel-2 imagery for large-scale crop mapping. This framework consists of three parts: 1) a Bayesian neural network (BNN) for crop classification, 2) a Semi-Supervised task to leverage massive satellite imagery without crop type information, and 3) an AL strategy to select the most informative samples to be labeled. The proposed framework was validated on five crop classes over six sites with different geographical conditions across the U.S. in testing years 2019–2021. The BSSAL framework produced a mean OA of 98.69% in the end-of-season experiment and significantly outperformed other comparison methods. Moreover, it achieved an average OA of 97.00% with in-season satellite imagery from the day of the year (DOY) 140 (early April) to 200 (mid-July) and greatly improved the timeliness of crop mapping. Furthermore, the BSSAL framework had the best performance in all study sites, showing the robustness of the global model on local testing. By monitoring the accuracy of each crop type along with the number of samples selected, the results showed the effectiveness of the proposed active data query strategy to random choice. Overall, this study provided a robust framework for large-scale crop mapping with less labeling budget and higher classification accuracy. The source code is available at: https://github.com/YXu556/BSSAL. © 2024 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)"
157,156,28,156_logs_logbased_spikelog_log,"logs,logbased,spikelog,log,allinfolog,anomaly,monitoring,anomalies,anomalous,supervised","Log anomaly is a manifestation of a software system error or security threat. Detecting such unusual behaviours across logs in real-time is the driving force behind large-scale autonomous monitoring technology that can rapidly alert zero-day attacks. Increasingly, AI methods are being used to process voluminous log datasets and reveal patterns of correlated anomaly. In this paper, we propose an enhanced approach to learning semantic-aware embeddings for logs called the Subword Encoder Neural network (SEN). Solving upon a key limitation of previous semantic log parsing works, the proposed work introduces the concept of learning word vectors from subword-level granularity using an attention encoder strategy. The learnt embeddings reflect the contextual/lexical relationships at the word level. As a result, the learnt word representations precisely capture new log messages previously not seen by the model. Furthermore, we develop a novel feature distillation algorithm termed Naive Bayes Feature Selector (NBFS) to extract useful log events. This probabilistic technique examines the occurrence pattern of events to only select the salient ones that can aid anomaly detection. To our best knowledge, this is the first attempt to associate affinity to log events based on the target task. Since the predictions can be traced to the log messages, the AI is inherently explainable too. The model outperforms state-of-the-art methods by a fair margin. It achieves a 0.99 detection F1-score on the benchmarked BGL, HDFS and OpenStack log datasets. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Now-a-days, failure detection and prediction have become a significant research focus on enhancing the reliability and availability of IT infrastructure components. Log analysis is an emerging domain aimed at diminishing downtime caused by IT infrastructure components' failure. However, it can be challenging due to poor log quality and large data sizes. The proposed system automatically classifies logs based on log level and semantic analysis, allowing for a precise understanding of the meaning of log entries. Using the BERT pre-trained model, semantic vectors are generated for various IT infrastructures, such as Server Applications, Cloud Systems, Operating Systems, Supercomputers, and Mobile Systems. These vectors are then used to train machine learning (ML) classifiers for log categorization. The trained models are competent in classifying logs by comprehending the context of different types of logs. Additionally, semantic analysis outperforms sentiment analysis when dealing with unobserved log records. The proposed system significantly reduces engineers' day-to-day error-handling work by automating the log analysis process. © 2023, International Journal of Advanced Computer Science and Applications. All Rights Reserved.,Large-scale services are generating massive logs, which trace the runtime states and critical events. Anomaly detection via logs is critical for service maintenance and reliability assurance. Existing log-based anomaly detection methods make use of the limited information in log data, resulting in their incapability of detecting diverse anomalies related to unused log features. In this paper, we propose AllInfoLog, a robust log-based anomaly detection method taking advantage of all log information, to detect diverse types of anomalies. To capture all log features, AllInfoLog utilizes four encoders to extract semantic, parameter, time, and other feature embeddings, respectively. The embeddings of all log features are then combined to train an attention-based Bi-LSTM model to detect diverse anomalies. The experimental evaluations on real-world log datasets, synthetic datasets, and unstable log datasets demonstrate AllInfoLog outperforms the state-of-the-art log-based anomaly detection methods from aspects of performance and robustness, and has effectiveness to detect diverse types of anomalies.  © 2004-2012 IEEE."
158,157,27,157_machining_milling_grinding_neural,"machining,milling,grinding,neural,tool,cnn,tools,training,lstm,mscnn","Local Interpretable and Model-agnostic Explanation (LIME), an explainable artificial intelligence (XAI) approach is adapted to identify the globally important time-frequency bands for predicting average surface roughness (Ra) in a smart grinding process. The smart grinding setup consisted of a Supertech CNC precision surface grinding machine, instrumented with a Dytran piezoelectric accelerometer attached to the tailstock quill along the tangential direction (Y-axis). For every grinding pass, vibration signatures were captured, and the ground truth surface roughness values were recorded using a Mahr Marsurf M300C portable surface roughness profilometer. The roughness values ranged from 0.06 to 0.14 microns over the complete set of experiments. Time-frequency domain spectrogram frames were extracted for each of the vibration signals collected during the grinding process. Convolutional Neural Networks (CNNs) were modeled to predict the surface roughness based on these spectrogram frames and their image augmentations. The best CNN model was able to predict the roughness values with an overall R2-score of 0.95, training R2-score of 0.99, and testing R2-score of 0.81 with only 80 sets of vibration signals corresponding to 4 experiments with 20 trials each. Although the data size is not large enough to guarantee such performance metrics in real-world scenarios, one can extract statistically consistent explanations underlying the relationships these complex deep learning models capture. The LIME methodology was implemented on the developed surface roughness CNN model to identify the important time-frequency bands (i.e., the superpixels of a spectrogram) influencing the predictions. Based on the identified important regions on the spectrogram frames, the corresponding frequency characteristics were determined that influence the surface roughness predictions. The important frequency range based on LIME results was approximately 11.7 to 19.1 kHz. The power of XAI was demonstrated by cutting down the sampling rate from 160 kHz to 30, 20, 10, and 5 kHz based on the important frequency range and considering Nyquist criteria. Separate CNN models were developed for these ranges by only extracting time-frequency contents below their corresponding Nyquist cut-offs. A proper data acquisition strategy is proposed by comparing the model performances to argue the selection of a sufficient sampling rate to capture the grinding process successfully and robustly. © 2023 The Society of Manufacturing Engineers,Accurate prediction of tool wear is essential to ensure the machining quality of parts. However, in the actual milling process, the data distribution varies greatly between sensor signals due to variations in individual tools and machining parameters; moreover, a single deep learning model is less reliable when processing a large volume of signals. All these problems make accurate tool wear prediction challenging. Therefore, we propose a multi-model method with two-stage. In the first stage, the tool wear data is initially divided into two parts. For each part, we design a correlation-aligned multiscale convolutional temporal attention gated recurrent neural network model to perform preliminary prediction, aiming at extracting the deep temporal features from diverse signals and mitigating the sensitivity of the features to the changes in data distributions. In the second stage, we adaptively aggregate the preliminary prediction from multiple models to obtain the final prediction via a joint decision-making module to extend the decision boundary of single model and improve the tool wear prediction performance. Finally, two sets of experiments are conducted for different tools and machining conditions. The experimental results show that our proposed method significantly reduces the root mean square error (RMSE) by 15% and the mean absolute error by 18% compared to other methods. © 2023 IOP Publishing Ltd.,Surface roughness and machining accuracy are essential indicators of the quality of parts in milling. With recent advancements in sensor technology and data processing, the cutting force signals collected during the machining process can be used for the prediction and determination of the machining quality. Deep-learning-based artificial neural networks (ANNs) can process large sets of signal data and can make predictions according to the extracted data features. During the final stage of the milling process of SUS304 stainless steel, we selected the cutting speed, feed per tooth, axial depth of cut, and radial depth of cut as the experimental parameters to synchronously measure the cutting force signals with a sensory tool holder. The signals were preprocessed for feature extraction using a Fourier transform technique. Subsequently, three different ANNs (a deep neural network, a convolutional neural network, and a long short-term memory network) were applied for training in order to predict the machining quality under different cutting conditions. Two training methods, namely whole-data training and training by data classification, were adopted. We compared the predictive accuracy and efficiency of the training process of these three models based on the same training data. The training results and the measurements after machining indicated that in predicting the surface roughness based on the feed per tooth classification, all the models had a percentage error within 10%. However, the convolutional neural network (CNN) and long short-term memory (LSTM) models had a percentage error of 20% based on the whole-data training, while that of the deep neural network (DNN) model was over 50%. The percentage error for the machining accuracy prediction based on the whole-data training of the DNN and CNN models was below 10%, while that of the LSTM model was as large as 20%. However, there was no significant improvement in the results of the classification training. In all the training processes, the CNN model had the best analytical efficiency, followed by the LSTM model. The DNN model performed the worst. © 2023 by the authors."
159,158,27,158_videos_visual_distortion_quality,"videos,visual,distortion,quality,vvc,perceptual,visually,distortions,images,perception","Ultra-high-definition (UHD) video has brought new challenges to objective video quality assessment (VQA) due to its high resolution and high frame rate. Most existing VQA methods are designed for non-UHD videos—when they are employed to deal with UHD videos, the processing speed will be slow and the global spatial features cannot be fully extracted. In addition, these VQA methods usually segment the video into multiple segments, predict the quality score of each segment, and then average the quality score of each segment to obtain the quality score of the whole video. This breaks the temporal correlation of the video sequences and is inconsistent with the characteristics of human visual perception. In this paper, we present a no-reference VQA method, aiming to effectively and efficiently predict quality scores for UHD videos. First, we construct a spatial distortion feature network based on a super-resolution model (SR-SDFNet), which can quickly extract the global spatial distortion features of UHD videos. Then, to aggregate the spatial distortion features of each UHD frame, we propose a time fusion network based on a reinforcement learning model (RL-TFNet), in which the actor network continuously combines multiple frame features extracted by SR-SDFNet and outputs an action to adjust the current quality score to approximate the subjective score, and the critic network outputs action values to optimize the quality perception of the actor network. Finally, we conduct large-scale experiments on UHD VQA databases and the results reveal that, compared to other state-of-the-art VQA methods, our method achieves competitive quality prediction performance with a shorter runtime and fewer model parameters. © 2023 by the authors.,To monitor objects of interest, such as wildlife and people, image-capturing devices are used to collect a large number of images with and without objects of interest. As we are recording valuable information about the behavior and activity of objects, the quality of images containing objects of interest should be better than that of images without objects of interest, even if the former exhibits more severe distortion than the latter. However, according to current methods, quality assessments produce the opposite results. In this study, we propose an end-to-end model, named DETR-IQA (detection transformer image quality assessment), which extends the capability to perform object detection and blind image quality assessment (IQA) simultaneously by adding IQA heads comprising simple multi-layer perceptrons at the top of the DETRs (detection transformers) decoder. Using IQA heads, DETR-IQA carried out blind IQAs based on the weighted fusion of the distortion degree of the region of objects of interest and the other regions of the image; the predicted quality score of images containing objects of interest was generally greater than that of images without objects of interest. Currently, the subjective quality score of all public datasets is in accordance with the distortion of images and does not consider objects of interest. We manually extracted the images in which the five predefined classes of objects were the main contents of the largest authentic distortion dataset, KonIQ-10k, which was used as the experimental dataset. The experimental results show that with slight degradation in object detection performance and simple IQA heads, the values of PLCC and SRCC were 0.785 and 0.727, respectively, and exceeded those of some deep learning-based IQA models that are specially designed for only performing IQA. With the negligible increase in the computation and complexity of object detection and without a decrease in inference speeds, DETR-IQA can perform object detection and IQA via multi-tasking and substantially reduce the workload. © 2023 by the authors.,No-reference (NR) video quality assessment (VQA) is a challenging problem due to the difficulty in model training caused by insufficient annotation samples. Previous work commonly utilizes transfer learning to directly migrate pre-trained models on the image database, which suffers from domain inadaptation. Recently, self-supervised representation learning has become a hot spot for the independence of large-scale labeled data. However, existing self-supervised representation learning method only considers the distortion types and contents of the video, there needs to investigate the intrinsic properties of videos for the VQA task. To amend this, here we propose a novel multi-task self-supervised representation learning framework to pre-train a video quality assessment model. Specifically, we consider the effects of distortion degrees, distortion types, and frame rates on the perceived quality of videos, and utilize them as guidance to generate self-supervised samples and labels. Then, we optimize the ability of the VQA model in capturing spatio-temporal differences between the original video and the distorted version using three pretext tasks. The resulting framework not only eases the requirements for the quality of the original video but also benefits from the self-supervised labels as well as the Siamese network. In addition, we propose a Transformer-based VQA model, where short-term spatio-temporal dependencies of videos are modeled by 3D-CNN and 2D-CNN, and then the long-term spatio-temporal dependencies are modeled by Transformer because of its excellent long-term modeling capability. We evaluated the proposed method on four public video quality assessment databases and found that it is competitive with all compared VQA algorithms.  © 1963-12012 IEEE."
160,159,27,159_emotionrelated_emotions_emotion_multimodal,"emotionrelated,emotions,emotion,multimodal,emotioncause,neural,emotional,supervised,affective,dcnn","Identifying the emotional causes of mental illnesses is key to effective intervention. Existing emotion-cause analysis approaches can effectively detect simple emotion-cause expressions where only one cause and one emotion exist. However, emotions may often result from multiple causes, implicitly or explicitly, with complex interactions among these causes. Moreover, the same causes may result in multiple emotions. How to model the complex interactions between multiple emotion spans and cause spans remains under-explored. To tackle this problem, a contrastive learning-based framework is presented to detect the complex emotion-cause pairs with the introduction of negative samples and positive samples. Additionally, we developed a large-scale emotion-cause dataset with complex emotion-cause instances based on subreddits associated with mental health. Our proposed approach was compared to prevailing CNN-based, LSTM-based, Transformer-based and GNN-based methods. Extensive experiments have been conducted and the quantifiable outcomes indicate that our proposed solution achieves competitive performance on simple emotion-cause pairs and significantly outperformed baseline methods in extracting complex emotion-cause pairs. Empirical studies further demonstrated that our proposed approach can be used to reveal the emotional causes of mental disorders for effective intervention. © 2023 The Author(s),In recent times, there has been a lot of focus placed on speech emotion recognition, often known as SER. SER is a fundamental approach to emotional human-machine interaction. Utilising DCNNs has resulted in significant advancements being made, notably with regard to the learning of high-level characteristics for SER. However, one of the primary challenges that arises when applying deep neural networks to SER is the problem of overfitting, which occurs commonly when too many parameters are chosen using insufficiently large datasets. The majority of the research that has been done to attempt to solve this problem has focused on translating the audio input into a picture and employing transfer learning methods. According to research, the patterns generated in this region include significant emotional aspects of the speaker. A new speech signal format called Chaogram was designed to project these patterns, which would result in three channels like RGB images, in order to give an input that works with VGG-based Deep-CNNs. The original phase space is then used to reconstruct the voice samples in a three-dimensional space. In the subsequent stage, the Chaogram photos' finer details were accentuated by image enhancing techniques. To learn Chaogram's high-level features and emotion classifications, the Visual Geometry Group (VGG) deep convolutional neural network (DCNN) is employed after being pre-trained on the massive ImageNet dataset using intelligent sensors. We next apply transfer learning to our datasets to further improve the provided model, which we combine with the proposed model. To enhance the hyper-parameter layout of architecture-determined CNNs, a new Deep-CNN with BWO (Beluga Whale Optimization) is introduced. In order to apply Deep–CNN–KL and to the field of SER, thispropose a new method of representing speech signals, which we call Chaograms, by mapping the signal onto a 2D image. On two publicly available emotion datasets, these findings demonstrate the ability of the proposed approach, which has the potential to significantly improve SER applications: EMO-DB and eNTERFACE05. Image enhancement methods that place more emphasis on such features may lead to more precise classification in the performance analysis. Classification accuracy can be considerably improved by adapting these images to work with the pre-trained Deep-CNN inputs. © 2024 The Authors,As deep learning technology research continues to progress, artificial intelligence technology is gradually empowering various fields. To achieve a more natural human-computer interaction experience, how to accurately recognize emotional state of speech interactions has become a new research hotspot. Sequence modeling methods based on deep learning techniques have promoted the development of emotion recognition, but the mainstream methods still suffer from insufficient multimodal information interaction, difficulty in learning emotion-related features, and low recognition accuracy. In this article, we propose a transformer-based deep-scale fusion network (TDFNet) for multimodal emotion recognition, solving the aforementioned problems. The multimodal embedding (ME) module in TDFNet uses pretrained models to alleviate the data scarcity problem by providing a priori knowledge of multimodal information to the model with the help of a large amount of unlabeled data. Furthermore, a mutual transformer (MT) module is introduced to learn multimodal emotional commonality and speaker-related emotional features to improve contextual emotional semantic understanding. In addition, we design a novel emotion feature learning method named the deep-scale transformer (DST), which further improves emotion recognition by aligning multimodal features and learning multiscale emotion features through GRUs with shared weights. To comparatively evaluate the performance of TDFNet, experiments are conducted with the IEMOCAP corpus under three reasonable data splitting strategies. The experimental results show that TDFNet achieves 82.08% WA and 82.57% UA in RA data splitting, which leads to 1.78% WA and 1.17% UA improvements over the previous state-of-the-art method, respectively. Benefiting from the attentively aligned mutual correlations and fine-grained emotion-related features, TDFNet successfully achieves significant improvements in multimodal emotion recognition.  © 2014 IEEE."
161,160,27,160_streetview_streetscape_streets_neighborhoods,"streetview,streetscape,streets,neighborhoods,urban,neighborhood,street,spatial,streetlevel,imagery","Street-level imagery has emerged as a valuable tool for observing large-scale urban spaces with unprecedented detail. However, previous studies have been limited to analyzing individual street-level images. This approach falls short in representing the characteristics of a spatial unit, such as a street or grid, which may contain varying numbers of street-level images ranging from several to hundreds. As a result, a more comprehensive and representative approach is required to capture the complexity and diversity of urban environments at different spatial scales. To address this issue, this study proposes a deep learning-based module called Vision-LSTM, which can effectively obtain vector representation from varying numbers of street-level images in spatial units. The effectiveness of the module is validated through experiments to recognize urban villages, achieving reliable recognition results (overall accuracy: 91.6%) through multimodal learning that combines street-level imagery with remote sensing imagery and social sensing data. Compared to existing image fusion methods, Vision-LSTM demonstrates significant effectiveness in capturing associations between street-level images. The proposed module can provide a more comprehensive understanding of urban spaces, enhancing the research value of street-level imagery and facilitating multimodal learning-based urban research. Our models are available at https://github.com/yingjinghuang/Vision-LSTM. © 2023 Elsevier Ltd,Restorative environments help people recover from mental fatigue and negative emotional and physical reactions to stress. Excellent restorative environments in urban streets help people focus and improve their daily behavioral performance, allowing them to regain efficient information processing skills and cognitive levels. High-density urban spaces create obstacles in resident interactions with the natural environment. For urban residents, the restorative function of the urban space is more important than that of the natural environment in the suburbs. An urban street is a spatial carrier used by residents on a daily basis; thus, the urban street has considerable practical value in terms of improving the urban environment to have effective restorative function. Thus, in this study, we explored a method to determine the perceived restorability of urban streets using street view data, deep learning models, and the Ordinary Least Squares (OLS), the multiscale geographically weighted regression (MGWR) model. We performed an empirical study in the Nanshan District of Shenzhen, China. Nanshan District is a typical high-density city area in China with a large population and limited urban resources. Using the street view images of the study area, a deep learning scoring model was developed, the SegNet algorithm was introduced to segment and classify the visual street elements, and a random forest algorithm based on the restorative factor scale was employed to evaluate the restorative perception of urban streets. In this study, spatial heterogeneity could be observed in the restorative perception data, and the MGWR models yielded higher R2 interpretation strength in terms of processing the urban street restorative data compared to the ordinary least squares and geographically weighted regression (GWR) models. The MGWR model is a regression model that uses different bandwidths for different visual street elements, thereby allowing additional detailed observation of the extent and relevance of the impact of different elements on restorative perception. Our research also supports the exploration of the size of areas where heterogeneity exists in space for each visual street element. We believe that our results can help develop informed design guidelines to enhance street restorative and help professionals develop targeted design improvement concepts based on the restorative nature of the urban street. Copyright © 2023 Han, Wang, He and Jung.,The physical presence of a street, called the “street view”, is a medium through which people perceive the urban form. A street’s spatial ratio is the main feature of the street view, and its measurement and quality are the core issues in the field of urban design. The traditional method of studying urban aspect ratios is manual on-site observation, which is inefficient, incomplete and inaccurate, making it difficult to reveal overall patterns and influencing factors. Street view images (SVI) provide large-scale urban data that, combined with deep learning algorithms, allow for studying street spatial ratios from a broader space-time perspective. This approach can reveal an urban forms’ aesthetics, spatial quality, and evolution process. However, current streetscape research mainly focuses on the creation and maintenance of spatial data infrastructure, street greening, street safety, urban vitality, etc. In this study, quantitative research of the Beijing street spatial ratio was carried out using street view images, a convolution neural network algorithm, and the classical street spatial ratio theory of urban morphology. Using the DenseNet model, the quantitative measurement of Beijing’s urban street location, street aspect ratio, and the street symmetry was realized. According to the model identification results, the law of the gradual transition of the street spatial ratio was depicted (from the open and balanced type to the canyon type and from the historical to the modern). Changes in the streets’ spatiotemporal characteristics in the central area of Beijing were revealed. Based on this, the clustering and distribution phenomena of four street aspect ratio types in Beijing are discussed and the relationship between the street aspect ratio type and symmetry is summarized, selecting a typical lot for empirical research. The classical theory of street spatial proportion has limitations under the conditions of high-density development in modern cities, and the traditional urban morphology theory, combined with new technical methods such as streetscape images and deep learning algorithms, can provide new ideas for the study of urban space morphology. © 2023 by the authors."
162,161,26,161_pruning_prune_pruned_networks,"pruning,prune,pruned,networks,neural,network,learning,compression,neurons,decoding","Channel pruning is one of the main methods of model compression for the deep neural network. Some of the existing pruning methods manually set parameters based on experience, which is very time-consuming, and pruning channels by greedy algorithms or heuristic algorithms will bring local optimal solutions. Some methods prune channels by automated methods, but the lack of theoretical guidance on network characteristics makes the pruning efficiency extremely low. In this article, we propose a new automated channel pruning method: by introducing additional losses into the network, to obtain the channel’s ability to discriminate information and adopt an automated pruning method based on reinforcement learning to iteratively perform channel selection and parameter optimization and prune the filtered channels. A large number of experiments have proved the effectiveness of our method. For example, for ResNet-110, on CIFAR-10, our method reduces FLOPs by 63.1%, the parameters are reduced by 62.7%, and there is almost no loss in accuracy, only 0.01% lower than the baseline model. On ImageNet, FLOPs are reduced by 50.3%, The parameters are reduced by 55.2%, and the accuracy is only lost by 0.22%. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Network pruning has been a hot topic in recent years, and many popular pruning methods rely on network design expertise. However, the pruning process usually involves manual intervention and can be difficult for users who lack prior knowledge. Automatic pruning using evolutionary algorithms shows great promise, but it must address the challenge of performing time-consuming model evaluations and searching through a large solution space. Dataset distillation is a technique that compresses the original dataset to decrease the cost of fine-tuning models. In this paper, we explore the potential of using the distilled dataset to exhibit a similar role as the real dataset in network pruning, and proposed the evolutionary pruning framework using distilled dataset. Specifically, the network pruning pipeline is carried out on the distilled dataset to significantly reduce the model evaluation cost, and the number of filters in the convolutional layer is directly coded to narrow the search space. In addition, a tailored evolutionary algorithm is proposed that takes the form of constrained optimization to search the most suitable pruned network. The experiments conducted on VGG16, VGG19, ResNet56, and ResNet110 demonstrate that the proposed method reduces at least 41.56% of the flops and achieves competitive results with little compromising accuracy. © 2023 Elsevier Inc.,Deep learning technology has found a promising application in lightweight model design, for which pruning is an effective means of achieving a large reduction in both model parameters and float points operations (FLOPs). The existing neural network pruning methods mostly start from the consideration of the importance of model parameters and design parameter evaluation metrics to perform parameter pruning iteratively. These methods were not studied from the perspective of network model topology, so they might be effective but not efficient, and they require completely different pruning for different datasets. In this article, we study the graph structure of the neural network and propose a regular graph pruning (RGP) method to perform a one-shot neural network pruning. Specifically, we first generate a regular graph and set its node-degree values to meet the preset pruning ratio. Then, we reduce the average shortest path-length (ASPL) of the graph by swapping edges to obtain the optimal edge distribution. Finally, we map the obtained graph to a neural network structure to realize pruning. Our experiments demonstrate that the ASPL of the graph is negatively correlated with the classification accuracy of the neural network and that RGP has a strong precision retention capability with high parameter reduction (more than 90%) and FLOPs reduction (more than 90%) (the code for quick use and reproduction is available at https://github.com/Holidays1999/Neural-Network-Pruning-through-its-RegularGraph-Structure). IEEE"
163,162,26,162_changing_changes_sensing_change,"changing,changes,sensing,change,detection,features,images,land,feature,image","Automatic change detection based on remote sensing is playing an increasingly important role in the national economy construction. To address the problem of limited change detection accuracy in existing single-level difference networks, this study proposes the Multi-level Difference Network (MDNet) for automatic change detection of ground targets from very high-resolution (VHR) remote sensing images. An early-difference network and a late-difference network are combined by MDNet to extract multi-level change features. The early-difference network can focus on change information throughout to reduce the spurious changes in the change detection results, and the late-difference network can provide deep features of a single image for reducing rough boundaries and scattered holes in the change detection results, thus improving the accuracy. However, not all high-level features extracted by MDNet contribute to the recognition of image differences, and the multi-level change features suffer from cross-channel heterogeneity. Stacking them directly on channels does not make effective use of change information, thus limiting the performance of MDNet. Therefore, the Multi-level Change Features Fusion Module (MCFFM) is proposed in this study for the effective fusion of multi-level change features. In the experiments, the publicly available open-pit mine change detection (OMCD) dataset was used first to achieve a change detection of open-pit mines over a large area, with an F1-score of 89.2%, increasing by 1.3% to 5.9% compared to the benchmark methods. Then, a self-made OMCD dataset was used to achieve an F1-score of 92.8% for the localized and fine-scale change detection in open-pit mines, which is an improvement of 0.7% to 5.4% compared to the benchmark methods. Finally, the Season-varying Change Detection Dataset is used to verify that the MDNet proposed can detect changes in other scenarios very well. The experimental results show that the proposed MDNet has significantly improved the performance of change detection on the three datasets compared with six advanced deep learning models, which will contribute to the development of change detection with VHR remote sensing images. © 2023 by the authors.,Deep learning instantiated by convolutional neural networks has achieved great success in high-resolution remote-sensing image change detection. However, such networks have a limited receptive field, being unable to extract long-range dependencies in a scene. As the transformer model with self-attention can better describe long-range dependencies, we introduce a hierarchical transformer model to improve the precision of change detection in high-resolution remote sensing images. First, the hierarchical transformer extracts abstract features from multitemporal remote sensing images. To effectively minimize the model’s complexity and enhance the feature representation, we limit the self-attention calculation of each transformer layer to local windows with different sizes. Then, we combine the features extracted by the hierarchical transformer and input them into a nested U-Net to obtain the change detection results. Furthermore, a simple but effective model fusion strategy is adopted to improve the change detection accuracy. Extensive experiments are carried out on two large-scale data sets for change detection, LEVIR-CD and SYSU-CD. The quantitative and qualitative experimental results suggest that the proposed method outperforms the advanced methods in terms of detection performance. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.,High-resolution remote sensing image change detection technology compares and analyzes bi-temporal or multitemporal high-resolution remote sensing images to determine the change areas. It plays an important role in land cover/use monitoring, natural disaster monitoring, illegal building investigation, military target strike effect analysis, and land and resource investigation. The change detection of high-resolution remote sensing images has developed rapidly from data accumulation to algorithm models because of the rapid development of technologies such as deep learning and earth observation in recent years. However, the current deep learning-based change detection methods are strongly dependent on large sample data, and the training model has insufficient cross-domain generalization ability. As a result, a prior semantic information-guided change detection framework (PSI-CD), which alleviates the change detection model’s dependence on datasets by making full use of prior semantic information, is proposed in this paper. The proposed method mainly includes two parts: one is a prior semantic information generation network that uses the semantic segmentation dataset to extract robust and reliable prior semantic information; the other is the prior semantic information guided change detection network that makes full use of prior semantic information to reduce the sample size of the change detection. To verify the effectiveness of the proposed method, we produced pixel-level semantic labels for the bi-temporal images of the public change detection dataset (LEVIR-CD). Then, we performed extensive experiments on the WHU and LEVIR-CD datasets, including comparisons with existing methods, experiments with different amounts of data, and ablation study, to show the effectiveness of the proposed method. Compared with other existing methods, our method has the highest IoU for all training samples and different amounts of training samples on WHU and LEVIR-CD, reaching a maximum of 83.25% and 83.80%, respectively. © 2023 by the authors."
164,163,26,163_geochemical_mineralization_mineral_geochemistry,"geochemical,mineralization,mineral,geochemistry,geothermal,geological,geosciences,geophysical,geologists,classification","A mineral prospectivity map (MPM) focusing on gold mineralization in the Larder Lake region of Northern Ontario, Canada, has been produced in this study. We have used the Random Forest (RF) algorithm to use 32 predictor maps integrating geophysical, geochemical, and geological datasets from various sources that represent vectors to gold mineralization. It is evident from the efficiency of classification curves that MPMs generated are robust. The unsupervised algorithms, K-means and principal component analysis (PCA) were used to investigate and visualize the clustering nature of large geochemical and geophysical datasets. We used RQ-mode PCA to compute variable and object loadings simultaneously, which allows the displays of observations and the variables at the same scale. PCA biplots of the Larder Lake geochemical data show that Au is strongly correlated with W, S, Pb and K, but inversely correlated with Fe, Mn, Co, Mg, Ca, and Ni. The known gold mineralization locations were well classified by RF with the accuracy of 95.63 %. Furthermore, partial least squares-discriminant analysis (PLS-DA) model combines 3D geophysical clusters and geochemical compositions, which indicates the Au-rich areas are characterized with low to mid resistivity – low susceptibility properties. We conclude that the Larder Lake-Cadillac deformation zone (LLCDZ) is relatively more fertile than the Lincoln-Nipissing shear zone (LNSZ) with respect to gold mineralization due to deeper penetrating faults. The intersection of the LLCDZ and network of high-angle NE-trending cross faults acts as key conduits for gold endowments in the Larder Lake area. This study innovatively combined multivariate geological, geochemical, and geophysical datasets via machine learning algorithms, which improves identification of geochemical anomalies and interpretation of spatial features associated with gold mineralization. © 2023 Elsevier B.V.,Several large-scale porphyry copper deposits (PCDs) with high economic value have been excavated in the Duolong ore district, Tibet, China. However, the high altitudes and harsh conditions in this area make traditional exploration difficult. Hydrothermal alteration minerals related to PCDs with diagnostic spectral absorption features in the visible–near-infrared–shortwave-infrared ranges can be effectively identified by remote sensing imagery. Mainly based on hyperspectral imagery supplemented by multispectral imagery and geochemical element data, the Duolong ore district was selected to conduct data-driven PCD prospectivity modelling. A total of 11 known deposits and 17 evidential layers of multisource geoscience information related to Cu mineralization constitute the input datasets of the predictive models. A deep learning convolutional neural network (CNN) model was applied to mineral prospectivity mapping, and its applicability was tested by comparison to conventional machine learning models, such as support vector machine and random forest. CNN achieves the greatest classification performance with an accuracy of 0.956. This is the first trial in Duolong to conduct mineral prospectivity mapping combined with remote imagery and geochemistry based on deep learning methods. Four metallogenic prospective sites were delineated and verified through field reconnaissance, indicating that the application of deep learning-based methods in PCD prospecting proposed in this paper is feasible by utilizing geoscience big data such as remote sensing datasets and geochemical elements. © 2023 by the authors.,Although mineral prospectivity modeling (MPM) has undergone decades of development, it has not yet been widely adopted in the global mineral exploration industry. Exploration geoscientists encounter challenges in understanding the internal working of many mineral prospectivity models due to their black box nature. Besides, their predictive results usually delineate undesirably large high-prospectivity areas, which are biased toward existing deposits, making MPM impractical. However, there are only a few data-driven methods for MPM that address both the interpretability of black box models and the issue of bias in high prospective areas, which may result from the intrinsic stochastic uncertainty of training samples, particularly toward well-known deposits. In this study, we construct and demonstrate a framework to improve the performance and reliability of data-driven MPM in the Qulong–Jiama mineral district of Tibet. Firstly, the mineral systems concept was applied to select appropriate targeting criteria and to derive corresponding evidential features. Secondly, model-agnostic methods, such as permutation feature importance, partial dependence plot, individual conditional expectation plot, and Shapely values, were applied to interpret the machine learning models. Finally, modulated prediction models and the spatial pattern of linked uncertainties were generated by an ensemble method that combines bootstrapping and the Random Forest algorithm. The final exploration targets, which were demarcated by cells with high modulated values and low uncertainties obtained by 50 predictive models, account for just ~ 3% of the study area. © 2023, International Association for Mathematical Geosciences."
165,164,26,164_blockchain_blockchains_blockchainbased_federated,"blockchain,blockchains,blockchainbased,federated,sidechains,cloud,ethereum,decentralized,distributed,secure","Federated learning (FL) has received considerable attention because it allows multiple devices to train models locally without revealing sensitive data. Well-trained local models are transmitted to a parameter server for further aggregation. The dependence on a trusted central server makes FL vulnerable to the single point of failure or attack. Blockchain is regarded as a state-of-the-art solution to decentralize the central server and provide attractive features simultaneously, such as immutability, traceability, and accountability. However, current popular blockchain systems cannot be combined with FL seamlessly. Since all local models should be collected before aggregation, the latency of FL is determined by the slowest device. The consensus process required by blockchain will increase the latency further, especially, when a large block is required for including the model. Moreover, forever-growing blockchain together with models will take up a lot of storage space, making it impractical to be deployed on lightweight devices. To address these problems, we propose a lightweight blockchain TORR for FL. A novel consensus protocol Proof of Reliability is designed to achieve fast consensus while mitigating the impact of stragglers. A storage protocol is designed based on erasure coding and periodic storage refreshing policy. With erasure coding, we take full advantage of the limited storage space of devices. With the periodic storage refreshing policy, we reduce the requirement for storage. Compared to the common blockchain-based FL system, TORR reduces the system latency, overall storage overhead, and peak storage overhead by up to 62%, 75.44%, and 51.77%, respectively.  © 2014 IEEE.,Cloud storage is widely used by large companies to store vast amounts of data and files, offering flexibility, financial savings, and security. However, information shoplifting poses significant threats, potentially leading to poor performance and privacy breaches. Blockchain-based cognitive computing can help protect and maintain information security and privacy in cloud platforms, ensuring businesses can focus on business development. To ensure data security in cloud platforms, this research proposed a blockchain-based Hybridized Data Driven Cognitive Computing (HD2C) model. However, the proposed HD2C framework addresses breaches of the privacy information of mixed participants of the Internet of Things (IoT) in the cloud. HD2C is developed by combining Federated Learning (FL) with a Blockchain consensus algorithm to connect smart contracts with Proof of Authority. The “Data Island” problem can be solved by FL’s emphasis on privacy and lightning-fast processing, while Blockchain provides a decentralized incentive structure that is impervious to poisoning. FL with Blockchain allows quick consensus through smart member selection and verification. The HD2C paradigm significantly improves the computational processing efficiency of intelligent manufacturing. Extensive analysis results derived from IIoT datasets confirm HD2C superiority. When compared to other consensus algorithms, the Blockchain PoA’s foundational cost is significant. The accuracy and memory utilization evaluation results predict the total benefits of the system. In comparison to the values 0.004 and 0.04, the value of 0.4 achieves good accuracy. According to the experiment results, the number of transactions per second has minimal impact on memory requirements. The findings of this study resulted in the development of a brand-new IIoT framework based on blockchain technology. © 2023 Tech Science Press. All rights reserved.,The growth of information technology has resulted in a massive escalation of data and the demand for data exploration, particularly in the machine learning sector. However, machine learning raises concerns about data privacy because algorithms require large amounts of data to learn and make accurate predictions. Such data often contain personal information about individuals, and there is a risk that this information could be accessed or misused by unauthorized parties. It is crucial for organizations that use machine learning to prioritize personal data protection and ensure that appropriate safeguards are in place to prevent privacy breaches. Federated learning (FL) and blockchain technology are two increasingly popular approaches to distributed computing. Federated learning is a distributed machine-learning approach that trains machine-learning models on decentralized datasets without centralizing the data. Federated learning offers several benefits, including improved data privacy. Ensuring the benefit of clients in federated learning is vital for the success of this distributed machine learning approach, especially when combined with blockchain technology, as it offers a secure and transparent way to store and verify data. In this study, we propose a combination of federated learning and blockchain as a solution to some of the challenges faced by both approaches. By leveraging the decentralized nature of federated learning and the security and transparency of blockchain, our approach tends to overcome issues such as data privacy and trustworthiness of results. The evaluation results demonstrated that the proposed approach has many potential applications in various domains.  © 2013 IEEE."
166,165,26,165_forests_forest_trees_vegetation,"forests,forest,trees,vegetation,tree,pine,treecrown,deciduous,lidar,canopy","Mapping forest disturbances is paramount to carbon monitoring, estimating environmental drivers, and developing strategies to enhance forest resilience. Existing forest change products from Landsat and Sentinel-2 have improved our understanding of large-scale disturbance patterns; however, their relatively coarse spatial resolution (10 to 30-m) leads to the use of mixed pixels which constrains their application for detecting heterogeneous survival or mortality outcomes occurring at the level of individual trees or canopies. PlanetScope multispectral imagery at 3-m and near-daily frequency offers new capabilities to detect and monitor diverse tree mortality patterns following disturbance across landscapes. This research proposes a framework to detect canopy-scale (3 × 3-m) tree/shrub mortality and survival using PlanetScope monthly time series. A 3D Spatio-Temporal Convolutional Neural Network (ST-CNN) deep learning model was designed to fully utilize the spatial context and the temporal change unique to canopy survival and mortality from the PlanetScope time series. As a crucial component for training robust and scalable deep learning models, a large set of labels was collected via a semi-automatic workflow by combining pre-disturbance lidar crown segmentation and post-disturbance aerial imagery interpretation. We applied the framework to detect canopy-scale mortality and survival following 15 large wildfires in California from 2018–2021. We sampled 1,176 384 × 384-m scenes from burned areas with pre-fire aerial lidar, containing >1.8M tree and shrub canopy polygons labeled as dead or alive following wildfire. Evaluated with an independent testing dataset, the optimized ST-CNN model detects heterogeneous patterns representing survival and mortality outcomes at 3-m resolution which accurately align with observed/labeled data. Tree mortality detection accuracy was high and stable in the Sierra Nevada and North Coast Mountains ecoregions (user's = 83%–86%; producer's = 81%–82%), but decreased slightly within the sparser Central Foothills and South Coast and Mountains (user's = 77%–81%; producer's = 58%–61%) often due to confusion between shrub and tree mortality. Producer's accuracy of tree mortality and survival increased with canopy height and remained stable (>75%) on canopies taller than 11-m. Further, a sensitivity analysis demonstrates the performance benefits of using spatial and/or temporal convolutions in the ST-CNN architecture for model prediction. Lastly, we demonstrate the scalability of the ST-CNN for regional-scale application on all large 2020 wildfires in California (?1.6 Mha burn area). The wall-to-wall post-fire maps showed an overall 3-m tree mortality rate of 58.8%, ranging from 32% to 94% among individual fires. The trained ST-CNN provides an ecologically detailed estimation of the pre-disturbance forest composition (trees, shrubs, non-woody) and their outcomes (survival or mortality) post-disturbance. These data will improve higher resolution monitoring and assessment of forest disturbance impacts, allow for better understanding of forest vulnerability, and support forest management strategies and actions. © 2023 The Author(s),Forest fragmentation has been increasingly exacerbated by deforestation, urbanization, and agricultural expansion. Monitoring the forest fragments via the lens of tree-crown scale leaf phenology is critical to understand tree species phenological responses to climate change and identify the fragment species vulnerable to environmental disturbance. Despite advances in remote sensing for phenology monitoring, detecting tree-crown scale leaf phenology in fragmented forests remains challenging. Simultaneous tracking of key spring phenological events that are crucial to ecosystem functions and climate change responses is also neglected. To address these challenges, we develop a novel tree-crown scale remote sensing phenological monitoring framework to characterize all the critical spring phenological events of individual trees of deciduous forest fragments, with Trelease Woods in Champaign, Illinois as a case study. The novel framework comprises four components: 1) generate high spatiotemporal resolution fusion imagery from multi-scale satellite time series with a hybrid deep learning fusion model; 2) calibrate PlanetScope imagery time series with fusion data using histogram matching; 3) model tree-crown scale phenology trajectory with a Beck logistic-based method; 4) detect a diversity of tree-crown scale phenological events using several phenological metric extraction methods (i.e., threshold- and curve feature-based methods). Combined with weekly in-situ phenological observations of 123 individual trees across 12 broadleaf species from 2017 to 2020, the framework effectively bridges the satellite- and field-based phenological measures for the key spring phenological events (i.e., budswell, budburst, leaf expansion, and leaf maturity events) at the tree-crown scale, particularly for large individuals (RMSE <1 week for most events). Calibration of PlanetScope imagery using multi-scale satellite fusion data in consideration of landscape fragmentation is critical for monitoring tree phenology of forest fragments. Compared to curve feature-based methods, threshold-based phenometric extraction methods demonstrate enhanced capability in detecting spring leaf phenological dynamics of individual trees. Among the phenological events, full leaf out and early leaf expansion events are retrieved with high accuracy using calibrated PlanetScope time series (RMSE from 3 to 5 days and R-squared higher than 0.8). With both intensive satellite and field phenological efforts, this novel framework is at the forefront of interpreting tree-crown scale remotely sensed phenological metrics in the context of biologically meaningful field phenological events in fragmented forest setting. © 2023 Elsevier Inc.,Tropical forests are a major component of the global carbon cycle and home to two-thirds of terrestrial species. Upper-canopy trees store the majority of forest carbon and can be vulnerable to drought events and storms. Monitoring their growth and mortality is essential to understanding forest resilience to climate change, but in the context of forest carbon storage, large trees are underrepresented in traditional field surveys, so estimates are poorly constrained. Aerial photographs provide spectral and textural information to discriminate between tree crowns in diverse, complex tropical canopies, potentially opening the door to landscape monitoring of large trees. Here we describe a new deep convolutional neural network method, Detectree2, which builds on the Mask R-CNN computer vision framework to recognize the irregular edges of individual tree crowns from airborne RGB imagery. We trained and evaluated this model with 3797 manually delineated tree crowns at three sites in Malaysian Borneo and one site in French Guiana. As an example application, we combined the delineations with repeat lidar surveys (taken between 3 and 6 years apart) of the four sites to estimate the growth and mortality of upper-canopy trees. Detectree2 delineated 65 000 upper-canopy trees across 14 km2 of aerial images. The skill of the automatic method in delineating unseen test trees was good (F1 score = 0.64) and for the tallest category of trees was excellent (F1 score = 0.74). As predicted from previous field studies, we found that growth rate declined with tree height and tall trees had higher mortality rates than intermediate-size trees. Our approach demonstrates that deep learning methods can automatically segment trees in widely accessible RGB imagery. This tool (provided as an open-source Python package) has many potential applications in forest ecology and conservation, from estimating carbon stocks to monitoring forest phenology and restoration. Python package available to install at https://github.com/PatBall1/Detectree2. © 2023 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London."
167,166,26,166_microscopy_microscope_electron_electrons,"microscopy,microscope,electron,electrons,nanometric,segmentation,atomgan,nanoparticles,nanoparticle,atomvision","The rise of automation and machine learning (ML) in electron microscopy has the potential to revolutionize materials research through autonomous data collection and processing. A significant challenge lies in developing ML models that rapidly generalize to large data sets under varying experimental conditions. We address this by employing a cycle generative adversarial network (CycleGAN) with a reciprocal space discriminator, which augments simulated data with realistic spatial frequency information. This allows the CycleGAN to generate images nearly indistinguishable from real data and provide labels for ML applications. We showcase our approach by training a fully convolutional network (FCN) to identify single atom defects in a 4.5 million atom data set, collected using automated acquisition in an aberration-corrected scanning transmission electron microscope (STEM). Our method produces adaptable FCNs that can adjust to dynamically changing experimental variables with minimal intervention, marking a crucial step towards fully autonomous harnessing of microscopy big data. © 2023, The Author(s).,The major role of many research studies in nanotechnology is to identify, count, and measure nanoparticles. Images with particles are often handled by hand, employing a computer program ruler. The lack of available algorithms and specialized software tools for analyzing microscope images are limited in this research area. So, this study aims to encourage the development of a comprehensive model to predict the nanoparticles type in scanning electron microscopy (SEM). We present a dataset of 750 ordered nanoscale materials. Palladium nanoparticles (Pd nanoparticles) are placed on a carbon surface using a scanning electron microscope. This paper proposes an intelligent optimization model to classify Nanoparticle types in SEM images (e.g., lines, intersections, networks, ellipses, and circles). The utilized dataset is unlabeled with the imbalanced distribution of classes. Moreover, it contains a large amount of redundant information. These kinds of problems affect the performance of the nanoparticle's classifier. The proposed model can be divided into four phases: preprocessing, feature extraction, feature selection, and classification. The first phase of the Nanoparticle classification model is concerned with image labeling, where some morphological operations and image filtration are proposed to help experts in labeling the SEM dataset. VGG-19 deep networks in combination with Grey Wolf Optimization (GWO) are used for feature extraction and selection phases. The imbalanced input data classification phase proposes a class weight balancing support vector machine (SVM) with different kernel functions. Detailed results of all experiments show that the proposed intelligent optimization model is promising in providing a high-performance classifier for nanoparticle types in SEM images. Overall performance measures are 97% and 98% for overall accuracy and F1-score, respectively. © 2023 Elsevier B.V.,Electron backscattering diffraction provides the analysis of crystalline phases at large scales (microns) while precession electron diffraction may be used to get 4D-STEM data to elucidate structure at nanometric resolution. Both are limited by the probe size and also exhibit some difficulties for the generation of large datasets, given the inherent complexity of image acquisition. The latter appoints the application of advanced machine learning techniques, such as deep learning adapted for several tasks, including pattern matching, image segmentation, etc. This research aims to show how Gabor filters provide an appropriate feature extraction technique for electron microscopy images that could prevent the need of large volumes of data to train deep learning models. The work presented herein combines an algorithm based on Gabor filters for feature extraction and an unsupervised learning method to perform particle segmentation of polyhedral metallic nanoparticles and crystal orientation mapping at atomic scale. Experimental results have shown that Gabor filters are convenient for electron microscopy images analysis, that even a nonsupervised learning algorithm can provide remarkable results in crystal segmentation of individual nanoparticles. This approach enables its application to dynamic analysis of particle transformation recorded with aberration-corrected microscopy, offering new possibilities of analysis at nanometric scale. © 2022 Wiley-VCH GmbH."
168,167,25,167_soil_rainfall_hydrological_precipitation,"soil,rainfall,hydrological,precipitation,vegetation,drought,meteorological,arid,climate,moisture","Large-scale surface soil moisture (SSM) distribution is very necessary for agricultural drought monitoring, water resource management, and climate change research. However, the current large-scale SSM products have relatively coarse spatial resolution, which limits their application. In this study, we estimate the 1 km daily SSM in China based on ensemble learning using a multi-source data set including in situ soil moisture measurements from 2980 meteorological stations, MODIS Surface Reflectance products, SMAP (Soil Moisture Active Passive) soil moisture products, ERA5-Land dataset, SRTM DEM and soil texture. Among them, in situ measurements are used as independent variables, and other data are used as dependent variables. In order to improve the spatio-temporal completeness of SSM, the missing value in SMAP soil moisture products were reconstructed using the Discrete Cosine Transformation-penalized Partial Least Square (DCT-PLS) method to provide spatially complete background field information for soil moisture retrieval. The results show that the reconstructed soil moisture value has high quality, and the DCT-PLS method can fully utilize the three-dimensional spatiotemporal information to fill the data gaps. Subsequently, the performance of four ensemble learning models of random forest (RF), extremely randomized trees (ERT), extreme gradient boosting (XGBoost), and light gradient boosting machine (LightGBM) for soil moisture retrieval was evaluated. The LightGBM outperformed the other three machine learning models, with a correlation coefficient (R2) of 0.88, a bias of 0.0004 m³/m³, and an unbiased root mean square error (ubRMSE) of 0.0366 m³/m³. The high correlation between the in situ soil moisture and the predicted values at each meteorological station further indicate that LightGBM can well capture the temporal variation of soil moisture. Finally, the model was used to map the 1 km daily SSM in China on the first day of each month from May to October 2018. This study can provide some reference and help for future long-term daily 1 km surface soil moisture mapping in China. © 2023 by the authors.,Accurate high-resolution soil moisture mapping is critical for surface studies as well as climate change research. Currently, regional soil moisture retrieval primarily focuses on a spatial resolution of 1 km, which is not able to provide effective information for environmental science research and agricultural water resource management. In this study, we developed a quantitative retrieval framework for high-resolution (250 m) regional soil moisture inversion based on machine learning, multisource data fusion, and in situ measurement data. Specifically, we used various data sources, including the normalized vegetation index, surface temperature, surface albedo, soil properties data, precipitation data, topographic data, and soil moisture products from passive microwave data assimilation as input parameters. The soil moisture products simulated based on ground model simulation were used as supplementary data of the in situ measurements, together with the measured data from the Maqu Observation Network as the training target value. The study was conducted in the Zoige region of the Tibetan Plateau during the nonfreezing period (May–October) from 2009 to 2018, using random forests for training. The random forest model had good accuracy, with a correlation coefficient of 0.885, a root mean square error of 0.024 m³/m³, and a bias of ?0.004. The ground-measured soil moisture exhibited significant fluctuations, while the random forest prediction was more accurate and closely aligned with the field soil moisture compared to the soil moisture products based on ground model simulation. Our method generated results that were smoother, more stable, and with less noise, providing a more detailed spatial pattern of soil moisture. Based on the permutation importance method, we found that topographic factors such as slope and aspect, and soil properties such as silt and sand have significant impacts on soil moisture in the southeastern Tibetan Plateau. This highlights the importance of fine-scale topographic and soil property information for generating high-precision soil moisture data. From the perspective of inter-annual variation, the soil moisture in this area is generally high, showing a slow upward trend, with small spatial differences, and the annual average value fluctuates between 0.3741 m3/m3 and 0.3943 m3/m3. The intra-annual evolution indicates that the monthly mean average soil moisture has a large geographical variation and a small multi-year linear change rate. These findings can provide valuable insights and references for regional soil moisture research. © 2023 by the authors.,Motivated by the lack of long-term global soil moisture products with both high spatial and temporal resolutions, a global 1km daily spatiotemporally continuous soil moisture product (GLASS SM) was generated from 2000 to 2020 using an ensemble learning model (eXtreme Gradient Boosting - XGBoost). The model was developed by integrating multiple datasets, including albedo, land surface temperature, and leaf area index products from the Global Land Surface Satellite (GLASS) product suite, as well as the European reanalysis (ERA5-Land) soil moisture product, in situ soil moisture dataset from the International Soil Moisture Network (ISMN), and auxiliary datasets (Multi-Error-Removed Improved-Terrain (MERIT) DEM and Global gridded soil information (SoilGrids)). Given the relatively large-scale differences between point-scale in situ measurements and other datasets, the triple collocation (TC) method was adopted to select the representative soil moisture stations and their measurements for creating the training samples. To fully evaluate the model performance, three validation strategies were explored: random, site independent, and year independent. Results showed that although the XGBoost model achieved the highest accuracy on the random test samples, it was clearly a result of model overfitting. Meanwhile, training the model with representative stations selected by the TC method could considerably improve its performance for site- or year-independent test samples. The overall validation accuracy of the model trained using representative stations on the site-independent test samples, which was least likely to be overfitted, was a correlation coefficient (R) of 0.715 and root mean square error (RMSE) of 0.079m3m-3. Moreover, compared to the model developed without station filtering, the validation accuracies of the model trained with representative stations improved significantly for most stations, with the median R and unbiased RMSE (ubRMSE) of the model for each station increasing from 0.64 to 0.74 and decreasing from 0.055 to 0.052m3m-3, respectively. Further validation of the GLASS SM product across four independent soil moisture networks revealed its ability to capture the temporal dynamics of measured soil moisture (R=0.69-0.89; ubRMSE = 0.033-0.048m3m-3). Lastly, the intercomparison between the GLASS SM product and two global microwave soil moisture datasets - the 1km Soil Moisture Active Passive/Sentinel-1 L2 Radiometer/Radar soil moisture product and the European Space Agency Climate Change Initiative combined soil moisture product at 0.25- indicated that the derived product maintained a more complete spatial coverage and exhibited high spatiotemporal consistency with those two soil moisture products. The annual average GLASS SM dataset from 2000 to 2020 can be freely downloaded from 10.5281/zenodo.7172664 (Zhang et al., 2022a), and the complete product at daily scale is available at http://glass.umd.edu/soil_moisture/ (last access: 12 May 2023).  © 2023 Yufang Zhang et al."
169,168,25,168_homomorphic_encryption_privacy_privacypreserving,"homomorphic,encryption,privacy,privacypreserving,cryptographic,encrypted,secure,cloud,private,security","Nowadays the wide application of Convolution Neural Network(CNN) has promoted the rapid development of new intelligence fields. However, the inference of complex CNN usually includes a large number of computational operations. It is hard for the resource-limited user to complete the complicated CNN inference by his own. Privacy-preserving convolution neural network inference schemes allow the user to leverage the edge/cloud server to complete CNN inference under the condition that the privacy is protected. Nevertheless, existing schemes either require time-consuming cryptographic techniques, such as secure multi-party computing and homomorphic encryption, or fail to protect the privacy of the trained CNN model. To address this problem, we propose a novel privacy-preserving CNN inference scheme with edge-assistance. In this scheme, the privacy of the input data and the trained model is well protected and no complex cryptographic technique is involved. We blind the data to be predicted and the trained model to the edge servers, and then the edge servers calculate the most time-consuming layers. The user only deals with computationally efficient layers, fast encryption and recovery. Analysis and experimental results demonstrate that the proposed scheme is secure and efficient, which obviously saves the computational overhead of the user. © 2022 Elsevier Ltd,Cardiovascular disease supposes a substantial fraction of healthcare systems. The invisible nature of these pathologies demands solutions that enable remote monitoring and tracking. Deep Learning (DL) has arisen as a solution in many fields, and in healthcare, multiple successful applications exist for image enhancement and health outside hospitals. However, the computational requirements and the need for large-scale datasets limit DL. Thus, we often offload computation onto server infrastructure, and various Machine-Learning-as-a-Service (MLaaS) platforms emerged from this need. These enable the conduction of heavy computations in a cloud infrastructure, usually equipped with high-performance computing servers. Unfortunately, the technical barriers persist in healthcare ecosystems since sending sensitive data (e.g., medical records or personally identifiable information) to third-party servers involves privacy and security concerns with legal and ethical implications. In the scope of Deep Learning for Healthcare to improve cardiovascular health, Homomorphic Encryption (HE) is a promising tool to enable secure, private, and legal health outside hospitals. Homomorphic Encryption allows for privacy-preserving computations over encrypted data, thus preserving the privacy of the processed information. Efficient HE requires structural optimizations to perform the complex computation of the internal layers. One such optimization is Packed Homomorphic Encryption (PHE), which encodes multiple elements on a single ciphertext, allowing for efficient Single Instruction over Multiple Data (SIMD) operations. However, using PHE in DL circuits is not straightforward, and it demands new algorithms and data encoding, which existing literature has not adequately addressed. To fill this gap, in this work, we elaborate on novel algorithms to adapt the linear algebra operations of DL layers to PHE. Concretely, we focus on Convolutional Neural Networks. We provide detailed descriptions and insights into the different algorithms and efficient inter-layer data format conversion mechanisms. We formally analyze the complexity of the algorithms in terms of performance metrics and provide guidelines and recommendations for adapting architectures that deal with private data. Furthermore, we confirm the theoretical analysis with practical experimentation. Among other conclusions, we prove that our new algorithms speed up the processing of convolutional layers compared to the existing proposals. 2023 Cabrero-Holgueras and Pastrana.,Today, the rapid development of deep learning has spread across all walks of life, and it can be seen in various fields such as image classification, automatic driving, and medical imaging diagnosis. Convolution Neural Networks (CNNs) are also widely used by the public as tools for deep learning. In real life, if local customers implement large-scale model inference first, they need to upload local data to the cloud, which will cause problems such as data leakage and privacy disclosure. To solve this problem, we propose a framework using homomorphic encryption technology. Our framework has made improvements to the batch operation and reduced the complexity of layer connection. In addition, we provide a new perspective to deal with the impact of the noise caused by the homomorphic encryption scheme on the accuracy during the inference. In our scheme, users preprocess the images locally and then send them to the cloud for encrypted inference without worrying about privacy leakage during the inference process. Experiments show that our proposed scheme is safe and efficient, which provides a safe solution for users who cannot process data locally. © 2023 by the authors."
170,169,25,169_peptides_peptide_peptidebased_proteins,"peptides,peptide,peptidebased,proteins,proteomics,amino,protein,deep2pep,antimicrobial,defensin","Functional peptides are easy to absorb and have low side effects, which has attracted increasing interest from pharmaceutical scientists. However, due to the limitations in the laboratory funding and human resources, it is difficult to screen the functional peptides from a large number of peptides with unknown functions. With the development of machine learning and Deep learning, the combination of computational methods and biological information provides an effective method for identifying peptide functions. To explore the value of multi-functional active peptides, a new deep learning method named Deep2Pep (Deep learning to Peptides) was constructed, which was based on sequence encoding, embedding, and language tokenizer. It can achieve predictions of peptides on antimicrobial, antihypertensive, antioxidant and antihyperglycemic by converting sequence information into digital vectors, combined BiLSTM, attention-residual algorithm, and BERT Encoder. The results showed that Deep2Pep had a Hamming Loss of 0.095, subset Accuracy of 0.737, and Macro F1-Score of 0.734. which outperformed other models. BiLSTM played a primary role in Deep2Pep, which BERT encoder was in an auxiliary position. Deep learning algorithms was used in this study to accurately predict the four active functions of peptides, and it was expected to provide effective references for predicting multi-functional peptides. © 2024 Elsevier Ltd,Background: Deep learning’s automatic feature extraction has proven to give superior performance in many sequence classification tasks. However, deep learning models generally require a massive amount of data to train, which in the case of Hemolytic Activity Prediction of Antimicrobial Peptides creates a challenge due to the small amount of available data. Results: Three different datasets for hemolysis activity prediction of therapeutic and antimicrobial peptides are gathered and the AMPDeep pipeline is implemented for each. The result demonstrate that AMPDeep outperforms the previous works on all three datasets, including works that use physicochemical features to represent the peptides or those who solely rely on the sequence and use deep learning to learn representation for the peptides. Moreover, a combined dataset is introduced for hemolytic activity prediction to address the problem of sequence similarity in this domain. AMPDeep fine-tunes a large transformer based model on a small amount of peptides and successfully leverages the patterns learned from other protein and peptide databases to assist hemolysis activity prediction modeling. Conclusions: In this work transfer learning is leveraged to overcome the challenge of small data and a deep learning based model is successfully adopted for hemolysis activity classification of antimicrobial peptides. This model is first initialized as a protein language model which is pre-trained on masked amino acid prediction on many unlabeled protein sequences in a self-supervised manner. Having done so, the model is fine-tuned on an aggregated dataset of labeled peptides in a supervised manner to predict secretion. Through transfer learning, hyper-parameter optimization and selective fine-tuning, AMPDeep is able to achieve state-of-the-art performance on three hemolysis datasets using only the sequence of the peptides. This work assists the adoption of large sequence-based models for peptide classification and modeling tasks in a practical manner. © 2022, The Author(s).,Many bioactive peptides demonstrated therapeutic effects over complicated diseases, such as antiviral, antibacterial, anticancer, etc. It is possible to generate a large number of potentially bioactive peptides using deep learning in a manner analogous to the generation of de novo chemical compounds using the acquired bioactive peptides as a training set. Such generative techniques would be significant for drug development since peptides are much easier and cheaper to synthesize than compounds. Despite the limited availability of deep learning-based peptide-generating models, we have built an LSTM model (called LSTM_Pep) to generate de novo peptides and fine-tuned the model to generate de novo peptides with specific prospective therapeutic benefits. Remarkably, the Antimicrobial Peptide Database has been effectively utilized to generate various kinds of potential active de novo peptides. We proposed a pipeline for screening those generated peptides for a given target and used the main protease of SARS-COV-2 as a proof-of-concept. Moreover, we have developed a deep learning-based protein-peptide prediction model (DeepPep) for rapid screening of the generated peptides for the given targets. Together with the generating model, we have demonstrated that iteratively fine-tuning training, generating, and screening peptides for higher-predicted binding affinity peptides can be achieved. Our work sheds light on developing deep learning-based methods and pipelines to effectively generate and obtain bioactive peptides with a specific therapeutic effect and showcases how artificial intelligence can help discover de novo bioactive peptides that can bind to a particular target. © 2023 American Chemical Society."
171,170,25,170_questionanswering_answering_answerer_questionanswer,"questionanswering,answering,answerer,questionanswer,answers,retrieval,qa,nlp,comprehension,questions","Machine reading comprehension (MRC) is one of the most challenging tasks and active fields in natural language processing (NLP). MRC systems aim to enable a machine to understand a given context in natural language and to answer a series of questions about it. With the advent of bi-directional deep learning algorithms and large-scale datasets, MRC achieved improved results. However, these models are still suffering from two research issues: textual ambiguities and semantic vagueness to comprehend the long passages and generate answers for abstractive MRC systems. To address these issues, this paper proposes a novel Extended Generative Pretrained Transformers-based Question Answering (ExtGPT-QA) model to generate precise and relevant answers to questions about a given context. The proposed architecture comprises two modified forms of encoder and decoder as compared to GPT. The encoder uses a positional encoder to assign a unique representation with each word in the sentence for reference to address the textual ambiguities. Subsequently, the decoder module involves a multi-head attention mechanism along with affine and aggregation layers to mitigate semantic vagueness with MRC systems. Additionally, we applied syntax and semantic feature engineering techniques to enhance the effectiveness of the proposed model. To validate the proposed model's effectiveness, a comprehensive empirical analysis is carried out using three benchmark datasets including SQuAD, Wiki-QA, and News-QA. The results of the proposed ExtGPT-QA outperformed state of art MRC techniques and obtained 93.25% and 90.52% F1-score and exact match, respectively. The results confirm the effectiveness of the ExtGPT-QA model to address textual ambiguities and semantic vagueness issues in MRC systems. © 2023 Ahmed et al.,With the fast growth of information science and engineering, a large number of textual data generated are valuable for natural language processing and its applications. Particularly, finding correct answers to natural language questions or queries requires spending tremendous time and effort in human life. While using search engines to discover information, users manually determine the answer to a given question on a range of retrieved texts or documents. Question answering relies heavily on the capability to automatically comprehend questions in human language and extract meaningful answers from a single text. In recent years, such question–answering systems have become increasingly popular using machine reading comprehension techniques. On the other hand, high-resource languages (e.g., English and Chinese) have witnessed tremendous growth in question-answering methodologies based on various knowledge sources. Besides, powerful BERTology-based language models only encode texts with a limited length. The longer texts contain more distractor sentences that affect the QA system performance. Vietnamese has a variety of question words in the same question type. To address these challenges, we propose ViQAS, a new question–answering system with multi-stage transfer learning using language models based on BERTology for a low-resource language such as Vietnamese. Last but not least, our QA system is integrated with Vietnamese characteristics and transformer-based evidence extraction techniques into an effective contextualized language model-based QA system. As a result, our proposed system outperforms our forty retriever-reader QA configurations and seven state-of-the-art QA systems such as DrQA, BERTserini, BERTBM25, XLMRQA, ORQA, COBERT, and NeuralQA on three Vietnamese benchmark question answering datasets. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,A man-made machine-reading comprehension (MRC) dataset is necessary to train the answer extraction part of existing Question Answering (QA) systems. However, a high-quality and well-structured dataset with question-paragraph-answer pairs is not usually found in the real world. Furthermore, updating or building an MRC dataset is a challenging and costly affair. To address these shortcomings, we propose a QA system that uses a large-scale English Community Question Answering (CQA) dataset (i.e., Stack Exchange) composed of 3,081,834 question-answer pairs. The QA system adopts a classifier-retriever-summarizer structure design. The question classifier and the answer retriever part are based on a Bidirectional Encoder Representations from Transformers (BERT) Natural Language Processing (NLP) model by Google, and the summarizer part introduces a deep learning-based Text-to-Text Transfer Transformer (T5) model to summarize the long answers. We instantiated the proposed QA system with 140 topics from the CQA dataset (including topics such as biology, law, politics, etc.) and conducted human and automatic evaluations. Our system presented encouraging results, considering that it provides high-quality answers to the questions in the test set and satisfied the requirements to develop a QA system without MRC datasets. Our results show the potential of building automatic and high-performance QA systems without being limited by man-made datasets, a significant step forward in the research of open-domain or specific-domain QA systems. © 2023 Elsevier B.V."
172,171,25,171_forecasting_forecast_inventory_prediction,"forecasting,forecast,inventory,prediction,lstm,sales,learning,regression,predict,retailers","Continued global economic instability and uncertainty is causing difficulties in predicting sales. As a result, many sectors and decision-makers are facing new, pressing challenges. In supply chain management, the food industry is a key sector in which sales movement and the demand forecasting for food products are more difficult to predict. Accurate sales forecasting helps to minimize stored and expired items across individual stores and, thus, reduces the potential loss of these expired products. To help food companies adapt to rapid changes and manage their supply chain more effectively, it is a necessary to utilize machine learning (ML) approaches because of ML’s ability to process and evaluate large amounts of data efficiently. This research compares two forecasting models for confectionery products from one of the largest distribution companies in Saudi Arabia in order to improve the company’s ability to predict demand for their products using machine learning algorithms. To achieve this goal, Support Vectors Machine (SVM) and Long Short-Term Memory (LSTM) algorithms were utilized. In addition, the models were evaluated based on their performance in forecasting quarterly time series. Both algorithms provided strong results when measured against the demand forecasting model, but overall the LSTM outperformed the SVM. © 2023,International Journal of Advanced Computer Science and Applications. All Rights Reserved.,To address the problem of the large subjective error of expert evaluation methods in supply chain management, the supply chain system is comprehensively analyzed, and a deep learning backpropagation (BP) neural network-based supply chain risk assessment model is constructed. First, the basic theories of supply chain and risk assessment are described, and the process of supply chain risk management is explained. Then, the ANN (artificial neural network) is discussed in detail. On this basis, the feasibility of the BP neural network applied in the risk assessment of the supply chain is analyzed. In addition, the risks of the supply chain system are analyzed under the support of the Internet of Things (IoT), and the indices for risk assessment of the supply chain are determined. The reliability analysis, validity analysis, and factor analysis of the evaluation indices are implemented using a questionnaire survey, based on which the risk assessment indices of the supply chain are determined as 7 first-level indices and 20 sesond-level indices. Finally, a BP neural network-based supply chain risk assessment model is established, and the simulation results are analyzed in MATLAB. The maximum relative error of the proposed BP neural network model for supply chain risk assessment is as low as 0.03076923%, and that calculated by the AHP (analytic hierarchy process) is 57.41%. Compared with that of AHP, the fitting degree of the BP neural network-based supply chain risk assessment model is much higher. Meanwhile, the simulation experiment indicates that the established risk assessment model has strong generalization ability and learning ability. This work not only provides technical support for the development of remanufacturing closed-loop supply chain systems but also contributes to the improvement of the accuracy of supply chain risk assessment. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The purposes are to improve the accuracy of inventory demand forecast, balance the indexes of enterprises, and reduce the costs of human, material and financial resources of enterprises and suppliers, thus reducing the supply chain costs and meeting the actual needs of enterprises. In terms of training a large amount of data, deep learning is better than traditional machine learning. The sales demand time series data and the previous material demand time series data are input and trained by back propagation (BP) neural network, and then the material demand value is output. Therefore, the historical data of sales demand forecast and material information are input, and the model is established by BP neural network, which not only takes into account the decisive factor of sales demand forecast, but also considers the material consumption, achieving more accurate forecast. The material demand budget of enterprises is analyzed and a material forecast demand model based on deep learning algorithm is proposed. The model uses a neural network to input the sales demand forecast data, material inventory information and material attribute information into the model, and then the model is trained by the training set in accordance with the error back propagation algorithm. Finally, the training effect of the model is tested by the test set. The results show that when the independent variables include sales demand forecast, material consumption forecast and material attribute information, the forecast error of both models is lower and the effect is better, compared with the material consumption data only as an independent variable. The forecast method based on neural network proposed increases the lead time of the forecast, give the supplier a longer time to prepare goods, and reduce the shortage or surplus of supply caused by the short lead time. Therefore, the material demand forecast model based on convolution neural network (CNN) algorithm provides an important reference for the enterprises, helps them improve their work efficiency and promotes the development of enterprises. This model achieves a great improvement on the accuracy of material demand forecast, and has a certain guiding significance in relevant theory and practice. © 2021, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden."
173,172,24,172_renewable_optimization_solar_hydropower,"renewable,optimization,solar,hydropower,electricity,energy,scheduling,generators,battery,grid","Using renewable energies such as wind and solar energy, two types of renewable energy, we may adjust the structure of the energy system, addressing both energy and environmental challenges at the same time. As a result of its impact on the environment, wind and solar energy generation are inherently unreliable sources of energy. Lithium batteries, a technology that is becoming increasingly mature in terms of energy storage, are a critical component of the answer to the problem of instability. In order to avoid waste and expense increases, the capacity should not be too large or too small, respectively. Power consumption restricts the amount of energy that may be stored, but industrial power usage is unpredictable and non-periodic. This is a significant task that needs the development of a model that can dispatch while still providing a reasonable amount of storage. In this paper, we develop a KNN classification model that considers the test cyclic of photovoltaic (PV) generation that includes battery installation, data on electricity consumption and data on PV generation in India. These metrics are used to develop an energy management model. The model aims at the reduction of operation cost and optimal storage of energy that should satisfy the grid demands. The results of simulation and the comparison of the theoretical results shows that the proposed model has higher optimisation of energy in the storage devices in case of distributed systems. © 2022,Future electric grids will face severe uncertainties with the unprecedented penetration of renewable energy sources, which may cause problems in grid exploitation. It is essential to evaluate the uncertainty of system performance in this grid; therefore, traditional exploitation methods may not be suitable for distributions grid such as multi-carrier microgrids when considering the electricity and gas grid constraints. This paper proposes an effective method for simultaneously optimizing different types of energy infrastructure in an environment with various uncertainties while considering grid constraints. To address the multi-energy synergy supply optimization problem in the micro energy grid, a coordinated two-stage programming approach is also suggested. A day-ahead and real-time phases are separated in the scheduling cycle to overcome the effects of the unpredictability of wind energy and solar power. The findings of the day-ahead forecast are then taken into account as uncertain variables for the higher-layer model. The demand response programming model and the energy storage revision model are lower-layer models considering the actual value as the realization of uncertain variables. The competitive swarm optimization algorithm is used to solve the problem mentioned above The Cumulative search optimization (CSO) uses global and local systems in particle swarm optimization and adopts a novel learning mechanism for creating competition between particle pairs. Comprehensive numeric examinations show that convergence speed and precision are critical, especially in solving large-scale problems. Therefore, we use chaos theory to improve its performance. Finally, the proposed method is tested and discussed on a system. The results showed that: (1) power-to-gas could convert extra electricity into natural gas; (2) the proposed two-stage optimization model and algorithm achieved different energy optimizations; (3) price-based demand response (DR) can level the energy load curve and maximize multi energy grid (MEG) revenue by using complementary specifications of energy price; and (4) the micro energy grid could communicate with the high-degree energy grid in order to gain more economic benefits. Accordingly, the incentive-based demand response (IBDR) output is reduced. The electricity and heating provided by convention gas turbine (CGT) is increased, which is fed to upper power grid (UPG) and upper heating grid (UHG). Since MEG's purchasing power from UPG during the peak period generates more total revenue than island operations. Based on the obtained results, it can be seen that the proposed algorithm was about 50% faster compared to other methods and its standard deviation was about 0.0013 with different implementations, which was much better compared to other models. © 2023,Battery storage is an important factor for power systems made up of renewable energy sources. Technologies for battery storage are crucial to accelerating the transition from fossil fuels to renewable energy. Between responding to electricity demand and using renewable energy sources, battery storage devices will become increasingly important. The aim of this study is to examine how battery storage affects a power system consisting of solar and hydroelectric energy and to draw conclusions about whether energy storage recommends a power system. The method involves designing a model of eight real cascade hydropower power plants and solving an optimization problem. This power system model is based on existing hydroelectric power plants powered by solar energy and batteries in the Turkish cities of Yozgat and Tokat. A case study with four different battery capacities in the system was carried out to assess the implications of energy storage in the power system. The stochastic nonlinear optimization problem was modeled for 72 h and solved with the MATLAB programming tool. The stochastic Quasi-Newton method performs very well in hybrid renewable problems arising from large-scale machine learning. When solar energy and batteries were added to the system, the maximum installed wind power was found to be 2 MW and 3.6 MW, respectively. In terms of profit and hydropower planning, a medium-proportion battery was found to be the most suitable. Increased variability in hydropower generation results from the installation of an energy storage system. © 2023 by the author."
174,173,24,173_parkinsonian_parkinsons_parkinsonism_tremor,"parkinsonian,parkinsons,parkinsonism,tremor,tremors,classifier,inceptiontime,neuroimaging,classify,neurological","Parkinson's disease (PD) is a common neurodegenerative movement disorder among older individuals. As one of the typical symptoms of PD, tremor is a critical reference in the PD assessment. A widely accepted clinical approach to assessing tremors in PD is based on part III of the Movement Disorder Society-Unified Parkinson's Disease Rating Scale (MDS-UPDRS). However, expert assessment of tremor is a time-consuming and laborious process that poses considerable challenges to the medical evaluation of PD. In this paper, we proposed a novel model, Global Temporal-difference Shift Network (GTSN), to estimate the MDS-UPDRS score of PD tremors based on video. The PD tremor videos were scored according to the majority vote of multiple raters. We used Eulerian Video Magnification (EVM) pre-processing to enhance the representations of subtle PD tremors in the videos. To make the model better focus on the tremors in the video, we proposed a special temporal difference module, which stacks the current optical flow to the result of inter-frame difference. The prediction scores were obtained from the Residual Networks (ResNet) embedded with a novel module, the Global Shift Module (GSM), which allowed the features of the current segment to include the global segment features. We carried out independent experiments using PD tremor videos of different body parts based on the scoring content of the MDS-UPDRS. On a fairly large dataset, our method achieved an accuracy of 90.6% for hands with rest tremors, 85.9% for tremors in the leg, and 89.0% for the jaw. An accuracy of 84.9% was obtained for postural tremors. Our study demonstrated the effectiveness of computer-assisted assessment for PD tremors based on video analysis. The latest version of the code is available at https://github.com/199507284711/PD-GTSN. © 2023 Elsevier B.V.,Although many studies have been conducted on machine learning (ML) models for Parkinson’s disease (PD) prediction using neuroimaging and movement analyses, studies with large population-based datasets are limited. We aimed to propose PD prediction models using ML algorithms based on the National Health Insurance Service-Health Screening datasets. We selected individuals who participated in national health-screening programs > 5 times between 2002 and 2015. PD was defined based on the ICD-code (G20), and a matched cohort of individuals without PD was selected using a 1:1 random sampling method. Various ML algorithms were applied for PD prediction, and the performance of the prediction models was compared. Neural networks, gradient boosting machines, and random forest algorithms exhibited the best average prediction accuracy (average area under the receiver operating characteristic curve (AUC): 0.779, 0.766, and 0.731, respectively) among the algorithms validated in this study. The overall model performance metrics were higher in men than in women (AUC: 0.742 and 0.729, respectively). The most important factor for predicting PD occurrence was body mass index, followed by total cholesterol, glucose, hemoglobin, and blood pressure levels. Smoking and alcohol consumption (in men) and socioeconomic status, physical activity, and diabetes mellitus (in women) were highly correlated with the occurrence of PD. The proposed health-screening dataset-based PD prediction model using ML algorithms is readily applicable, produces validated results, and could be a useful option for PD prediction models. © 2022, The Author(s).,Background: Since both essential tremor (ET) and Parkinson’s disease (PD) are movement disorders and share similar clinical symptoms, it is very difficult to recognize the differences in the presentation, course, and treatment of ET and PD, which leads to misdiagnosed commonly. Purpose: Although neuroimaging biomarker of ET and PD has been investigated based on statistical analysis, it is unable to assist the clinical diagnosis of ET and PD and ensure the efficiency of these biomarkers. The aim of the study was to identify the neuroimaging biomarkers of ET and PD based on structural magnetic resonance imaging (MRI). Moreover, the study also distinguished ET from PD via these biomarkers to validate their classification performance. Methods: This study has developed and implemented a three-level machine learning framework to identify and distinguish ET and PD. First of all, at the model-level assessment, the searchlight-based machine learning method has been used to identify the group differences of patients (ET/PD) with normal controls (NCs). And then, at the feature-level assessment, the stability of group differences has been tested based on structural brain atlas separately using the permutation test to identify the robust neuroimaging biomarkers. Furthermore, the identified biomarkers of ET and PD have been applied to classify ET from PD based on machine learning techniques. Finally, the identified biomarkers have been compared with the previous findings of the biology-level assessment. Results: According to the biomarkers identified by machine learning, this study has found widespread alterations of gray matter (GM) for ET and large overlap between ET and PD and achieved superior classification performance (PCA + SVM, accuracy = 100%). Conclusions: This study has demonstrated the significance of a machine learning framework to identify and distinguish ET and PD. Future studies using a large data set are needed to confirm the potential clinical application of machine learning techniques to discern between PD and ET. © 2022, The Author(s)."
175,174,24,174_dairy_milk_cow_cows,"dairy,milk,cow,cows,lactating,prediction,meat,infrared,feedlot,physicochemical","In the beverages industry, milk foaming is done to enhance the flavor, texture, and visual appeal of milk-based beverages. It is very crucial to study milk foam properties not just to create visually appealing and rich in taste beverages but also to estimate the adulterants present in it. Machine learning is being used in every field nowadays as it can analyze large datasets quickly and help in making data-driven decisions. This paper is a demonstration of how a futuristic apparatus will detect the best type of milk for beverages and identify milk adulteration using machine learning. In the current study, machine learning methods are employed to assess milk foam properties. This study aims to choose the best type of milk for foam-based milk beverages preparations and detect surfactants often used in low concentrations for foaming but act as adulterants at high concentrations. Surfactants alter the foaming properties of milk in different ways depending on their charge and are therefore used in the dairy industry. By using machine learning techniques, the impact of three different surfactants, having distinct ionic properties, on three distinct types of milk have been analyzed. It was found that foaming properties of milk were highly correlated to each other. “Random forest classifier” turned out to be the most effective among all the machine learning models in both the tasks. Heating and addition of sodium dodecyl sulfate (SDS) improved foaming. The findings of this study can be used for deriving valuable insights about the dairy industry. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Cheese whey addition to milk is a type of fraud with high prevalence and severe economic effects, resulting in low yield for dairy products, nutritional reduction of milk and milk-derived products, and even some safety concerns. Nevertheless, methods to detect fraudulent addition of cheese whey to milk are expensive and time consuming, and are thus ineffective as screening methods. The Fourier-transform infrared (FTIR) spectroscopy technique is a promising alternative to identify this type of fraud because a large number of data are generated, and useful information might be extracted to be used by machine learning models. The objective of this work was to evaluate the use of FTIR with machine learning methods, such as classification tree and multilayer perceptron neural networks to detect the addition of cheese whey to milk. A total of 520 samples of raw milk were added with cheese whey in concentrations of 1, 2, 5, 10, 15, 20, 25, and 30%; and 65 samples were used as control. The samples were stored at 7, 20, and 30°C for 0, 24, 48, 72, and 168 h, and analyzed using FTIR equipment. Complementary results of 520 samples of authentic raw milk were used. Selected components (fat, protein, casein, lactose, total solids, and solids nonfat) and freezing point (°C) were predicted using FTIR and then used as input features for the machine learning algorithms. Performance metrics included accuracy as high as 96.2% for CART (classification and regression trees) and 97.8% for multilayer perceptron neural networks, with precision, sensitivity, and specificity above 95% for both methods. The use of milk composition and freezing point predicted using FTIR, associated with machine learning techniques, was highly efficient to differentiate authentic milk from samples added with cheese whey. The results indicate that this is a potential method to be used as a high-performance screening process to detected milk adulterated with cheese whey in milk quality laboratories. © 2022 American Dairy Science Association,In this research communication we compare three different approaches for developing dry matter intake (DMI) prediction models based on milk mid-infrared spectra (MIRS), using data collected from a research herd over five years. In dairy production, knowledge of individual DMI could be important and useful, but DMI can be difficult and expensive to measure on most commercial farms as cows are commonly group-fed. Instead, this parameter is often estimated based on the age, body weight, stage of lactation and body condition score of the cow. Recently, milk MIRS have also been used as a tool to estimate DMI. There are different methods available to create prediction models from large datasets. The main data used were total DMI calculated as a 3-d average, coupled with milk MIRS data available fortnightly. Data on milk yield and lactation stage parameters were also available for each animal. We compared the performance of three prediction approaches: partial least-squares regression, support vector machine regression and random forest regression. The full milk MIRS alone gave low to moderate prediction accuracy (R2 = 0.07-0.40), regardless of prediction modelling approach. Adding more variables to the model improved R2 and decreased the prediction error. Overall, partial least-squares regression proved to be the best method for predicting DMI from milk MIRS data, while MIRS data together with milk yield and concentrate DMI at 3-30 d in milk provided good prediction accuracy (R2 = 0.52-0.65) regardless of the prediction tool used.  Copyright © The Author(s), 2023. Published by Cambridge University Press on behalf of Hannah Dairy Research Foundation."
176,175,24,175_registration_deeplearningbased_multimodal_images,"registration,deeplearningbased,multimodal,images,deformation,learningbased,voxel,deformations,mris,deep","Learning-based image registration approaches typically learn to map from input images to a transformation matrix. Regarding the current deep-learning-based image rigid registration approaches learn a transformation matrix in a one-shot way. Our purpose is to present a deep reinforcement learning (DRL) based method for image registration to explicitly model the step-wise nature of the human registration process. We cast an image registration process as a Markov Decision Process (MDP) where actions are defined as global image adjustment operations. Then we train our proxy to learn the optimal action sequences to achieve a good registration. More specifically, we propose a DRL proxy incorporating an attention mechanism to address the challenge of large differences in appearance between images from different modalities. Registration experiments on 3D CT-MR image pairs of patients with nasopharyngeal carcinoma and on publicly available 3D PET-MR image pairs show that our approach significantly outperforms other methods, and achieves state-of-the-art performance in multi-m-modal medical image registration.  © 2013 IEEE.,In recent years, deep learning (DL)-based registration technology has significantly improved the calculation speed of medical image registration. Existing DL-based registration methods generally use raw data features to predict the deformation field. However, this strategy may not be very effective for difficult registration tasks. Hence, in this study, we propose a similarity attention-based convolutional neural network (CNN) for accurate and robust three-dimensional medical image registration. We first introduce a similarity-based local attention model as an auxiliary module for building a displacement searching space, instead of a direct displacement prediction based on raw data. The proposed model can help the network focus on spatial correspondences with high similarities and ignore those with low similarities. A multi-scale CNN is then integrated with the similarity-based local attention for providing non-local attention, lightweight network, and coarse-to-fine registration. We evaluated the proposed method for various applications, such as the registration of large-scope abdominal computerized tomography (CT) images and chest CT images acquired at different respiratory phases, and atlas registration in magnetic resonance imaging. The experimental results demonstrate that the proposed method can provide a more accurate and robust registration performance than state-of-the-art registration methods. © 2022 Elsevier Ltd,Medical image registration is an essential task in researching and applying medical images. Doctors can observe and extract relevant pathological features to quickly analyze the disease by registered images to diagnose the infection. After more than ten years of research and development, medical image registration has achieved good research results in traditional and deep learning methods. However, most existing methods only focus on unidirectional medical image registration research and rarely consider bidirectional medical image registration research. This paper proposes a new, unsupervised bidirectional medical image registration method based on this aspect. This method guarantees the registration effect in the forward and reverses directions and adds a cascade connection-based channel attention network to the registration model to enable better automatic learning of the registration model, optimizes feature weights, and extracts essential information from images to improve registration performance. We verified the effectiveness of our method by conducting experiments on large-scale 3D brain MRI images and achieved a comparable registration speed and effect with most existing medical image registration methods. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
177,176,24,176_audio_microphone_acoustic_microphones,"audio,microphone,acoustic,microphones,auditory,soundscapes,polyphonic,hearing,sound,neural","Noise exposure influences the comfort and well-being of people in several contexts, such as work or learning environments. For instance, in offices, different kind of noises can increase or drop the employees' productivity. Thus, the ability of separating sound sources in real contexts plays a key role in assessing sound environments. Long-term monitoring provide large amounts of data that can be analyzed through machine and deep learning algorithms. Based on previous works, an entire working day was recorded through a sound level meter. Both sound pressure levels and the digital audio recording were collected. Then, a dual clustering analysis was carried out to separate the two main sound sources experienced by workers: traffic and speech noises. The first method exploited the occurrences of sound pressure levels via Gaussian mixture model and K-means clustering. The second analysis performed a semi-supervised deep clustering analyzing the latent space of a variational autoencoder. Results show that both approaches were able to separate the sound sources. Spectral matching and the latent space of the variational autoencoder validated the assumptions underlying the proposed clustering methods.  © 2023 Acoustical Society of America.,Deep learning can be used for audio signal classification in a variety of ways. It can be used to detect and classify various types of audio signals such as speech, music, and environmental sounds. Deep learning models are able to learn complex patterns of audio signals and can be trained on large datasets to achieve high accuracy. To employ deep learning for audio signal classification, the audio signal must first be represented in a suitable form. This can be done using signal representation techniques such as using spectrograms, Mel-frequency Cepstral coefficients, linear predictive coding, and wavelet decomposition. Once the audio signal is represented in a suitable form, it can then be fed into a deep learning model. Various deep learning models can be utilized for audio classification. We provide an extensive survey of current deep learning models used for a variety of audio classification tasks. In particular, we focus on works published under five different deep neural network architectures, namely Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Autoencoders, Transformers and Hybrid Models (hybrid deep learning models and hybrid deep learning models with traditional classifiers). CNNs can be used to classify audio signals into different categories such as speech, music, and environmental sounds. They can also be used for speech recognition, speaker identification, and emotion recognition. RNNs are widely used for audio classification and audio segmentation. RNN models can capture temporal patterns of audio signals and be used to classify audio segments into different categories. Another approach is to use autoencoders for learning the features of audio signals and then classifying the signals into different categories. Transformers are also well-suited for audio classification. In particular, temporal and frequency features can be extracted to identify the characteristics of the audio signals. Finally, hybrid models for audio classification either combine various deep learning architectures (i.e. CNN-RNN) or combine deep learning models with traditional machine learning techniques (i.e. CNN-Support Vector Machine). These hybrid models take advantage of the strengths of different architectures while avoiding their weaknesses. Existing literature under different categories of deep learning are summarized and compared in detail. © 2013 IEEE.,Sound is one of the primary forms of sensory information that we use to perceive our surroundings. Usually, a sound event is a sequence of an audio clip obtained from an action. The action can be rhythm patterns, music genre, people speaking for a few seconds, etc. The sound event classification address distinguishes what kind of audio clip it is from the given audio sequence. Nowadays, it is a common issue to solve in the following pipeline: audio pre-processing?perceptual feature extraction?classification algorithm. In this paper, we improve the traditional sound event classification algorithm to identify unknown sound events by using the deep learning method. The compact cluster structure in the feature space for known classes helps recognize unknown classes by allowing large room to locate unknown samples in the embedded feature space. Based on this concept, we applied center loss and supervised contrastive loss to optimize the model. The center loss tries to minimize the intra- class distance by pulling the embedded feature into the cluster center, while the contrastive loss disperses the inter-class features from one another. In addition, we explored the performance of self-supervised learning in detecting unknown sound events. The experimental results demonstrate that our proposed open-set sound event classification algorithm and self-supervised learning approach achieve sustained performance improvements in various datasets. © 2024, The Author(s)."
178,177,24,177_supervised_classification_classifiers_classifier,"supervised,classification,classifiers,classifier,labeling,labeled,learning,datasets,annotation,label","Active learning is an important technology to solve the lack of data in crack detection model training. However, the sampling strategies of most existing active learning methods for crack detection are based on the uncertainty or representation of the samples, which cannot effectively balance the exploitation and exploration of active learning. To solve this problem, this study proposes an active learning method for crack detection based on subset searching and weighted sampling. First, a new active learning framework is established to successively search subsets with large uncertainty from the candidate dataset, and select training samples with large diversity from the subsets to update the crack detection model. Second, to realize the active learning process, a subset searching method based on sample relative error is proposed to adaptively select subsets with large uncertainty, and a weighted sampling method based on flow-based deep generative network is introduced to select training samples with large diversity form the subsets. Third, a termination criterion for active learning directly based on the prediction accuracy of the trained model is proposed to adaptively determine the maximum number of iterations. Finally, the proposed method is tested using two open-source crack datasets. The experimental comparison results on the Bridge Crack Library dataset show that the proposed method has higher calculation efficiency and prediction accuracy in crack detection than the uncertainty-based and representation-based active learning methods. The test results on the DeepCrack dataset show that the crack detection model trained by the proposed method has good transferability on different datasets with multi-scale concrete cracks and scenes. © The Author(s) 2023.,Active learning is usually used in scenarios where few labels are available and manual labeling is expensive. To improve model performance, it is necessary to find the most valuable instance among all instances and label it to maximize the benefits of labeling. In practical scenarios, it is often more efficient to query a group of instances instead of a individual instance during each iteration. To achieve this goal, we need to explore the similarities between instances to ensure the informativeness and diversity. Many ad-hoc algorithms are proposed for batch mode active learning, and there are generally two major issues. One is that similarity measurement among instances often only relies on the expression of features but it is not well integrated with the classification algorithm model. This will cut down the precise measurement of diversity. The other is that in order to explore the decision boundary, these algorithms often choose the instance near the boundary. It is difficult to get the true boundary when there are few labeled instances. As a large number of instances continue to be labeled, information between instances is less used, and the performance will be greatly improved if it is properly used. In our work, we propose an adaptive algorithm based on deep neural networks to solve the two problems mentioned above. During the training phase, we established a paired network to improve the accuracy of the classification model, and the network can project the instance to a new feature space for more accurate similarity measurement. When batch labeling instances, we use the adaptive algorithm to select the instance by balancing the maximum uncertainty (exploration) and diversity (exploitation). Our algorithm has been validated for heart failure prediction tasks in real-world EHR datasets. Due to the no public of EHR data, we also conducted validation on two other classic classification tasks. Our algorithm is superior to the baseline method in both accuracy and convergence rate. © 2023,Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. However, the growing availability of data streams has led to an increase in the number of approaches that focus on online active learning, which involves continuously selecting and labeling observations as they arrive in a stream. This work aims to provide an overview of the most recently proposed approaches for selecting the most informative observations from data streams in real time. We review the various techniques that have been proposed and discuss their strengths and limitations, as well as the challenges and opportunities that exist in this area of research. © 2023, The Author(s)."
179,178,24,178_forecasting_fuzzy_timeseries_fcms,"forecasting,fuzzy,timeseries,fcms,fcm,learning,neurofuzzy,cognitive,map,maps","As a soft computing method, applying fuzzy cognitive map (FCM) to time series prediction has become a timely issue pursued by numerous researchers. Although many FCM construction methods have emerged, most of them exhibit obvious limitations in weight learning especially for long-term or complex time series. Either the weight calculation is computationally expensive, or it cannot achieve gratifying accuracy. In this paper, a new method for constructing FCM is proposed which extracts concepts from data by exploiting triangular membership function, and the weights of high-order FCM are subtly obtained by transforming the learning problem of FCM into a convex optimization problem with constraints. Since then, FCM with optimized weights is used to represent fuzzy logical relationships of time series and implement prediction further. Fifteen benchmark time series,such as Soybean Price time series, Yahoo stock time series, Condition monitoring of hydraulic systems time series etc. are applied to verify prediction performance of the proposed method. Accordingly, experiment results show that the proposed numerical prediction method of time series is effective and can acquire better prediction accuracy with lower computation time than other recent advanced methods. In addition, the influence of parameters of the method is analyzed individually.  © 2013 IEEE.,Complex fuzzy sets are an extension of type-1 fuzzy sets with complex-valued membership functions. Over the last 20 years, time-series forecasting has emerged as the most important application of complex fuzzy sets, with neuro-fuzzy systems employing them shown to be accurate and compact forecasting models. In the complex fuzzy sets literature, two dominant approaches to designing forecasters can be observed: sinusoidal membership functions versus complex-valued Gaussian membership functions. To date, however, there has never been a systematic investigation that compares the performance of these two membership types (or their combination) within a common architecture. We propose a new neuro-fuzzy architecture using complex fuzzy sets that has been designed for large-scale learning problems. This architecture employs randomized learning to speed up network training. In designing this architecture, we empirically compared sinusoidal complex fuzzy sets and complex Gaussian fuzzy sets. Across multiple variations of the architecture, we find that the complex Gaussian fuzzy sets lead to significantly more accurate forecasts on moderate-to-large time series datasets, while still keeping the overall size of the network compact. © 2023 Elsevier B.V.,Among various soft computing approaches for time series forecasting, fuzzy cognitive maps (FCMs) have shown remarkable results as a tool to model and analyze the dynamics of complex systems. FCMs have similarities to recurrent neural networks and can be classified as a neuro-fuzzy method. In other words, FCMs are a mixture of fuzzy logic, neural network, and expert system aspects, which act as a powerful tool for simulating and studying the dynamic behavior of complex systems. The most interesting features are knowledge interpretability, dynamic characteristics and learning capability. The goal of this survey paper is mainly to present an overview on the most relevant and recent FCM-based time series forecasting models proposed in the literature. In addition, this article considers an introduction on the fundamentals of FCM model and learning methodologies. Also, this survey provides some ideas for future research to enhance the capabilities of FCM in order to cover some challenges in the real-world experiments such as handling non-stationary data and scalability issues. Moreover, equipping FCMs with fast learning algorithms is one of the major concerns in this area. © 2022, The Author(s), under exclusive licence to Springer Nature B.V."
180,179,24,179_steganography_steganographic_steganalysis_steganalytic,"steganography,steganographic,steganalysis,steganalytic,steganalyzers,jpeg,watermarking,hiding,watermark,hide","Stochastic computing is a relatively new approach to computing that has gained interest in recent years due to its potential for low-power and high-noise environments. It is a method of computing that uses probability to represent and manipulate data, therefore it has applications in areas such as signal processing, machine learning, and cryptography. Stochastic steganography involves hiding a message within a cover image using a statistical model. Unlike traditional steganography techniques that use deterministic algorithms to embed the message, stochastic steganography uses a probabilistic approach to hide the message in a way that makes it difficult for an adversary to detect. Due to this error robustness and large bit streams stochastic computing, they are well suited for high capacity and secure image steganography. In this paper, as per the authors’ best knowledge, image steganography using stochastic computing based on linear feedback shift register (LFSR) is proposed for the first time. In the proposed technique, the cover image is converted to stochastic representation instead of the binary one, and then a secret image is embedded in it. The resulting stego image has a high PSNR value transmitted with no visual trace of the hidden image. The final results are stego image with PSNR starting from 30 dB and a maximum payload up to 40 bits per pixel (bpp) with an effective payload up to 28 bpp. The proposed method achieves high security and high capability of the number of stored bits in each pixel. Thus, the proposed method can prove a vital solution for high capacity and secure image steganography, which can then be extended to other types of steganography. © 2024, The Author(s).,With the rapid proliferation of urbanization, massive data in social networks are collected and aggregated in real time, making it possible for criminals to use images as a cover to spread secret information on the Internet. How to determine whether these images contain secret information is a huge challenge for multimedia computing security. The steganalysis method based on deep learning can effectively judge whether the pictures transmitted on the Internet in urban scenes contain secret information, which is of great significance to safeguarding national and social security. Image steganalysis based on deep learning has powerful learning ability and classification ability, and its detection accuracy of steganography images has surpassed that of traditional steganalysis based on manual feature extraction. In recent years, it has become a hot topic of the information hiding technology. However, the detection accuracy of existing deep learning based steganalysis methods still needs to be improved, especially when detecting arbitrary-size and multi-source images, their detection efficientness is easily affected by cover mismatch. In this manuscript, we propose a steganalysis method based on Inverse Residuals structured Siamese network (abbreviated as SiaIRNet method, Siamese-Inverted-Residuals-Network Based method). The SiaIRNet method uses a siamese convolutional neural network (CNN) to obtain the residual features of subgraphs, including three stages of preprocessing, feature extraction, and classification. Firstly, a preprocessing layer with high-pass filters combined with depth-wise separable convolution is designed to more accurately capture the correlation of residuals between feature channels, which can help capture rich and effective residual features. Then, a feature extraction layer based on the Inverse Residuals structure is proposed, which improves the ability of the model to obtain residual features by expanding channels and reusing features. Finally, a fully connected layer is used to classify the cover image and the stego image features. Utilizing three general datasets, BossBase-1.01, BOWS2, and ALASKA#2, as cover images, a large number of experiments are conducted comparing with the state-of-the-art steganalysis methods. The experimental results show that compared with the classical SID method and the latest SiaStegNet method, the detection accuracy of the proposed method for 15 arbitrary-size images is improved by 15.96% and 5.86% on average, respectively, which verifies the higher detection accuracy and better adaptability of the proposed method to multi-source and arbitrary-size images in urban scenes. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.,Because the JPEG recompression in social networks changes the DCT coefficients of uploaded images, applying image steganography in popular image-sharing social networks requires robustness. Currently, most robust steganography algorithms rely on the resistance of embedding to the general JPEG recompression process. The operations in a specific compression channel are usually ignored, which reduces the robustness performance. Besides, to acquire the robust cover image, the state-of-the-art robust steganography needs to upload the cover image to social networks several times, which may be insecure regarding behavior security. In this paper, a robust steganography method based on the softmax outputs of a trained classifier and protocol message embedding is proposed. In the proposed method, a deep learning-based robustness classifier is trained to model the specific process of the JPEG recompression channel. The prediction result of the classifier is used to select the robust DCT blocks to form the embedding domain. The selection information is embedded as the protocol messages into the middle-frequency coefficients of DCT blocks. To further improve the recovery possibility of the protocol message, a robustness enhancement method is proposed. It decreases the predicted non-robust possibility of the robustness classifier by modifying low-frequency coefficients of DCT blocks. The experimental results show that the proposed method has better robustness performance compared with state-of-the-art robust steganography and does not have the disadvantage regarding behavior security. The method is universal and can be implemented in different JPEG compression channels after fine-tuning the classifier. Moreover, it has better security performance compared with the state-of-the-art method when embedding large-sized secret messages. © 2023, The Author(s)."
181,180,24,180_smartwatch_smartwatches_wearable_accelerometermeasured,"smartwatch,smartwatches,wearable,accelerometermeasured,accelerometer,biomarkers,weartime,vigilance,fitness,monitoring","Importance: Higher physical activity levels are associated with lower risks of cancer, cardiovascular disease, and diabetes, but associations with many common and less severe health conditions are not known. These conditions impose large health care burdens and reduce quality of life. Objectives: To investigate the association between accelerometer-measured physical activity and the subsequent risk of hospitalization for 25 common reasons for hospitalization and to estimate the proportion of these hospitalizations that might have been prevented if participants had higher levels of physical activity. Design, Setting, and Participants: This prospective cohort study used data from a subset of 81 717 UK Biobank participants aged 42 to 78 years. Participants wore an accelerometer for 1 week (between June 1, 2013, and December 23, 2015) and were followed up over a median (IQR) of 6.8 (6.2-7.3) years; follow-up for the current study ended in 2021 (exact date varied by location). Exposures: Mean total and intensity-specific accelerometer-measured physical activity. Main Outcomes and Measures: Hospitalization for the most common health conditions. Cox proportional hazards regression analysis was used to estimate hazard ratios (HRs) and 95% CIs for mean accelerometer-measured physical activity (per 1-SD increment) and risks of hospitalization for 25 conditions. Population-attributable risks were used to estimate the proportion of hospitalizations for each condition that might be prevented if participants increased their moderate to vigorous physical activity (MVPA) by 20 minutes per day. Results: Among 81 717 participants, the mean (SD) age at accelerometer assessment was 61.5 (7.9) years; 56.4% were female, and 97.0% self-identified as White. Higher levels of accelerometer-measured physical activity were associated with lower risks of hospitalization for 9 conditions: gallbladder disease (HR per 1 SD, 0.74; 95% CI, 0.69-0.79), urinary tract infections (HR per 1 SD, 0.76; 95% CI, 0.69-0.84), diabetes (HR per 1 SD, 0.79; 95% CI, 0.74-0.84), venous thromboembolism (HR per 1 SD, 0.82; 95% CI, 0.75-0.90), pneumonia (HR per 1 SD, 0.83; 95% CI, 0.77-0.89), ischemic stroke (HR per 1 SD, 0.85; 95% CI, 0.76-0.95), iron deficiency anemia (HR per 1 SD, 0.91; 95% CI, 0.84-0.98), diverticular disease (HR per 1 SD, 0.94; 95% CI, 0.90-0.99), and colon polyps (HR per 1 SD, 0.96; 95% CI, 0.94-0.99). Positive associations were observed between overall physical activity and carpal tunnel syndrome (HR per 1 SD, 1.28; 95% CI, 1.18-1.40), osteoarthritis (HR per 1 SD, 1.15; 95% CI, 1.10-1.19), and inguinal hernia (HR per 1 SD, 1.13; 95% CI, 1.07-1.19), which were primarily induced by light physical activity. Increasing MVPA by 20 minutes per day was associated with reductions in hospitalization ranging from 3.8% (95% CI, 1.8%-5.7%) for colon polyps to 23.0% (95% CI, 17.1%-28.9%) for diabetes. Conclusions and Relevance: In this cohort study of UK Biobank participants, those with higher physical activity levels had lower risks of hospitalization across a broad range of health conditions. These findings suggest that aiming to increase MVPA by 20 minutes per day may be a useful nonpharmaceutical intervention to reduce health care burdens and improve quality of life.. © 2023 American Medical Association. All rights reserved.,Introduction: Obsessive-compulsive disorders (OCD) are marked by distress, negative emotions, mental processes and behaviors that are reflected in physiological signals such as heart rate, electrodermal activity, and skin temperature. Continuous monitoring of physiological signals associated with OCD symptoms may make measures of OCD more objective and facilitate close monitoring of prodromal symptoms, treatment progress and risk of relapse. Thus, we explored the feasibility of capturing OCD events in the real world using an unobtrusive wrist worn biosensor and machine learning models. Methods: Nine adolescents (ages 10–17 years) with mild to moderate-severe OCD were recruited from child and adolescent mental health services. Participants were asked to wear the biosensor in the lab during conditions of rest and exposure to OCD symptom-triggering stimuli and for up to 8 weeks in their everyday lives and register OCD events. We explored the relationships among physiological data, registered OCD events, age, OCD symptom severity and symptom types. In the machine learning models, we considered detection of OCD events as a binary classification problem. A nested cross-validation strategy with either random 10-folds, leave-one-subject-out, or leave-week(s)-out in both layers was used. We compared the performance of four models: logistic regression, random forest (RF), feedforward neural networks, and mixed-effect random forest (MERF). To explore the ability of the models to detect OCD events in new patients, we assessed the performance of participant-based generalized models. To explore the ability of models to detect OCD events in future, unseen data from the same patients, we compared the performance of temporal generalized models trained on multiple patients with personalized models trained on single patients. Results: Eight of the nine participants collected biosensor signals totaling 2, 405 h and registered 1, 639 OCD events. Better performance was obtained when generalizing across time compared to across patients. Generalized temporal models trained on multiple patients were found to perform better than personalized models trained on single patients. RF and MERF models outperformed the other models in terms of accuracy in all cross-validation strategies, reaching 70% accuracy in random and participant cross-validation. Conclusion: Our pilot results suggest that it is possible to detect OCD episodes in the everyday lives of adolescents using physiological signals captured with a wearable biosensor. Large scale studies are needed to train and test models capable of detecting and predicting episodes. Clinical trial registration: ClinicalTrials.gov: NCT05064527, registered October 1, 2021. Copyright © 2023 Lønfeldt, Olesen, Das, Mora-Jensen, Pagsberg and Clemmensen.,Introduction: Advances in wearable sensor technology have enabled the collection of biomarkers that may correlate with levels of elevated stress. While significant research has been done in this domain, specifically in using machine learning to detect elevated levels of stress, the challenge of producing a machine learning model capable of generalizing well for use on new, unseen data remain. Acute stress response has both subjective, psychological and objectively measurable, biological components that can be expressed differently from person to person, further complicating the development of a generic stress measurement model. Another challenge is the lack of large, publicly available datasets labeled for stress response that can be used to develop robust machine learning models. In this paper, we first investigate the generalization ability of models built on datasets containing a small number of subjects, recorded in single study protocols. Next, we propose and evaluate methods combining these datasets into a single, large dataset to study the generalization capability of machine learning models built on larger datasets. Finally, we propose and evaluate the use of ensemble techniques by combining gradient boosting with an artificial neural network to measure predictive power on new, unseen data. In favor of reproducible research and to assist the community advance the field, we make all our experimental data and code publicly available through Github at https://github.com/xalentis/Stress. This paper's in-depth study of machine learning model generalization for stress detection provides an important foundation for the further study of stress response measurement using sensor biomarkers, recorded with wearable technologies. Methods: Sensor biomarker data from six public datasets were utilized in this study. Exploratory data analysis was performed to understand the physiological variance between study subjects, and the complexity it introduces in building machine learning models capable of detecting elevated levels of stress on new, unseen data. To test model generalization, we developed a gradient boosting model trained on one dataset (SWELL), and tested its predictive power on two datasets previously used in other studies (WESAD, NEURO). Next, we merged four small datasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of 99 subjects, and applied feature engineering to generate additional features utilizing statistical summaries, with sliding windows of 25 s. We name this large dataset, StressData. In addition, we utilized random sampling on StressData combined with another dataset (EXAM) to build a larger training dataset consisting of 200 synthesized subjects, which we name SynthesizedStressData. Finally, we developed an ensemble model that combines our gradient boosting model with an artificial neural network, and tested it using Leave-One-Subject-Out (LOSO) validation, and on two additional, unseen publicly available stress biomarker datasets (WESAD and Toadstool). Results: Our results show that previous models built on datasets containing a small number (<50) of subjects, recorded in single study protocols, cannot generalize well to new, unseen datasets. Our presented methodology for generating a large, synthesized training dataset by utilizing random sampling to construct scenarios closely aligned with experimental conditions demonstrate significant benefits. When combined with feature-engineering and ensemble learning, our method delivers a robust stress measurement system capable of achieving 85% predictive accuracy on new, unseen validation data, achieving a 25% performance improvement over single models trained on small datasets. The resulting model can be used as both a classification or regression predictor for estimating the level of perceived stress, when applied on specific sensor biomarkers recorded using a wearable device, while further allowing researchers to construct large, varied datasets for training machine learning models that closely emulate their exact experimental conditions. Conclusion: Models trained on small, single study protocol datasets do not generalize well for use on new, unseen data and lack statistical power. Machine learning models trained on a dataset containing a larger number of varied study subjects capture physiological variance better, resulting in more robust stress detection. Feature-engineering assists in capturing these physiological variance, and this is further improved by utilizing ensemble techniques by combining the predictive power of different machine learning models, each capable of learning unique signals contained within the data. While there is a general lack of large, labeled public datasets that can be utilized for training machine learning models capable of accurately measuring levels of acute stress, random sampling techniques can successfully be applied to construct larger, varied datasets from these smaller sample datasets, for building robust machine learning models. © 2023 The Author(s)"
182,181,23,181_dopamine_reinforcement_behavioral_reward,"dopamine,reinforcement,behavioral,reward,impulsivity,dopaminergic,motivational,fmri,rewards,addiction","Many real-world situations require navigating decisions for both reward and threat. While there has been significant progress in understanding mechanisms of decision-making and mediating neurocircuitry separately for reward and threat, there is limited understanding of situations where reward and threat contingencies compete to create approach-avoidance conflict (AAC). Here, we leverage computational learning models, independent component analysis (ICA), and multivariate pattern analysis (MVPA) approaches to understand decision-making during a novel task that embeds concurrent reward and threat learning and manipulates congruency between reward and threat probabilities. Computational modeling supported a modified reinforcement learning model where participants integrated reward and threat value into a combined total value according to an individually varying policy parameter, which was highly predictive of decisions to approach reward vs avoid threat during trials where the highest reward option was also the highest threat option (i.e., approach-avoidance conflict). ICA analyses demonstrated unique roles for salience, frontoparietal, medial prefrontal, and inferior frontal networks in differential encoding of reward vs threat prediction error and value signals. The left frontoparietal network uniquely encoded degree of conflict between reward and threat value at the time of choice. MVPA demonstrated that delivery of reward and threat could accurately be decoded within salience and inferior frontal networks, respectively, and that decisions to approach reward vs avoid threat were predicted by the relative degree to which these reward vs threat representations were active at the time of choice. This latter result suggests that navigating AAC decisions involves generating mental representations for possible decision outcomes, and relative activation of these representations may bias subsequent decision-making towards approaching reward or avoiding threat accordingly. © 2022 The Authors,Background: Neural activation during reward processing is thought to underlie critical behavioral changes that take place during the transition to adolescence (e.g., learning, risk-taking). Though literature on the neural basis of reward processing in adolescence is booming, important gaps remain. First, more information is needed regarding changes in functional neuroanatomy in early adolescence. Another gap is understanding whether sensitivity to different aspects of the incentive (e.g., magnitude and valence) changes during the transition into adolescence. We used fMRI from a large sample of preadolescent children to characterize neural responses to incentive valence vs. magnitude during anticipation and feedback, and their change over a period of two years. Methods: Data were taken from the Adolescent Cognitive and Brain DevelopmentSM (ABCD®) study release 3.0. Children completed the Monetary Incentive Delay task at baseline (ages 9–10) and year 2 follow-up (ages 11–12). Based on data from two sites (N = 491), we identified activation-based Regions of Interest (ROIs; e.g., striatum, prefrontal regions, etc.) that were sensitive to trial type (win $5, win $0.20, neutral, lose $0.20, lose $5) during anticipation and feedback phases. Then, in an independent subsample (N = 1470), we examined whether these ROIs were sensitive to valence and magnitude and whether that sensitivity changed over two years. Results: Our results show that most ROIs involved in reward processing (including the striatum, prefrontal cortex, and insula) are specialized, i.e., mainly sensitive to either incentive valence or magnitude, and this sensitivity was consistent over a 2-year period. The effect sizes of time and its interactions were significantly smaller (0.002??2?0.02) than the effect size of trial type (0.06??2?0.30). Interestingly, specialization was moderated by reward processing phase but was stable across development. Biological sex and pubertal status differences were few and inconsistent. Developmental changes were mostly evident during success feedback, where neural reactivity increased over time. Conclusions: Our results suggest sub-specialization to valence vs. magnitude within many ROIs of the reward circuitry. Additionally, in line with theoretical models of adolescent development, our results suggest that the ability to benefit from success increases from pre- to early adolescence. These findings can inform educators and clinicians and facilitate empirical research of typical and atypical motivational behaviors during a critical time of development. © 2023 The Author(s),Tobacco use and its harmful health-related problems have become one of the largest modern preventable public health issues. Current research strongly suggests that smoking during adolescence enhances addictive smoking behaviors during life, which can be related to adolescence as a critical ontogenetic period characterized by behaviors that can increase the probability of risk-related behaviors such as sensation and novelty seeking. Adolescent development is also a period of maturation of frontal and subcortical neural systems, brain changes that underlie higher impulsivity tendencies to promote adequate learning and adaptations necessary to succeed the novel challenges of the adult life, but those changes also enhance vulnerabilities to the addictive effects of drugs. Consistent with this, tobacco use affects brain development processes which underlie longterm psychobiological alterations and the enhanced risks for tobacco addiction during adult life. Thus, the present review describes current psychobiological approaches to understand general addiction processes and tobacco addiction, highlighting the behavioral and neural short-term effects of tobacco use during adolescence and its long-term effects during adulthood. Current research has advanced on four aspects for the understanding of both the psychobiology of adolescent development and the effects of drugs of abuse during this time. The first aspect is behavioral, as adolescence is related to important changes on motivational and emotional behaviors such as sensation seeking. Other important behavioral changes are social approach, a higher variety of opportunity for personal choices, and development of personal independence. Research on a second aspect has focused on cognition. A review of research is presented showing enhanced abilities during adolescence development for reading, abstract and logical thinking, and novel problem solving. Stress reactivity is the third aspect of reviewed psychobiological mechanisms. The stress biological system undergoes important changes during adolescence, including changes on stress-related hormones and neural architecture. An important issue is that exposure to early and/or chronic stressful circumstances during adolescence could be related to higher risk to the start and maintenance of addiction states, as suggested by research assessing the disruptive effects of stress on psychobiological homeostatic processes needed to maintain stable biological and emotional regulation. The fourth aspect is psychobiology. In this section research is reviewed related to the development of monoaminergic brain circuits underlying motivation, novelty-seeking, impulsivity, and addiction processes. Using as model the previous review integration, the effects of nicotine are discussed, the essential addictive component of tobacco, on the neurochemical systems underlying tobacco addiction. Following this, important research is introduced that describes psychobiological changes during adolescence and evidence of vulnerability to addiction during this life stage. Then, current research on both short-term and long-term effects of tobacco or nicotine administration during adolescence on the brain, behavior, and cognition is introduced. The current research advances and discussions on the psychobiology of addictions in general, and tobacco addiction in particular, have been possible to a large extent from the use of animal models and preclinical research, since animal models have become crucial to identify learning, motivational, emotional, and cognitive mechanisms that underlie addictive processes, and making possible to perform experimental procedures to discover the functioning and participation of biological components. One example of such components is the cholinergic system, which is activated by nicotine and is part of the neurochemical machinery on different brain areas important for both tobacco addiction and adolescence development such as the dorsal striatum, amygdala, ventral tegmental area, nucleus accumbens, prefrontal cortex, and hippocampus. The present review and research divulgation written in Spanish are expected to clarify modern research on addiction and encourage current scientific education on the vulnerabilities and predispositions for tobacco abuse in Latin-American countries © 2023, Interdisciplinaria.All Rights Reserved."
183,182,23,182_satellite_imagery_landsat_lakes,"satellite,imagery,landsat,lakes,multispectral,lake,wetland,images,ocean,shorelines","Shallow water bathymetry is of great significance in understanding, managing, and protecting coastal ecological environments. Many studies have shown that both empirical models and deep learning models can achieve promising results from satellite imagery bathymetry inversion. However, the spectral information available today in multispectral or/and hyperspectral satellite images has not been explored thoroughly in many models. The Band-optimized Bidirectional Long Short-Term Memory (BoBiLSTM) model proposed in this paper feeds only the optimized bands and band ratios to the deep learning model, and a series of experiments were conducted in the shallow waters of Molokai Island, Hawaii, using hyperspectral satellite imagery (PRISMA) and multispectral satellite imagery (Sentinel-2) with ICESat-2 data and multibeam scan data as training data, respectively. The experimental results of the BoBiLSTM model demonstrate its robustness over other compared models. For example, using PRISMA data as the source image, the BoBiLSTM model achieves RMSE values of 0.82 m (using ICESat-2 as the training data) and 1.43 m (using multibeam as the training data), respectively, and because of using the bidirectional strategy, the inverted bathymetry reaches as far as a depth of 25 m. More importantly, the BoBiLSTM model does not overfit the data in general, which is one of its advantages over many other deep learning models. Unlike other deep learning models, which require a large amount of training data and all available bands as the inputs, the BoBiLSTM model can perform very well using equivalently less training data and a handful of bands and band ratios. With ICESat-2 data becoming commonly available and covering many shallow water regions around the world, the proposed BoBiLSTM model holds potential for bathymetry inversion for any region around the world where satellite images and ICESat-2 data are available. © 2023 by the authors.,Remote sensing of the Earth's surface water is critical in a wide range of environmental studies, from evaluating the societal impacts of seasonal droughts and floods to the large-scale implications of climate change. Consequently, a large literature exists on the classification of water from satellite imagery. Yet, previous methods have been limited by (1) the >10 m spatial resolution of public satellite imagery, (2) classification schemes that operate at the pixel level, and (3) the need for multiple spectral bands. We advance the state-of-the-art by (1) using commercial satellite imagery with panchromatic and multispectral resolutions of ? 30 cm and ? 1.2 m, respectively, (2) developing multiple fully convolutional neural networks (FCN) that can learn the morphological features of water bodies in addition to their spectral properties, and (3) FCN that can classify water even from panchromatic imagery. This study focuses on rivers in the Arctic, using images from the Quickbird-2, WorldView-1, WorldView-2, WorldView-3, and GeoEye satellites. Because no training data are available at such high resolutions, we construct those manually. First, we use the red, green, blue, and near-infrared bands of the 8-band multispectral sensors. Those trained models all achieve excellent precision and recall over 90% on validation data, aided by on-the-fly preprocessing of the training data specific to satellite imagery. In a novel approach, we then use results from the multispectral model to generate training data for FCN that only require panchromatic imagery, of which considerably more is available. Despite the smaller feature space, these models still achieve a precision and recall of over 85%. We provide our open-source codes and trained model parameters to the remote sensing community, which paves the way to a wide range of environmental hydrology applications at vastly superior accuracies and 1–2 orders of magnitude higher spatial resolution than previously possible. © 2022 Elsevier Inc.,Landsat is the longest-running environmental satellite program and has been used for surface water mapping since its launch in 1972. However, its sustained 30 m resolution since 1982 prohibits the detection of small water bodies, which are globally far more prevalent than large. Remote sensing image resolution is increasingly being enhanced through single image super resolution (SR), a machine learning task typically performed by neural networks. Here, we show that a 10× SR model (Enhanced Super Resolution Generative Adversarial Network, or ESRGAN) trained entirely with Planet SmallSat imagery (3 m resolution) improves the detection of small and sub-pixel lakes in Landsat imagery (30 m) and produces images (3 m resolution) with preserved radiometric properties. We test the utility of these Landsat SR images for small lake detection by applying a simple water classification to SR and original Landsat imagery and comparing their lake counts, sizes, and locations with independent, high-resolution water maps made from coincident airborne camera imagery. SR images appear realistic and have fewer missed detections (type II error) compared to low resolution (LR), but exhibit errors in lake location and shape, and yield increasing false detections (type I error) with decreasing lake size. Even so, lakes between ~500 and ~10,000 m2 in area are better detected with SR than with native-resolution Landsat 8 imagery. SR transformation achieves an F-1 score for water detection of 0.75 compared to 0.73 from native resolution Landsat. We conclude that SR enhancement improves the detection of small lakes sized several Landsat pixels or less, with a minimum mapping unit (MMU) of ~ 2/3 of a Landsat pixel–a significant improvement from previous studies. We also apply the SR model to a historical Landsat 5 image and find similar performance gains, using an independent 1985 air photo map of 242 small Alaskan lakes. This demonstration of retroactively generated 3 m imagery dating to 1985 has exciting applications beyond water detection and paves the way for further SR land cover classification and small object detection from the historical Landsat archive. However, we caution that the approach presented is suitable for landscape-scale inventories of lake counts and lake size distributions, but not for specific geolocational positions of individual lakes. Much work remains to be done surrounding technical and ethical guidelines for the creation, use, and dissemination of SR satellite imagery. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group."
184,183,23,183_tracking_tracker_trackers_trackingbydetection,"tracking,tracker,trackers,trackingbydetection,tracklets,tracklet,siamese,tracknas,features,detections","Unmanned aerial vehicle (UAV)-based tracking has shown large potential in various domains such as transportation, logistics, public safety, and more. However, deploying deep learning (DL)-based tracking algorithms on UAVs is challenging because of limitations in computing resources, battery capacity, and maximum load. Discriminative correlation filter (DCF)-based trackers have become a popular choice in the UAV tracking community owing to their ability to provide superior efficiency while consuming fewer resources. However, the limited representation learning ability of DCF-based trackers leads to lower precision in complex scenarios compared to DL-based methods. Filter pruning is a prevalent practice for deploying deep neural networks on edge devices with constrained resources, and it may be an effective way to solve problems encountered when deploying deep learning trackers on UAVs. However, the application of filter pruning to UAV tracking is underexplored, and a straightforward and useful pruning standard is desirable. This paper proposes using Fisher pruning to reduce the SiamFC++ model for UAV tracking, resulting in the F-SiamFC++ tracker. The proposed tracker achieves a remarkable balance between precision and efficiency, as demonstrated through exhaustive experiments on four popular UAV benchmarks: UAVDT, DTB70, UAV123@10fps, and Vistrone2018, showing state-of-the-art performance. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Over the last decade, Siamese network architectures have emerged as dominating tracking paradigms, which have led to significant progress. These architectures are made up of a backbone network and a head network. The backbone network comprises two identical feature extraction sub-branches, one for the target template and one for the search candidate. The head network takes both the template and candidate features as inputs and produces a local similarity score for the target object in each location of the search candidate. Despite promising results that have been attained in visual tracking, challenges persist in developing efficient and lightweight models due to the inherent complexity of the task. Specifically, manually designed tracking models that rely heavily on the knowledge and experience of relevant experts are lacking. In addition, the existing tracking approaches achieve excellent performance at the cost of large numbers of parameters and vast amounts of computations. A novel Siamese tracking approach called TrackNAS based on neural architecture search is proposed to reduce the complexity of the neural architecture applied in visual tracking. First, according to the principle of the Siamese network, backbone and head network search spaces are constructed, constituting the search space for the network architecture. Next, under the given resource constraints, the network architecture that meets the tracking performance requirements is obtained by optimizing a hybrid search strategy that combines distributed and joint approaches. Then, an evolutionary method is used to lighten the network architecture obtained from the search phase to facilitate deployment to devices with resource constraints (FLOPs). Finally, to verify the performance of TrackNAS, comparison and ablation experiments are conducted using several large-scale visual tracking benchmark datasets, such as OTB100, VOT2018, UAV123, LaSOT, and GOT-10k. The results indicate that the proposed TrackNAS achieves competitive performance in terms of accuracy and robustness, and the number of network parameters and computation volume are far smaller than those of other advanced Siamese trackers, meeting the requirements for lightweight deployment to resource-constrained devices. © 2023 by the authors.,Pedestrian tracking is a challenging task in the area of visual object tracking research and it is a vital component of various vision-based applications such as surveillance systems, human-following robots, and autonomous vehicles. In this paper, we proposed a single pedestrian tracking (SPT) framework for identifying each instance of a person across all video frames through a tracking-by-detection paradigm that combines deep learning and metric learning-based approaches. The SPT framework comprises three main modules: detection, re-identification, and tracking. Our contribution is a significant improvement in the results by designing two compact metric learning-based models using Siamese architecture in the pedestrian re-identification module and combining one of the most robust re-identification models for data associated with the pedestrian detector in the tracking module. We carried out several analyses to evaluate the performance of our SPT framework for single pedestrian tracking in the videos. The results of the re-identification module validate that our two proposed re-identification models surpass existing state-of-the-art models with increased accuracies of 79.2% and 83.9% on the large dataset and 92% and 96% on the small dataset. Moreover, the proposed SPT tracker, along with six state-of-the-art (SOTA) tracking models, has been tested on various indoor and outdoor video sequences. A qualitative analysis considering six major environmental factors verifies the effectiveness of our SPT tracker under illumination changes, appearance variations due to pose changes, changes in target position, and partial occlusions. In addition, quantitative analysis based on experimental results also demonstrates that our proposed SPT tracker outperforms the GOTURN, CSRT, KCF, and SiamFC trackers with a success rate of 79.7% while beating the DiamSiamRPN, SiamFC, CSRT, GOTURN, and SiamMask trackers with an average of 18 tracking frames per second. © 2023 by the authors."
185,184,23,184_dementia_alzheimers_neurodegenerative_biomarker,"dementia,alzheimers,neurodegenerative,biomarker,biomarkers,cognitive,impairment,neurodegeneration,amyloid,aging","Background: Dementia has become a major public health concern due to its heavy disease burden. Mild cognitive impairment (MCI) is a transitional stage between healthy aging and dementia. Early identification of MCI is an essential step in dementia prevention. Objective: Based on machine learning (ML) methods, this study aimed to develop and validate a stable and scalable panel of cognitive tests for the early detection of MCI and dementia based on the Chinese Neuropsychological Consensus Battery (CNCB) in the Chinese Neuropsychological Normative Project (CN-NORM) cohort. Methods: CN-NORM was a nationwide, multicenter study conducted in China with 871 participants, including an MCI group (n=327, 37.5%), a dementia group (n=186, 21.4%), and a cognitively normal (CN) group (n=358, 41.1%). We used the following 4 algorithms to select candidate variables: the F-score according to the SelectKBest method, the area under the curve (AUC) from logistic regression (LR), P values from the logit method, and backward stepwise elimination. Different models were constructed after considering the administration duration and complexity of combinations of various tests. Receiver operating characteristic curve and AUC metrics were used to evaluate the discriminative ability of the models via stratified sampling cross-validation and LR and support vector classification (SVC) algorithms. This model was further validated in the Alzheimer's Disease Neuroimaging Initiative phase 3 (ADNI-3) cohort (N=743), which included 416 (56%) CN subjects, 237 (31.9%) patients with MCI, and 90 (12.1%) patients with dementia. Results: Except for social cognition, all other domains in the CNCB differed between the MCI and CN groups (P < .008). In feature selection results regarding discrimination between the MCI and CN groups, the Hopkins Verbal Learning Test-5 minutes Recall had the best performance, with the highest mean AUC of up to 0.80 (SD 0.02) and an F-score of up to 258.70. The scalability of model 5 (Hopkins Verbal Learning Test-5 minutes Recall and Trail Making Test-B) was the lowest. Model 5 achieved a higher level of discrimination than the Hong Kong Brief Cognitive test score in distinguishing between the MCI and CN groups (P < .05). Model 5 also provided the highest sensitivity of up to 0.82 (range 0.72-0.92) and 0.83 (range 0.75-0.91) according to LR and SVC, respectively. This model yielded a similar robust discriminative performance in the ADNI-3 cohort regarding differentiation between the MCI and CN groups, with a mean AUC of up to 0.81 (SD 0) according to both LR and SVC algorithms. Conclusions: We developed a stable and scalable composite neurocognitive test based on ML that could differentiate not only between patients with MCI and controls but also between patients with different stages of cognitive impairment. This composite neurocognitive test is a feasible and practical digital biomarker that can potentially be used in large-scale cognitive screening and intervention studies. © Dongmei Gu, Xiaozhen Lv, Chuan Shi, Tianhong Zhang, Sha Liu, Zili Fan, Lihui Tu, Ming Zhang, Nan Zhang, Liming Chen, Zhijiang Wang, Jing Wang, Ying Zhang, Huizi Li, Luchun Wang, Jiahui Zhu, Yaonan Zheng, Huali Wang, Xin Yu, Alzheimer's Disease Neuroimaging Initiative (ADNI). Originally published in the Journal of Medical Internet Research (https://www.jmir.org),01.12.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.,Background and Objective: Dementia is a broad term for a complex range of conditions that affect the brain, such as Alzheimer’s disease (AD). Dementia affects a lot of people in the elderly community, hence there is a huge demand to better understand this condition by using cost effective and quick methods, such as neuropsychological tests, since pathological assessments are invasive and demand expensive resources. One of the promising initiatives that deals with dementia and Mild Cognitive Impairment (MCI) is the Alzheimer’s Disease Neuroimaging Initiative (ADNI), which includes cognitive tests, such as Clinical Dementia Rating (CDR) scores. The aim of this research is to investigate non-invasive dementia indicators, such as cognitive features, that are typically diagnosed by clinical assessment within ADNI’s data to understand their effect on dementia. Methods: To achieve the aim, machine learning techniques have been utilized to classify patients into Cognitively Normal (CN), MCI, or having dementia, based on the sum of CDR scores (CDR-SB) besides demographic variables. Particularly, the performance of Support Vector Machine (SVM), K-nearest neighbors (KNN), Decision Trees (C4.5), Probabilistic Naïve Bayes (NB), and Rule Induction (RIPPER) is measured with respect to different evaluation measures, including specificity, sensitivity, and harmonic mean (F-measure), among others, on a large number of cases and controls from the ADNI dataset. Results: The results indicate competitive performance when classifying subjects from the baseline selected variables using machine learning technology. Though we observed fairly good results across all machine learning algorithms utilized, there was still variation in the performance ability, indicating that some algorithms, such as NB and C4.5, are better suited to the task of classifying dementia status based on our baseline data. Conclusions: Using cognitive tests, such as CDR-SB scores, with demographic attributes to pinpoint to dementia using machine learning can be seen a less invasive approach that could be good for clinical use to aid in the diagnosis of dementia. This study gives an indication that a comprehensive assessment tool, such as CDR, may be adequate in assessing and assigning a dementia class to patients, upon their visit, in order to speed further clinical procedures. © 2023 by the authors.,Neuroimaging-based brain-age estimation via machine learning has emerged as an important new approach for studying brain aging. The difference between one's estimated brain age and chronological age, the brain age gap (BAG), has been proposed as an Alzheimer's Disease (AD) biomarker. However, most past studies on the BAG have been cross-sectional. Quantifying longitudinal changes in an individual's BAG temporal pattern would likely improve prediction of AD progression and clinical outcome based on neurophysiological changes. To fill this gap, our study conducted predictive modeling using a large neuroimaging dataset with up to 8 years of follow-up to examine the temporal patterns of the BAG's trajectory and how it varies by subject-level characteristics (sex, APOE?4 carriership) and disease status. Specifically, we explored the pattern and rate of change in BAG over time in individuals who remain stable with normal cognition or mild cognitive impairment (MCI), as well as individuals who progress to clinical AD. Combining multimodal imaging data in a support vector regression model to estimate brain age yielded improved performance over single modality. Multilevel modeling results showed the BAG followed a linear increasing trajectory with a significantly faster rate in individuals with MCI who progressed to AD compared to cognitively normal or MCI individuals who did not progress. The dynamic changes in the BAG during AD progression were further moderated by sex and APOE?4 carriership. Our findings demonstrate the BAG as a potential biomarker for understanding individual specific temporal patterns related to AD progression. © 2022"
186,185,23,185_reliability_kriging_simulation_krigingbased,"reliability,kriging,simulation,krigingbased,adaptive,estimating,numerical,sampling,uncertainty,accuracy","Estimating the small failure probability of highly reliable structures is often computationally expensive in reliability analysis. The adaptive kriging-based reliability analysis methods have been widely used to solve this issue. However, the kriging refinement phase of these methods may not achieve adequate efficiency due to a large candidate sample pool (CSP) size and unnecessary limit state function (LSF) evaluations. In this work, an efficient adaptive kriging refinement method (EAKRM) is proposed to alleviate the computational burden. First, a CSP generation strategy is developed using the radius sequence to gradually generate uniform samples along the direction vector until the failure region appears. Considering points with a high risk of misjudgment and located outside of CSP, a two-stage training point selection strategy is then proposed based on the learning function U to determine the most valuable training point. Finally, a correlation-based stopping criterion is presented by quantifying the consistency of the kriging predictive signs between two successive refinement processes. Three mathematical examples and three engineering examples are employed to illustrate the effectiveness of the proposed EAKRM. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,The reliability analysis methods based on the Kriging model have been explored to update the design of experiments (DoE), but the number of calls to the performance function is high. In this paper, a new active learning method combining the weight information entropy function (WH) and the adaptive candidate sample pool is proposed to improve the reliability analysis efficiency. Based on the information entropy function (H), the learning function WH is constructed to consider Kriging variance and the joint probability density function (PDF). In this proposed method, the sample points can be assigned different weight by the learning function WH according to the important degrees of sample points. The point which not only nears the limit state function (LSF), but also has a high probability density function value and large Kriging variance is assigned more weight than others. To select the sample points with lower confidence level, the adaptive candidate sample pool is generated by Markov Chain Monte Carlo (MCMC) simulation. The Kriging model can be updated efficiently by the proposed method. Four numerical examples and an engineering example with implicit performance function are used to verify the efficiency and accuracy of the proposed method. The results show that the proposed method can significantly improve the computational efficiency of the reliability analysis without losing accuracy. © IMechE 2022.,With increasing complexity of engineering problems, various traditional reliability analysis methods are facing rising challenges in terms of computational efficiency and accuracy. Surrogate models, especially Kriging model, have received growing attention and been widely used in reliability analyses by the virtue of their advantages for achieving high computational efficiency and ensuring high numerical accuracy. Nevertheless, there have been still two significant problems in the Kriging model-assisted reliability analyses due to the absence of prior knowledge: i.e. (1) the size of candidate sample pool tends to be quite large in order to ensure prediction of a convergent failure probability; and (2) local prediction accuracy of limiting state surface by Kriging model is generally excessive. These above two issues can often result in high computational cost for Kriging-based reliability analyses. To enhance computational efficiency, a new method that combines adaptive Kriging and n-hypersphere rings, named an AK-HRn method, is proposed in this study. First, the n-hypersphere rings, which can update its position and radius adaptively, is adopted to divide the design space into potential safety domains and potential failure domains. Second, these potential failure domains are used as the sampling domains for implementing importance sampling method to generate a suitably-sized candidate sample pool. Third, a novel learning function is presented to enrich the design of experiment (DoE), which avoids excessive local prediction accuracy of Kriging models by establishing the rejection domains. Finally, the efficiency and robustness of AK-HRn is compared with other Kriging-based reliability analysis methods through four illustrative numerical examples and one 6-DOF industrial robot case study. Comparison shows that the proposed AK-HRn method has high efficiency and robustness to solve complex reliability analysis problems. © 2023 Elsevier B.V."
187,186,23,186_scheduling_scheduler_machines_jobs,"scheduling,scheduler,machines,jobs,dispatching,salesman,agents,agent,reinforcement,learning","Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this article presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to construct production-adaptive operation and machine features to support high-quality decision-making. Experimental results using synthetic data as well as public benchmarks corroborate that the proposed approach outperforms both traditional PDRs and the state-of-the-art DRL method. Moreover, it achieves results comparable to exact methods in certain cases and demonstrates favorable generalization ability to large-scale and real-world unseen FJSP tasks. IEEE,Job-shop scheduling problem (JSP) is a mathematical optimization problem widely used in industries like manufacturing, and flexible JSP (FJSP) is also a common variant. Since they are NP-hard, it is intractable to find the optimal solution for all cases within reasonable times. Thus, it becomes important to develop efficient heuristics to solve JSP/FJSP. A kind of method of solving scheduling problems is construction heuristics, which constructs scheduling solutions via heuristics. Recently, many methods for construction heuristics leverage deep reinforcement learning (DRL) with graph neural networks (GNN). In this paper, we propose a new approach, named residual scheduling, to solving JSP/FJSP. In this new approach, we remove irrelevant machines and jobs such as those finished, such that the states include the remaining (or relevant) machines and jobs only. Our experiments show that our approach reaches state-of-the-art (SOTA) among all known construction heuristics on most well-known open JSP and FJSP benchmarks. In addition, we also observe that even though our model is trained for scheduling problems of smaller sizes, our method still performs well for scheduling problems of large sizes in terms of makespan. Interestingly in our experiments, our approach even reaches zero makespan gap for 49 among 60 JSP instances whose job numbers are more than 100 on 15 machines. © 2013 IEEE.,The flexible job shop scheduling (FJSS) is important in real-world factories due to the wide applicability. FJSS schedules the operations of jobs to be executed by specific machines at the appropriate time slots based on two decision steps, namely, the job sequencing (i.e., the sequence of jobs executed on a machine) and the job routing (i.e., the route of a job to a machine). Most current studies utilize either deep reinforcement learning (DRL) or multi-agent reinforcement learning (MARL) for FJSS with a large search space. However, these studies suffer from two major limitations: no integration between DRL and MARL, and independent agents without cooperation. To this end, we propose a new model for FJSS, called DeepMAG based on Deep reinforcement learning with Multi-Agent Graphs. DeepMAG has two key contributions. (1) Integration between DRL and MARL. DeepMAG integrates DRL with MARL by associating a different agent to each machine and job. Each agent exploits DRL to find the best action on the job sequencing and routing. After a job-associated agent chooses the best machine, the job becomes a job candidate for the machine to proceed to its next operation, while a machine-associated agent selects the next job from its job candidate set to be processed. (2) Cooperative agents. A multi-agent graph is built based on the operation relationships among machines and jobs. An agent cooperates with its neighboring agents to take one cooperative action. Finally, we conduct experiments to evaluate the performance of DeepMAG and experimental results show that it outperforms the state-of-the-art techniques. © 2022 Elsevier B.V."
188,187,23,187_metabolomics_spectrometry_metabolites_metabolite,"metabolomics,spectrometry,metabolites,metabolite,spectrometers,spectrometer,datasets,spectra,spec2vec,chromatogram","Tandem mass spectrometry (MS/MS) shows great promise in the research of metabolomics, providing an abundance of information on compounds. Due to the rapid development of mass spectrometric techniques, a large number of MS/MS spectral data sets have been produced from different experimental environments. The massive data brings great challenges into the spectral analysis including compound identification and spectra clustering. The core challenge in MS/MS spectral analysis is how to describe a spectrum more quantitatively and effectively. Recently, emerging deep-learning-based technologies have brought new opportunities to handle this challenge in which high-quality descriptions of MS/MS spectra can be obtained. In this study, we propose a novel contrastive learning-based method for the representation of MS/MS spectra, called CLERMS, which is based on transformer architecture. Specifically, an optimized model architecture equipped with a sinusoidal embedder and a novel loss function composed of InfoNCE loss and MSE loss has been proposed for the attainment of good embedding from the peak information and the metadata. We evaluate our method using a GNPS data set, and the results demonstrate that the learned embedding can not only distinguish spectra from different compounds but also reveal the structural similarity between them. Additionally, the comparison between our method and other methods on the performance of compound identification and spectra clustering shows that our method can achieve significantly better results.  © 2023 American Chemical Society.,Metabolite annotation continues to be the widely accepted bottleneck in nontargeted metabolomics workflows. Annotation of metabolites typically relies on a combination of high-resolution mass spectrometry (MS) with parent and tandem measurements, isotope cluster evaluations, and Kendrick mass defect (KMD) analysis. Chromatographic retention time matching with standards is often used at the later stages of the process, which can also be followed by metabolite isolation and structure confirmation utilizing nuclear magnetic resonance (NMR) spectroscopy. The measurement of gas-phase collision cross-section (CCS) values by ion mobility (IM) spectrometry also adds an important dimension to this workflow by generating an additional molecular parameter that can be used for filtering unlikely structures. The millisecond timescale of IM spectrometry allows the rapid measurement of CCS values and allows easy pairing with existing MS workflows. Here, we report on a highly accurate machine learning algorithm (CCSP 2.0) in an open-source Jupyter Notebook format to predict CCS values based on linear support vector regression models. This tool allows customization of the training set to the needs of the user, enabling the production of models for new adducts or previously unexplored molecular classes. CCSP produces predictions with accuracy equal to or greater than existing machine learning approaches such as CCSbase, DeepCCS, and AllCCS, while being better aligned with FAIR (Findable, Accessible, Interoperable, and Reusable) data principles. Another unique aspect of CCSP 2.0 is its inclusion of a large library of 1613 molecular descriptors via the Mordred Python package, further encoding the fine aspects of isomeric molecular structures. CCS prediction accuracy was tested using CCS values in the McLean CCS Compendium with median relative errors of 1.25, 1.73, and 1.87% for the 170 [M - H]-, 155 [M + H]+, and 138 [M + Na]+adducts tested. For superclass-matched data sets, CCS predictions via CCSP allowed filtering of 36.1% of incorrect structures while retaining a total of 100% of the correct annotations using a ?CCSthreshold of 2.8% and a mass error of 10 ppm. © 2022 American Chemical Society. All rights reserved.,The majority of tandem mass spectrometry (MS/MS) spectra in untargeted metabolomics and exposomics studies lack any annotation. Our deep learning framework, Integrated Data Science Laboratory for Metabolomics and Exposomics—Mass INTerpreter (IDSL_MINT) can translate MS/MS spectra into molecular fingerprint descriptors. IDSL_MINT allows users to leverage the power of the transformer model for mass spectrometry data, similar to the large language models. Models are trained on user-provided reference MS/MS libraries via any customizable molecular fingerprint descriptors. IDSL_MINT was benchmarked using the LipidMaps database and improved the annotation rate of a test study for MS/MS spectra that were not originally annotated using existing mass spectral libraries. IDSL_MINT may improve the overall annotation rates in untargeted metabolomics and exposomics studies. The IDSL_MINT framework and tutorials are available in the GitHub repository at https://github.com/idslme/IDSL_MINT . Scientific contribution statement. Structural annotation of MS/MS spectra from untargeted metabolomics and exposomics datasets is a major bottleneck in gaining new biological insights. Machine learning models to convert spectra into molecular fingerprints can help in the annotation process. Here, we present IDSL_MINT, a new, easy-to-use and customizable deep-learning framework to train and utilize new models to predict molecular fingerprints from spectra for the compound annotation workflows. © 2024, The Author(s)."
189,188,23,188_forecast_forecasting_turbine_wind,"forecast,forecasting,turbine,wind,turbines,lstm,renewable,meteorological,prediction,wavelet","In order to solve the security threat brought by the volatility and randomness of large-scale distributed wind power, this paper proposed a wind power prediction model which integrates two-layer decomposition and deep learning, effectively realizing the accurate prediction of wind power series with non-stationary characteristics. Initially, pearson correlation coefficient (PCC) is employed to identify primary meteorological variables as input series. Second, the wind power series are smoothed by implementing complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), and then all subseries are decomposed and obtained by utilizing empirical wavelet transform (EWT) for the components with the highest complexity. Subsequently, hidden information related to wind speed, wind direction, and wind power series are extracted through the bidirectional temporal convolutional network (BiTCN), and the obtained information is fed into a bidirectional long short-term memory network (BiLSTM) optimized by attention mechanism for prediction. Finally, the predicted values of all components are summed to derive the final prediction results. In addition, the significant advantages of the prediction model in this paper are verified by five comparison experiments. The mean absolute error (MAE) and root mean square error (RMSE) of the model's one-step prediction in the January dataset are 2.1647 and 2.8456, respectively. © 2023,Large-scale wind power grid connection increases the uncertainty of the power system, which reduces the economy and security of power system operations. Wind power prediction technology provides the wind power sequence for a period of time in the future, which provides key technical support for the reasonable development of the power generation plan and the arrangement of spare capacity. For large-scale wind farm groups, we propose a cluster model of wind power prediction based on multi-task learning, which can directly output the power prediction results of multiple wind farms. Firstly, the spatial and temporal feature matrix is constructed based on the meteorological forecast data provided by eight wind farms, and the dimensionality of each attribute is reduced by the principal component analysis algorithm to form the spatial fusion feature set. Then, a network structure with bidirectional gated cycle units is constructed, and a multi-output network structure is designed based on the Multi-gate Mixture-of-Experts (MMoE) framework to design the wind power group prediction model. Finally, the data provided by eight wind farms in Jilin, China, was used for experimental analysis, and the predicted average normalized root mean square error is 0.1754, meaning the prediction precision meets the scheduling requirement, which verifies the validity of the wind power prediction model. © 2023 by the authors.,Accurate and effective short-term wind power forecasting is vital for the large-scale integration of wind power generation into the power grid. However, due to the intermittence and volatility of wind resources, short-term wind power forecasting is challenging. To address the issue that the existing decomposition forecasting methods ignore the coupling relationship between wind power series and multiple meteorological series, this study proposes a short-term wind power forecasting method based on multivariate signal decomposition and variable selection. First, multivariate variational mode decomposition (MVMD) is used to perform time-frequency synchronous analysis on wind power and multidimensional meteorological series, thereby decomposing them into the same predefined number of frequency-aligned intrinsic mode functions (IMFs). Secondly, elastic net (EN) is used for supervised variable selection on all IMFs to provide a high-quality training set for the forecasting model, thereby enhancing precision and interpretability. Next, a hybrid deep neural network combining convolutional neural network (CNN), bidirectional long-short term memory (BiLSTM) neural network, and multi-head attention (MHA) mechanism is employed to model the output curve of a group of wind turbines in a wind farm. Finally, the proposed method is comprehensively evaluated through four sets of comparative experiments and multiple evaluation metrics on data gathered from the Mahuangshan first wind farm in China with four forecasting horizons: 15-min ahead, 30-min ahead, 45-min ahead, and 1-h ahead. The experimental results show that the proposed method significantly outperforms fifteen existing deep learning methods in terms of precision and stability. © 2024 Elsevier Ltd"
190,189,23,189_peptidemhc_receptor_peptide_receptors,"peptidemhc,receptor,peptide,receptors,peptides,antigen,antigens,tcrpmhc,tcrpeptidemhc,antigenic","Understanding how a T-cell receptor (TCR) recognizes its specific ligand peptide is crucial for gaining an insight into biological functions and disease mechanisms. Despite its importance, experimentally determining TCR–peptide–major histocompatibility complex (TCR–pMHC) interactions is expensive and time-consuming. To address this challenge, computational methods have been proposed, but they are typically evaluated by internal retrospective validation only, and few researchers have incorporated and tested an attention layer from language models into structural information. Therefore, in this study, we developed a machine learning model based on a modified version of Transformer, a source–target attention neural network, to predict the TCR–pMHC interaction solely from the amino acid sequences of the TCR complementarity-determining region (CDR) 3 and the peptide. This model achieved competitive performance on a benchmark dataset of the TCR–pMHC interaction, as well as on a truly new external dataset. Additionally, by analyzing the results of binding predictions, we associated the neural network weights with protein structural properties. By classifying the residues into large- and small-attention groups, we identified statistically significant properties associated with the largely attended residues such as hydrogen bonds within CDR3. The dataset that we created and the ability of our model to provide an interpretable prediction of TCR–peptide binding should increase our knowledge about molecular recognition and pave the way for designing new therapeutics. Copyright © 2023 Koyama, Hashimoto, Nagao and Mizuguchi.,Cognate target identification for T-cell receptors (TCRs) is a significant barrier in T-cell therapy development, which may be overcome by accurately predicting TCR interaction with peptide-bound major histocompatibility complex (pMHC). In this study, we have employed peptide embeddings learned from a large protein language model- Evolutionary Scale Modeling (ESM), to predict TCR-pMHC binding. The TCR-ESM model presented outperforms existing predictors. The complementarity-determining region 3 (CDR3) of the hypervariable TCR is located at the center of the paratope and plays a crucial role in peptide recognition. TCR-ESM trained on paired TCR data with both CDR3? and CDR3? chain information performs significantly better than those trained on data with only CDR3?, suggesting that both TCR chains contribute to specificity, the relative importance however depends on the specific peptide-MHC targeted. The study illuminates the importance of MHC information in TCR-peptide binding which remained inconclusive so far and was thought dependent on the dataset characteristics. TCR-ESM outperforms existing approaches on external datasets, suggesting generalizability. Overall, the potential of deep learning for predicting TCR-pMHC interactions and improving the understanding of factors driving TCR specificity are highlighted. The prediction model is available at http://tcresm.dhanjal-lab.iiitd.edu.in/ as an online tool. © 2023 The Authors,Introduction: T-cell receptor (TCR) recognition of foreign peptides presented by the major histocompatibility complex (MHC) initiates the adaptive immune response against pathogens. While a large number of TCR sequences specific to different antigenic peptides are known to date, the structural data describing the conformation and contacting residues for TCR-peptide-MHC complexes is relatively limited. In the present study we aim to extend and analyze the set of available structures by performing highly accurate template-based modeling of these complexes using TCR sequences with known specificity. Methods: Identification of CDR3 sequences and their further clustering, based on available spatial structures, V- and J-genes of corresponding T-cell receptors, and epitopes, was performed using the VDJdb database. Modeling of the selected CDR3 loops was conducted using a stepwise introduction of single amino acid substitutions to the template PDB structures, followed by optimization of the TCR-peptide-MHC contacting interface using the Rosetta package applications. Statistical analysis and recursive feature elimination procedures were carried out on computed energy values and properties of contacting amino acid residues between CDR3 loops and peptides, using R. Results: Using the set of 29 complex templates (including a template with SARS-CoV-2 antigen) and 732 specificity records, we built a database of 1585 model structures carrying substitutions in either TCR? or TCR? chains with some models representing the result of different mutation pathways for the same final structure. This database allowed us to analyze features of amino acid contacts in TCR - peptide interfaces that govern antigen recognition preferences and interpret these interactions in terms of physicochemical properties of interacting residues. Conclusion: Our results provide a methodology for creating high-quality TCR-peptide-MHC models for antigens of interest that can be utilized to predict TCR specificity. Copyright © 2023 Shcherbinin, Karnaukhov, Zvyagin, Chudakov and Shugay."
191,190,23,190_soils_soil_soilforming_soiltexture,"soils,soil,soilforming,soiltexture,soillandscape,vegetation,topography,topsoil,topographic,texture","Insights into the controlling factors of soil organic carbon (SOC) stock variation are necessary both for our scientific understanding of the terrestrial carbon balance and to support policies that intend to promote carbon storage in soils to mitigate climate change. In recent years, complex statistical and algorithmic tools from the field of machine learning have become popular for modelling and mapping SOC stocks over large areas. In this paper, we report on the development of a statistical method for interpreting complex models, which we implemented for the study of SOC stock variation. We fitted a random forest machine learning model with 2206 measurements of SOC stocks for the 0-50 cm depth interval from mainland France and used a set of environmental covariates as explanatory variables. We introduce Shapley values, a method from coalitional game theory, and use them to understand how environmental factors influence SOC stock prediction: what is the functional form of the association in the model between SOC stocks and environmental covariates, and how does the covariate importance vary locally from one location to another and between carbon-landscape zones? Results were validated both in light of the existing and well-described soil processes mediating soil carbon storage and with regards to previous studies in the same area. We found that vegetation and topography were overall the most important drivers of SOC stock variation in mainland France but that the set of most important covariates varied greatly among locations and carbon-landscape zones. In two spatial locations with equivalent SOC stocks, there was nearly an opposite pattern in the individual covariate contribution that yielded the prediction - in one case climate variables contributed positively, whereas in the second case climate variables contributed negatively - and this effect was mitigated by land use. We demonstrate that Shapley values are a methodological development that yield useful insights into the importance of factors controlling SOC stock variation in space. This may provide valuable information to understand whether complex empirical models are predicting a property of interest for the right reasons and to formulate hypotheses on the mechanisms driving the carbon sequestration potential of a soil.  © 2023 Alexandre M. J.-C. Wadoux et al.,Accurate soil quality evaluation is an important prerequisite for improving soil management systems and remediating soil pollution. However, traditional soil quality evaluation methods are cumbersome to calculate, and suffer from low efficiency and low accuracy, which often lead to large deviations in the evaluation results. This study aims to provide a new and accurate soil quality evaluation method based on graph convolution network (GCN). In this study, soil organic matter (SOM), alkaline hydrolysable nitrogen (AN), available potassium (AK), salinity, and heavy metals (iron (Fe), copper (Cu), manganese (Mn), and zinc (Zn)) were determined and evaluated using the soil quality index (SQI). Then, the graph convolution network (GCN) was first introduced in the soil quality evaluation to construct an evaluation model, and its evaluation results were compared with those of the SQI. Finally, the spatial distribution of the evaluation results of the GCN model was displayed. The results showed that soil salinity had the largest coefficient of variation (86%), followed by soil heavy metals (67%) and nutrients (30.3%). The soil salinization and heavy metal pollution were at a low level in this area, and the soil nutrients and soil quality were at a high level. The evaluation accuracy of the GCN model for soil salinity/heavy metals, soil nutrients, and soil quality were 0.91, 0.84, and 0.90, respectively. Therefore, the GCN model has a high accuracy and is feasible to be applied in the soil quality evaluation. This study provides a new, simple, and highly accurate method for soil quality evaluation. © 2023 by the authors.,Readily available environmental covariates in current digital soil mapping usually do not indicate the spatial differences between deep soil attributes. This, to a large extent, leads to a decrease in the accuracy of 3D soil mapping with depth, which seriously affects the quality of soil information generated. This study tested the hypothesis that spatialized laboratory soil spectral information can be used as environmental covariates to improve the accuracy of 3D soil attribute mapping and proposed a new type of environmental covariable. In the first step, with soil-forming environmental covariates and independent soil profiles, laboratory vis-NIR spectral data of soil samples resampled into six bands in Anhui province, China, were spatially interpolated to generate spatial distributions of soil spectral measurements at multiple depths. In the second step, we constructed three sets of covariates using the laboratory soil spectral distribution maps at multiple depths: conventional soil-forming variables (C), conventional soil-forming variables plus satellite remote sensing wavebands (C+SRS) and conventional soil-forming variables plus spatialized laboratory soil spectral information (C+LSS). In the third step, we used the three sets of environmental covariates to develop random forest models for predicting soil attributes (pH; CEC, cation exchange capacity; Silt; SOC, soil organic carbon; TP, total phosphorus) at multiple depths. We compared the 3D soil mapping accuracies between these three sets of covariates based on another dataset of 132 soil profiles (collected in the 1980s). The results show that the use of spatialized laboratory soil spectral information as additional environmental covariates has a 50% improvement in prediction accuracy compared with that of only conventional covariates, and a 30% improvement in prediction accuracy compared with that of the satellite remote sensing wavebands as additional covariates. This indicates that spatialized laboratory soil spectral information can improve the accuracy of 3D digital soil mapping. © 2023 by the authors."
192,191,23,191_summarizers_summarizing_summarization_summaries,"summarizers,summarizing,summarization,summaries,textual,text,summary,nlp,sentences,content","Journaling is a widely adopted technique, known to improve mental health and well-being by enabling reflection on past events. Large amounts of text in digital journaling applications could hinder the reflection process due to information overload. Abstractive summarization can solve this problem by generating short summaries to quickly glance at and reminisce. In this paper, we present an investigation of the utility of large language models in the context of autobiographical text summarization. We study two approaches to adapt a self-supervised learning (SSL) model to the domain of autobiographical text. One model employs transfer learning using our new autobiographical text summary dataset to fine-tune the SSL model. The second model leverages existing news datasets for high-quality text summarization mixed with our autobiographical summary dataset. We conducted mixed methods research to analyze the performance of these two models. Through objective evaluation using ROUGE and BART scores, we find that both these approaches perform significantly better than the SSL model fine-tuned with only high-quality news datasets, showing the importance of domain adaptation and autobiographical text summary dataset for this task. Secondly, through a subjective evaluation on a crowd-sourcing platform, we evaluated the summaries generated from these models on various quality criteria such as grammar, non-redundancy, structure, and coherence. We found that on all criteria, these summaries score >4 out of 5, and the two models show comparable results. We deployed a proof-of-concept web-based journaling application to assess the practical real-world implications of incorporating abstractive summarization in a digital journaling context. We found that the participants showed a high consensus that the summaries generated by the system captured the main idea of their journal entry (80% of the 75 participants gave a Likert scale rating of (Formula presented.) out of 7.0, with the overall mean rating of 5.56 ± 1.32) while being factually correct, and they found it to be a useful feature of a journaling application. Finally, we conducted human evaluation studies to compare the quality of the summaries generated from a commercial tool ChatGPT and mixed distribution fine-tuned SSL model, and present insights into these systems in the context of autobiographical abstractive text summarization. We have made our model, dataset, and subjective evaluation questionnaire openly available to the research community. © 2023 Taylor & Francis Group, LLC.,With the development of pre-trained language models and large-scale datasets, automatic text summarization has attracted much attention from the community of natural language processing, but the progress of automatic summarization evaluation has stagnated. Although there have been efforts to improve automatic summarization evaluation, ROUGE has remained one of the most popular metrics for nearly 20 years due to its competitive evaluation performance. However, ROUGE is not perfect, there are studies have shown that it is suffering from inaccurate evaluation of abstractive summarization and limited diversity of generated summaries, both caused by lexical bias. To avoid the bias of lexical similarity, more and more meaningful embedding-based metrics have been proposed to evaluate summaries by measuring semantic similarity. Due to the challenge of accurately measuring semantic similarity, none of them can fully replace ROUGE as the default automatic evaluation toolkit for text summarization. To address the aforementioned problems, we propose a compromise evaluation framework (ROUGE-SEM) for improving ROUGE with semantic information, which compensates for the lack of semantic awareness through a semantic similarity module. According to the differences in semantic similarity and lexical similarity, summaries are classified into four categories for the first time, including good-summary, pearl-summary, glass-summary, and bad-summary. In particular, the back-translation technique is adopted to rewrite pearl-summary and glass-summary that are inaccurately evaluated by ROUGE to alleviate lexical bias. Through this pipeline framework, summaries are first classified by candidate summary classifier, then rewritten by categorized summary rewriter, and finally scored by rewritten summary scorer, which are efficiently evaluated in a manner consistent with human behavior. When measured using Pearson, Spearman, and Kendall rank coefficients, our proposal achieves comparable or higher correlations with human judgments than several state-of-the-art automatic summarization evaluation metrics in dimensions of coherence, consistency, fluency, and relevance. This also suggests that improving ROUGE with semantics is a promising direction for automatic summarization evaluation. © 2023 Elsevier Ltd,With the rapid and unprecedented growth of textual data in recent years, there is a remarkable need for automatic text summarization models to retrieve useful information from these large numbers of textual documents without human intervention within a reasonable time. Text summarization is commonly performed based on extractive and abstractive paradigms. Although different machine learning and deep learning based methods have been proposed for the task of text summarization during the last decades, they are still in their early steps of development and their potential has yet to be fully explored. Accordingly, a new summarization model is proposed in this paper which takes advantage of both extractive and abstractive text summarization models as a single unified model based on the strategy gradient of reinforcement learning. The proposed model also employs the combination of convolutional neural network and gated recurrent unit in both extraction and abstraction modules besides attention mechanism. Moreover, language models, namely Word2Vec and BERT, are used as the backbone of the proposed model to better express sentence semantics as a word vector. We conducted our experiments on widely-studied text summarization datasets (CNN\Daily Mail and DUC-2004) and according to the empirical results, not only the proposed model achieved higher accuracy compared to both extractive and abstractive summarization models in terms of ROUGE metric but also its generated summaries presented higher saliency and readability based on human evaluation. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
193,192,23,192_biometrics_biometric_recognition_cnn,"biometrics,biometric,recognition,cnn,fingerprint,fingerprints,iris,classification,features,convolutional","Biometric systems have been used extensively in the identification and verification of persons. Fingerprint biometrics stands out as the most effective due to their characteristics of Permanence, uniqueness, ergonomics, throughput, low cost, and lifelong usability. By reducing the number of comparisons, biometric recognition systems can effectively deal with large-scale databases. Fingerprint classification is an important task used to reduce the number of comparisons by dividing fingerprints into classes. Deep learning models have demonstrated impressive classification performance in fingerprint classification tasks. The high-level features of deep learning models can affect the transfer learning in deep learning models. Furthermore, the high-level features involve high computational costs that can render difficulty in the deployment of the applications. This work proposes an improved system for fingerprint classification through the truncation of layers and transfer learning. Our approach modifies the ResNet50 model to improve its network inference speed and performance in fingerprint classification by removing some deep convolutional layers. We then finetune the modified model and train it using a fingerprint dataset. The results show that the finetuned modified model improves classification accuracy at a reduced computational cost. At only 5.1M parameters, our model obtained a classification accuracy of 93.3% and precision of 93.4% performing better than previous studies based on its size-performance ratio. © 2023 John Wiley & Sons, Ltd.,Finger vein recognition is an advanced biometric recognition technology that offers high precision and high security. It recognizes or authenticates individuals using irradiating vein texture images collected from fingers with near-infrared light. In this paper, we propose a new finger vein recognition model (MMRAN) based on a multiscale and multistage residual attention network. First, to fully adapt to the low-resolution, grayscale pixels, and linear patterns of finger vein images, we designed an architecture that combines a fusion residual attention block (FRAB) and a multistage residual attention connection (MRAC). The FRAB contains two distinct subpaths: the main vein path (MVP) and the guided attention path (GAP). The MVP extracts finger vein features at multiple scales by using a multibranch residual structure, while the GAP uses an hourglass network to generate weight maps to guide the setting of eigenvalues at corresponding locations in the main vein feature map. MRAC integrates venous features extracted at different learning stages through the above two pathways. The proposed multiscale and multistage extraction model is effective at extracting various types of digital vein features, including those whose shapes change in width, direction, curvature, and so on. We combine the various dimensions of vein features through multistage learning to further improve the model performance for extracting high-level abstract features. To evaluate the performance of our proposed model, we conducted a large number of experiments on five publicly available finger vein datasets. The experimental results show that the proposed model not only achieves a recognition accuracy above 98%, which is an improvement compared to the current state-of-the-art methods, but it can also be implemented with fewer parameters, which improves training and inference. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The usage of biometric identification has increased in recent years, with numerous public and commercial organizations incorporating biometric technologies into their infrastructures. One of the technologies is iris recognition which has been used as a biometric recognition compared to other modalities to combat identity abuse due to its ability to eliminate risk of collisions or false matches even when comparing large populations. The use of CNN is proven to provide high accuracy; however, this technology involves the need for a large dataset and higher computational cost. Therefore, this study uses a combined model of Convolutional Neural Network (CNN) and Vision Transformer (ViT) in identifying and verifying an iris image. By using the proposed learning rate, it proves that the novel hybrid model is capable to achieve up to 93.66% accuracy in recognizing iris images. The cross-entropy loss function was implemented to reduce the loss and it was able to predict the class label more correctly. In addition, the model was thoroughly tested on three publicly available iris databases, achieving satisfactory iris recognition results. Furthermore, this model has the potential to be used in other biometrics such as face and retina recognitions. © 2024, Semarak Ilmu Publishing. All rights reserved."
194,193,22,193_traffic_reinforcement_congestion_freeways,"traffic,reinforcement,congestion,freeways,roads,autonomous,bus,packet,dqn,signal","This paper proposes a reinforcement learning (RL)-based traffic control strategy integrated with attention mechanism for large-scale adaptive traffic signal control (ATSC) system. The proposed attention RL integrates attention mechanism into a multiagent RL model, namely multiagent proximal policy optimization (MAPPO), so as to enable more effective, scalable, and stable learning in complex ATSC environments. In the attention RL, decentralized policies are trained using a centrally computed critic that shares an attention model, while the attention model selects relevant intersections for each agent to estimate the global critic. This framework effectively reduces the computational complexity and stabilizes the training process, enhancing the ability of RL agents to control large-scale traffic networks. The proposed control strategy is tested in both a large synthetic traffic grid and a large real-world traffic network of Yangzhou city using the microscopic traffic simulation tool, SUMO. Experimental results demonstrate that the proposed approach learns stable and sustainable policies that achieve lower congestion level and faster recovery, which outperforms other state-of-art RL-based approaches, as well as a gap-based actuated controller.  © 2024 American Society of Civil Engineers.,The optimization of intersection signal control can improve traffic efficiency, reduce congestion degree, and improve traffic safety. Aiming at implementing the coordinated adaptive traffic signal control (ATSC) across large-scale arterial network, multi-agent reinforcement learning (MARL) has been widely concerned and lucubrated. Nevertheless, the existing MARL-based ATSC studies suffers from several limitations: (1) While most existing researches focused on the mobility performance of controlled corridor, there calls for a methodology that aims at combine multi-objective performance on traffic safety, efficiency, and network coordination simultaneously; (2) Most methods ignore the correlations between multiple agents, nor considers the spatial–temporal dependencies among the corelated neighboring intersections due to high communications requirements, which can hardly be achieved in real adaptive coordination control. To overcome the aforementioned difficulties, a multi-objective reinforcement learning model (NACRL) for network-wide coordinated signal control is proposed. Firstly, to enforce a coordinated network control with safety and efficiency considerations, a reward mechanism inspecting both traffic safety and traffic efficiency indicators was designed to achieve ideal performance in terms of mobility, safety and smooth. Secondly, the proposed NACRL conducted a centralized training-decentralized execution framework, this overcomes the critical limitation of data transmission in the field implementation while explicitly analyzing the traffic state over the entire network instead of examining each isolated intersection. Last but not least, the proposed algorithm utilized the attention mechanism to dynamically capture the sophisticated spatial–temporal dependencies over the complex arterial network, which aids the better coordinated control over multi-agents deployed at the intersections across the corridor. To testify the effeteness of the proposed algorithm, extensive experiments were implemented in both large-scale synthetic traffic grid and real-world arterial network. The experiment demonstrated that the proposed NACRL algorithm outperforms other state-of-the-art baselines with simultaneously improved performance in terms of traffic safety, traffic efficiency and network coordination, as well as improved algorithm convergence and interpretability. © 2023 Elsevier Ltd,Cooperation between intersections in large-scale road networks is critical in traffic congestion. Currently, most traffic signals cooperate via pre-defined timing phases, which is extremely inefficient in real-time traffic scenarios. Most existing studies on multi-agent reinforcement learning (MARL) traffic signal control have focused on designing efficient communication methods, but have ignored the importance of how agents interact in cooperative communication. To achieve more efficient cooperation among traffic signals and alleviate urban traffic congestion, this study constructs a Graph Cooperation Q-learning Network Traffic Signal Control (GCQN-TSC) model, which is a graph cooperation network with an embedded self-attention mechanism that enables agents to adjust their attention in real time according to the dynamic traffic flow information, perceive the traffic environment quickly and effectively in a larger range, and help agents achieve more effective collaboration. Moreover, the Deep Graph Q-learning (DGQ) algorithm is proposed in this model to optimize the traffic signal control strategy according to the spatio-temporal characteristics of different traffic scenes and provide the optimal signal phase for each intersection. This study also integrates the ecological traffic concept into MARL traffic signal control, which aims to reduce traffic exhaust emissions. Finally, the proposed GCQN-TSC is experimentally validated both in a synthetic traffic grid and a real-world traffic network using the SUMO simulator. The experimental results show that GCQN-TSC outperforms other traffic signal control methods in almost all performance metrics, including average queue length and waiting time, as it can aggregate information acquired from collaborative agents and make network-level signal optimization decisions. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
195,194,22,194_fraudagents_frauds_classification_fraud,"fraudagents,frauds,classification,fraud,fraudsters,fraudulent,banking,outlier,credit,dataset","Research into machine learning methods for fraud detection is of paramount importance, largely due to the substantial financial implications associated with fraudulent activities. Our investigation is centered around the Credit Card Fraud Dataset and the Medicare Part D dataset, both of which are highly imbalanced. The Credit Card Fraud Detection Dataset is large data and contains actual transactional content, which makes it an ideal benchmark for credit card fraud detection. The Medicare Part D dataset is big data, providing researchers the opportunity to examine national trends and patterns related to prescription drug usage and expenditures. This paper presents a detailed comparison of One-Class Classification (OCC) and binary classification algorithms, utilizing eight distinct classifiers. OCC is a more appealing option, since collecting a second label for binary classification can be very expensive and not possible to obtain within a reasonable time frame. We evaluate our models based on two key metrics: the Area Under the Precision-Recall Curve (AUPRC)) and the Area Under the Receiver Operating Characteristic Curve (AUC). Our results show that binary classification consistently outperforms OCC in detecting fraud within both datasets. In addition, we found that CatBoost is the most performant among the classifiers tested. Moreover, we contribute novel results by being the first to publish a performance comparison of OCC and binary classification specifically for fraud detection in the Credit Card Fraud and Medicare Part D datasets. © 2023, Springer Nature Switzerland AG.,In recent years, with the rapid development of Internet technology, the number of credit card users has increased significantly. Subsequently, credit card fraud has caused a large amount of economic losses to individual users and related financial enterprises. At present, traditional machine learning methods (such as SVM, random forest, Markov model, etc.) have been widely studied in credit card fraud detection, but these methods are often have difficulty in demonstrating their effectiveness when faced with unknown attack patterns. In this paper, a new Unsupervised Attentional Anomaly Detection Network-based Credit Card Fraud Detection framework (UAAD-FDNet) is proposed. Among them, fraudulent transactions are regarded as abnormal samples, and autoencoders with Feature Attention and GANs are used to effectively separate them from massive transaction data. Extensive experimental results on Kaggle Credit Card Fraud Detection Dataset and IEEE-CIS Fraud Detection Dataset demonstrate that the proposed method outperforms existing fraud detection methods. © 2023 by the authors.,The rise in technology, particularly the increase in online shopping, has made it easier for cybercriminals to obtain and exploit stolen payment card information. Traditional fraud detection systems are finding it increasingly challenging to keep up with the rapid pace of technological advancement, leading to a surge in payment card fraud. Hence, it is essential for companies to continually update their fraud detection methods to keep up with the latest tactics employed by fraudsters. Machine learning algorithms have the ability to analyze large datasets and quickly identify anomalies or deviations from normal behaviour, making them a highly effective tool for payment card fraud detection. By detecting fraud early, organizations can minimize their financial losses and prevent further damage. In this study, we generated a credit card fraud dataset that comprises three types of fraud cases. The dataset is imbalanced, with a ratio of fraudulent transactions at 0.004, making it close to real-world data. To handle the imbalance in the dataset related to credit card fraud detection, we employed popular machine learning models such as Random Forest, Decision Tree, Logistic Regression, and XGBoost. The results showed that XGBoost and Random Forest outperformed the other models on both the training and test sets. However, the Decision Tree algorithm with unlimited depth had the highest average accuracy on the training set and the lowest average accuracy on the test set, indicating that this algorithm should be avoided due to overfitting. In conclusion, our study highlights the significance of using machine learning algorithms for payment card fraud detection. The results demonstrate that XGBoost and Random Forest are the most effective models for detecting credit card fraud in imbalanced datasets. By employing these models, organizations can improve their fraud detection capabilities and minimize the financial impact of payment card fraud. © 2023 Little Lion Scientific."
196,195,22,195_alloys_alloy_aluminum_ensemble,"alloys,alloy,aluminum,ensemble,materials,dataset,entropy,prediction,metal,elements","Obtaining a suitable chemical composition for high-entropy alloys (HEAs) with superior mechanical properties and good biocompatibility is still a formidable challenge through conventional trial-and-error methods. Here, based on a large amount of experimental data, a machine learning technique may be used to establish the relationship between the composition and the mechanical properties of the biocompatible HEAs. Subsequently, first-principles calculations are performed to verify the accuracy of the prediction results from the machine learning model. The predicted Young’s modulus and yield strength of HEAs performed very well in the previous experiments. In addition, the effect on the mechanical properties of alloying an element is investigated in the selected Ti-Zr-Hf-Nb-Ta HEA with the high crystal symmetry. Finally, the Ti8-Zr20-Hf16-Nb35-Ta21 HEA predicted by the machine learning model exhibits a good combination of biocompatibility and mechanical performance, attributed to a significant electron flow and charge recombination. This work reveals the importance of these strategies, combined with machine learning and first-principles calculations, on the development of advanced biocompatible HEAs. © 2023 by the authors.,High entropy alloys (HEAs) are an important material class in the development of next-generation structural materials, but the astronomically large composition space cannot be efficiently explored by experiments or first-principles calculations. Machine learning (ML) methods might address this challenge, but ML of HEAs has been hindered by the scarcity of HEA property data. In this work, the EMTO-CPA method was used to generate a large HEA dataset (spanning a composition space of 14 elements) containing 7086 cubic HEA structures with structural properties, 1911 of which have the complete elastic tensor calculated. The elastic property dataset was used to train a ML model with the Deep Sets architecture. The Deep Sets model has better predictive performance and generalizability compared to other ML models. Association rule mining was applied to the model predictions to describe the compositional dependence of HEA elastic properties and to demonstrate the potential for data-driven alloy design. © 2022, The Author(s).,High Entropy Alloys (HEAs) are a novel category of materials with unique physical and mechanical properties, which makes them a promising candidate for various engineering applications. However, the accurate identification of different phases present in HEAs, controlling its superb properties, is still a challenging task. To address this issue, this study proposes a comprehensive approach based on machine learning (ML) and deep learning (DL) to detect the phases present in HEAs. A deep neural network (DNN) was successfully developed that uses unique feature values of a number of HEAs to accurately predict different phases present in the alloy system. The DNN architecture consists of multiple layers that allow it to learn and extract the relevant features from the data, and then classify them into their respective phases. To train and validate the DNN model, a large dataset of HEAs (with known phases and their corresponding feature values) was collected from the literature. This dataset was then used to train the DNN model using a supervised learning approach. The DNN model was also validated using a separate set of HEAs with unknown phases to test its accuracy and reliability. Comparison with known results demonstrated that the DNN model can accurately predict the phases present in HEAs with high accuracy. After implementing all ML and DL algorithms, a voting ensemble (containing six best performing algorithms) was constructed. The final accuracy was obtained as high as 84% which is quite reasonable for classification problem. Further, it was also observed how significant each feature is in determining the phase(s) of HEAs. The proposed approach offers a promising method for phase detection in HEAs, which can lead to a deeper understanding of their properties and facilitate the design of new materials for various applications. © 2023 Elsevier Ltd"
197,196,22,196_classifiers_classification_cardiac_cardiovascular,"classifiers,classification,cardiac,cardiovascular,prediction,datasets,dataset,boosting,coronary,heart","For the design and implementation of CDSS, computation time and prognostic accuracy are very important. To analyze the large collection of a dataset for detecting and diagnosis disease ML techniques are used. According to the reports of World Health Organizations, HD is a major cause of death and killer in urban and rural areas or worldwide. The main reason for this is a shortage of doctors and delay in the diagnosis. In this research work, heart disease is a diagnosis by the data mining techniques and used the clinical parameters of patients for early stages diagnosis. The intend of this learning to develop a representation that relies on the prediction method for coronary heart disease. This proposed work used the approach of self-diagnosis Algorithm, Fuzzy Artificial neural network, and NCA & PCA and imputation methods. By the use of this technique computation time for prediction of Coronary HD can be reduced. For the implementation of this the two datasets are using such as Cleveland and Statlog datasets that is collected from the UCI kaggle the ML repository. The datasets for the disease prediction measure are used to accurately calculate the difference between variables and to determine whether they are correlated or not. For this classification model, the performance measure is calculated in requisites of their accuracy, precision, recall, and specificity. This approach is evaluated on the heart disease datasets for improving the accuracy performance results obtained. The outcome for KNN+SDA+NCA+FuzzyANN for Cleveland dataset accuracy achieved 98.56 %.and for Statlog dataset 98.66 %.. © 2022 The authors.,For the design and implementation of Clinical decision support system, computation time and prognostic accuracy are very important. To analyze the large collection of a dataset for detecting and diagnosis disease machine learning techniques are used. According to the reports of World Health Organizations, heart disease is a major cause of death and killer in urban and rural areas or worldwide. The main reason for this is a shortage of doctors and delay in the diagnosis. The outcome presaging of this disease is a very challenging job. This proposed work used the approach of self-diagnosis algorithm, fuzzy artificial neural network, and NCA and PCA and imputation methods. By the use of this technique reduces the computation time for prediction of Coronary heart disease. For the implementation of this the two datasets are using such as Cleveland and Statlog datasets. In this research work, heart disease is a diagnosis by used the clinical parameters of patients for early stages. Classifiers used for that random forest algorithm, ANN, and K-NN algorithm. The datasets for the disease prediction measure are used to accurately calculate the difference between variables and to determine whether they are correlated or not. For this classification model, the performance measure is calculated in requisites of their accuracy, precision, recall, and specificity. This approach is evaluated on the heart disease datasets for improving the accuracy performance results obtained. The experimental results obtained by NCA provide greater performance in terms of performance metrics for the multiple classifiers like RF, DT, NB, SVM. The aim of this study to develop the classification model as well as show a comparative analysis between existing systems is discussed. In this paper for Cleveland dataset has taken 303 instances and 14 attributes are used. This dataset has preprocessed dataset. This paper depicts the higher accuracy score with the multiple classifiers such as Random forest,SVM, and NB using NCA is 99.34% and for DT is 98%. Overall NCA is best in terms of classification accuracy. The result shows not only accuracy for the proposed method but obtains better results for multiple classifiers that exhibit their reliability.in medical science, this approach is used to predict heart disease at its early stages.The outcome for KNN + SDA + NCA + Fuzzy ANN for Cleveland dataset accuracy achieved 98.56% and for Statlog dataset 98.66%. This result describes the hybrid method used for prediction of heart disease and achieved better results. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Heart attack is a life-threatening condition which is mostly caused due to coronary disease resulting in death in human beings. Detecting the risk of heart diseases is one of the most important problems in medical science that can be prevented and treated with early detection and appropriate medical management; it can also help to predict a large number of medical needs and reduce expenses for treatment. Predicting the occurrence of heart diseases by machine learning (ML) algorithms has become significant work in healthcare industry. This study aims to create a such system that is used for predicting whether a patient is likely to develop heart attacks, by analysing various data sources including electronic health records and clinical diagnosis reports from hospital clinics. ML is used as a process in which computers learn from data in order to make predictions about new datasets. The algorithms created for predictive data analysis are often used for commercial purposes. This paper presents an overview to forecast the likelihood of a heart attack for which many ML methodologies and techniques are applied. In order to improve medical diagnosis, the paper compares various algorithms such as Random Forest, Regression models, K-nearest neighbour imputation (KNN), Naïve Bayes algorithm etc. It is found that the Random Forest algorithm provides a better accuracy of 88.52% in forecasting heart attack risk, which could herald a revolution in the diagnosis and treatment of cardiovascular illnesses. © 2023 The Authors. Healthcare Technology Letters published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology."
198,197,22,197_translations_translator_translating_translation,"translations,translator,translating,translation,multilingual,corpus,kazakhenglish,corpora,rnn,chinesevietnamese","Blockchain technology can create a shared platform for English translation and reserve a large number of practical corpus resources, thus improving the quality of machine translation. This paper first introduces the research status of foreign language corpus and blockchain English translation in China. Then it introduces the basic principles of BPNN and particle swarm optimization and constructs the PSO-BP model. Experiments show that the prediction accuracy of BPNN optimized by particle swarm optimization algorithm is greatly improved, the convergence speed is faster, and it will not fall into the local optimal trap. Finally, this paper proposes the implementation path of blockchain in corpus translation application: (1) build ""blockchain+ AI"" English translation corpus and (2) improve the machine English translation software of the ""blockchain+ AI"" English translation training platform. © 2023 IGI Global. All rights reserved.,Neural Machine Translation (NMT) brings promising improvements in translation quality, but until recently, these models rely on large-scale parallel corpora. As such corpora only exist on a handful of language pairs, the translation performance is far from the desired effect in the majority of low-resource languages. Thus, developing low-resource language translation techniques is crucial and it has become a popular research field in neural machine translation. In this article, we make an overall review of existing deep learning techniques in low-resource NMT. We first show the research status as well as some widely used low-resource datasets. Then, we categorize the existing methods and show some representative works detailedly. Finally, we summarize the common characters among them and outline the future directions in this field.  © 2022 Association for Computing Machinery.,The development of the machine translation field was driven by people’s need to communicate with each other globally by automatically translating words, sentences, and texts from one language into another. The neural machine translation approach has become one of the most significant in recent years. This approach requires large parallel corpora not available for low-resource languages, such as the Kazakh language, which makes it difficult to achieve the high performance of the neural machine translation models. This article explores the existing methods for dealing with low-resource languages by artificially increasing the size of the corpora and improving the performance of the Kazakh–English machine translation models. These methods are called forward translation, backward translation, and transfer learning. Then the Sequence-to-Sequence (recurrent neural network and bidirectional recurrent neural network) and Transformer neural machine translation architectures with their features and specifications are concerned for conducting experiments in training models on parallel corpora. The experimental part focuses on building translation models for the high-quality translation of formal social, political, and scientific texts with the synthetic parallel sentences from existing monolingual data in the Kazakh language using the forward translation approach and combining them with the parallel corpora parsed from the official government websites. The total corpora of 380,000 parallel Kazakh–English sentences are trained on the recurrent neural network, bidirectional recurrent neural network, and Transformer models of the OpenNMT framework. The quality of the trained model is evaluated with the BLEU, WER, and TER metrics. Moreover, the sample translations were also analyzed. The RNN and BRNN models showed a more precise translation than the Transformer model. The Byte-Pair Encoding tokenization technique showed better metrics scores and translation than the word tokenization technique. The Bidirectional recurrent neural network with the Byte-Pair Encoding technique showed the best performance with 0.49 BLEU, 0.51 WER, and 0.45 TER. © Copyright 2023 Karyukin et al."
199,198,22,198_surveillance_footage_videos_webcam,"surveillance,footage,videos,webcam,cameras,cctv,crimenet,recognition,imcam,detection","Due to the ever increasing number of closed circuit television (CCTV) cameras worldwide, it is the need of the hour to automate the screening of video content. Still, the majority of video content is manually screened to detect some anomalous incidence or activity. Automatic abnormal event detection such as theft, burglary, or accidents may be helpful in many situations. However, there are significant difficulties in processing video data acquired by several cameras at a central location, such as bandwidth, latency, large computing resource needs, and so on. To address this issue, an edge-based visual surveillance technique has been implemented, in which video analytics are performed on the edge nodes to detect aberrant incidents in the video stream. Various deep learning models were trained to distinguish 13 different categories of aberrant incidences in video. A customized Bi-LSTM model outperforms existing cutting-edge approaches. This approach is used on edge nodes to process video locally. The user can receive analytics reports and notifications. The experimental findings suggest that the proposed system is appropriate for visual surveillance with increased accuracy and lower cost and processing resources. © 2024 by the authors.,Surveillance cameras are increasingly being used worldwide due to the proliferation of digital video capturing, storage, and processing technologies. However, the large volume of video data generated makes it difficult for humans to perform real-time analysis, and even manual approaches can result in delayed detection of events. Automatic violence detection in surveillance footage has therefore gained significant attention in the scientific community as a way to address this challenge. With the advancement of machine learning algorithms, automatic video recognition tasks such as violence detection have become increasingly feasible. In this study, we investigate the use of smart networks that model the dynamic relationships between actors and/or objects using 3D convolutions to capture both the spatial and temporal structure of the data. We also leverage the knowledge learned by a pre-trained action recognition model for efficient and accurate violence detection in surveillance footage. We extend and evaluate several public datasets featuring diverse and challenging video content to assess the effectiveness of our proposed methods. Our results show that our approach outperforms state-of-the-art methods, achieving approximately a 2% improvement in accuracy with fewer model parameters. Additionally, our experiments demonstrate the robustness of our approach under common compression artifacts encountered in remote server processing applications.  © 2013 IEEE.,Purpose: This paper aims to implement and extend the You Only Live Once (YOLO) algorithm for detection of objects and activities. The advantage of YOLO is that it only runs a neural network once to detect the objects in an image, which is why it is powerful and fast. Cameras are found at many different crossroads and locations, but video processing of the feed through an object detection algorithm allows determining and tracking what is captured. Video Surveillance has many applications such as Car Tracking and tracking of people related to crime prevention. This paper provides exhaustive comparison between the existing methods and proposed method. Proposed method is found to have highest object detection accuracy. Design/methodology/approach: The goal of this research is to develop a deep learning framework to automate the task of analyzing video footage through object detection in images. This framework processes video feed or image frames from CCTV, webcam or a DroidCam, which allows the camera in a mobile phone to be used as a webcam for a laptop. The object detection algorithm, with its model trained on a large data set of images, is able to load in each image given as an input, process the image and determine the categories of the matching objects that it finds. As a proof of concept, this research demonstrates the algorithm on images of several different objects. This research implements and extends the YOLO algorithm for detection of objects and activities. The advantage of YOLO is that it only runs a neural network once to detect the objects in an image, which is why it is powerful and fast. Cameras are found at many different crossroads and locations, but video processing of the feed through an object detection algorithm allows determining and tracking what is captured. For video surveillance of traffic cameras, this has many applications, such as car tracking and person tracking for crime prevention. In this research, the implemented algorithm with the proposed methodology is compared against several different prior existing methods in literature. The proposed method was found to have the highest object detection accuracy for object detection and activity recognition, better than other existing methods. Findings: The results indicate that the proposed deep learning–based model can be implemented in real-time for object detection and activity recognition. The added features of car crash detection, fall detection and social distancing detection can be used to implement a real-time video surveillance system that can help save lives and protect people. Such a real-time video surveillance system could be installed at street and traffic cameras and in CCTV systems. When this system would detect a car crash or a fatal human or pedestrian fall with injury, it can be programmed to send automatic messages to the nearest local police, emergency and fire stations. When this system would detect a social distancing violation, it can be programmed to inform the local authorities or sound an alarm with a warning message to alert the public to maintain their distance and avoid spreading their aerosol particles that may cause the spread of viruses, including the COVID-19 virus. Originality/value: This paper proposes an improved and augmented version of the YOLOv3 model that has been extended to perform activity recognition, such as car crash detection, human fall detection and social distancing detection. The proposed model is based on a deep learning convolutional neural network model used to detect objects in images. The model is trained using the widely used and publicly available Common Objects in Context data set. The proposed model, being an extension of YOLO, can be implemented for real-time object and activity recognition. The proposed model had higher accuracies for both large-scale and all-scale object detection. This proposed model also exceeded all the other previous methods that were compared in extending and augmenting the object detection to activity recognition. The proposed model resulted in the highest accuracy for car crash detection, fall detection and social distancing detection. © 2023, Emerald Publishing Limited."
200,199,22,199_driving_autonomous_planning_lanechanging,"driving,autonomous,planning,lanechanging,highway,learns,driver,adversarial,road,steering","The traffic environment and driving behaviors are of great complexity and uncertainty in our physical world. Therefore, training in the digital world with low cost and diverse complexities become popular for autonomous driving in recent years. However, the current training methods tend to be limited to static data sets and deterministic models that do not sufficiently take into account the uncertainty and diversity prevalent in real traffic scenarios. These approaches also limit more possibilities for the comprehensive development and optimization of vision systems. In this paper, we develop a parallel training method based on artificial systems, computational experiments, and parallel execution (ACP) for the intelligent optimization and learning of the aforementioned agents in uncertain driving spaces. Parallel training creates a virtual driving space following the instruction of the ACP approach and conducts large-scale rehearsal experiments for possible scenarios. By enhancing the diversity of virtual scenarios, intelligent vehicles are trained to respond and adapt to the diverse uncertainties in the physical real-world driving space. Specifically, parallel training first proposes a standard operating procedure for intelligent driving systems, namely the projection-emergence-convergence-operation (PECO) loop. Digital quadruplets for parallel training, i.e., physical, descriptive, predictive, and prescriptive coaches, are also proposed. With the guidance of parallel training, virtual and real-world driving spaces are set up in parallel and interact frequently. They are closely linked and unified in opposition to each other, ultimately building a parallel driving system that fulfills safety, security, sustainability, sensitivity, service, and smartness (6S).  © 2016 IEEE.,An open problem in autonomous vehicle safety validation is building reliable models of human driving behavior in simulation. This work presents an approach to learn neural driving policies from real world driving demonstration data. We model human driving as a sequential decision making problem that is characterized by non-linearity and stochasticity, and unknown underlying cost functions. Imitation learning is an approach for generating intelligent behavior when the cost function is unknown or difficult to specify. Building upon work in inverse reinforcement learning (IRL), Generative Adversarial Imitation Learning (GAIL) aims to provide effective imitation even for problems with large or continuous state and action spaces, such as modeling human driving. This article describes the use of GAIL for learning-based driver modeling. Because driver modeling is inherently a multi-agent problem, where the interaction between agents needs to be modeled, this paper describes a parameter-sharing extension of GAIL called PS-GAIL to tackle multi-agent driver modeling. In addition, GAIL is domain agnostic, making it difficult to encode specific knowledge relevant to driving in the learning process. This paper describes Reward Augmented Imitation Learning (RAIL), which modifies the reward signal to provide domain-specific knowledge to the agent. Finally, human demonstrations are dependent upon latent factors that may not be captured by GAIL. This paper describes Burn-InfoGAIL, which allows for disentanglement of latent variability in demonstrations. Imitation learning experiments are performed using NGSIM, a real-world highway driving dataset. Experiments show that these modifications to GAIL can successfully model highway driving behavior, accurately replicating human demonstrations and generating realistic, emergent behavior in the traffic flow arising from the interaction between driving agents.  © 2000-2011 IEEE.,Planning appropriate driving trajectory for route following is an important function for autonomous driving. Behavioral cloning, which allows automatic trajectory learning and improvement, has been effectively used in driving trajectory planning. However, existing behavioral cloning methods always rely on large scales of time-consuming, laborious, and reliable labels. To address this problem, this paper proposes a new off-policy imitation learning method for autonomous driving using task knowledge distillation. This novel method clones human driving behavior and effectively transfers the driving strategies to domain shift scenarios. The experiment results indicate that our method can lead to satisfactory route-following performance in realistic urban driving scenes and can transfer the driving strategies to new unknown scenes under various illumination and weather scenarios for autonomous driving.  © 2016 IEEE."
201,200,22,200_optimization_regularization_minimax_overparameterized,"optimization,regularization,minimax,overparameterized,regularized,algorithms,optimal,gradient,hessian,multipliers","In a class of large-scale distributed optimization, the calculation of RELM based on the Moore–Penrose inverse matrix is prohibitively expensive, which hinders the formulation of a computationally efficient optimization model. Attempting to improve the model’s convergence performance, this paper proposes a low computing cost Alternating Direction Method of Multipliers (ADMM), where the original update in ADMM is solved inexactly with approximate curvature information. Based on quasi-Newton techniques, the ADMM approach allows us to solve convex optimization with reasonable accuracy and computational effort. By introducing this algorithm into the RELM model, the model fitting problem can be decomposed into a set of subproblems that can be executed in parallel to achieve efficient classification performance. To avoid the storage of expensive Hessian for large problems, BFGS with limited memory is proposed with computational efficiency. And the optimal parameter values of the step-size search method are obtained through Wolfe line search strategy. To demonstrate the superiority of our methods, numerical experiments are conducted on eight real-world datasets. Results on problems arising in machine learning suggest that the proposed method is competitive with other similar methods, both in terms of better computational efficiency as well as accuracy. © 2023 by the authors.,Alternating direction method of multipliers (ADMM) receives much attention in the field of optimization and computer science, etc. The generalized ADMM (G-ADMM) proposed by Eckstein and Bertsekas incorporates an acceleration factor and is more efficient than the original ADMM. However, G-ADMM is not applicable in some models where the objective function value (or its gradient) is computationally costly or even impossible to compute. In this paper, we consider the two-block separable convex optimization problem with linear constraints, where only noisy estimations of the gradient of the objective function are accessible. Under this setting, we propose a stochastic linearized generalized ADMM (called SLG-ADMM) where two subproblems are approximated by some linearization strategies. And in theory, we analyze the expected convergence rates and large deviation properties of SLG-ADMM. In particular, we show that the worst-case expected convergence rates of SLG-ADMM are and for solving general convex and strongly convex problems, respectively, where N is the iteration number, similarly hereinafter, and with high probability, SLG-ADMM has and constraint violation bounds and objective error bounds for general convex and strongly convex problems, respectively. © The Author(s), 2023. Published by Cambridge University Press.,As a classical machine learning model, support vector machine (SVM) has attracted much attention due to its rigorous theoretical foundation and powerful discriminative performance. The doubly regularized SVM (DRSVM) is an important variant of SVM based on elastic-net regularization, which considers both the sparsity and stability of the model. To tackle the problems of explosive increases in data dimensions and data volume, the alternating direction method of multipliers (ADMM) algorithm can be used to train the DRSVM model. ADMM is an effective iterative algorithm for solving convex optimization problems by decomposing a large issue into a series of solvable subproblems, which is also well suited for distributed computing. However, lack of guaranteed convergence and slow convergence rate are two critical limitations of ADMM. In this paper, a 3-block ADMM algorithm based on the over-relaxation technique is proposed to accelerate DRSVM training, namely, the over-relaxed DRSVM (O-RDRSVM). The main strategy of the over-relaxation technique is to further append the information from the previous iteration to the next iteration to improve the convergence of ADMM. We also propose a distributed version of O-RDRSVM to handle parallel and distributed computing faster, termed DO-RDRSVM. Moreover, we develop a fast O-RDRSVM algorithm (FO-RDRSVM) and a fast DO-RDRSVM algorithm (FDO-RDRSVM), which further reduce the computational cost of O-RDRSVM and DO-RDRSVM by employing the matrix inversion lemma. The convergence analyses ensure the effectiveness of our algorithms for DRSVM training. Finally, extensive experiments on public datasets demonstrate the advantages of our algorithms in terms of convergence rate and training time while maintaining accuracy and sparsity comparable to those of previous works. © 2023 Elsevier B.V."
202,201,22,201_groundwater_aquifers_aquifer_basin,"groundwater,aquifers,aquifer,basin,basins,geomorphology,runoff,geological,rainfall,alluvial","Groundwater quality management is pivotal for ensuring public health and ecological resilience. However, the conventional water quality indices often face challenges related to parameter selection, geographic coverage, and scalability. The integration of machine learning and spatial analysis represents a promising methodological shift, allowing for high accuracy and adaptive management strategies. The Safe Groundwater Project in Unsupplied Areas (2017–2020) employed a comprehensive Groundwater Quality Index (GQI) to evaluate potable groundwater quality across South Korea, utilizing a large dataset comprising 28 water quality parameters and 3552 wells. This study revealed that over 50 % of the evaluated wells (Total 8326 wells) were inappropriate as sources of drinking water, indicating a pressing need for policy revision. The averaged neural network model achieved a high predictive accuracy of approximately 95 % for GQI grades, outperforming other classification models. The introduction of 2D spatial analysis in conjunction with machine learning algorithms notably increased the predictive accuracy for unevenly distributed groundwater samples. Moreover, this combined approach enabled the intuitive visualization of groundwater vulnerability across various regions, which can inform targeted interventions for effective resource allocation and management. This research represents a methodologically robust, interdisciplinary approach that holds significant implications for a framework for future groundwater quality management and vulnerability assessment. © 2023 The Authors,Groundwater quality is typically measured through water sampling and lab analysis. The field-based measurements are costly and time-consuming when applied over a large domain. In this study, we developed a machine learning-based framework to map groundwater quality in an unconfined aquifer in the north of Iran. Groundwater samples were provided from 248 monitoring wells across the region. The groundwater quality index (GWQI) in each well was measured and classified into four classes: very poor, poor, good, and excellent, according to their cut-off values. Factors affecting groundwater quality, including distance to industrial centers, distance to residential areas, population density, aquifer transmissivity, precipitation, evaporation, geology, and elevation, were identified and prepared in the GIS environment. Six machine learning classifiers, including extreme gradient boosting (XGB), random forest (RF), support vector machine (SVM), artificial neural networks (ANN), k-nearest neighbor (KNN), and Gaussian classifier model (GCM), were used to establish relationships between GWQI and its controlling factors. The algorithms were evaluated using the receiver operating characteristic curve (ROC) and statistical efficiencies (overall accuracy, precision, recall, and F-1 score). Accuracy assessment showed that ML algorithms provided high accuracy in predicting groundwater quality. However, RF was selected as the optimum model given its higher accuracy (overall accuracy, precision, and recall = 0.92; ROC = 0.95). The trained RF model was used to map GWQI classes across the entire region. Results showed that the poor GWQI class is dominant in the study area (covering 66% of the study area), followed by good (19% of the area), very poor (14% of the area), and excellent (< 1% of the area) classes. An area of very poor GWQI was observed in the north. Feature analysis indicated that the distance to industrial locations is the main factor affecting groundwater quality in the region. The study provides a cost-effective methodology in groundwater quality modeling that can be duplicated in other regions with similar hydrological and geological settings. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,The Mississippi Alluvial Plain, located in the south-central United States, is undergoing long-term groundwater-level declines within the surficial Mississippi River Valley alluvial aquifer (hereinafter referred to as “alluvial aquifer”), which has raised concerns about future groundwater availability. In some parts of the alluvial aquifer, groundwater availability for common uses such as irrigation, public supply, and domestic use is limited by quality (for example, high salinity) rather than quantity of water stored in the aquifer. The Mississippi Alluvial Plain region has an abundance of water-quality measurements in the alluvial aquifer and deeper aquifers; however, large areas lack direct measurements of salinity to evaluate regional groundwater availability. Statistical models can interpolate between wells to fill in spatial data gaps. In 2021, the U.S. Geological Survey trained two boosted regression tree (BRT) machine-learning models on specific conductance data available between 1942 and 2020 to predict spatially continuous surfaces of groundwater salinity at multiple depths for the alluvial aquifer and deeper aquifers. Well construction information, water levels, and surficial variables such as geomorphology and soils were included as explanatory variables in this baseline model. Additionally, subsurface electrical resistivity data from the first aquifer-wide aerial electromagnetic (AEM) survey for the region were incorporated to create a geophysical model. This work expands on prior BRT salinity predictions of the alluvial aquifer and extends predictions south to the Gulf of Mexico, where groundwater salinity is high. AEM survey data were not available for the southern extent of the alluvial aquifer at the time of modeling. A BRT model was trained without (baseline) and with (geophysical) AEM variables to test the ability of the models to predict salinity where explanatory data are missing and response data are sparse. Additionally, model sensitivity to AEM survey data was evaluated to better understand how AEM variables influence specific conductance predictions. Model performance was improved with the addition of geophysical data, which added three-dimensional information, thereby improving salinity predictions at depth. Groundwater specific conductance predictions can help inform other geophysical investigations in the southern extent of the study area, where high groundwater specific conductance can obfuscate changes in aquifer sediment resistivity and could limit groundwater resources for agricultural, public supply, and domestic uses. © 2023, US Geological Survey. All rights reserved."
203,202,22,202_cyberbullying_bullying_nlp_twitter,"cyberbullying,bullying,nlp,twitter,tweets,victimization,corpus,hateful,speech,hate","Online hate speech has flourished on social networking sites due to the widespread availability of mobile computers and other Web knowledge. Extensive research has shown that online exposure to hate speech has real-world effects on marginalized communities. Research into methods of automatically identifying hate speech has garnered significant attention. Hate speech can affect any demographic, while some populations are more vulnerable than others. Relying solely on progressive learning is insufficient for achieving the goal of automatic hate speech identification. It need access to large amounts of labelled data to train a model. Inaccurate statistics on hate speech and preconceived notions have been the biggest obstacles in the field of hate speech research for a long time. This research provides a novel strategy for meeting these needs by combining a transfer-learning attitude-based BERT (Bidirectional Encoder Representations from Transformers) with a coral reef optimization-based approach (CROA). A feature selection (FC) optimization strategy for coral reefs, a coral reefs optimization method mimics coral behaviours for reef location and development. We might think of each potential answer to the problem as a coral trying to establish itself in the reefs. The results are refined at each stage by applying specialized operators from the coral reefs optimization algorithm. When everything is said and done, the optimal solution is chosen. We also use a cutting-edge fine-tuning method based on transfer learning to assess BERT’s ability to recognize hostile contexts in social media communications. The paper evaluates the proposed approach using Twitter datasets tagged for racist, sexist, homophobic, or otherwise offensive content. The numbers show that our strategy achieves 5%–10% higher precision and recall compared to other approaches. © 2024 by author(s).,Recently, cyberbullying has become one of the most important topics on social media. Online social media users have recognised this as a severe problem, and in recent years, effective detection models have been developed. This has taken on substantial importance. The numerous forms of cyberbullying on social media are highlighted by this poll. Currently, research is being done to identify cyberbullying using AI approaches. We talk about various machine learning and natural language processing (NLP) methods that are used to identify cyberbullying. Additionally, the difficulties and potential directions for future research in the area of AI detection of cyberbullying have been discussed. Attacks on victims of cyberbullying have surged by 40% in 2020’s pandemic season. 20% of the increase in juvenile suicides is attributable to cyberbullying. Attacks involving cyberbullying are expected to reach an all-time high in 2025, according to 60% of experts. 38% of respondents report daily exposure to cyberbullying on social media platforms. Even though many people are aware of cyberattacks, cyberbullying has begun to rise alarmingly. By keeping track of the signs of cyberbullying before it occurs, internet service providers can develop more precise classifications for the behaviour to prevent it. Large data sets can also be processed using deep learning techniques. © 2023, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.,There is an increased demand for detecting online hate speech, especially with the recent changing policies of hate content and free-of-speech right of online social media platforms. Detecting hate speech will reduce its negative impact on social media users. A lot of effort in the Natural Language Processing (NLP) field aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, which adds more challenges to hate speech detection tasks. Word representation from its domain will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibility of leveraging domain-specific word embedding as features and a bidirectional LSTM-based deep model as a classifier to automatically detect hate speech. This approach guarantees that the word is assigned its negative meaning, which is a very helpful technique to detect coded words. Furthermore, we investigate the use of the transfer learning language model (BERT) on the hate speech problem as a binary classification task as it provides high-performance results for many NLP tasks. The experiments showed that domain-specific word embedding with the bidirectional LSTM-based deep model achieved a 93% f1-score, while BERT achieved 96% f1-score on a combined balanced dataset from available hate speech datasets. The results proved that the performance of pre-trained models is influenced by the size of the trained data. Although there is a huge variation in the corpus size, the first approach achieved a very close result compared to BERT, which is trained on a huge data corpus, this is because it is trained on data related to the same domain. The first approach was very helpful to detect coded words while the second approach achieved better performance because it is trained on much larger data. To conclude, it is very helpful to build large pre-trained models from rich domains specific content in current social media platforms. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC."
204,203,22,203_molecular_molecule_graphbased_molecules,"molecular,molecule,graphbased,molecules,networks,learning,moleculestm,graphganfed,pharmaceutical,embeddings","Models based on machine learning can enable accurate and fast molecular property predictions, which is of interest in drug discovery and material design. Various supervised machine learning models have demonstrated promising performance, but the vast chemical space and the limited availability of property labels make supervised learning challenging. Recently, unsupervised transformer-based language models pretrained on a large unlabelled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, we present molecular embeddings obtained by training an efficient transformer encoder model, MoLFormer, which uses rotary positional embeddings. This model employs a linear attention mechanism, coupled with highly distributed training, on SMILES sequences of 1.1 billion unlabelled molecules from the PubChem and ZINC datasets. We show that the learned molecular representation outperforms existing baselines, including supervised and self-supervised graph neural networks and language models, on several downstream tasks from ten benchmark datasets. They perform competitively on two others. Further analyses, specifically through the lens of attention, demonstrate that MoLFormer trained on chemical SMILES indeed learns the spatial relationships between atoms within a molecule. These results provide encouraging evidence that large-scale molecular language models can capture sufficient chemical and structural information to predict various distinct molecular properties, including quantum-chemical properties. © 2022, The Author(s), under exclusive licence to Springer Nature Limited.,Although substantial efforts have been made using graph neural networks (GNNs) for artificial intelligence (AI)-driven drug discovery, effective molecular representation learning remains an open challenge, especially in the case of insufficient labeled molecules. Recent studies suggest that big GNN models pre-trained by self-supervised learning on unlabeled datasets enable better transfer performance in downstream molecular property prediction tasks. However, the approaches in these studies require multiple complex self-supervised tasks and large-scale datasets, which are time-consuming, computationally expensive and difficult to pre-train end-to-end. Here, we design a simple yet effective self-supervised strategy to simultaneously learn local and global information about molecules, and further propose a novel bi-branch masked graph transformer autoencoder (BatmanNet) to learn molecular representations. BatmanNet features two tailored complementary and asymmetric graph autoencoders to reconstruct the missing nodes and edges, respectively, from a masked molecular graph. With this design, BatmanNet can effectively capture the underlying structure and semantic information of molecules, thus improving the performance of molecular representation. BatmanNet achieves state-of-the-art results for multiple drug discovery tasks, including molecular properties prediction, drug–drug interaction and drug–target interaction, on 13 benchmark datasets, demonstrating its great potential and superiority in molecular representation learning. © The Author(s) 2023. Published by Oxford University Press. All rights reserved.,Molecular property prediction is an essential task in drug discovery. Recently, deep neural networks have accelerated the discovery of compounds with improved molecular profiles for effective drug development. In particular, graph neural networks (GNNs) have played a pivotal role in identifying promising drug candidates with desirable molecular properties. However, it is common for only a few molecules to share the same set of properties, which presents a low-data problem unanswered by regular machine learning (ML) approaches. Transformer networks have also emerged as a promising solution to model the long-range dependence in molecular embeddings and achieve encouraging results across a wide range of molecular property prediction tasks. Nonetheless, these methods still require a large number of data points per task to achieve acceptable performance. In this study, we propose a few-shot GNN-Transformer architecture, FS-GNNTR to face the challenge of low-data in molecular property prediction. The proposed model accepts molecules in the form of molecular graphs to model the local spatial context of molecular graph embeddings while preserving the global information of deep representations. Furthermore, we introduce a two-module meta-learning framework to iteratively update model parameters across few-shot tasks and predict new molecular properties with limited available data. Finally, we conduct multiple experiments on small-sized biological datasets for molecular property prediction, Tox21 and SIDER, and our results demonstrate the superior performance of FS-GNNTR compared to simpler graph-based baselines. The code and data underlying this article are available in the repository, https://github.com/ltorres97/FS-GNNTR. © 2023 The Author(s)"
205,204,22,204_fpga_hardware_programmable_benchmarks,"fpga,hardware,programmable,benchmarks,processor,implementations,computing,architectures,gpu,gpus","Support Vector Machines (SVM) are widely used techniques in the field of classification problems because of their ability to effectively deal with datasets that have complex non-linear structures and a high dimensionality. The compute-intensive training algorithm associated with SVM makes it challenging to keep an up-to-date model that accurately reflects the characteristics of newly arriving data points in real-time systems. This paper proposes a novel training algorithm for incremental learning from large datasets, based on a variant of Sequential Minimal Optimization (SMO). High-Level Synthesis (HLS) was used for implementing the Field Programmable Gate Array (FPGA) based Intellectual Property (IP) Core, which includes the computationally intensive kernel computation portion of the training algorithm. In addition to the kernel computation, the inference phase of the SVM classifier is built into the IP core, and its use can be switched on the fly. The computational latency and memory bandwidth of an IP core are optimized using loop pipelining and DMA burst data transfer. With the help of hardware/software co-design, the IP core is integrated into the design of a flexible and re-usable System on Chip (SoC) called PYNQ Overlay. The experiments show that the overlay outperforms the embedded processor, multiple hardware SVM classifiers, and hardware accelerated Convolutional Neural Networks (CNN) in terms of real-time efficiency. The Overlay makes much less use of the resources available on the chip in comparison to the majority of the CNN accelerators. The overlay achieves an average classification accuracy that is only 1% lower than that of an ARM Cortex-A9 processor, according to experimental results on six datasets. Furthermore, it can increase training speed by an average of 31.82x and inference speed by an average of 31.74x. In addition, the proposed Overlay design achieves a 2.3x improvement in average training speed, as measured in Mega bits per second, compared to existing SVM training implementations, along with incremental learning and multi-class classification support. © 2023 Elsevier B.V.,Field-programmable gate arrays (FPGAs) have grown to be an important platform for integrated circuit design and hardware emulation. However, with the dramatic increase in design scale, it has become a key challenge to partition very large scale integration into multi-FPGA systems. Fast estimation of FPGA on-chip resource usage for individual sub-circuit blocks early in the circuit design flow will provide an essential basis for reasonable circuit partition. It will also help FPGA designers to tune the circuits in hardware description language. In this article, we propose a framework for fast estimation of the on-chip resources consumed by register transfer level (RTL) designs with machine learning methods. We extensively collect RTL designs as a dataset, extract features from the result of a parser tool and analyze their roles, and train a targeted three-stage ensemble learning model. A 5,513× speedup is achieved while having 27% relative absolute error. Although the effect is sufficient to support RTL circuit partition, we discuss how the estimation quality continues to be improved.  © 2022 Association for Computing Machinery.,With the recent advances in hardware technologies like advanced CPUs and GPUs and the large availability of open-source libraries, machine learning has penetrated various domains, including Electronics Design Automation (EDA). EDA consists of multiple stages, from high-level synthesis and logic synthesis to placement and routing. Traditionally, estimating resources and areas from one level of design abstraction to the next level uses mathematical, statistical, and analytical approaches. However, as the technology node decreases and the number of cells inside the chip increases, the traditional estimation methods fail to correlate with the actual post-route values. Machine-learning (ML) based methodologies pave a strong path towards accurately estimating post-route values. In this paper, we present a comprehensive survey of the existing literature in the ML application field in EDA, emphasizing FPGA design automation tools. We discuss how ML is applied in different stages to predict congestion, power, performance, and area (PPA), both for High-Level Synthesis (HLS) and Register Transfer Level (RTL)-based FPGA designs, application of design space exploration and application in Computer-Aided Design (CAD) tool parameter settings to optimize timing and area requirements. Reinforcement learning is widely applied in both FPGA and ASIC physical design flow, a topic of discussion in this paper. We also discuss various ML models like classical regression and classification ML, convolution neural networks, reinforcement learning, and graph convolution network and their application in EDA.  © 2013 IEEE."
206,205,21,205_crowds_crowd_crowded_surveillance,"crowds,crowd,crowded,surveillance,people,camera,organizers,datasets,convolutional,fullysupervised","Labeling is onerous for crowd counting as it should annotate each individual in crowd images. Recently, several methods have been proposed for semi-supervised crowd counting to reduce the labeling efforts. Given a limited labeling budget, they typically select a few crowd images and densely label all individuals in each of them. Despite the promising results, we argue the None-or-All labeling strategy is suboptimal as the densely labeled individuals in each crowd image usually appear similar while the massive unlabeled crowd images may contain entirely diverse individuals. To this end, we propose to break the labeling chain of previous methods and make the first attempt to reduce spatial labeling redundancy for semi-supervised crowd counting. First, instead of annotating all the regions in each crowd image, we propose to annotate the representative ones only. We analyze the region representativeness from both vertical and horizontal directions of initially estimated density maps, and formulate them as cluster centers of Gaussian Mixture Models. Additionally, to leverage the rich unlabeled regions, we exploit the similarities among individuals in each crowd image to directly supervise the unlabeled regions via feature propagation instead of the error-prone label propagation employed in the previous methods. In this way, we can transfer the original spatial labeling redundancy caused by individual similarities to effective supervision signals on the unlabeled regions. Extensive experiments on the widely-used benchmarks demonstrate that our method can outperform previous best approaches by a large margin.  © 1979-2012 IEEE.,Crowd localization is a new computer vision task, evolved from crowd counting. Different from the latter, it provides more precise location information for each instance, not just counting numbers for the whole crowd scene, which brings greater challenges, especially in extremely congested crowd scenes. In this paper, we focus on how to achieve precise instance localization in high-density crowd scenes, and to alleviate the problem that the feature extraction ability of the traditional model is reduced due to the target occlusion, the image blur, etc. To this end, we propose a Dilated Convolutional Swin Transformer (DCST) for congested crowd scenes. Specifically, a window-based vision transformer is introduced into the crowd localization task, which effectively improves the capacity of representation learning. Then, the well-designed dilated convolutional module is inserted into some different stages of the transformer to enhance the large-range contextual information. Extensive experiments evidence the effectiveness of the proposed methods and achieve the state-of-the-art performance on five popular datasets. Especially, the proposed model achieves F1-measure of 77.5% and MAE of 84.2 in terms of localization and counting performance, respectively. © 2022 Elsevier B.V.,Aiming at the problem that the existing crowd counting methods cannot achieve accurate crowd counting and map visualization in a large scene, a crowd density estimation and mapping method based on surveillance video and GIS (CDEM-M) is proposed. Firstly, a crowd semantic segmentation model (CSSM) and a crowd denoising model (CDM) suitable for high-altitude scenarios are constructed by transfer learning. Then, based on the homography matrix between the video and remote sensing image, the crowd areas in the video are projected to the map space. Finally, according to the distance from the crowd target to the camera, the camera inclination, and the area of the crowd polygon in the geographic space, a BP neural network for the crowd density estimation is constructed. The results show the following: (1) The test accuracy of the CSSM was 96.70%, and the classification accuracy of the CDM was 86.29%, which can achieve a high-precision crowd extraction in large scenes. (2) The BP neural network for the crowd density estimation was constructed, with an average error of 1.2 and a mean square error of 4.5. Compared to the density map method, the MAE and RMSE of the CDEM-M are reduced by 89.9 and 85.1, respectively, which is more suitable for a high-altitude camera. (3) The crowd polygons were filled with the corresponding number of points, and the symbol was a human icon. The crowd mapping and visual expression were realized. The CDEM-M can be used for crowd supervision in stations, shopping malls, and sports venues. © 2023 by the authors."
207,206,21,206_deepmedic_cnnrnn_deepinfusion_cnnbased,"deepmedic,cnnrnn,deepinfusion,cnnbased,intracranial,angiography,trainingvalidation,deep,aneurysm,aneurysms","To investigate the performance of a joint convolutional neural networks-recurrent neural networks (CNN-RNN) using an attention mechanism in identifying and classifying intracranial hemorrhage (ICH) on a large multi-center dataset; to test its performance in a prospective independent sample consisting of consecutive real-world patients. All consecutive patients who underwent emergency non-contrast-enhanced head CT in five different centers were retrospectively gathered. Five neuroradiologists created the ground-truth labels. The development dataset was divided into the training and validation set. After the development phase, we integrated the deep learning model into an independent center’s PACS environment for over six months for assessing the performance in a real clinical setting. Three radiologists created the ground-truth labels of the testing set with a majority voting. A total of 55,179 head CT scans of 48,070 patients, 28,253 men (58.77%), with a mean age of 53.84 ± 17.64 years (range 18–89) were enrolled in the study. The validation sample comprised 5211 head CT scans, with 991 being annotated as ICH-positive. The model's binary accuracy, sensitivity, and specificity on the validation set were 99.41%, 99.70%, and 98.91, respectively. During the prospective implementation, the model yielded an accuracy of 96.02% on 452 head CT scans with an average prediction time of 45 ± 8 s. The joint CNN-RNN model with an attention mechanism yielded excellent diagnostic accuracy in assessing ICH and its subtypes on a large-scale sample. The model was seamlessly integrated into the radiology workflow. Though slightly decreased performance, it provided decisions on the sample of consecutive real-world patients within a minute. © 2022, The Author(s).,Background and purpose: Multiple attempts at intracranial hemorrhage (ICH) detection using deep-learning techniques have been plagued by clinical failures. We aimed to compare the performance of a deep-learning algorithm for ICH detection trained on strongly and weakly annotated datasets, and to assess whether a weighted ensemble model that integrates separate models trained using datasets with different ICH improves performance. Methods: We used brain CT scans from the Radiological Society of North America (27,861 CT scans, 3,528 ICHs) and AI-Hub (53,045 CT scans, 7,013 ICHs) for training. DenseNet121, InceptionResNetV2, MobileNetV2, and VGG19 were trained on strongly and weakly annotated datasets and compared using independent external test datasets. We then developed a weighted ensemble model combining separate models trained on all ICH, subdural hemorrhage (SDH), subarachnoid hemorrhage (SAH), and small-lesion ICH cases. The final weighted ensemble model was compared to four well-known deep-learning models. After external testing, six neurologists reviewed 91 ICH cases difficult for AI and humans. Results: InceptionResNetV2, MobileNetV2, and VGG19 models outperformed when trained on strongly annotated datasets. A weighted ensemble model combining models trained on SDH, SAH, and small-lesion ICH had a higher AUC, compared with a model trained on all ICH cases only. This model outperformed four deep-learning models (AUC [95% C.I.]: Ensemble model, 0.953[0.938–0.965]; InceptionResNetV2, 0.852[0.828–0.873]; DenseNet121, 0.875[0.852–0.895]; VGG19, 0.796[0.770–0.821]; MobileNetV2, 0.650[0.620–0.680]; p < 0.0001). In addition, the case review showed that a better understanding and management of difficult cases may facilitate clinical use of ICH detection algorithms. Conclusion: We propose a weighted ensemble model for ICH detection, trained on large-scale, strongly annotated CT scans, as no model can capture all aspects of complex tasks. Copyright © 2023 Kang, Park, Ryu, Schellingerhout, Kim, Kim, Park, Lee, Han, Jeong and Kim.,The accuracy of computed tomography angiography (CTA) image interpretation depends on the radiologist. This study aims to develop a new method for automatically detecting intracranial aneurysms from CTA images using deep learning, based on a convolutional neural network (CNN) implemented on the DeepMedic platform. Ninety CTA scans of patients with intracranial aneurysms are collected and divided into two datasets: training (80 subjects) and test (10 subjects) datasets. Subsequently, a deep learning architecture with a three-dimensional (3D) CNN model is implemented on the DeepMedic platform for the automatic segmentation and detection of intracranial aneurysms from the CTA images. The samples in the training dataset are used to train the CNN model, and those in the test dataset are used to assess the performance of the established system. Sensitivity, positive predictive value (PPV), and false positives are evaluated. The overall sensitivity and PPV of this system for detecting intracranial aneurysms from CTA images are 92.3% and 100%, respectively, and the segmentation sensitivity is 92.3%. The performance of the system in the detection of intracranial aneurysms is closely related to their size. The detection sensitivity for small intracranial aneurysms (? 3 mm) is 66.7%, whereas the sensitivity of detection for large (> 10 mm) and medium-sized (3–10 mm) intracranial aneurysms is 100%. The deep learning architecture with a 3D CNN model on the DeepMedic platform can reliably segment and detect intracranial aneurysms from CTA images with high sensitivity. © 2022, The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine."
208,207,20,207_latentfactorizationoftensors_tensor_tensors_parallelization,"latentfactorizationoftensors,tensor,tensors,parallelization,factor,latent,sgd,lfa,hessianvector,pca","High-dimensional and incomplete (HDI) data are omnipresent in a variety of Big Data-related applications. Latent feature analysis (LFA) is a typical representation learning method that can extract useful yet latent knowledge from HDI data via low-rank embedding. Existing LFA-based models mostly adopt a single-metric-based modeling strategy, where the representation designed for the embedding Loss function is fixed and exclusive. However, real-world HDI data are commonly heterogeneous and have large diverse underlying patterns, making a single-metric-based model cannot represent such HDI data in a comprehensive and unbiased fashion. Motivated by this discovery, this paper proposes a multi-metric latent feature (MMLF) model whose ideas are two-fold: 1) two vector spaces and three Lp-norms are simultaneously adopted to develop six LFA variants, each of which possesses a unique merit, and 2) all the variants are aggregated with a tailored, self-adaptive weighting strategy. As such, the proposed MMLF enjoys the merits originated from a set of disparate metric spaces all at once, achieving the comprehensive and unbiased representation of HDI data. Theoretical study guarantees that MMLF attains evident performance gain. Extensive experiments on ten real-world HDI matrices, spanning a wide range of industrial and scientific areas, verify that the proposed MMLF significantly outperforms nine state-of-the-art, shallow and deep counterparts. IEEE,Latent factor analysis (LFA) is efficient in knowledge discovery from a high-dimensional and incomplete (HDI) matrix frequently encountered in industrial big data-related applications. A stochastic gradient descent (SGD) algorithm is commonly adopted as a learning algorithm for LFA owing to its high efficiency. However, its sequential nature makes it less scalable when processing large-scale data. Although alternating SGD decouples an LFA process to achieve parallelization, its performance relies on its hyper-parameters that are highly expensive to tune. To address this issue, this paper presents three extended alternating SGD algorithms whose hyper-parameters are made adaptive through particle swarm optimization. Correspondingly, three Parallel Adaptive LFA (PAL) models are proposed and achieve highly efficient latent factor acquisition from an HDI matrix. Experiments have been conducted on four HDI matrices collected from industrial applications, and the benchmark models are LFA models based on state-of-the-art parallel SGD algorithms including the alternative SGD, Hogwild!, distributed gradient descent, and sparse matrix factorization parallelization. The results demonstrate that compared with the benchmarks, with 32 threads, the proposed PAL models achieve much speedup gain. They achieve the highest prediction accuracy for missing data on most cases. <italic>Note to Practitioners</italic>&#x2014;HDI data are commonly encountered in many industrial big data-related applications, where rich knowledge and patterns can be extracted efficiently. An SGD based-LFA model is popular in addressing HDI data due to its efficiency. Yet when dealing with large-scale HDI data, its serial nature greatly reduces its scalability. Although alternating SGD can decouple an LFA process to implement parallelization, its performance depends on its hyper-parameter whose tuning is tedious. To address this vital issue, this study proposes three extended alternating SGD algorithms whose hyper-parameters are made via through a particle swarm optimizer. Based on them, three models are realized, which are able to efficiently obtain latent factors from HDI matrices. Compared with the existing and state-of-the-art models, they enjoy their hyper-parameter-adaptive learning process, as well as highly competitive computational efficiency and representation learning ability. Hence, they provide practitioners with more scalable solutions when addressing large HDI data from industrial applications. IEEE,High-dimensional and incomplete (HDI) data are commonly encountered in various big data-related applications concerning the complex interactions among numerous nodes, such as the user-item iterations in a recommender system. A stochastic gradient descent (SGD)-based latent factor analysis (LFA) model can perform efficient representation learning to such HDI data, thereby extracting useful knowledge from them. However, a standard SGD algorithm updates a latent factor based on the current stochastic gradient only, without the considerations on the past information, making a resultant model suffer from slow convergence. To address this critical issue, this paper proposes an Adaptive Non-linear PID-incorporated SGD (ANPS) algorithm with two-fold ideas: 1) rebuilding the instant learning error when computing the stochastic gradient following the principle of a nonlinear PID controller to incorporate past update information into the learning scheme efficiently, and 2) implementing gain parameter adaptation following the principle of particle swarm optimization (PSO). Experiments on six widely-adopted HDI datasets demonstrate that compared with state-of-the-art LFA models, an ANPS-based LFA model achieves significant advantage in both efficiency and accuracy. Moreover, its flexible gain parameter adaptation mechanism greatly boosts its practicability for real issues. <italic>Note to Practitioners</italic>&#x2014;In many industrial applications like recommender systems, social network systems, and cloud service systems, people usually encounter numerous nodes and their highly-incomplete relationships. An HDI matrix is commonly adopted to describe such specific relationships. One of the major challenges is to acquire useful knowledge from an HDI matrix efficiently and accurately for various data analysis tasks, e.g., accurate recommendation, community detection, and web service selection. An SGD-based LFA model has been widely adopted to tackle this issue. However, it suffers from slow convergence that leads to considerable time cost on large-scale datasets. This study proposes an ANPS algorithm following the principle of a nonlinear PID controller. With it, an ANPS-based LFA model is achieved, which possesses fast convergence rate on an industrial HDI matrix. The proposed ANPS algorithm can be leveraged for different types of various machine learning models, thereby improving their utility and scalability in practice. IEEE"
209,208,20,208_renal_kidney_dialysis_kidneys,"renal,kidney,dialysis,kidneys,prediction,glomerular,predictors,ckd,hypertension,predict","Introduction: Chronic kidney disease (CKD) is a progressive disease with high incidence but early imperceptible symptoms. Since China’s rural areas are subject to inadequate medical check-ups and single disease screening programme, it could easily translate into end-stage renal failure. This study aimed to construct an early warning model for CKD tailored to impoverished areas by employing machine learning (ML) algorithms with easily accessible parameters from ten rural areas in Shanxi Province, thereby, promoting a forward shift of treatment time and improving patients’ quality of life. Methods: From April to November 2019, CKD opportunistic screening was carried out in 10 rural areas in Shanxi Province. First, general information, physical examination data, blood and urine specimens were collected from 13,550 subjects. Afterward, feature selection of explanatory variables was performed using LASSO regression, and target datasets were balanced using the SMOTE (synthetic minority over-sampling technique) algorithm, i.e., albuminuria-to-creatinine ratio (ACR) and ?1-microglobulin-to-creatinine ratio (MCR). Next, Bagging, Random Forest (RF) and eXtreme Gradient Boosting (XGBoost) were employed for classification of ACR outcomes and MCR outcomes, respectively. Results: 12,330 rural residents were included in this study, with 20 explanatory variables. The cases with increased ACR and increased MCR represented 1,587 (12.8%) and 1,456 (11.8%), respectively. After conducting LASSO, 14 and 15 explanatory variables remained in these two datasets, respectively. Bagging, RF, and XGBoost performed well in classification, with the AUC reaching 0.74, 0.87, 0.87, 0.89 for ACR outcomes and 0.75, 0.88, 0.89, 0.90 for MCR outcomes. The five variables contributing most to the classification of ACR outcomes and MCR outcomes constituted SBP, TG, TC, and Hcy, DBP and age, TG, SBP, Hcy and FPG, respectively. Overall, the machine learning algorithms could emerge as a warning model for CKD. Conclusion: ML algorithms in conjunction with rural accessible indexes boast good performance in classification, which allows for an early warning model for CKD. This model could help achieve large-scale population screening for CKD in poverty-stricken areas and should be promoted to improve the quality of life and reduce the mortality rate. Copyright © 2023 Song, Liu, Qiu, Qing, Li, Zhao, Li, Li and Zhou.,Chronic kidney disease (CKD) remains one of the most prominent global causes of mortality worldwide, necessitating accurate prediction models for early detection and prevention. In recent years, machine learning (ML) techniques have exhibited promising outcomes across various medical applications. This study introduces a novel ML-driven monogram approach for early identification of individuals at risk for developing CKD stages 3–5. This retrospective study employed a comprehensive dataset comprised of clinical and laboratory variables from a large cohort of diagnosed CKD patients. Advanced ML algorithms, including feature selection and regression models, were applied to build a predictive model. Among 467 participants, 11.56% developed CKD stages 3–5 over a 9-year follow-up. Several factors, such as age, gender, medical history, and laboratory results, independently exhibited significant associations with CKD (p < 0.05) and were utilized to create a risk function. The Linear regression (LR)-based model achieved an impressive R-score (coefficient of determination) of 0.954079, while the support vector machine (SVM) achieved a slightly lower value. An LR-based monogram was developed to facilitate the process of risk identification and management. The ML-driven nomogram demonstrated superior performance when compared to traditional prediction models, showcasing its potential as a valuable clinical tool for the early detection and prevention of CKD. Further studies should focus on refining the model and validating its performance in diverse populations. © 2023, The Author(s).,Rationale & Objective: Chronic kidney disease (CKD) is a major cause of morbidity and mortality. To date, there are no widely used machine-learning models that can predict progressive CKD across the entire disease spectrum, including the earliest stages. The objective of this study was to use readily available demographic and laboratory data from Sonic Healthcare USA laboratories to train and test the performance of machine learning-based predictive risk models for CKD progression. Study Design: Retrospective observational study Setting & Participants: The study population was composed of deidentified laboratory information services data procured from a large US outpatient laboratory network. The retrospective data set included 110,264 adult patients over a 5-year period with initial estimated glomerular filtration rate (eGFR) values between 15-89 mL/min/1.73 m2. Predictors: Patient demographic and laboratory characteristics. Outcomes: Accelerated (ie, >30%) eGFR decline associated with CKD progression within 5 years. Analytical Approach: Machine-learning models were developed using random forest survival methods, with laboratory-based risk factors analyzed as potential predictors of significant eGFR decline. Results: The 7-variable risk classifier model accurately predicted an eGFR decline of >30% within 5 years and achieved an area under the curve receiver-operator characteristic of 0.85. The most important predictor of progressive decline in kidney function was the eGFR slope. Other key contributors to the model included initial eGFR, urine albumin-creatinine ratio, serum albumin (initial and slope), age, and sex. Limitations: The cohort study did not evaluate the role of clinical variables (eg, blood pressure) on the performance of the model. Conclusions: Our progressive CKD classifier accurately predicts significant eGFR decline in patients with early, mid, and advanced disease using readily obtainable laboratory data. Although prospective studies are warranted, our results support the clinical utility of the model to improve timely recognition and optimal management for patients at risk for CKD progression. Plain-Language Summary: Defined by a significant decrease in estimated glomerular filtration rate (eGFR), chronic kidney disease (CKD) progression is strongly associated with kidney failure. However, to date, there are no broadly used resources that can predict this clinically significant event. Using machine-learning techniques on a diverse US population, this cohort study aimed to address this deficiency and found that a 5-year risk prediction model for CKD progression was accurate. The most important predictor of progressive decline in kidney function was the eGFR slope, followed by the urine albumin-creatinine ratio and serum albumin slope. Although further study is warranted, the results showed that a machine-learning model using readily obtainable laboratory information accurately predicts CKD progression, which may inform clinical diagnosis and management for this at-risk population. © 2023 The Authors"
210,209,20,209_reinforcement_cooperation_evolutionary_cooperativecompetitive,"reinforcement,cooperation,evolutionary,cooperativecompetitive,coevolutionary,strategies,strategy,adaptive,games,rewards","Recent studies have raised concerns on the inevitability of chaos in congestion games with large learning rates. We further investigate this phenomenon by exploring the learning dynamics in simple two-resource congestion games, where a continuum of agents learns according to a simplified experience-weighted attraction algorithm. The model is characterized by three key parameters: a population intensity of choice (learning rate), a discount factor (recency bias or exploration parameter), and the cost function asymmetry. The intensity of choice captures agents’ economic rationality in their tendency to approximately best respond to the other agent’s behavior. The discount factor captures a type of memory loss of agents, where past outcomes matter exponentially less than the recent ones. Our main findings reveal that while increasing the intensity of choice destabilizes the system for any discount factor, whether the resulting dynamics remains predictable or becomes unpredictable and chaotic depends on both the memory loss and the cost asymmetry. As memory loss increases, the chaotic regime gives place to a periodic orbit of period 2 that is globally attracting except for a countable set of points that lead to the equilibrium. Therefore, memory loss can suppress chaotic behaviors. The results highlight the crucial role of memory loss in mitigating chaos and promoting predictable outcomes in congestion games, providing insights into designing control strategies in resource allocation systems susceptible to chaotic behaviors. © 2024 Author(s).,In evolutionary game theory, the emergence and maintenance of group cooperative behavior is usually challenged by the lure of high-payoff defection behavior.Recently, besides imitation rules, the adaptive ability of individuals under limited information is also the key to adjust strategies, for example, individuals based on reinforcement learning rules by judging whether previous performance is satisfactory. In realistic scenarios, individuals with rich experience usually lead those who are inexperienced and play a guiding role in the group. Here we propose a multi-guider game model. Players on each layer of the network play different roles and follow different strategy update rules. Specifically, guiders use reinforcement learning rules to update their strategies in the upper network, and guided players use payoff-based imitation rules to update their strategies in the lower network. As the evolution progresses, guided players in the lower layer begin to reference the experienced guiders in the upper layer to update their strategies. A large number of Monte Carlo simulation results show that inexperienced individuals in the group are able to learn from the experience of others with the experienced guidance of multiple guiders. In addition to the improvement of group decision-making, the cooperative behavior can also be maintained at a higher level in the simulation of social dilemma. © 2023,Imitation and aspiration learning rules are frequently observed in humans and animals. The former is an act of copying other’s action, whereas the latter is characterized by the self-evaluation. Here we study the coexistence of these learning mechanisms in structured populations. Both rules have been combined focusing on two different scenarios: (I) adoption of either update rule with a certain probability, and (II) grouping the entire population according to the update rules. We present two pair approximation models, illustrating both scenarios, which yield a nice agreement—under weak selection—with that of agent-based simulations. For weak selection and large population size, we find that the condition for cooperation to dominate defection is similar in both heterogeneous and homogeneous update rules. We examine several variants of the mixed model such as time-evolving aspirations alongside strategies and the coevolution of strategies and update rules. In the former case, our simulation reveals that Prisoner’s dilemma and, in some cases, Stag-hunt experience overall less aspiration levels compared to other games such as Chicken or Trivial. The coevolution of strategies and update rules demonstrates a better cooperation, in contrast to the fixed update rule case, exhibiting the possibility of asymptotic coexistence of both learning mechanisms. © 2023 IOP Publishing Ltd & London Mathematical Society."
211,210,20,210_ev_charging_evpv_microgrid,"ev,charging,evpv,microgrid,discharging,evs,scheduling,evcs,pricing,evcss","EV drivers have experienced a charging inconvenience due to a limited number of charging facilities and mileage anxiety due to the limited driving distance for a single full charge. This paper developed a user-friendly online EV charging guidance algorithm to cope with the two aforementioned issues using multi-agent deep reinforcement learning. First, three models, i.e., the traffic network model, charging station model, and EV driver model, are established, respectively, considering the traffic condition, the potential competition of future charging demand at charging stations, and the drivers&#x2019; mileage anxiety. Second, the charging guidance process is modeled as a Markov decision process, and charging stations are taken as agents. The attentional multi-agent actor-critic algorithm based on the centralized training with decentralized execution framework is built. Finally, compared to the comparison algorithm, the performance does not diminish with the increase in the number of agents, indicating that the approach has the scalability to be applied to large-scale agent systems. The model still has the generalization in extreme scenarios such as traffic road and charger failures. The testing time within various numbers of charging stations is about 23ms per EV, which is sufficient to apply the proposed model to real-time decision-making and online recommendation. IEEE,With the rapid growth of the number of Electric Vehicles (EVs), access to large-scale EVs will bring serious safety hazards to the operation planning of the power system. It needs to be supported by an effective EV charging and discharging behavior control strategy to meet the operation demand of the power system. An optimization model with the objectives of minimizing grid load variance and minimizing user charging cost is established. An improved hybrid algorithm is proposed for the optimal allocation of charging and discharging power of EVs by combining particle swarm optimization (PSO) algorithm and gravitational search algorithm (GSA). The performance of variant algorithm is tested using CEC2005 benchmarking functions sets and applied to the solution of the ordered charge–discharge optimal scheduling model. The results show that the convergence accuracy of the algorithm is better than the traditional algorithm, and it can effectively balance exploration and exploitation ability of the particles. In addition, the scheduling analysis is performed for different charging strategies of EVs. The scheduling results show that with the same optimization weights, implementing the ordered charging and discharging strategy can significantly reduce the charging cost of users and the load variance of the grid. Thus, the operational stability of the grid and the economic benefits for users are improved. © 2024 The Authors,The rapid growth of electric vehicles (EVs) is an unstoppable worldwide development trend. An optimal charging strategy for large-scale EVs is able to deal with the randomness of EVs charging and satisfy charging demands of users while ensuring safe and economic operation of the power system. The current centralized and model-based methods failed to overcome the randomness charging problem of the large-scale EVs. Thus, the paper proposes a decentralized approach based on model-free deep reinforcement learning (DRL) to determine the optimal strategy for reducing EVs charging cost considering power limit of the charging station (CS), users' charging demands and fair charging fees. First, a decentralized framework and a dynamic energy boundary (DEB) model of single EV which discretizes the charging demand are proposed. Second, the problem as a Markov Decision Process (MDP) with unknown transition probability is formulated. Moreover, the recurrent deep deterministic policy gradient (RDDPG) based approach is proposed to determine the charging strategy for all charging piles in the CS. Finally, digital simulation studies are conducted to demonstrate the effectiveness of the proposed approach in charging cost reduction and fair charging fees. In addition, the RDDPG-based approach has great scalability which can apply a small-scale model to solve a large-scale problem without being retrained. © 2022"
212,211,19,211_molecular_synthesizability_molecule_molecules,"molecular,synthesizability,molecule,molecules,generative,drugex,autoencoder,novo,drug,synthetic","Motivation: In the field of pharmacochemistry, it is a time-consuming and expensive process for the new drug development. The existing drug design methods face a significant challenge in terms of generation efficiency and quality. Results: In this paper, we proposed a novel molecular generation strategy and optimization based on A2C reinforcement learning. In molecular generation strategy, we adopted transformer-DNN to retain the scaffolds advantages, while accounting for the generated molecules' similarity and internal diversity by dynamic parameter adjustment, further improving the overall quality of molecule generation. In molecular optimization, we introduced heterogeneous parallel supercomputing for large-scale molecular docking based on message passing interface communication technology to rapidly obtain bioactive information, thereby enhancing the efficiency of drug design. Experiments show that our model can generate high-quality molecules with multi-objective properties at a high generation efficiency, with effectiveness and novelty close to 100%. Moreover, we used our method to assist shandong university school of pharmacy to find several candidate drugs molecules of anti-PEDV.  © 2023 The Author(s). Published by Oxford University Press.,Rational drug design often starts from specific scaffolds to which side chains/substituents are added or modified due to the large drug-like chemical space available to search for novel drug-like molecules. With the rapid growth of deep learning in drug discovery, a variety of effective approaches have been developed for de novo drug design. In previous work we proposed a method named DrugEx, which can be applied in polypharmacology based on multi-objective deep reinforcement learning. However, the previous version is trained under fixed objectives and does not allow users to input any prior information (i.e. a desired scaffold). In order to improve the general applicability, we updated DrugEx to design drug molecules based on scaffolds which consist of multiple fragments provided by users. Here, a Transformer model was employed to generate molecular structures. The Transformer is a multi-head self-attention deep learning model containing an encoder to receive scaffolds as input and a decoder to generate molecules as output. In order to deal with the graph representation of molecules a novel positional encoding for each atom and bond based on an adjacency matrix was proposed, extending the architecture of the Transformer. The graph Transformer model contains growing and connecting procedures for molecule generation starting from a given scaffold based on fragments. Moreover, the generator was trained under a reinforcement learning framework to increase the number of desired ligands. As a proof of concept, the method was applied to design ligands for the adenosine A2A receptor (A2AAR) and compared with SMILES-based methods. The results show that 100% of the generated molecules are valid and most of them had a high predicted affinity value towards A2AAR with given scaffolds. © 2023, The Author(s).,In the past few years, a number of machine learning (ML)-based molecular generative models have been proposed for generating molecules with desirable properties, but they all require a large amount of label data of pharmacological and physicochemical properties. However, experimental determination of these labels, especially bioactivity labels, is very expensive. In this study, we analyze the dependence of various multi-property molecule generation models on biological activity label data and propose Frag-G/M, a fragment-based multi-constraint molecular generation framework based on conditional transformer, recurrent neural networks (RNNs), and reinforcement learning (RL). The experimental results illustrate that, using the same number of labels, Frag-G/M can generate more desired molecules than the baselines (several times more than the baselines). Moreover, compared with the known active compounds, the molecules generated by Frag-G/M exhibit higher scaffold diversity than those generated by the baselines, thus making it more promising to be used in real-world drug discovery scenarios.  © 2023 American Chemical Society."
213,212,19,212_phishing_classifiers_classifier_spam,"phishing,classifiers,classifier,spam,classification,cyberattacks,urls,malicious,feature,emails","Phishing is a persistent and major threat on the internet that is growing steadily and dangerously. It is a type of cyber-attack, in which phisher mimics a legitimate website page to harvest victim’s sensitive information, such as usernames, emails, passwords and bank or credit card details. To prevent such attacks, several phishing detection techniques have been proposed such as AI based, 3rd party, heuristic and content based. However, these approaches suffer from a number of limitations that needs to be addressed in order to detect phishing URLs. Firstly, features extracted in the past are extensive, with a limitation that it takes a considerable amount of time to extract such features. Secondly, several approaches selected important features using statistical methods, while some propose their own features. Although both methods have been implemented successfully in various approaches, however, these methods produce incorrect results without amplification of domain knowledge. Thirdly, most of the literature has used pre-classified and smaller datasets, which fail to produce exact efficiency and precision on large and real world datasets. Fourthly, the previous proposed approaches lack in advanced evaluation measures. Hence, in this paper, effective machine learning framework is proposed, which predicts phishing URLs without visiting the webpage nor utilizing any 3rd party services. The proposed technique is based on URL and uses full URL, protocol scheme, hostname, path area of the URL, entropy feature, suspicious words and brand name matching using TF-IDF technique for the classification of phishing URLs. The experiments are carried out on six different datasets using eight different machine learning classifiers, in which Random Forest achieved a significant higher accuracy than other classifiers on all the datasets. The proposed framework with only 30 features achieved a higher accuracy of 96.25% and 94.65% on the Kaggle datasets. The comparative results show that the proposed model achieved an accuracy of 92.2%, 91.63%, 94.80, 96.85% on benchmark datasets, which is higher than the existing approaches. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Phishing attacks are one of the most challenging social engineering cyberattacks due to the large amount of entities involved in online transactions and services. In these attacks, criminals deceive users to hijack their credentials or sensitive data through a login form which replicates the original website and submits the data to a malicious server. Many anti-phishing techniques have been developed in recent years, using different resource such as the URL and HTML code from legitimate index websites and phishing ones. These techniques have some limitations when predicting legitimate login websites, since, usually, no login forms are present in the legitimate class used for training the proposed model. Hence, in this work we present a methodology for phishing website detection in real scenarios, which uses URL, HTML, and web technology features. Since there is not any updated and multipurpose dataset for this task, we crafted the Phishing Index Login Websites Dataset (PILWD), an offline phishing dataset composed of 134,000 verified samples, that offers to researchers a wide variety of data to test and compare their approaches. Since approximately three-quarters of collected phishing samples request the introduction of credentials, we decided to crawl legitimate login websites to match the phishing standpoint. The developed approach is independent of third party services and the method relies on a new set of features used for the very first time in this problem, some of them extracted from the web technologies used by the on each specific website. Experimental results show that phishing websites can be detected with 97.95% accuracy using a LightGBM classifier and the complete set of the 54 features selected, when it was evaluated on PILWD dataset. © 2022,Phishing attacks are evolving with more sophisticated techniques, posing significant threats. Considering the potential of machine-learning-based approaches, our research presents a similar modern approach for web phishing detection by applying powerful machine learning algorithms. An efficient layered classification model is proposed to detect websites based on their URL structure, text, and image features. Previously, similar studies have used machine learning techniques for URL features with a limited dataset. In our research, we have used a large dataset of 20,000 website URLs, and 22 salient features from each URL are extracted to prepare a comprehensive dataset. Along with this, another dataset containing website text is also prepared for NLP-based text evaluation. It is seen that many phishing websites contain text as images, and to handle this, the text from images is extracted to classify it as spam or legitimate. The experimental evaluation demonstrated efficient and accurate phishing detection. Our layered classification model uses support vector machine (SVM), XGBoost, random forest, multilayer perceptron, linear regression, decision tree, naïve Bayes, and SVC algorithms. The performance evaluation revealed that the XGBoost algorithm outperformed other applied models with maximum accuracy and precision of 94% in the training phase and 91% in the testing phase. Multilayer perceptron also worked well with an accuracy of 91% in the testing phase. The accuracy results for random forest and decision tree were 91% and 90%, respectively. Logistic regression and SVM algorithms were used in the text-based classification, and the accuracy was found to be 87% and 88%, respectively. With these precision values, the models classified phishing and legitimate websites very well, based on URL, text, and image features. This research contributes to early detection of sophisticated phishing attacks, enhancing internet user security. © 2023 by the authors."
214,213,19,213_colorization_colorizing_cnnbased_attention,"colorization,colorizing,cnnbased,attention,blur,lightenformer,brightness,illumination,deblurring,lownormallight","The images captured under low-light conditions are characterized by low brightness and poor contrast, which affects the accuracy of computer vision tasks. In recent years, there have been a variety of low-light image enhancement (LLIE) models based on deep learning, but they have not been able to fully extract the multiscale information of multiple stages, resulting in poor generalization performance and instability of the model. Currently, a large number of multistage networks cause color distortion and stylization of images due to excessive transmission of noncritical information. To address these issues, we propose an LLIE via multistage feature fusion network. Our network consists of three stages. In the first two stages of the LLIE, S-UNet, which combines UNet and spatial weighted residual channel attention block (SWRCAB), helps the network extract more critical multiscale information and occupy a small amount of computing resources. In the third stage, we fuse the SWRCAB and a nonlocal sparse block into the original enhancement network to enhance the original resolution pixel-by-pixel. We also propose a fusion attention mechanism, which can provide real and effective supervision and control the transmission of a small amount of critical feature information for each stage. In addition, we add illumination guidance for image segmentation at the beginning of each stage of the network, excepting that the model can better focus on the dark part of the low-light image and avoid overexposure. We conduct experiments on multiple benchmark datasets to qualitatively and quantitatively demonstrate that the proposed method is more competitive than the state-of-the-art methods.  © 2022 SPIE and IS&T.,With the development of the field of deep learning, image recognition, enhancement and other technologies have been widely used.However, dark lighting environments in reality, such as insufficient light at night, cause or block photographic images in low brightness, severe noise, and a large number of details are lost, resulting in a huge loss of image content and information, which hinders further analysis and use. Such problems not only exist in the traditional deep learning field, but also exist in criminal investigation, scientific photography and other fields, such as the accuracy of low-light image. However, in the current research results, there is no perfect means to deal with the above problems. Therefore, the study of low-light image enhancement has important theoretical significance and practical application value for the development of smart cities. In order to improve the quality of low-light enhanced images, this paper tries to introduce the luminance attention mechanism to improve the enhancement efficiency. The main contents of this paper are summarized as follows: using the attention mechanism, we proposed a method of low-light image enhancement based on the brightness attention mechanism and generative adversarial networks. This method uses brightness attention mechanism to predict the illumination distribution of low-light image and guides the enhancement network to enhance the image adaptiveness in different luminance regions. At the same time, u-NET network is designed and constructed to improve the modeling process of low-light image. We verified the performance of the algorithm on the synthetic data set and compared it with traditional image enhancement methods (HE, SRIE) and deep learning methods (DSLR). The experimental results show that our proposed network model has relatively good enhancement quality for low-light images, and improves the overall robustness, which has practical significance for solving the problem of low-light image enhancement. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The low visibility and dull colors associated with low-light images are not only difficult to satisfy the photographer, but also hinder further visual tasks. In this study, we propose an attention-based dual-color space fusion network to enhance low-light images. By introducing HSV color space, the network can solve the problems of dim color and insufficient contrast existing in previous RGB color space methods. In both color spaces, the network adopts a large number of attention mechanisms to highlight important features in real time. To better achieve feature fusion across color space, we design adaptive large-kernel attention (ALKA) and feature extraction module (FEM), respectively. The ALKA adaptively selects features for enriching the encoding input in the HSV color space during the RGB color space encoding process. The FEM assists the network in more comprehensive supervision of the enhancement process and generates the attention map to transmit the marking features in the previous color space. At the end of the network, we also design channel enhancement module (CEM) to protect the texture details lost during the scale change. We have conducted experiments on a variety of public datasets, and both subjective and objective comparisons have demonstrated the particularity and prominence of our method. © 2023 Elsevier B.V."
215,214,18,214_abstract_available_no_,"abstract,available,no,,,,,,,","[No abstract available],[No abstract available],[No abstract available]"
216,215,18,215_ligandbased_inhibitors_pharmacophore_proteins,"ligandbased,inhibitors,pharmacophore,proteins,kinase,protein,molecular,machinelearning,screening,amino","Data-driven drug discovery exploits a comprehensive set of big data to provide an efficient path for the development of new drugs. Currently, publicly available bioassay data sets provide extensive information regarding the bioactivity profiles of millions of compounds. Using these large-scale drug screening data sets, we developed a novel in silico method to virtually screen hit compounds against protein targets, named BEAR (Bioactive compound Enrichment by Assay Repositioning). The underlying idea of BEAR is to reuse bioassay data for predicting hit compounds for targets other than their originally intended purposes, i.e., “assay repositioning”. The BEAR approach differs from conventional virtual screening methods in that (1) it relies solely on bioactivity data and requires no physicochemical features of either the target or ligand. (2) Accordingly, structurally diverse candidates are predicted, allowing for scaffold hopping. (3) BEAR shows stable performance across diverse target classes, suggesting its general applicability. Large-scale cross-validation of more than a thousand targets showed that BEAR accurately predicted known ligands (median area under the curve = 0.87), proving that BEAR maintained a robust performance even in the validation set with additional constraints. In addition, a comparative analysis demonstrated that BEAR outperformed other machine learning models, including a recent deep learning model for ABC transporter family targets. We predicted P-gp and BCRP dual inhibitors using the BEAR approach and validated the predicted candidates using in vitro assays. The intracellular accumulation effects of mitoxantrone, a well-known P-gp/BCRP dual substrate for cancer treatment, confirmed nine out of 72 dual inhibitor candidates preselected by primary cytotoxicity screening. Consequently, these nine hits are novel and potent dual inhibitors for both P-gp and BCRP, solely predicted by bioactivity profiles without relying on any structural information of targets or ligands. © 2023 American Chemical Society.,Purpose: Patients with diabetes mellitus have an elevated chance of developing cataracts, a degenerative vision-impairing condition often needing surgery. The process of the reduction of glucose to sorbitol in the lens of the human eye that causes cataracts is managed by the Aldose Reductase Enzyme (AR), and it is been found that AR inhibitors may mitigate the onset of diabetic cataracts. There exists a large pool of natural and synthetic AR inhibitors that can prevent diabetic complications, and the development of a machine-learning (ML) prediction model may bring new AR inhibitors with better characteristics into clinical use. Methods: Using known AR inhibitors and their chemical-physical descriptors we created the ML model for prediction of new AR inhibitors. The predicted inhibitors were tested by computational docking to the binding site of AR. Results: Using cross-validation in order to find the most accurate ML model, we ended with final cross-validation accuracy of 90%. Computational docking testing of the predicted inhibitors gave a high level of correlation between the ML prediction score and binding free energy. Conclusions: Currently known AR inhibitors are not used yet for patients for several reasons. We think that new predicted AR inhibitors have the potential to possess more favorable characteristics to be successfully implemented after clinical testing. Exploring new inhibitors can improve patient well-being and lower surgical complications all while decreasing long-term medical expenses. © 2023 The Authors,The Zika virus (ZIKV) is a neurotropic arbovirus considered a global threat to public health. Although there have been several efforts in drug discovery projects for ZIKV in recent years, there are still no antiviral drugs approved to date. Here, we describe the results of a global collaborative crowdsourced open science project, the OpenZika project, from IBM's World Community Grid (WCG), which integrates different computational and experimental strategies for advancing a drug candidate for ZIKV. Initially, molecular docking protocols were developed to identify potential inhibitors of ZIKV NS5 RNA-dependent RNA polymerase (NS5 RdRp), NS3 protease (NS2B-NS3pro), and NS3 helicase (NS3hel). Then, a machine learning (ML) model was built to distinguish active vs inactive compounds for the cytoprotective effect against ZIKV infection. We performed three independent target-based virtual screening campaigns (NS5 RdRp, NS2B-NS3pro, and NS3hel), followed by predictions by the ML model and other filters, and prioritized a total of 61 compounds for further testing in enzymatic and phenotypic assays. This yielded five non-nucleoside compounds which showed inhibitory activity against ZIKV NS5 RdRp in enzymatic assays (IC50 range from 0.61 to 17 ?M). Two compounds thermally destabilized NS3hel and showed binding affinity in the micromolar range (Kd range from 9 to 35 ?M). Moreover, the compounds LabMol-301 inhibited both NS5 RdRp and NS2B-NS3pro (IC50 of 0.8 and 7.4 ?M, respectively) and LabMol-212 thermally destabilized the ZIKV NS3hel (Kd of 35 ?M). Both also protected cells from death induced by ZIKV infection in in vitro cell-based assays. However, while eight compounds (including LabMol-301 and LabMol-212) showed a cytoprotective effect and prevented ZIKV-induced cell death, agreeing with our ML model for prediction of this cytoprotective effect, no compound showed a direct antiviral effect against ZIKV. Thus, the new scaffolds discovered here are promising hits for future structural optimization and for advancing the discovery of further drug candidates for ZIKV. Furthermore, this work has demonstrated the importance of the integration of computational and experimental approaches, as well as the potential of large-scale collaborative networks to advance drug discovery projects for neglected diseases and emerging viruses, despite the lack of available direct antiviral activity and cytoprotective effect data, that reflects on the assertiveness of the computational predictions. The importance of these efforts rests with the need to be prepared for future viral epidemic and pandemic outbreaks.  © 2022 American Chemical Society."
217,216,18,216_corpus_linguistic_multilingual_language,"corpus,linguistic,multilingual,language,annotated,languages,corpora,debates,lexiconbased,texts","Multilingual text analysis is increasingly important to address the current narrow focus of English and other Indo-European languages in comparative studies. However, there has been a lack of a comprehensive approach to evaluate the validity of multilingual text analytic methods across different language contexts. To address this issue, we propose that the validity of multilingual text analysis should be studied through the lens of transferability, which assesses the extent to which the performance of a multilingual text analytic method can be maintained when switching from one language context to another. We first formally conceptualize transferability in multilingual text analysis as a measure of whether the method is equivalent across language groups (linguistic transferability) and societal contexts (contextual transferability). We propose a model-agnostic approach to evaluate transferability using (1) natural and synthetic data pairs, (2) manual annotation of errors, and (3) the Local Interpretable Model-Agnostic Explanations (LIME) technique. As an application of our approach, we analyze the transferability of a multilingual BERT (mBERT) model fine-tuned with annotated manifestos and media texts from five Indo-European language-speaking countries of the Comparative Agendas Project. The transferability is then evaluated using natural and synthetic parliamentary data from the UK, Basque, Hong Kong, and Taiwan. Through the evaluation of transferability, this study sheds light on the common causes that lead to prediction errors in multilingual text classification using mBERT. © The authors.,The social science toolkit for computational text analysis is still very much in the making. We know surprisingly little about how to produce valid insights from large amounts of multilingual texts for comparative social science research. In this paper, we test several recent innovations from deep transfer learning to help advance the computational toolkit for social science research in multilingual settings. We investigate the extent to which prior language and task knowledge stored in the parameters of modern language models is useful for enabling multilingual research; we investigate the extent to which these algorithms can be fruitfully combined with machine translation; and we investigate whether these methods are accurate, practical and valid in multilingual settings – three essential conditions for lowering the language barrier in practice. We use two datasets with texts in 12 languages from 27 countries for our investigation. Our analysis shows, that, based on these innovations, supervised machine learning can produce substantively meaningful outputs. Our BERT-NLI model trained on only 674 or 1,674 texts in only one or two languages can validly predict political party families’ stances towards immigration in eight other languages and ten other countries. © The authors.,Machine Learning is an interesting tool for stance recognition in a large-scale context, in terms of data size, but also regarding the topics and themes addressed or the languages employed by the participants. Public consultations of citizens using online participatory democracy platforms offer this kind of setting and are good use cases for automatic stance recognition systems. In this paper, we propose to use three datasets of public consultations, in order to train a model able to classify the stance of a citizen within a text, towards a proposal or a debate question. We studied stance detection in several contexts: using data from an online platform without interactions between users, using multilingual data from online debates that are in one language, and using data from online intra-multilingual debates, which can contain several languages inside the same unique debate discussion. We propose several baselines and methods in order to take advantage of the different available data, by comparing the results of models using out-of-dataset annotations, and binary or ternary annotations from the target dataset. We finally proposed a self-supervised learning method to take advantage of unlabelled data. We annotated both the datasets with ternary stance labels and made them available. © 2023 by the authors."
218,217,18,217_bloodnet_wbc_leukocytes_cells,"bloodnet,wbc,leukocytes,cells,leukocyte,classify,monocytes,marrow,classification,cell","Leukemia is a cancer of blood-producing cells, including the bone marrow. Abnormal white blood cells travel through blood vessels and multiply rapidly. Healthy cells in the body become a minority, and the imbalance increases the chances of infection in the body. Leukemia or blood cancer is the most common cancer in children ages 2-14. Most leukemia in children is treated. Acute lymphocytic leukemia (ALL) is a type of cancer in the blood and bone marrow. It progresses rapidly when immature white blood cells are formed instead of mature ones. Treatments for acute lymphocytic leukemia include drugs and blood transfusions directly into veins, chemotherapy, and all transplantation, which involve transferring organs or tissues within the body or from one person to another. In this paper, Pattern Recognition of Acute Lymphoblastic Leukemia has been proposed using Computational Deep Learning. Pattern recognition technology uses mathematical algorithms to identify patterns in large datasets of data. Analyzing the data, the algorithms can identify patterns indicative of certain states or conditions. In the case of ALL, the algorithm would look for patterns in white blood cell count data that indicate the presence of ALL. These patterns may include changes in the number of white blood cells over time, changes in the composition of the white blood cells, or changes in the levels of certain proteins or gene expressions associated with ALL. The proposed ALLDM model achieved 81.53% (DDS) and 87.92% (SDS) of chemotherapy management, 79.16% (DDS) and 94.31% (SDS) of Stem Cell Transplantation Management, 63.77% (DDS) and 87.37% (SDS) of Radiation therapy Management and 88.92% (DDS) and 85.86% (SDS) of Targeted therapy drugs management. © 2013 IEEE.,Background and objectives: Visual analysis of cell morphology has an important role in the diagnosis of hematological diseases. Morphological cell recognition is a challenge that requires experience and in-depth review by clinical pathologists. Within the new trend of introducing computer-aided diagnostic tools in laboratory medicine, models based on deep learning are being developed for the automatic identification of different types of cells in peripheral blood. In general, well-annotated large image sets are needed to train the models to reach a desired classification performance. This is especially relevant when it comes to discerning between cell images in which morphological differences are subtle and when it comes to low prevalent diseases with the consequent difficulty in collecting cell images. The objective of this work is to develop, train and validate SyntheticCellGAN (SCG), a new system for the automatic generation of artificial images of white blood cells, maintaining morphological characteristics very close to real cells found in practice in clinical laboratories. Methods: SCG is designed with two sequential generative adversarial networks. First, a Wasserstein structure is used to transform random noise vectors into low resolution images of basic mononuclear cells. Second, the concept of image-to-image translation is used to build specific models that transform the basic images into high-resolution final images with the realistic morphology of each cell type target: 1) the five groups of normal leukocytes (lymphocytes, monocytes, eosinophils, neutrophils and basophils); 2) atypical promyelocytes and hairy cells, which are two relevant cell types of complex morphology with low abundance in blood smears. Results: The images of the SCG system are evaluated with four experimental tests. In the first test we evaluated the generated images with quantitative metrics for GANs. In the second test, morphological verification of the artificial images is performed by expert clinical pathologists with 100% accuracy. In the third test, two classifiers based on convolutional neural networks (CNN) previously trained with images of real cells are used. Two sets of artificial images of the SCG system are classified with an accuracy of 95.36% and 94%, respectively. In the fourth test, three CNN classifiers are trained with artificial images of the SCG system. Real cells are identified with an accuracy ranging from 87.7% to 100%. Conclusions: The SCG system has proven effective in creating images of all normal leukocytes and two low-prevalence cell classes associated with diseases such as acute promyelocyte leukemia and hairy cell leukemia. Once trained, the system requires low computational cost and can help augment high-quality image datasets to improve automatic recognition model training for clinical laboratory practice. © 2022 The Author(s),Accurate and early detection of anomalies in peripheral white blood cells plays a crucial role in the evaluation of well-being in individuals and the diagnosis and prognosis of hematologic diseases. For example, some blood disorders and immune system-related diseases are diagnosed by the differential count of white blood cells, which is one of the common laboratory tests. Data is one of the most important ingredients in the development and testing of many commercial and successful automatic or semi-automatic systems. To this end, this study introduces a free access dataset of normal peripheral white blood cells called Raabin-WBC containing about 40,000 images of white blood cells and color spots. For ensuring the validity of the data, a significant number of cells were labeled by two experts. Also, the ground truths of the nuclei and cytoplasm are extracted for 1145 selected cells. To provide the necessary diversity, various smears have been imaged, and two different cameras and two different microscopes were used. We did some preliminary deep learning experiments on Raabin-WBC to demonstrate how the generalization power of machine learning methods, especially deep neural networks, can be affected by the mentioned diversity. Raabin-WBC as a public data in the field of health can be used for the model development and testing in different machine learning tasks including classification, detection, segmentation, and localization. © 2022, The Author(s)."
219,218,18,218_verbs_linguistic_linguistics_lexicons,"verbs,linguistic,linguistics,lexicons,language,phonological,languages,lexicon,semantic,lexical","The Discriminative Lexicon is a theory of the mental lexicon that brings together insights from various other theories: words are the relevant cognitive units in morphology, the meaning of a word is represented by its distribution in utterances, word forms and their meaning are learned by minimizing prediction errors, and fully connected networks successfully capture language learning. In this article we model comprehension and production of Kinyarwanda verb forms in the Discriminative Lexicon model. Kinyarwanda is a highly inflectional language, and therefore particularly interesting, because its paradigms are almost unlimited in size. Can knowledge of its enormous paradigms be modeled only on the basis of words? To answer this question we modeled a data set of 11,528 verb forms, hand-annotated for meaning and their grammatical functions, in the Linear Discriminative Learning (LDL), a two-layered, fully connected computational implementation of the Discriminative Lexicon model. We also extracted 573 verbs from our data set for which meanings are available that are based on empirical word embeddings obtained from large text corpora, and modeled them in LDL. Both comprehension and production is learned accurately: Kinyarwanda verb forms can be comprehended and produced relying on words as cognitive units, in a two-layered network, in which prediction errors are minimized.  © 2023 the author(s), published by De Gruyter, Berlin/Boston.,To attain native-like proficiency in second-language word usage, learners have to discover intricate semantic categories in the target language. We investigated the factors influencing the development of two aspects of second-language learners’ semantic categories: the category center and category boundary of word meanings. In the experiment, second-language learners of Japanese, whose first language is Mandarin, were asked to produce the best verb for 28 videos depicting various cutting and breaking events. Descriptive analyses were conducted to compare the verb patterns used by second-language learners with those of native speakers. The second-language learners’ verb use pattern suggested their struggle in delineating the semantic ranges of breaking verbs in a native-like manner. Model analyses further revealed that different factors contribute to learning two different aspects of word meanings. The learning category center of word meaning depended on the similarity between the lexical domains in the first and second languages. On the contrary, the success of learning the semantic boundaries of verbs required a large input frequency and smaller semantic coverage, and smaller category ambiguity. The results suggest that constructing a semantic domain in the second language should be evaluated from at least two different aspects of semantic representation. © 2024 Saji et al. This is an open access article distributed under the terms of the Creative Commons Attribution License,,Purpose: Research indicates that when teaching grammatical forms to children, the verbs used to model specific grammatical inflections matter. When learning grammatical forms, children have higher performance when they hear many unique verb forms that vary in their frequency and phonological complexity. In this tutorial, we demonstrate a method for identifying and characterizing a large number of verbs based on their frequency and complexity. Method: We selected verbs from an open-access database of transcribed child language samples. We extracted verbs produced by 5-to 8.9-year-old children in four morphosyntactic contexts: regular past tense-ed, third person singular-s, is/are + verb+ing,and do/does questions. We ranked verbs based on their fre-quency of occurrence across transcripts. We also coded the phonological com-plexity of each verb. We coded each verb as high or low frequency and high or low phonological complexity. Results: The synthesis yielded 129 unique verbs used in the regular past tense-ed context, 107 verbs used in the third person singular-s context, 69 verbs used in the is/are + verb+ing context, and 16 verbs used in the do/does ques-tion context. We created tables for each form that include the frequency rank-ings and phonological complexity scores for every verb. Conclusions: Clinicians may use the verb lists, frequency ratings, and phonolo-gical complexity scores to help identify verbs to incorporate into assessment and intervention sessions with children. Researchers and clinicians may use the step-by-step approach presented in the tutorial to identify verbs or other syn-tactic components used in different morphosyntactic contexts or produced by individuals of different demographics in different speaking contexts. © 2023 The Authors."
220,219,18,219_dentistry_dental_tooth_teeth,"dentistry,dental,tooth,teeth,segmentation,dentists,dentitions,caries,molars,dentition","Objective: This study aims to investigate the effect of number of data on model performance, for the detection of tooth numbering problem on dental panoramic radiographs, with the help of image processing and deep learning algorithms. Study Design: The data set consists of 3000 anonymous dental panoramic X-rays of adult individuals. Panoramic X-rays were labeled on the basis of 32 classes in line with the FDI tooth numbering system. In order to examine the relationship between the number of data used in image processing algorithms and model performance, four different datasets which include 1000, 1500, 2000 and 2500 panoramic X-rays, were used. The training of the models was carried out with the YOLOv4 algorithm and trained models were tested on a fixed test dataset with 500 data and compared based on F1 score, mAP, sensitivity, precision and recall metrics. Results: The performance of the model increased as the number of data used during the training of the model increased. Therefore, the last model trained with 2500 data showed the highest success among all the trained models. Conclusion: Dataset size is important for dental enumeration, and large samples should be considered as more reliable. © 2023, The Author(s) under exclusive licence to Japanese Society for Oral and Maxillofacial Radiology.,Neural networks and artificial intelligence find more applications in dentistry. It treats dental caries, the most prevalent type of dental illness worldwide. Even though dental caries can be prevented and treated, they typically cause dental discomfort and tooth loss. For dental caries to be treated quickly and effectively, comprehensive detection may be needed, a combination of techniques that include eye inspection, probing, using a dental probe, and using a hand-held mirror, and the individual application of each of these techniques, can quickly identify large caries cavities. Long-established caries detection techniques help to locate only partially hidden but still accessible holes. Deep learning (DL) techniques have produced remarkable diagnostic results in radiology. This study aimed to classify various radiographic extensions on panoramic films using DL techniques, identify caries lesions using these techniques, and compare the results to those of dentists with extensive training. Faster region-based convolutional neural networks (R-CNN) is a newly discovered field of medical research that is rapidly expanding and has produced outstanding results in diagnosing and prognosis of pathology and radiology conditions. In this study, dental cavities were detected and analysed using periapical radiographs to evaluate the accuracy of the Faster R-CNN algorithm. Because these three caries were derived from the oral panoramic images, we designed You Only Look Once Version 3 (YOLOv3) as a U-shaped network with a large-scale axial attention module. We also compare the effectiveness of YOLOv3's segmentation to that of other industrial standards. Experiments show that our proposed method, Fast R-CNN–YOLOv3, achieves higher accuracy in segmenting the three distinct caries level. The proposed model (R-CNN–YOLOv3) achieved an effective result with a precision of 97.183%. © 2023 John Wiley & Sons Ltd.,Objectives: This study developed and validated a deep learning-based method to automatically segment and number teeth in panoramic radiographs across primary, mixed, and permanent dentitions. Methods: A total of 6,046 panoramic radiographs were collected and annotated. The dataset encompassed primary, mixed and permanent dentitions and dental abnormalities such as tooth number anomalies, dental diseases, dental prostheses, and orthodontic appliances. A deep learning-based algorithm consisting of a U-Net-based region of interest extraction model, a Hybrid Task Cascade-based teeth segmentation and numbering model, and a post-processing procedure was trained on 4,232 images, validated on 605 images, and tested on 1,209 images. Precision, recall and Intersection-over-Union (IoU) were used to evaluate its performance. Results: The deep learning-based teeth identification algorithm achieved good performance on panoramic radiographs, with precision and recall for teeth segmentation and numbering exceeding 97%, and the IoU between predictions and ground truths reaching 92%. It generalized well across all three dentition stages and complex real-world cases. Conclusions: By utilizing a two-stage training framework with a large-scale heterogeneous dataset, the automatic teeth identification algorithm achieved a performance level comparable to that of dental experts. Clinical Significance: Deep learning can be leveraged to aid clinical interpretation of panoramic radiographs across primary, mixed, and permanent dentitions, even in the presence of real-world complexities. This robust teeth identification algorithm could contribute to the future development of more advanced, diagnosis- or treatment-oriented dental automation systems. © 2023"
221,220,18,220_energetic_atoms_molecules_hydrogen,"energetic,atoms,molecules,hydrogen,ions,explosives,energy,intermolecular,bonds,crystals","The current rise in performance prediction techniques for energetic compounds provides the possibility of prejudging the effectiveness of derived patterns. In this work, 2,4-diamino-6-chloropyrimidine was used as the precursor, and the three possible derived pathways were systematically explored (guest oxidant, nitration, and N-oxidation). Theoretical calculations were adopted to predict the energy and stability parameters of possible products. The calculations indicated that constructing bridged-ring energetic molecules with N-oxide and nitro group is an effective way to balance energy and safety. Based on this protocol, we synthesized a series of pyrimidine-based energetic molecules within three steps and tested and analyzed their physicochemical properties, verifying the consistency between experimental results and theoretical predictions. This work provides a research model for determining the feasibility and effectiveness of the derivative pathway based on a specific energetic compound precursor and can offer guidance for the directed and large-scale synthesis of high-energy and low-sensitivity explosive molecules. © 2023 Elsevier Ltd,The aim of the present work is to show a simple quantitative theoretical study on the relaxation mechanisms of the 4F3/2 levels of the Nd3+ ions in the widely used meta-phosphate laser glass system (P2O5–Al2O3–BaO–K2O) for the studied concentration range of the Nd3+ ions from 0.10 × 1020 to 8.15 × 1020 ions/cm3. Auzel's diffusion-limited model for energy transfer has been adopted in the present study for estimating zero concentration level lifetime, ?0 of 4F3/2 levels in the Nd3+ ions present in the studied glass matrix. Primarily, the relaxation mechanisms are dominated by radiative decay (krad) and energy transfer to hydroxyl groups (kOH) present in the glass network structure up to the dopant, Nd3+ concentration 0.78 × 1020 ions/cm3. Later on, with increasing dopant ion concentration, fluorescence from trivalent Nd3+ ions is unfortunately quenched by the interaction between Nd3+ ions, also known as concentration quenching. The concentration quenching of the fluorescence lifetime has been analysed using proposed Inokuti-Hirayama (I–H), Yokota-Tanimoto (Y-T) and Brushtein (B) models and found to be migration-assisted cross-relaxation mechanisms at or beyond 1.47 × 1020 ions/cm3 dopant concentration. The diffusion coefficient, D derived from the Y-T model and the energy migration rate, Wm attained from the B model has demonstrated diffusion-based energy migration initially at about >1.47 × 1020 ions/cm3 Nd3+ concentration and after that, it has switched to a hopping mechanism with further increasing ion concentration, >2.97 × 1020 ions/cm3. Critical quenching concentration, Q and critical radius, R0 for the studied glass composition have also been determined and correlated to the experimental results. No evidence for ion clustering has been found within the studied dopant ion concentration range (0.10 × 1020 - 8.15 × 1020) ions/cm3 with respect to the estimated R0. Finally, the contribution of the individual relaxation process to the experimentally recorded emission spectra and measured lifetime, ?exp has been investigated in order to identify the most suitable Nd3+ ion concentration for developing large-aperture high-energy/high-peak-power lasers and optical amplifiers. Therefore, the present work is expected to be significant in aiding to formulate an effective glass composition for construction of compact high-power lasers and optical amplifiers. © 2023 Elsevier B.V.,The deuteration of energetic materials contributes to high signal-to-noise ratios (SNRs) in neutron diffraction, thus allowing the structures of energetic materials to be effectively investigated. This study developed the synthesis methods of deuterated energetic materials through chemical synthesis or newly developed one-pot H/D exchange. Using these methods, it synthesized nine deuterated energetic materials in a concise and low-cost manner: deuterated 1,3,5-triamino-2,4,6-trinitrobenzene (TATB-d6, 1), 1,3,5,7-tetranitro-1,3,5,7-tetraazacyclooctane (HMX-d8, 2), 1,3,5-trinitro-1,3,5-triazacyclohexane (RDX-d6, 3), dihydroxylammonium 5,5?-bis(tetrazole-1-oate) (TKX-50-d8, 4), nitroguanidine (NQ-d4, 5), 1,1-diamino-2,2-dinitroethylene (FOX-7-d4, 6), 2,6-diamino-3,5-dinitropyrazine-1-oxide (LLM-105-d4, 7), trinitrotoluene (TNT-d3, 8), and 3-nitro-1,2,4-triazol-5-one (NTO-d2, 9). Furthermore, the single crystals of HMX-d8 (2) and RDX-d6 (3) were obtained, and the ?-, ?-, ?-, and ?-polymorphs of HMX-d8 (2) were prepared accordingly. The deuterated energetic materials were characterized and analyzed using infrared spectroscopy (IR), nuclear magnetic resonance (NMR) spectroscopy, differential scanning calorimetry (DSC), thermogravimetry (TG), X-ray diffraction (XRD), and neutron diffraction. Besides, this study determined the decomposition activation energy (Ea), pre-exponential factor (A), decomposition rate constant (k), and critical explosion temperature (Tb) of TATB-d6 (1), HMX-d8 (2), and RDX-d6 (3) via DSC experiments at different heating rates. The NMR and neutron diffraction data show that these deuterated energetic materials have high deuteration rates of more than 95%. The DSC and TG analyses indicate that the deuterated energetic materials exhibit slightly higher decomposition temperatures than their nondeuterated counterparts. Furthermore, neutron diffraction shows that the deuterated energetic materials feature high SNRs. © 2023 The Authors"
222,221,18,221_underwaterimage_underwater_deepsea_seapixgan,"underwaterimage,underwater,deepsea,seapixgan,texture,images,vision,haze,illumination,image","Optical imaging instruments have been widely deployed in underwater-engineering systems, playing an important role in underwater-object localization, detection, and recognition. However, underwater images suffer from the adverse effects of local distortion and global haze. Consequently, raw underwater images have limitations when used for display and vision tasks. Super resolution (SR) has been increasingly exploited for perceptual image quality improvement. However, underwater-image SR is a relatively underexplored area, most existing methods being unable to reduce the adverse effects of underwater images. Moreover, in contrast to land-based high-performance platforms, the power supply and computational resources of underwater platforms are limited, making them difficult to use in large-scale models. To solve these problems, this study developed a novel range-dependency learning network to present the short- and long-range dependency of multiscale features. Such a mechanism could provide more detailed and accurate texture information for underwater-image SR, improving underwater-image SR performance. Moreover, a channel-splitting module was designed to generate the channel bands which could extract texture details and global structural information at different scales while reducing the number of parameters, thus accelerating the training speed of the model and maintaining good performance. Our novel network could reach an optimal tradeoff between the underwater-image SR performance and efficiency, which was demonstrated by experimental comparisons and an ablation study. © 2023,The absorption and scattering properties of the water medium cause various types of distortion in underwater images, which seriously affects the accuracy and effectiveness of subsequent processing. The application of supervised learning algorithms in underwater image enhancement is limited by the difficulty of obtaining a large number of underwater paired images in practical applications. As a solution, we propose an unsupervised representation disentanglement based underwater image enhancement method (URD-UIE). URD-UIE disentangles content information (e.g., texture, semantics) and style information (e.g., chromatic aberration, blur, noise, and clarity) from underwater images and then employs the disentangled information to generate the target distortion-free image. Our proposed method URD-UIE adopts an unsupervised cycle-consistent adversarial translation architecture and combines multiple loss functions to impose specific constraints on the output results of each module to ensure the structural consistency of underwater images before and after enhancement. The experimental results demonstrate that the URD-UIE technique effectively enhances the quality of underwater images when training with unpaired data, resulting in a significant improvement in the performance of the standard model for underwater object detection and semantic segmentation. © 2023 Elsevier Ltd,Due to underwater light absorption and scattering, underwater images usually suffer from severe color attenuation and contrast reduction. Most mainstream underwater image processing methods based on deep learning require a large amount of underwater paired training data, leading to a complex network structure, longer training time, and higher computational cost. To address this problem, a novel Zero-Reference Deep Network for Underwater Image Enhancement (Zero-UIE) is proposed in this paper, which transforms the enhancement of an underwater image into a specific parameter map estimation by using a deep network. The underwater curve model based on the classical haze image formation principle is specially designed to remove underwater color dispersion and cast. A lightweight deep network is designed to estimate the dynamic adjustment parameters of the underwater curve model, and then adjust the dynamic range of the given image pixels according to the model. A set of non-reference loss functions are designed according to the characteristics of underwater images, which can implicitly drive the network learning. In addition, adaptive color compensation can be optionally used as the pre-processing step to further improve the robustness and visual performance. The significant contribution of the proposed method is zero reference, i.e., it does not require any paired or unpaired reference data for training. Extensive experiments on various benchmarks demonstrate that the proposed method is superior to state-of-the-art methods subjectively and objectively, which is competitive and applicable to diverse underwater conditions. Most importantly, it is an innovative exploration of zero reference for underwater image enhancement. © 1976-2012 IEEE."
223,222,18,222_feature_prediction_features_cnnbigru,"feature,prediction,features,cnnbigru,convolution,convolutional,maintenance,machinery,equipment,track","Data imbalance and large data probability distribution discrepancies are major factors that reduce the accuracy of remaining useful life (RUL) prediction of high-reliability rotating machinery. In feature extraction, most deep transfer learning models consider the overall features but rarely attend to the local target features that are useful for RUL prediction; insufficient attention paid to local features reduces the accuracy and reliability of prediction. By considering the contribution of input data to the modeling output, a deep learning model that incorporates the attention mechanism in feature selection and extraction is proposed in our work; an unsupervised clustering method for classification of rotating machinery performance state evolution is put forward, and a similarity function is used to calculate the expected attention of input data to build an input data extraction attention module; the module is then fused with a gated recurrent unit (GRU), a variant of a recurrent neural network, to construct an attention-GRU model that combines prediction calculation and weight calculation for RUL prediction. Tests on public datasets show that the attention-GRU model outperforms traditional GRU and LSTM in RUL prediction, achieves less prediction error, and improves the performance and stability of the model. © 2023 by the authors.,Remaining useful life (RUL) prediction is of great significance for improving maintenance efficiency and ensuring the reliability of rotating machinery. In recent years, there are a large number of deep-learning-based methods for RUL prediction of rotating machinery. However, the effect of conventional end-to-end RUL prediction methods relies on the distribution consistency of training data and test data, and conventional health indicator extrapolation RUL prediction methods are susceptible to interference from abnormal fluctuations in the health curve. To overcome the problem, this paper proposes a new RUL prediction method for rotating machinery using health indicators constructed by the residual hybrid network with self-attention mechanism (Res-HSA). First of all, we propose the residual hybrid network combined with self-attention mechanism to extract the high-level degenerate feature. Then, the health assessment model based on Res-HSA is proposed to generate the health indicators of the equipment. To assist in network training, the segmented data labels based on the degradation rule are applied to optimize the labels of training sets. Finally, to address the problem of abnormal fluctuations in the health curve, a fitting interval selection method is used to optimize conventional curve fitting schemes to calculate RUL. Two public datasets, IEEE-PHM-2012-challenge datasets and C-MAPSS datasets, are used to verify the effectiveness of the proposed method. The experiment results on two public datasets show that the RUL prediction method proposed in this paper has good prediction performance. Compared to the state-of-the-art method, the method proposed in this article reaches the most advanced level in some test projects, while the rest of the projects can be very close to the most advanced method. © 2023 Elsevier Ltd,The remaining useful life (RUL) prediction is important for improving the safety, supportability, maintainability, and reliability of modern industrial equipment. The traditional data-driven rolling bearing RUL prediction methods require a substantial amount of prior knowledge to extract degraded features. A large number of recurrent neural networks (RNNs) have been applied to RUL, but their shortcomings of long-term dependence and inability to remember long-term historical information can result in low RUL prediction accuracy. To address this limitation, this paper proposes an RUL prediction method based on adaptive shrinkage processing and a temporal convolutional network (TCN). In the proposed method, instead of performing the feature extraction to preprocess the original data, the multi-channel data are directly used as an input of a prediction network. In addition, an adaptive shrinkage processing sub-network is designed to allocate the parameters of the soft-thresholding function adaptively to reduce noise-related information amount while retaining useful features. Therefore, compared with the existing RUL prediction methods, the proposed method can more accurately describe RUL based on the original historical data. Through experiments on a PHM2012 rolling bearing data set, a XJTU-SY data set and comparison with different methods, the predicted mean absolute error (MAE) is reduced by 52% at most, and the root mean square error (RMSE) is reduced by 64% at most. The experimental results show that the proposed adaptive shrinkage processing method, combined with the TCN model, can predict the RUL accurately and has a high application value. © 2022 by the authors."
224,223,18,223_mooc_moocs_recommender_tutoring,"mooc,moocs,recommender,tutoring,recommendation,ranking,recommendations,courses,students,embeddings","The exponential growth of Massive Open Online Courses (MOOCs) surges the needs of advanced models for personalized Online Education Services (OES). Existing solutions successfully recommend MOOCs courses via deep learning models, they however generate weak 'course embeddings' with original profiles, which contain noisy and few enrolled courses. On the other hand, existing algorithms provide recommendation orders according to the score of each course while ignoring personalized demands of users. To tackle the above challenges, we propose a Meta Hierarchical Reinforced Ranking approach MHRR, which consists of a meta hierarchical reinforcement learning pre-trained mechanism and an over-parameterized ranking regressor to enhance the representation learning of courses and learners while refining the ranking result of recommended courses. Specifically, MHRR combines a user profile reviser and a meta embedding generator to provide course embedding representation enhancement for recommender services. Furthermore, MHRR transforms learned representations generated from recommender services with Gaussian kernel approximation to over-parameterize the downstream learning to rank (LTR) models with representations in ultra-high dimensionality. We deploy MHRR on a real-world MOOCs platform and evaluate it with a large number of baseline models. The results show that MHRR outperforms baseline algorithms on two major metrics, including Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). Also, we conduct a 7-day online evaluation using the realistic traffic of a large-scale real-world MOOCs platform, where we can still observe significant improvement in real-world applications. MHRR performs consistently both in the online and offline evaluation.  © 2008-2012 IEEE.,Massive Open Online Courses (MOOCs) have received unprecedented attention, in which learners can obtain a large number of learning objects anytime and anywhere. However, the increasing information overload on MOOCs inhibits the appropriate choice of learning objects by learners, leading to a low efficiency and high dropout rates in the learning process of this human-computer interaction scenario. E-learning recommendation systems have been studied to present learning objects directly to learners, thereby relieving such problem. However, in MOOC platforms, recommendation network structures which can selectively extract implicit feature such as heterogeneous learning preference and knowledge organization of learning objects are still not comprehensively studied. To this end, we propose a learning object recommendation model based on heterogeneous learning behavior and knowledge graph. To generate a unified representation of each entity and relation, we first propose an Attentive Composition based Graph Convolutional Network (ACGCN). By introducing an attention mechanism, information is amplified when updating the representation of the heterogeneous graph, which eliminates the impact of noise and improves the robustness of the model. Then, a Dense Feature based Operation-Aware Network (DFOAN) is utilized to capture implicit and complex learners' interactive behaviors, and to further provide a recommendation. Experimental results using two real-world datasets revealed that our proposed model has the best precision, recall, F1, and accuracy scores compared to those of several existing models.  © 1989-2012 IEEE.,Massive Open Online Courses (MOOCs), which provide learners with a large-scale, open-access learning opportunity, have drawn a lot of attention recently. The amount of information available on MOOCs is increasing, making it challenging for learners to choose the best course materials and leading to low learning efficiency and high dropout rates. To address these problems, Recommendation Systems (RSs) have been researched as a direct approach of delivering educational content to learners while also attracting their interest. However, a course often consists of various learning concepts, each of which covers a different topic. Explicitly proposing courses can cause the lack of learners' attention to a particular knowledge concept. We introduce a Top-N Knowledge Concept Recommendations in MOOCs using a Neural Co-Attention Model, called NCO-A, that integrates significant heterogeneous data with recommendations based on knowledge concepts. The NCO-A model's effectiveness has been proven by extensive experiments on three real-world datasets.  © 2013 IEEE."
225,224,18,224_classifiers_classifier_classification_ensembles,"classifiers,classifier,classification,ensembles,ensemble,decisiontree,pruning,predictors,datasets,predictor","Ensemble learning algorithms such as bagging often generate unnecessarily large models, which consume extra computational resources and may degrade the generalization ability. Pruning can potentially reduce ensemble size as well as improve performance; however, researchers have previously focused more on pruning classifiers rather than regressors. This is because, in general, ensemble pruning is based on two metrics: diversity and accuracy. Many diversity metrics are known for problems dealing with a finite set of classes defined by discrete labels. Therefore, most of the work on ensemble pruning is focused on such problems: classification, clustering, and feature selection. For the regression problem, it is much more difficult to introduce a diversity metric. In fact, the only such metric known to date is a correlation matrix based on regressor predictions. This study seeks to address this gap. First, we introduce the mathematical condition that allows checking whether the regression ensemble includes redundant estimators, i.e., estimators, whose removal improves the ensemble performance. Developing this approach, we propose a new ambiguity-based pruning (AP) algorithm that bases on error-ambiguity decomposition formulated for a regression problem. To check the quality of AP, we compare it with the two methods that directly minimize the error by sequentially including and excluding regressors, as well as with the state-of-art Ordered Aggregation algorithm. Experimental studies confirm that the proposed approach allows reducing the size of the regression ensemble with simultaneous improvement in its performance and surpasses all compared methods. © 2023 The authors.,Decision Trees (DTs) are a class of supervised learning models that are widely used for both classification and regression applications. They are well-known for their interpretability and robustness, which have led them to remain popular even 60 years after they were first proposed. However, because traditional tree algorithms use greedy methods that are prone to suboptimality, several works have explored the usage of evolutionary algorithms instead. Although these algorithms are often reported to outperform the traditional greedy approach, their computational cost is much higher, since the evolutionary component requires a large number (millions or billions) of function evaluations in order to produce a single tree. Aiming to reduce this computational cost, in this work we propose an encoding that allows the training and evaluation of DTs using only matrix operations. The proposed procedure is shown to be much faster than the traditional tree implementation for complete trees with depths ranging from 2 to 6, and for datasets ranging in size from 100 to 100,000 observations. In particular, the results show speedups of nearly up to 20 times, especially when the dataset is large and the desired tree is small enough to be interpretable. The proposed procedure also benefits from GPU parallelization, although it is still highly performing without it. Furthermore, we propose an evolutionary algorithm, called Coral Reef Optimization for Decision Trees (CRO-DT), that integrates this encoding with a pre-existing ensemble algorithm to evolve better univariate trees. The results obtained show that the proposed CRO-DT is competitive with traditional and modern tree algorithms, consistently producing models of good quality across 14 tested UCI Datasets. We conclude that for most relevant situations, the proposed matrix encoding provides significant speedups over the traditional implementation, and also may serve as a basis for high quality evolutionary DT algorithms. © 2023 Elsevier B.V.,Multi-class ensemble classification remains a popular focus of investigation within the research community. The popularization of cloud services has sped up their adoption due to the ease of deploying large-scale machine-learning models. It has also drawn the attention of the industrial sector because of its ability to identify common problems in production. However, there are challenges to conform an ensemble classifier, namely a proper selection and effective training of the pool of classifiers, the definition of a proper architecture for multi-class classification, and uncertainty quantification of the ensemble classifier. The robustness and effectiveness of the ensemble classifier lie in the selection of the pool of classifiers, as well as in the learning process. Hence, the selection and the training procedure of the pool of classifiers play a crucial role. An (ensemble) classifier learns to detect the classes that were used during the supervised training. However, when injecting data with unknown conditions, the trained classifier will intend to predict the classes learned during the training. To this end, the uncertainty of the individual and ensemble classifier could be used to assess the learning capability. We present a novel approach for anomaly detection using ensemble classification and evidence theory. A pool selection strategy is presented to build a solid ensemble classifier. We present an architecture for multi-class ensemble classification and an approach to quantify the uncertainty of the base classifiers and the ensemble classifier. We address the problem of detecting unknown conditions (while feeding out-of-distribution data), presenting a novel approach that monitors the uncertainty of the ensemble classifier using evidence theory. Finally, we use the benchmark Tennessee Eastman to perform experiments to test the ensemble classifier's prediction and anomaly detection capabilities.  © 2013 IEEE."
226,225,18,225_birds_wildlife_recordings_vocalizations,"birds,wildlife,recordings,vocalizations,vocalization,bioacoustics,vocalisations,bioacoustic,bird,audio","There is a need for monitoring biodiversity at multiple spatial and temporal scales to aid conservation efforts. Autonomous recording units (ARUs) can provide cost-effective, long-term and systematic species monitoring data for sound-producing wildlife, including birds, amphibians, insects and mammals over large areas. Modern deep learning can efficiently automate the detection of species occurrences in these sound data with high accuracy. Further, citizen science can be leveraged to scale up the deployment of ARUs and collect reference vocalizations needed for training and validating deep learning models. In this study we develop a convolutional neural network (CNN) acoustic classification pipeline for detecting 54 bird species in Sonoma County, California USA, with sound and reference vocalization data collected by citizen scientists within the Soundscapes to Landscapes project (www.soundscapes2landscapes.org). We trained three ImageNet-based CNN architectures (MobileNetv2, ResNet50v2, ResNet100v2), which function as a Mixture of Experts (MoE), to evaluate the usefulness of several methods to enhance model accuracy. Specifically, we: 1) quantify accuracy with fully-labeled 1-min soundscapes for an assessment of real-world conditions; 2) assess the effect on precision and recall of additional pre-training with an external sound archive (xeno-canto) prior to fine-tuning with vocalization data from our study domain; and, 3) assess how detections and errors are influenced by the presence of coincident biotic and non-biotic sounds (i.e., soundscape components). In evaluating accuracy with soundscape data (n = 37 species) across CNN probability thresholds and models, we found acoustic pre-training followed by fine-tuning improved average precision by 10.3% relative to no pre-training, although there was a small average 0.8% reduction in recall. In selecting an optimal CNN architecture for each species based on maximum F(? = 0.5), we found our MoE approach had total precision of 84.5% and average species precision of 85.1%. Our data exhibit multiple issues arising from applying citizen science and acoustic monitoring at the county scale, including deployment of ARUs with relatively low fidelity and recordings with background noise and overlapping vocalizations. In particular, human noise was significantly associated with more incorrect species detections (false positives, decreased precision), while physical interference (e.g., recorder hit by a branch) and geophony (e.g., wind) was associated with the classifier missing detections (false negatives, decreased recall). Our process surmounted these obstacles, and our final predictions allowed us to demonstrate how deep learning applied to acoustic data from low-cost ARUs paired with citizen science can provide valuable bird diversity data for monitoring and conservation efforts. © 2023,Open audio databases such as Xeno-Canto are widely used to build datasets to explore bird song repertoire or to train models for automatic bird sound classification by deep learning algorithms. However, such databases suffer from the fact that bird sounds are weakly labelled: a species name is attributed to each audio recording without timestamps that provide the temporal localization of the bird song of interest. Manual annotations can solve this issue, but they are time consuming, expert-dependent, and cannot run on large datasets. Another solution consists in using a labelling function that automatically segments audio recordings before assigning a label to each segmented audio sample. Although labelling functions were introduced to expedite strong label assignment, their classification performance remains mostly unknown. To address this issue and reduce label noise (wrong label assignment) in large bird song datasets, we introduce a data-centric novel labelling function composed of three successive steps: 1) time-frequency sound unit segmentation, 2) feature computation for each sound unit, and 3) classification of each sound unit as bird song or noise with either an unsupervised DBSCAN algorithm or the supervised BirdNET neural network. The labelling function was optimized, validated, and tested on the songs of 44 West-Palearctic common bird species. We first showed that the segmentation of bird songs alone aggregated from 10% to 83% of label noise depending on the species. We also demonstrated that our labelling function was able to significantly reduce the initial label noise present in the dataset by up to a factor of three. Finally, we discuss different opportunities to design suitable labelling functions to build high-quality animal vocalizations with minimum expert annotation effort. © 2022 Elsevier B.V.,Automated bioacoustic analysis aids understanding and protection of both marine and terrestrial animals and their habitats across extensive spatiotemporal scales, and typically involves analyzing vast collections of acoustic data. With the advent of deep learning models, classification of important signals from these datasets has markedly improved. These models power critical data analyses for research and decision-making in biodiversity monitoring, animal behaviour studies, and natural resource management. However, deep learning models are often data-hungry and require a significant amount of labeled training data to perform well. While sufficient training data is available for certain taxonomic groups (e.g., common bird species), many classes (such as rare and endangered species, many non-bird taxa, and call-type) lack enough data to train a robust model from scratch. This study investigates the utility of feature embeddings extracted from audio classification models to identify bioacoustic classes other than the ones these models were originally trained on. We evaluate models on diverse datasets, including different bird calls and dialect types, bat calls, marine mammals calls, and amphibians calls. The embeddings extracted from the models trained on bird vocalization data consistently allowed higher quality classification than the embeddings trained on general audio datasets. The results of this study indicate that high-quality feature embeddings from large-scale acoustic bird classifiers can be harnessed for few-shot transfer learning, enabling the learning of new classes from a limited quantity of training data. Our findings reveal the potential for efficient analyses of novel bioacoustic tasks, even in scenarios where available training data is limited to a few samples. © 2023, The Author(s)."
227,226,18,226_soils_soil_spectrometers_spectroscopy,"soils,soil,spectrometers,spectroscopy,spectra,hyperspectral,minerals,spectral,nitrogen,calibration","Infrared spectroscopy in the visible to near-infrared (vis–NIR) and mid-infrared (MIR) regions is a well-established approach for the prediction of soil properties. Different data fusion and training approaches exist, and the optimal procedures are yet undefined and may depend on the heterogeneity present in the set and on the considered scale. The objectives were to test the usefulness of partial least squares regressions (PLSRs) for soil organic carbon (SOC), total carbon (Ct), total nitrogen (Nt) and pH using vis–NIR and MIR spectroscopy for an independent validation after standard calibration (use of a general PLSR model) or using memory-based learning (MBL) with and without spiking for a national spectral database. Data fusion approaches were simple concatenation of spectra, outer product analysis (OPA) and model averaging. In total, 481 soils from an Austrian forest soil archive were measured in the vis–NIR and MIR regions, and regressions were calculated. Fivefold calibration-validation approaches were carried out with a region-related split of spectra to implement independent validations with n ranging from 47 to 99 soils in different folds. MIR predictions were generally superior over vis–NIR predictions. For all properties, optimal predictions were obtained with data fusion, with OPA and spectra concatenation outperforming model averaging. The greatest robustness of performance was found for OPA and MBL with spiking with R2 ? 0.77 (N), 0.85 (SOC), 0.86 (pH) and 0.88 (Ct) in the validations of all folds. Overall, the results indicate that the combination of OPA for vis–NIR and MIR spectra with MBL and spiking has a high potential to accurately estimate properties when using large-scale soil spectral libraries as reference data. However, the reduction of cost-effectiveness using two spectrometers needs to be weighed against the potential increase in accuracy compared to a single MIR spectroscopy approach. © 2023 The Authors. European Journal of Soil Science published by John Wiley & Sons Ltd on behalf of British Society of Soil Science.,Large and publicly available soil spectral libraries, such as the USDA National Soil Survey Center–Kellogg Soil Survey Laboratory (NSSC-KSSL) mid-infrared (MIR) spectral library, are enormously valuable resources enabling laboratories around the world to make rapid low-cost estimates of a number of soil properties. A limitation to widespread sharing of soil spectral data is the need to ensure that spectra collected on a secondary spectrometer are compatible with the spectra in the primary or reference library. Various spectral preprocessing and calibration transfer techniques have been proposed to overcome this limitation. We tested the transferability of models developed using the USDA NSSC-KSSL MIR library to a secondary instrument. For the soil properties, total carbon (TC), pH, and clay content, we found that good performance (ratio of performance to deviation [RPD] = 4.9, 2.0, and 3.6, respectively) could be achieved on an independent test set with Savitzky-Golay smoothing and first derivative preprocessing of the secondary spectra using a memory-based learning chemometric approach. We tested three calibration transfer techniques (direct standardization [DS], piecewise direct standardization [PDS], and spectral space transformation [SST]) using different size transfer sets selected to be representative of the entire NSSC-KSSL library. Among the transfer methods, SST consistently outperformed DS and PDS with 50 transfer samples being an optimal number for transfer model development. For TC and pH, performance was improved using the SST transfer (RPD = 7.7 and 2.2, respectively) primarily through the elimination of bias. Calibration transfer could not improve predictions for clay. These findings suggest that calibration transfer may not always be necessary, but users should test to confirm this assumption using a small set of representative samples scanned at both laboratories. © 2022 The Authors. Soil Science Society of America Journal published by Wiley Periodicals LLC on behalf of Soil Science Society of America.,Global pressures to improve soil organic carbon sequestration and soil health in general amongst the world's agricultural soils are creating a demand for improved practice to drive positive and sustainable changes in the natural capital of soils. Incentive programs aimed to promote this must be informed by accurate observations of the state of soils, both temporally and spatially. Soil spectral inference is a useful method for capturing the state of soils cost-effectively, but the price of standard laboratory grade visible and near-infrared (Vis-NIR) sensors can limit its application. Further, the acquisition of spectra by these laboratory grade sensors is performed primarily in air-dried and ground condition, adding a time lag to information retrieval. Recently, low-cost, portable miniaturised near-infrafred (NIR) spectrometers have become available and have shown to be a viable alternative for the measurement of several agronomically important soil properties, which are also vital to the maintenance of soil health, including soil organic carbon (SOC), and cation exchange capacity (CEC). However, the implementation of new spectrometers, to new locations requires the creation of new spectral libraries, an expensive and labour-intensive process requiring large amounts of soil analytical and spectral data gathering. Thus, existing, laboratory grade Vis-NIR spectral libraries present a high-quality and high-resolution resource to leverage. This work demonstrates how existing spectral library resources can be accessed with cheaper, portable miniaturised NIR spectrometers with appropriate spectral filtering, and appropriate transformation matrices. In addition, the work shows that by correcting for the influences of spectral differences between soils scanned in field condition, and those prepared for analysis in the laboratory, greater uptake of spectral inference as a tool to evaluate the state of soils can be enabled. This work also demonstrates how large existing laboratory grade spectral libraries such as the CSIRO national Australian Vis-NIR soil spectral library can be queried and using memory-based learning or similar methods, such as RS-Local, and the most appropriate samples may be identified to be used for the prediction of soil properties. This work builds off an existing framework for the use of soil spectral inference for monitoring the state of soil, the Australian 2021 Soil Organic Carbon Credits Methodology Determination. Methods are demonstrated for the prediction of nine agronomically important soil properties, SOC, pH in water, pH in CaCl2, electrical conductivity, CEC, and exchangeable Ca, K, Mg and Na. For SOC a model using only 20 local samples was produced in this work with a Lin's concordance correlation coefficient (LCCC) of 0.72, surpassing both the minimum requirement under the carbon credits methodology determination (LCCC 0.6), and a 50 sample local only model (LCCC 0.61). This example demonstrates that a significant further potential cost saving in laboratory analysis across soil monitoring projects can be achieved through selectively leveraging a large spectral library resource. © 2023"
228,227,17,227_suicidality_suicidal_suicide_adolescents,"suicidality,suicidal,suicide,adolescents,depression,predictors,psychopathology,abusedependence,youth,psychological","Suicide is a major global health concern and a prominent cause of death in adolescents. Previous research on suicide prediction has mainly focused on clinical or adult samples. To prevent suicides at an early stage, however, it is important to screen for risk factors in a community sample of adolescents. We compared the accuracy of logistic regressions, elastic net regressions, and gradient boosting machines in predicting suicide attempts by 17-year-olds in the Millennium Cohort Study (N = 7,347), combining a large set of self- and other-reported variables from different categories. Both machine learning algorithms outperformed logistic regressions and achieved similar balanced accuracies (.76 when using data 3 years before the self-reported lifetime suicide attempts and.85 when using data from the same measurement wave). We identified essential variables that should be considered when screening for suicidal behavior. Finally, we discuss the usefulness of complex machine learning models in suicide prediction. © The Author(s) 2023.,Background: Suicide is the second leading cause of death in adolescents, and self-harm is one of the strongest predictors of death by suicide. The rates of adolescents presenting to emergency departments (EDs) for suicidal thoughts and behaviors (STBs) have increased. Still, existing follow-up after ED discharge is inadequate, leaving a high-risk period for reattempts and suicide. There is a need for innovative evaluation of imminent suicide risk factors in these patients, focusing on continuous real-time evaluations with low assessment burden and minimal reliance on patient disclosure of suicidal intent. Objective: This study examines prospective longitudinal associations between observed real-time mobile passive sensing, including communication and activity patterns, and clinical and self-reported assessments of STB over 6 months. Methods: This study will include 90 adolescents recruited on their first outpatient clinic visit following their discharge from the ED due to a recent STB. Participants will complete brief weekly assessments and be monitored continuously for their mobile app usage, including mobility, activity, and communication patterns, over 6 months using the iFeel research app. Participants will complete 4 in-person visits for clinical assessment at baseline and at the 1-, 3-, and 6-month follow-ups. The digital data will be processed, involving feature extraction, scaling, selection, and dimensionality reduction. Passive monitoring data will be analyzed using both classical machine learning models and deep learning models to identify proximal associations between real-time observed communication, activity patterns, and STB. The data will be split into a training and validation data set, and predictions will be matched against the clinical evaluations and self-reported STB events (ie, labels). To use both labeled and unlabeled digital data (ie, passively collected), we will use semisupervised methods in conjunction with a novel method that is based on anomaly detection notions. Results: Participant recruitment and follow-up started in February 2021 and are expected to be completed by 2024. We expect to find prospective proximal associations between mobile sensor communication, activity data, and STB outcomes. We will test predictive models for suicidal behaviors among high-risk adolescents. Conclusions: Developing digital markers of STB in a real-world sample of high-risk adolescents presenting to ED can inform different interventions and provide an objective means to assess the risk of suicidal behaviors. The results of this study will be the first step toward large-scale validation that may lead to suicide risk measures that aid psychiatric follow-up, decision-making, and targeted treatments. This novel assessment could facilitate timely identification and intervention to save young people’s lives. ©Shira Barzilay, Shai Fine, Shannel Akhavan, Liat Haruvi-Catalan, Alan Apter, Anat Brunstein-Klomek, Lior Carmi, Mishael Zohar, Inbar Kinarty, Talia Friedman, Silvana Fennig.,Introduction: False positives in retrospective binary suicide attempt classification models are commonly attributed to sheer classification error. However, when machine learning suicide attempt classification models are trained with a multitude of psycho-socio-environmental factors and achieve high accuracy in suicide risk assessment, false positives may turn out to be at high risk of developing suicidal behavior or attempting suicide in the future. Thus, they may be better viewed as “true alarms,” relevant for a suicide prevention program. In this study, using large population-based longitudinal dataset, we examine three hypotheses: (1) false positives, compared to the true negatives, are at higher risk of suicide attempt in future, (2) the suicide attempts risk for the false positives increase as a function of increase in specificity threshold; and (3) as specificity increases, the severity of risk factors between false positives and true positives becomes more similar. Methods: Utilizing the Gradient Boosting algorithm, we used a sample of 11,369 Norwegian adolescents, assessed at two timepoints (1992 and 1994), to classify suicide attempters at the first time point. We then assessed the relative risk of suicide attempt at the second time point for false positives in comparison to true negatives, and in relation to the level of specificity. Results: We found that false positives were at significantly higher risk of attempting suicide compared to true negatives. When selecting a higher classification risk threshold by gradually increasing the specificity cutoff from 60% to 97.5%, the relative suicide attempt risk of the false positive group increased, ranging from minimum of 2.96 to 7.22 times. As the risk threshold increased, the severity of various mental health indicators became significantly more comparable between false positives and true positives. Conclusion: We argue that the performance evaluation of machine learning suicide classification models should take the clinical relevance into account, rather than focusing solely on classification error metrics. As shown here, the so-called false positives represent a truly at-risk group that should be included in suicide prevention programs. Hence, these findings should be taken into consideration when interpreting machine learning suicide classification models as well as planning future suicide prevention interventions for adolescents. Copyright © 2023 Haghish, Laeng and Czajkowski."
229,228,17,228_optical_oam_oammdm_ocm,"optical,oam,oammdm,ocm,optica,oambased,oamsk,affinitynet,orbital,selfmixing","The main difficulties of using the deep learning model to decouple the orbital angular momentum (OAM) in Free-Space Optical (FSO) communication are that the model requires a large number of training data sets and the model's convergence speed is low. In this paper, transfer learning and depthwise separable convolution are combined to improve the computational speed of the model and to reduce the requirement of training data set size. The recognition accuracies of 4-OAM and 8-OAM based on the measured data with noise are studied respectively and the OAM transmission in atmospheric turbulence are simulated to test the robustness of the model. In addition, the proposed method can be trained on the expanded data set of 38 experimental data collection, and the test classification results can reach 99.5%. Meanwhile, the minimum accuracy of the model in testing data of different transmission distances and turbulence intensities is 81.25%, indicating good robustness. The paper's work on the OAM pattern detection has great significance. © 2023 Elsevier B.V.,Nonlinear impairment in a high-speed orbital angular momentum (OAM) mode-division multiplexing (MDM) optical fiber communication system presents high complexity and strong stochasticity due to the massive optoelectronic devices. In this paper, we propose an Affinity Network (AffinityNet) nonlinear equalizer for an OAM-MDM intensity-modulation direct-detection (IM/DD) transmission with four OAM modes. The labeled training and testing signals from the OAM-MDM system can be regarded as “small sample” and “large target”, respectively. AffinityNet can be used to build an accurate nonlinear model using “small sample” based on few-shot learning and can predict the stochastic characteristic nonlinearity of OAM-MDM with a high level of generalization. As a result, the AffinityNet nonlinear equalizer can effectively compensate the stochastic nonlinearity in the OAM-MDM system, despite the large difference between the training and testing signals due to the stochastic nonlinear impairment. An experiment was conducted on a 400 Gbit/s transmission with four OAM modes using a pulse amplitude modulation-8 (PAM-8) signal over a 2 km ring-core fiber (RCF). Our experimental results show that the proposed nonlinear equalizer outperformed the conventional Volterra equalizer with improvements in receiver sensitivity of 1.7, 1.8, 3, and 3.3 dB for the four OAM modes at the 15% forward error correction (FEC) threshold, respectively. In addition, the proposed equalizer outperformed a convolutional neural network (CNN) equalizer with improvements in receiver sensitivity of 0.8, 0.5, 0.9, and 1.4 dB for the four OAM modes at the 15% FEC threshold. In the experiment, a complexity reduction of 37% and 83% of the AffinityNet equalizer is taken compared to the conventional Volterra equalizer and CNN equalizer, respectively. The proposed equalizer is a promising candidate for a high-speed OAM-MDM optical fiber communication system. © 2023 OSA - The Optical Society. All rights reserved.,Orbital angular momentum (OAM) has recently obtained tremendous research interest in free-space optical communications (FSO). During signal transmission within the free-space link, atmospheric turbulence (AT) poses a significant challenge as it diminishes the signal strength and introduce intermodal crosstalk, significantly reducing OAM mode detection accuracy. This issue directly impacts the performance of OAM-based communication systems and leads to a reduction in received information. To address this critical bottleneck of low mode recognition accuracy in OAM-based FSO-communications, a deep learning method based on vision transformers (ViT) is proposed for what we believe is for the first time. Designed carefully by numerous experts, the advanced self-attention mechanism of ViT captures more global information from the input image. To train the model, pretraining on a large dataset, named IMAGENET is conducted. Subsequently, we performed fine-tuning on our specific dataset, consisting of OAM beams that have undergone varying AT strengths. The computer simulation shows that based on ViT method, the multiple OAM modes can be recognized with a high accuracy (nearly 100%) under weak-to-moderate turbulence and with almost 98% accuracy even under long transmission distance with strong turbulence (CN2 = 1 × 10?14). Our findings highlight that leveraging ViT enables robust detection of complex OAM beams, mitigating the adverse effects caused by atmospheric turbulence. © 2023 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement."
230,229,17,229_pose_poses_3d_dataset,"pose,poses,3d,dataset,multiperson,videos,posegu,datasets,body,human","3D pose estimation has recently gained substantial interests in computer vision domain. Existing 3D pose estimation methods have a strong reliance on large size well-annotated 3D pose datasets, and they suffer poor model generalization on unseen poses due to limited diversity of 3D poses in training sets. In this work, we propose PoseGU, a novel human pose generator that generates diverse poses with access only to a small size of seed samples, while equipping the Counterfactual Risk Minimization to pursue an unbiased evaluation objective. Extensive experiments demonstrate PoseGU outperforms almost all the state-of-the-art 3D human pose methods under consideration over three popular benchmark datasets. Empirical analysis also proves PoseGU generates 3D poses with improved data diversity and better generalization ability. © 2023 Elsevier Inc.,Deep learning advances have made it possible to recoverfull 3-D meshes of human models from individual images. However, the extension of this notion to videos for recovering temporally coherent poses is still underexplored. A major challenge in this direction is the lack of appropriately annotated video data for learning the desired computational models. The existing human pose datasets only provide 2-D or 3-D skeleton joint annotations, whereas the datasets are also insufficiently recorded in constrained environments. We first contribute a technique to synthesize monocular action videos with rich 3-D annotations that are suitable for learning computational models for full mesh 3-D human pose recovery. Compared to the existing methods that simply 'texture map' clothes onto the 3-D human pose models, our approach incorporates Physics-based realistic cloth deformations with human body movements. The generated videos cover a large variety of human actions, poses, and visual appearances, while the annotations record accurate human pose dynamics and human body surface information. Our second major contribution is an end-to-end trainable recurrent neural network for full pose mesh recovery from monocular videos. Using the proposed video data and a long short-term memory recurrent structure, our network explicitly learns to model the temporal coherence in videos and imposes geometric consistency over the recovered meshes. We establish the effectiveness of the proposed model with quantitative and qualitative analysis using the proposed and benchmark datasets.  © 2020 IEEE.,Background: In computer vision, simultaneously estimating human pose, shape, and clothing is a practical issue in real life, but remains a challenging task owing to the variety of clothing, complexity of deformation, shortage of large-scale datasets, and difficulty in estimating clothing style. Methods: We propose a multistage weakly supervised method that makes full use of data with less labeled information for learning to estimate human body shape, pose, and clothing deformation. In the first stage, the SMPL human-body model parameters were regressed using the multi-view 2D key points of the human body. Using multi-view information as weakly supervised information can avoid the deep ambiguity problem of a single view, obtain a more accurate human posture, and access supervisory information easily. In the second stage, clothing is represented by a PCAbased model that uses two-dimensional key points of clothing as supervised information to regress the parameters. In the third stage, we predefine an embedding graph for each type of clothing to describe the deformation. Then, the mask information of the clothing is used to further adjust the deformation of the clothing. To facilitate training, we constructed a multi-view synthetic dataset that included BCNet and SURREAL. Results: The Experiments show that the accuracy of our method reaches the same level as that of SOTA methods using strong supervision information while only using weakly supervised information. Because this study uses only weakly supervised information, which is much easier to obtain, it has the advantage of utilizing existing data as training data. Experiments on the DeepFashion2 dataset show that our method can make full use of the existing weak supervision information for fine-tuning on a dataset with little supervision information, compared with the strong supervision information that cannot be trained or adjusted owing to the lack of exact annotation information. Conclusions: Our weak supervision method can accurately estimate human body size, pose, and several common types of clothing and overcome the issues of the current shortage of clothing data. © 2022 Beijing Zhongke Journal Publishing Co. Ltd"
231,230,17,230_anomaly_autoencoder_anomalies_intrusion,"anomaly,autoencoder,anomalies,intrusion,detection,anomalous,detecting,detect,surveillance,dcnn","Visual surface anomaly detection focuses on the classification (CLS) and location (LOC) of regions that deviate from the normal appearance, and generally, only normal samples are provided for training. The reconstruction-based method is widely used, which locates the anomalies by analyzing the reconstruction error. However, there are two problems unsettled in the reconstruction-based method. First, the reconstruction error in the normal regions is sometimes large. This might mislead the model to take the normal regions as anomalies, which is named an overkill problem. Second, it has been observed that the anomalous regions sometimes cannot be repaired to normal, which results in a small reconstruction error in the anomalous regions. This misleads the model to take the anomalies as normal, which is called an anomaly escape problem. Aiming at the above two problems, we propose a model named dual-branch autoencoder with prior information (DBPI) which is mainly composed of a dual-branch AE structure and a GA unit. To alleviate the overkill problem, a natural idea is to reduce the reconstruction error in the normal regions, and therefore a dual-branch AE is proposed. The dual-branch AE reconstructs two images with consistent normal regions and different anomalous regions. By analyzing the reconstruction error between the above two reconstructed images, the anomalies can be detected without causing overkill. For the anomaly escape problem, an effective solution is to add prior information of normal appearance to the reconstructive network, which assists in repairing the anomalous regions and increasing the reconstruction error in the anomalous regions. Since the mathematical expectation map of the training data contains crucial features of the normal appearance, we utilize it as the prior information of the normal appearance. And the prior information is selectively introduced by the proposed gated attention (GA) unit, which effectively assists in reconstructing a normal image and further mitigates the anomaly escape problem. On the average precision (AP) metric for the anomaly detection benchmark dataset MVTec, the proposed unsupervised method outperforms the current state-of-the-art reconstruction-based method self-supervised predictive convolutional attentive block (SSPCAB) by 7.4%. Meanwhile, our unsupervised method also exhibits comparable performance to the best supervised methods on the surface defect detection DAGM dataset.  © 1963-2012 IEEE.,Anomaly detection refers to the problem of uncovering patterns in a given data set that do not conform to the expected behavior. Recently, owing to the continuous development of deep representation learning, a large number of anomaly detection approaches based on deep learning models have been developed and achieved promising performance. In this work, an image anomaly detection approach based on contrastive learning framework is proposed. Rather than adopting ResNet or other CNN-based deep neural networks as in most of the previous deep learning-based image anomaly detection approaches to learn representations from training samples, a contrastive learning framework is developed for anomaly detection in which Transformer is adopted for extracting better representations. Then, we develop a triple contrastive loss function and embed it into the proposed contrastive learning framework to alleviate the problem of catastrophic collapse that is often encountered in many anomaly detection approaches. Furthermore, a nonlinear Projector is integrated with our model to improve the performance of anomaly detection. The effectiveness of our image anomaly detection approach is validated through experiments on multiple benchmark data sets. According to the experimental results, our approach can obtain better or comparative performance in comparison with state-of-the-art anomaly detection approaches. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Large-scale sensor and data acquisition systems, integrated with deep learning methodologies, play a pivotal role in enhancing the sustainability and security of smart city environments, exemplifying the critical significance of anomaly detection techniques. Anomaly detection in complex industrial scenarios presents various challenges, such as intricate working environments, limited anomaly samples, and lack of a priori information. Unsupervised anomaly detection based on knowledge distillation enables anomaly detection using only normal samples. However, the similarity in structure between teacher and student models, along with identical input data flow, hampers accurate anomaly detection and localization. To address these issues, we propose MNMC, an unsupervised anomaly detection model consisting of a mixed noise generation module emulating real defects, a mutual constraint module, and an anomaly segmentation module. Firstly, to enhance the student network's ability to learn robust features, we construct a hybrid noise model comprising dead-leaves noise and perlin noise. This generates features with structural texture and distributional characteristics closer to real anomalies. Secondly, we design a mutual constraint framework to further improve the learning ability of the student network for normal features by constraining representations containing only a single noise. Lastly, for the detection of anomalies at different scales, we propose a new evaluation metric based on equal importance of normal and anomalous regions. Through ablation experiments, we demonstrate the effectiveness of the simulated real defect generation module and the mutual constraints module. Performance experiments on the MVTec dataset show that our method achieves competitive results compared to the current state-of-the-art anomaly detection methods. © 2023 Elsevier B.V."
232,231,17,231_domaininvariant_adaptation_domainlevel_domains,"domaininvariant,adaptation,domainlevel,domains,unsupervised,domain,memoryadaptnet,rsis,segmentation,scenes","In unsupervised domain adaptation (UDA) of remote sensing images (RSIs), the huge interdomain discrepancies and intradomain variances lead to complicated class-level relations. Specifically, the instances of the same class differ greatly, while instances of different classes are similar, whether across different RSIs domains or within the same RSIs domain. However, existing methods cannot fully consider these problems, limiting the performance of UDA semantic segmentation of RSIs. To this end, this article proposes a novel cross-domain multiprototypes learning method, the core idea of which is to abstract the cross-domain and intradomain class-level relations into multiple prototypes. Specifically, the multiple prototypes belonging to different classes can detailedly describe complex interclass relations, and the multiple prototypes within the same class can better model rich intraclass relations. Furthermore, the source and target samples are jointly used for prototypes calculation, to fully fuse the feature information of different RSIs. In a nutshell, utilizing the samples from different RSIs domains to learn multiple prototypes for each class can achieve better domain alignment at the class level. In addition, considering that RSIs simultaneously contain large targets with wide coverage and important small targets, two masked consistency learning strategies are designed to better explore the contextual structure of target RSIs and improve the quality of pseudo labels for prototype updating. The global consistency strategy can strengthen the utilization of global context relations, while the local consistency strategy can further improve the learning of local context details. Therefore, the proposed method is actually a prototype and context-enhanced learning method for UDA semantic segmentation of RSIs. Extensive experiments demonstrate that the proposed method can achieve better performance than existing state-of-the-art UDA methods.  © 1980-2012 IEEE.,Unsupervised domain adaptation (UDA) aims at adapting a model from the source domain to the target domain by tackling the issue of domain shift. Cross-domain segmentation of remote sensing images (RSIs) remains a big challenge due to the unique properties of RSIs. On the one hand, the divergence of data distribution in different local regions leads to negative transfer by directly applying the global alignment method in RSIs. On the other hand, the underlying category-level structure in the target domain is often ignored, which confuses the decision of semantic boundaries on the dispersed category features caused by large intraclass variance and small interclass variance in RSIs. In this study, we propose a novel fine-grained adaptation framework combining two stages of global-local alignment and category-level alignment to solve the above-mentioned problems. In the first stage of global-local adaptation, an attention map is derived from an intermediate discriminator and focuses on hard-to-align regions to mitigate negative transfer due to global adversarial learning. In the second stage of category-level adaptation, the category feature compact module is utilized to address the issue of dispersed features in the target domain attained by the cross-domain network, which will facilitate the fine-grained alignment of categories. Experiments under various scenarios, including geographic location variation and spectral band composition variation, demonstrate that the local adaptation and category-level adaptation of RSIs are complementary in the cross-domain segmentation, and the integrated framework helps achieve outstanding performance for UDA semantic segmentation of RSIs. © 2008-2012 IEEE.,Semantic segmentation techniques for remote sensing images (RSIs) have been widely developed and applied. However, most segmentation methods depend on sufficiently annotated data for specific scenarios. When a large change occurs in the target scenes, model performance drops significantly. Therefore, unsupervised domain adaptation (UDA) for semantic segmentation is proposed to alleviate the reliance on expensive per-pixel densely labeled data. In this paper, two key issues of existing domain adaptive (DA) methods are considered: (1) the factors that cause data distribution shifts in RSIs may be complex and diverse, and existing DA approaches cannot adaptively optimize for different domain discrepancy scenarios; (2) domain-invariant feature alignment, based on adversarial training (AT), is prone to excessive feature perturbation, leading to over robust models. To address these issues, we propose an AdvCDA method that guides the model to adapt adversarial perturbation consistency. We combine consistency regularization to consider interdomain feature alignment as perturbation information in the feature space, and thus propose a joint AT and self-training (ST) DA method to further promote the generalization performance of the model. Additionally, we propose a confidence estimation mechanism that determines network stream training weights so that the model can adaptively adjust the optimization direction. Extensive experiments have been conducted on Potsdam, Vaihingen, and LoveDA remote sensing datasets, and the results demonstrate that the proposed method can significantly improve the UDA performance in various cross-domain scenarios. © 2023 by the authors."
233,232,17,232_tweets_twitter_depressive_depression,"tweets,twitter,depressive,depression,depressed,distress,mood,stress,health,illness","The recent coronavirus disease (COVID-19) has become a pandemic and has affected the entire globe. During the pandemic, we have observed a spike in cases related to mental health, such as anxiety, stress, and depression. Depression significantly influences most diseases worldwide, making it difficult to detect mental health conditions in people due to unawareness and unwillingness to consult a doctor. However, nowadays, people extensively use online social media platforms to express their emotions and thoughts. Hence, social media platforms are now becoming a large data source that can be utilized for detecting depression and mental illness. However, the existing approaches often overlook data sparsity in tweets and the multimodal aspects of social media. In this article, we propose a novel multimodal framework that combines textual, user-specific, and image analysis to detect depression among social media users. To provide enough context about the user&#x2019;s emotional state, we propose the following: 1) an extrinsic feature by harnessing the URLs present in tweets and 2) extracting textual content present in images posted in tweets. We also extract five sets of features belonging to different modalities to describe a user. In addition, we introduce a deep learning model, the visual neural network (VNN), to generate embeddings of user-posted images, which are used to create the visual feature vector for prediction. We contribute a curated COVID-19 dataset of depressed and nondepressed users for research purposes and demonstrate the effectiveness of our model in detecting depression during the COVID-19 outbreak. Our model outperforms the existing state-of-the-art methods over a benchmark dataset by 2%&#x2013;8% and produces promising results on the COVID-19 dataset. Our analysis highlights the impact of each modality and provides valuable insights into users&#x2019; mental and emotional states. IEEE,Human mental health (HMH) is a pervasive and impactful condition that profoundly affects an individual’s cognitive, emotional, and behavioural aspects in a negative manner. Among various mental health disorders, depression is particularly prevalent, with approximately 20% of women experiencing at least one depressive episode during their lifetime. Identifying depression early on is crucial for timely intervention and support. This study examines user-generated content from major social platforms like Twitter, Facebook, and Instagram, aiming to detect potential signs of depression through behavioural symptoms such as mood changes, loss of interest, altered sleep patterns, focus difficulties, and impaired decision-making. Leveraging natural language processing and machine learning, sentiment analysis deciphers emotional context in posts and comments. A new efficient methodology utilizing Bidirectional Encoder Representations from Transformers (BERT) is proposed for efficient analysis of the posts and comments. Knowledge distillation transfers insights from a large BERT model to a smaller one, enhancing accuracy. Integrating word2vec and BERT with bidirectional long short-term memory (Bi-LSTM), the approach effectively analyses depression and anxiety indicators in social media data. Comparative assessments highlight the system’s excellence, achieving a remarkable 98.5% accuracy through knowledge distillation. The proposed methodology marks a substantial stride in identifying mental health signals from social media, facilitating better early intervention and support for those facing depression and anxiety-related challenges. © 2023 by author(s).,The mental health of college students is a growing concern, and gauging the mental health needs of college students is difficult to assess in real-time and in scale. To address this gap, researchers and practitioners have encouraged the use of passive technologies. Social media is one such ""passive sensor"" that has shown potential as a viable ""passive sensor"" of mental health. However, the construct validity and in-practice reliability of computational assessments of mental health constructs with social media data remain largely unexplored. Towards this goal, we study how assessing the mental health of college students using social media data correspond with ground-truth data of on-campus mental health consultations. For a large U.S. public university, we obtained ground-truth data of on-campus mental health consultations between 2011–2016, and collected 66,000 posts from the university’s Reddit community. We adopted machine learning and natural language methodologies to measure symptomatic mental health expressions of depression, anxiety, stress, suicidal ideation, and psychosis on the social media data. Seasonal auto-regressive integrated moving average (SARIMA) models of forecasting on-campus mental health consultations showed that incorporating social media data led to predictions with r = 0.86 and SMAPE = 13.30, outperforming models without social media data by 41%. Our language analyses revealed that social media discussions during high mental health consultations months consisted of discussions on academics and career, whereas months of low mental health consultations saliently show expressions of positive affect, collective identity, and socialization. This study reveals that social media data can improve our understanding of college students’ mental health, particularly their mental health treatment needs. © 2022, The Author(s)."
234,233,16,233_faces_facial_face_portrait,"faces,facial,face,portrait,dreamface,generative,adversarial,pose,gan,3d","Age invariant face recognition (AIFR) is a challenging problem in the area of the face recognition. To handle large age gap for face recognition, we proposed a robust approach based on deep learning for face recognition under a large age gap. The presented approach consists of four important steps. The pre-processing is done for face detection. Age face generation is processed with the help of modified age conditional generative adversarial network (acGAN). Generated age face images are mixed with train dataset and augmentation is applied to increase the size of training data for handling biasness of the deep learning models towards dataset size. A modified residual convolutional neural network is applied for training and testing of face images. The performance has been evaluated using two-fold cross-validation on standard and challenging LAG dataset. The proposed approach achieved the 92.5% recognition accuracy, which is better than the existing face recognition approaches for a large age gap. Copyright © 2023 Inderscience Enterprises Ltd.,Recent years have seen growing interest in 3D human face modeling due to its wide applications in digital human, character generation and animation. Existing approaches overwhelmingly emphasized on modeling the exterior shapes, textures and skin properties of faces, ignoring the inherent correlation between inner skeletal structures and appearance. In this paper, we present SCULPTOR, 3D face creations with Skeleton Consistency Using a Learned Parametric facial generaTOR, aiming to facilitate the easy creation of both anatomically correct and visually convincing face models via a hybrid parametric-physical representation. At the core of SCULPTOR is LUCY, the first large-scale shape-skeleton face dataset in collaboration with plastic surgeons. Named after the fossils of one of the oldest known human ancestors, our LUCY dataset contains high-quality Computed Tomography (CT) scans of the complete human head before and after orthognathic surgeries, which are critical for evaluating surgery results. LUCY consists of 144 scans of 72 subjects (31 male and 41 female), where each subject has two CT scans taken pre- and post-orthognathic operations. Based on our LUCY dataset, we learned a novel skeleton consistent parametric facial generator, SCULPTOR, which can create unique and nuanced facial features that help define a character and at the same time maintain physiological soundness. Our SCULPTOR jointly models the skull, face geometry and face appearance under a unified data-driven framework by separating the depiction of a 3D face into shape blend shape, pose blend shape and facial expression blend shape. SCULPTOR preserves both anatomic correctness and visual realism in facial generation tasks compared with existing methods. Finally, we showcase the robustness and effectiveness of SCULPTOR in various fancy applications unseen before, like archaeological skeletal facial completion, bone-aware character fusion, skull inference from images, face generation with lipo-Level change and facial animations, etc.  © 2022 Owner/Author.,Recently, deep learning-based methods have shown significant results in 3D face reconstruction. By harnessing the power of convolutional neural networks, significant progress has been made in recovering 3D face shapes from single images using the 3D Morphable Model approach. However, training neural networks typically requires a large amount of data, while face images with ground-truth 3D face shapes are scarce. In this paper, we propose an unsupervised learning framework for accurate 3D face reconstruction from a single image. Our key idea is to process the images generated by a differentiable renderer and leverage the advantages of Generative Adversarial Networks to train a powerful neural renderer that produces highly realistic face images resembling the input image. We then modify traditional fitting methods to exploit the advantages of the neural renderer in finding optimal face parameters for improved 3D face reconstruction. The experimental results demonstrate that our method is capable of generating more accurate 3D face reconstruction results. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
235,234,16,234_neonatal_newborns_infant_pregnancies,"neonatal,newborns,infant,pregnancies,predict,pregnancy,predictive,predictors,pregnant,insemination","Background: Birth weight is a significant determinant of the likelihood of survival of an infant. Babies born at low birth weight are 25 times more likely to die than at normal birth weight. Low birth weight (LBW) affects one out of every seven newborns, accounting for about 14.6 percent of the babies born worldwide. Moreover, the prevalence of LBW varies substantially by region, with 7.2 per cent in the developed regions and 13.7 per cent in Africa, respectively. Ethiopia has a large burden of LBW, around half of Africa. These newborns were more likely to die within the first month of birth or to have long-term implications. These are stunted growth, low IQ, overweight or obesity, developing heart disease, diabetes, and early death. Therefore, the ability to predict the LBW is the better preventive measure and indicator of infant health risks. Method: This study implemented predictive LBW models based on the data obtained from the Ethiopia Demographic and Health Survey 2016. This study was employed to compare and identify the best-suited classifier for predictive classification among Logistic Regression, Decision Tree, Naive Bayes, K-Nearest Neighbor, Random Forest (RF), Support Vector Machine, Gradient Boosting, and Extreme Gradient Boosting. Results: Data preprocessing is conducted, including data cleaning. The Normal and LBW are the binary target category in this study. The study reveals that RF was the best classifier and predicts LBW with 91.60 percent accuracy, 91.60 percent Recall, 96.80 percent ROC-AUC, 91.60 percent F1 Score, 1.05 percent Hamming loss, and 81.86 percent Jaccard score. Conclusion: The RF predicted the occurrence of LBW more accurately and effectively than other classifiers in Ethiopia Demographic Health Survey. Gender of the child, marriage to birth interval, mother’s occupation and mother’s age were Ethiopia’s top four critical predictors of low birth weight in Ethiopia. © 2022, The Author(s).,Background: Low birthweight (LBW) is a leading cause of neonatal mortality in the United States and a major causative factor of adverse health effects in newborns. Identifying high-risk patients early in prenatal care is crucial to preventing adverse outcomes. Previous studies have proposed various machine learning (ML) models for LBW prediction task, but they were limited by small and imbalanced data sets. Some authors attempted to address this through different data rebalancing methods. However, most of their reported performances did not reflect the models' actual performance in real-life scenarios. To date, few studies have successfully benchmarked the performance of ML models in maternal health; thus, it is critical to establish benchmarks to advance ML use to subsequently improve birth outcomes. Objective: This study aimed to establish several key benchmarking ML models to predict LBW and systematically apply different rebalancing optimization methods to a large-scale and extremely imbalanced all-payer hospital record data set that connects mother and baby data at a state level in the United States. We also performed feature importance analysis to identify the most contributing features in the LBW classification task, which can aid in targeted intervention. Methods: Our large data set consisted of 266,687 birth records across 6 years, and 8.63% (n=23,019) of records were labeled as LBW. To set up benchmarking ML models to predict LBW, we applied 7 classic ML models (ie, logistic regression, naive Bayes, random forest, extreme gradient boosting, adaptive boosting, multilayer perceptron, and sequential artificial neural network) while using 4 different data rebalancing methods: random undersampling, random oversampling, synthetic minority oversampling technique, and weight rebalancing. Owing to ethical considerations, in addition to ML evaluation metrics, we primarily used recall to evaluate model performance, indicating the number of correctly predicted LBW cases out of all actual LBW cases, as false negative health care outcomes could be fatal. We further analyzed feature importance to explore the degree to which each feature contributed to ML model prediction among our best-performing models. Results: We found that extreme gradient boosting achieved the highest recall score-0.70-using the weight rebalancing method. Our results showed that various data rebalancing methods improved the prediction performance of the LBW group substantially. From the feature importance analysis, maternal race, age, payment source, sum of predelivery emergency department and inpatient hospitalizations, predelivery disease profile, and different social vulnerability index components were important risk factors associated with LBW. Conclusions: Our findings establish useful ML benchmarks to improve birth outcomes in the maternal health domain. They are informative to identify the minority class (ie, LBW) based on an extremely imbalanced data set, which may guide the development of personalized LBW early prevention, clinical interventions, and statewide maternal and infant health policy changes. © Yang Ren, Dezhi Wu, Yan Tong, Ana López-DeFede, Sarah Gareau. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 31.05.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.,Context • Women with hypertensive disorders of pregnancy often need to have labor induced. The use of cervical double balloons to trigger cervical ripening, combined with the use of oxytocin, has been widely used for labor induction in recent years. In the evaluation of factors affecting the success rate of labor induction, previous predictive models have been limited to use of linear correlation, which simplifies the complex relationship between a large number of variables. Objective • The study intended to retrospectively analyze the factors influencing the outcomes of cervical dilatation using a cervical double balloon in the induction of labor for pregnant women with hypertensive disorders and to establish a predictive model based on the random forest (RF) method that is able to manage multifeatured data, provide fast training speeds, offer high predictive accuracy, and analyze the impact of various features. Design • The research team performed a retrospective analysis of data. Setting • The study took place at the Fujian Provincial Maternity and Child Health Hospital at the Affiliated Hospital of Fujian Medical University in Fuzhou, China. Participants • Participants were 201 women in late pregnancy who came to the hospital for delivery between January 2014 and December 2018, who had hypertensive disorders of pregnancy, and for whom doctors induced labor using a cervical double balloon. Intervention • The research team divided participants into an intervention group, who had a successful induced labor, and a control group, who had a failed induced labor. Outcome Measures • The research team analyzed the medical records of the groups using the RF method of ensemble learning and the multifactor logical regression method. The team used the receiver operating characteristic curve (ROC) to evaluate the working efficiency of the two models. The RF prediction model examined the factors influencing induced labor: the pregnancy method, the ultrasound EFW, the amniotic fluid index (AFI), the serum LDH level of the pregnant women, the placental volume, the cervical Bishop score before use of the balloon, the duration of the balloon’s use, and the hours of use of oxytocin after balloon removal. Results • The success rate for induced labor with use of a cervical double balloon for women with hypertensive disorders during pregnancy was 77.18%. The incidence of postpartum hemorrhage was 4.7% and of fetal distress was 12.7%. The most important 10 features were: (1) hours of oxytocin use, (2) fetal weight, (3) placental volume, (4) AFI, (5) LDH, (6) BMI, (7) the Bishop score before use of the COOK balloon, (8) duration of the balloon’s use, (9) pregnancy method, and (10) weight gain during pregnancy. The area under the ROC curve for successful induction for the RF model was 0.983. The multivariate logistic regression model based on RF showed that multiple births, high cervical Bishop scores before labor induction, less time for use of oxytocin after balloon removal, and a small placental volume were independent risk factors, with the area under the ROC curve for successful induction being 0.918. Conclusions • Medical practitioners can use the cervical double balloon effectively for the induction of labor for women with hypertensive disorders during the third trimester of pregnancy, and the prediction model for induction of labor based on RF had a good working efficiency. © 2023, InnoVision Communications. All rights reserved."
236,235,16,235_magnetic_magnetomechanical_magnetics_magnet,"magnetic,magnetomechanical,magnetics,magnet,neural,coil,motors,motor,modeling,fem","A novel multiobjective optimization model is presented for the interior permanent magnet synchronous motors (IPMSMs). First of all, in the model initialization stage, appropriate design variables are determined for the actual topology. Also, with reference to the engineering needs, the key electromagnetic characteristics relating to the no-load and on-load magnetic fields are selected as the main optimization objectives in order to achieve global performance improvement. Next, in the model prediction stage, two performance prediction models are proposed, studied, and implemented in parallel. One is an analytical model (AM) based on the improved subdomain approach and the magnetic equivalent circuit. It is utilized to predict the no-load electromagnetic characteristics of IPMSM. The complex structure and core nonlinearity of IPMSM are also reasonably accounted for. The other is a surrogate model (SM) based on the intelligent machine learning language (support vector regression). It is utilized to predict the on-load electromagnetic characteristics of IPMSM. The reliance on finite-element analysis is further also minimized. The proposed AM and SM have commendable behavior in terms of analysis speed, prediction accuracy, and storage consumption. Meanwhile, AM screens credible samples as well as provides robust support for the construction of SM. All of these signs build a solid foundation for a substantial boost in optimization efficiency. Afterward, in the model optimization stage, the advanced nondominated sorting genetic algorithm III is investigated to complete the final multiobjective optimization. Ultimately, a large number of calculations, simulations, and experiments have highlighted the effectiveness, rationality, and engineering practicality of this research. IEEE,A saturated iron-core type superconducting fault current limiter (SI-SFCL) can effectively restrict the magnitude of the fault current and alleviate the strain on circuit breakers in DC power systems. Design of a superconducting coil (SC), which is one of the key tasks in the SI-SFCL design, requires guaranteeing a sufficient magnetic field, ensuring optimization of the shape and size, minimizing the wire cost, and satisfying the safety and stability of operation. Generally, finite element method (FEM) is used to calculate and evaluate the operating characteristics of SCs, from which it is possible to determine their optimal design parameters. When the coil is complex and large, the simulation time may range from hours to days, and if input parameters change even slightly, the simulations have to be redone from scratch. Recent advances in deep learning represent the ability to be effective for modeling and optimizing complex problems from training data or in real-Time. In this paper, we presented a combination of the FEM simulation and deep Q-network (DQN) algorithm to optimize the SC design of a lab-scale SI-SFCL for a DC power system. The detailed design process and options for the SC of SI-SFCL were proposed. In order to analyze the characteristics related to the electromagnetic properties and operational features of the SC, a 3D FEM model was developed. Then, a DQN model was constructed and integrated with the FEM simulation for training and optimizing the design parameters of the SC in real-Time. The obtained results of this study have the potential to effectively optimize the design parameters of large-scale SI-SFCL development for high-voltage DC power systems.  © 2023 Kim et al.,Purpose: Under the condition of small data set, a prediction model of motor magnetic field is established based on deep learning method. This paper aims to complete the magnetic field prediction quickly and accurately. Design/methodology/approach: An improved Linknet model is proposed to predict the motor magnetic field. This is a digital twin technology, which can predict the function values of other points according to the function values of typical sampling points. The results of magnetic field distribution are represented by color images. By predicting the pixels of the image, the corresponding magnetic field distribution is obtained. The model not only considers the correlation between pixels but also retains the spatial information in the original input image and can well learn the mapping relationship between motor structure and magnetic field. Findings: The model can speed up the calculation while ensuring the accuracy and has obvious advantages in large-scale calculation and real-time simulation. Originality/value: Under the condition of small data set, the model can well learn the mapping relationship between motor structure and magnetic field, so as to complete the magnetic field prediction quickly and accurately. In the future, according to the characteristics of magnetic field distribution, it will lay a foundation for solving the problems of rapid optimization, real-time simulation and physical field control of electrical equipment. © 2022, Emerald Publishing Limited."
237,236,16,236_recognition_retrieval_descriptor_images,"recognition,retrieval,descriptor,images,features,cnn,similarity,sketches,image,recognize","Introduction of the World Wide Web (WWW) and progressions in the field of multimedia and computer technology have enlarged the amount of image collections and databases including art galleries, digital libraries, and medical imageries that extend to millions of images. From these large-scale datasets, the retrieval procedure of images is carried out by classical approaches like chi square distance, colour histogram and text-based image retrieval, which however requires more time for getting the desired images. Thus, there is a need of proposing an effective retrieval system for images by handling these large numbers of images. For this reason, a new Content-Based Image Retrieval (CBIR) system has been implemented in this paper for retrieving the desired images of users from the collected images. Initially, different kinds of images like medical images, texture images, and environmental images etc., are collected from the standard image database. The deep feature extraction using Visual Geometry Group network (VGG-16), Inception v3, and Xception are used to get the features of all the images in the database. The parameters in these three deep learning techniques are tuned with an enhanced optimization algorithm of Modified Bypass-Rider Optimization Algorithm (MB-ROA). The features from VGG16, Inception and Xception are used for optimal weighted fused feature selection using enhanced ROA under training phase. During testing with query images, it undergoes deep feature extraction with the same set of techniques and, then fed into the optimal weighted fused feature selection to get the features of query image. Then, the multi-similarity function considering the cosine, Euclidean and Jaccard similarity are checked between the optimal features of database images and query image. The database images with the minimum similarity with the query image are retrieved from the database. The overall precision and recall to retrieve the top ten images related to the provided query image from the Corel dataset are 77.25% and 76.89%, respectively, while 70.15% and 70.26%, respectively from the VisTex dataset. The experimental findings show that the recommended model is effective at retrieving images from the database © 2022, International Journal of Intelligent Engineering and Systems.All Rights Reserved.,Query by Image Content (QBIC), subsequently known as Content-Based Image Retrieval (CBIR), offers an advantageous solution in a variety of applications, including medical, meteorological, search by image, and other applications. Such CBIR systems primarily use similarity matching algorithms to compare image content to get matched images from datasets. They essentially measure the spatial distance between extracted visual features from a query image and its similar versions in the dataset. One of the most challenging query retrieval problems is Facial Sketched-Real Image Retrieval (FSRIR), which is based on content similarity matching. These facial retrieval systems are employed in a variety of contexts, including criminal justice. The difficulties of retrieving such sorts come from the composition of the human face and its distinctive parts. In addition, the comparison between these types of images is made within two different domains. Besides, to our knowledge, there is a few large-scale facial datasets that can be used to assess the performance of the retrieval systems. The success of the retrieval process is governed by the method used to estimate similarity and the efficient representation of compared images. However, by effectively representing visual features, the main challenge-posing component of such systems might be resolved. Hence, this paper has several contributions that fill the research gap in content-based similarity matching and retrieval. The first contribution is extending the Chinese University Face Sketch (CUFS) dataset by including augmented images, introducing to the community a novel dataset named Extended Sketched-Real Image Retrieval (ESRIR). The CUFS dataset has been extended from 100 images to include 53,000 facial sketches and 53,000 real facial images. The paper second contribution is presenting three new systems for sketched-real image retrieval based on convolutional autoencoder, InfoGAN, and Vision Transformer (ViT) unsupervised models for large datasets. Furthermore, to meet the subjective demands of the users due to the prevalence of multiple query formats, the third contribution of the paper is to train and assess the performance of the proposed models on two additional facial datasets of different image types. Recently, the majority of people have preferred searching for brand logo images, but it may be tricky to separate certain brand logo features their alternatives and even from other features in an image. Thus, the fourth contribution is to compare logo image retrieval performance based on visual features derived from each of the three suggested retrieval systems. The paper also presents cloud-based energy and computational complexity saving approaches on large-scale datasets. Due to the ubiquity of touchscreen devices, users often make drawings based on their fantasies for certain object image searches. Thus, the proposed models are tested and assessed on a tough dataset of doodle-scratched human artworks. They are also studied on a multi-category dataset to cover practically all possible image types and situations. The results are compared with those of the most recent algorithms found in the literature. The results show that the proposed systems outperform the recent counterparts.  © 2013 IEEE.,Since the onset of civilization, sketches have been used to portray our visual world, and they continue to do so in many different disciplines today. As in specific government agencies, establishing similarities between sketches is a crucial aspect of gathering forensic evidence in crimes, in addition to satisfying the user’s subjective requirements in searching and browsing for specific sorts of images (i.e., clip art images), especially with the proliferation of smartphones with touchscreens. With such a kind of search, quickly and effectively drawing and retrieving sketches from databases can occasionally be challenging, when using keywords or categories. Drawing some simple forms and searching for the image in that way could be simpler in some situations than attempting to put the vision into words, which is not always possible. Modern techniques, such as Content-Based Image Retrieval (CBIR), may offer a more useful solution. The key engine of such techniques that poses various challenges might be dealt with using effective visual feature representation. Object edge feature detectors are commonly used to extract features from different image sorts. However, they are inconvenient as they consume time due to their complexity in computation. In addition, they are complicated to implement with real-time responses. Therefore, assessing and identifying alternative solutions from the vast array of methods is essential. Scale Invariant Feature Transform (SIFT) is a typical solution that has been used by most prevalent research studies. Even for learning-based methods, SIFT is frequently used for comparison and assessment. However, SIFT has several downsides. Hence, this research is directed to the utilization of handcrafted-feature-based Oriented FAST and Rotated BRIEF (ORB) to capture visual features of sketched images to overcome SIFT limitations on small datasets. However, handcrafted-feature-based algorithms are generally unsuitable for large-scale sets of images. Efficient sketched image retrieval is achieved based on content and separation of the features of the black line drawings from the background into precisely-defined variables. Each variable is encoded as a distinct dimension in this disentangled representation. For representation of sketched images, this paper presents a Sketch-Based Image Retrieval (SBIR) system, which uses the information-maximizing GAN (InfoGAN) model. The establishment of such a retrieval system is based on features acquired by the unsupervised learning InfoGAN model to satisfy users’ expectations for large-scale datasets. The challenges with the matching and retrieval systems of such kinds of images develop when drawing clarity declines. Finally, the ORB-based matching system is introduced and compared to the SIFT-based system. Additionally, the InfoGAN-based system is compared with state-of-the-art solutions, including SIFT, ORB, and Convolutional Neural Network (CNN). © 2022 by the authors."
238,237,16,237_imputation_datasets_imputing_imputed,"imputation,datasets,imputing,imputed,data,incomplete,missing,impute,autoencoder,generative","A missing value indicates that a particular attribute of an instance of a learning problem is not recorded. They are very common in many real-life datasets. In spite of this, however, most machine learning methods cannot handle missing values. Thus, they should be imputed before training. Gaussian Processes (GPs) are non-parametric models with accurate uncertainty estimates that combined with sparse approximations and stochastic variational inference scale to large data sets. Sparse GPs (SGPs) can be used to get a predictive distribution for missing values. We present a hierarchical composition of sparse GPs that is used to predict the missing values at each dimension using the observed values from the other dimensions. Importantly, we consider that the input attributes to each sparse GP used for prediction may also have missing values. The missing values in those input attributes are replaced by the predictions of the previous sparse GPs in the hierarchy. We call our approach missing GP (MGP). MGP can impute all observed missing values. It outputs a predictive distribution for each missing value that is then used in the imputation of other missing values. We evaluate MGP on one private clinical data set and on four UCI datasets with a different percentage of missing values. Furthermore, we compare the performance of MGP with other state-of-the-art methods for imputing missing values, including variants based on sparse GPs and deep GPs. Our results show that the performance of MGP is significantly better. © 2023 The Author(s),The presence of missing values is a pervasive and unavoidable phenomenon in sensor data. Despite numerous efforts from researchers to address this issue through imputation techniques, particularly in deep learning models, the unique data distributions and periods inherent in real-world sensor data are often neglected. This paper presents a novel, multistage deep learning-based imputation framework with adaptability to missing value imputation. The framework incorporates a mixture measurement index that accounts for both low- and higher-order statistical aspects of data distribution and a more adaptive evaluation metric, which improves upon traditional mean squared error. Additionally, a multistage imputation strategy and dynamic data length adjustment are integrated into the imputation process to account for variations in data periods. Empirical results on diverse sensor data demonstrate the superiority of the proposed framework, particularly in addressing large segment imputation issues, as evidenced by improved imputation performance. The implementation and experimental results have been made publicly available on GitHub. © 2023,Noticeable growth in the use of intelligent devices has resulted in the generation of vast amounts of data from sensor devices. When dealing with large amounts of data, it is common to observe databases with large amounts of missing values. This is a challenge for data miners because various methods for data analysis only work well on complete databases. A traditional approach to handling missing data is to discard instances of missing values and only use complete cases for analysis. However, research has shown that this approach is not practical especially when large amounts of data are missing. This led to an increased need to develop strategies for replacing missing values with plausible values through imputation. This study presents an imputation strategy called med.BFMVI for recovering missing values before training downstream classification models. Experiments simulated missingness from 10% to 40% using MCAR and MAR mechanisms and the performance of the proposed technique was measured against state-of-the-art techniques. Overall, the proposed algorithm recorded the best imputation accuracy as opposed to benchmark techniques and showed significant improvements on downstream learning.  © 2013 IEEE."
239,238,16,238_odometry_pose_camera_slam,"odometry,pose,camera,slam,3d,landmarks,tracking,3d3d,robotics,scenes","Camera pose estimation has long relied on geometry-based approaches and sparse 2D-3D keypoint correspondences. With the advent of deep learning methods, the estimation of camera pose parameters, i.e., the six parameters that describe position and rotation denoted by 6 Degrees of Freedom (6-DoF), has decreased from tens of meters to a few centimeters in median error for indoor applications. For outdoor applications, errors can be quite large and highly dependent on the variations in occlusion, contrast, brightness, repetitive structures, or blur introduced by camera motion. To address these limitations, we introduce, B-Pose, a Bayesian Convolutional deep network capable of not only automatically estimating the camera's pose parameters from a single RGB image but also provides a measure of uncertainty in the parameter estimation. Reported experiments on outdoor and indoor datasets demonstrate that B-Pose outperforms SOTA techniques and generalizes better to unseen RGB images. A strong correlation is shown between the prediction error and the model's uncertainty, indicating that the prediction is almost always incorrect whenever the model's uncertainty is high. © 2016 IEEE.,— Estimating monocular depth and ego-motion via unsupervised learning has emerged as a promising approach in autonomous driving, mobile robots, and augmented reality (AR)/VR applications. It avoids intensive efforts to collect a large amount of ground truth and further improves the scene construction density and long-term tracking accuracy in simultaneous localization and mapping (SLAM) systems. However, existing approaches are susceptible to illumination variations and blurry pictures due to fast movements in real-world driving scenarios. In this article, we propose a novel unsupervised learning framework to fuse the complementary strength of visual and inertial measurements for monocular depth estimation. It learns both forward and backward inertial sequences at multiple subspaces to produce environment-independent and scale-consistent motion features and selectively weights inertial and visual modalities to adapt to various scenes and motion states. In addition, we explore a novel virtual stereo model to adopt such depth estimates in the monocular SLAM system, thus improving the system efficiency and accuracy. Extensive experiments on the KITTI, EuRoC, and TUM datasets have shown our effectiveness in terms of monocular depth estimation, SLAM initialization efficiency, and pose estimation accuracy compared with the state-of-the-art. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.,Efficient localization plays a significant role in mobile autonomous robots' navigation systems. Traditional visual simultaneous localization systems based on point feature matching suffer from two shortcomings. First one is that the method of tracking features is not robust for environments with frequent changes in brightness. Another one is the large of consecutive visual keyframes can consume expensive computational and storage resources in complex environments. To solve these problems, an end-to-end real-time six degrees of freedom object pose estimation algorithm is proposed to solve the robust and efficient challenges through a deep learning model. First, preprocessing operations such as cropping, averaging, and timestamp alignment are performed on datasets to reduce computational cost and time. Second, the processed dataset is fed into our neural network model to extract the most effective features for matching. Finally, the robot's current 3D translation and 4D angle information are predicted and output to achieve an end-to-end localization system. A broad range of experiments are performed on both indoor and outdoor datasets. The experimental results demonstrate that the translation and orientation accuracy in outdoor scenes improved by 32.9% and 31.4%, respectively. The average improvement of localization accuracy in indoor scenes is 38.4%, and the angle improvement is 13.1%. Moreover, the effectiveness of predicting the global motion trajectories of sequential images algorithm has been verified and is superior to other convolutional neural network methods.  © 2023 Niansheng Chen et al."
240,239,16,239_topologyoptimized_optimization_topology_mesh,"topologyoptimized,optimization,topology,mesh,topological,neural,optimized,deep,mapnet,shapes","Classical optimization methods require finite element analysis in iterations, which increase the computing time and decrease the algorithmic efficiency. The deep learning model can potentially realize real-time topology optimization design, but it normally requires large training set. This paper presents a real-time topology optimization algorithm based on the Moving Morphable Component (MMC) method using a Convolutional Neural Network (CNN). The optimization algorithm uses a new data pre-processing method, which can preserve the numerical characteristics and smoothness of the structure boundary, hence it can help CNN to capture data features with a limited sample set. The topology optimization boundary information of the optimized result is used as the sample set label to avoid the components dislocation phenomenon. The new algorithm effectiveness has been verified with several examples. The trained model can significantly improve the optimization efficiency of the MMC method and offer accurate results with a clear structure boundary. © 2023 Elsevier Ltd,In the context of mechanical engineering design, the field of machine learning accelerated topology optimization is dominated by image-based models trained in a supervised manner on datasets with a limited diversity of boundary conditions. State-of-the-art methods show poor generalization capabilities and are strongly coupled to finite element mesh resolution, hindering scalability. In this paper, we leverage the explicit topology parameterization of the moving morphable components (MMC) framework to train a deep learning model that directly generates geometric design variables using a model architecture that is independent of the finite element mesh used for structural analysis. The developed model is trained on a large dataset of boundary conditions. Despite achieving state-of-the-art regression loss, evaluations reveal that direct-design approaches generate topologies with poor mechanical performance. Specifically, the model-generated topologies have, on average, a stiffness 11.48% lower than conventional MMC designs, as evidenced by in-distribution and out-of-distribution test samples. We demonstrate that this is due to the incompatibility between the regression loss function typically used in literature and the topology optimization objective of compliance minimization. To address this issue, we propose a novel acceleration approach that leverages the trained model to generate improved initial designs for conventional optimization. Specifically, the deep learning model is used to generate an initial design, which is then refined by conventional optimization to arrive at a final, optimal design. This approach shows a computation time-saving of 36.84% without sacrificing the final mechanical performance of the optimal topology compared to conventional optimization starting from a uniform initial layout. © 2023 Elsevier Ltd,Topology optimization design provides innovative structures with excellent thermal, mechanical and acoustic performance for modern engineering. Moving Morphable Component (MMC), as an emerging explicit topology optimization method, can effectively avoid many optimization problems such as the checkerboard phenomenon, however, its optimization iteration process still consumes considerable time, which makes real-time structural topology optimization impossible. Therefore, a lightweight and high-efficiency convolutional neural network, the improved convolutional block attention U-Net (Cba-U-Net) model, is proposed for topology-optimized configuration prediction, which avoids its own tedious iterative computation process and acquires the topology configuration in real-time. It is demonstrated that the proposed network not only obtains accurate topology-optimized configurations in negligible time but also has an accuracy rate of 91.42% compared to other deep learning models. The improved Cba-U-Net model is suitable not only for Moving Morphable Components but also for other optimization algorithms, such as Solid Isotropic Material with Penalization (SIMP) and Evolutionary Structural Optimization Method (ESO). By combining deep learning with topological optimization algorithms, this form of optimization is highly generalizable for practical large-scale projects. © 2022 Elsevier Ltd"
241,240,16,240_vulnerabilities_vulnerability_vulnerable_vulnerabilityindicative,"vulnerabilities,vulnerability,vulnerable,vulnerabilityindicative,syntaxbased,code,security,lstm,snippet,obfuscations","Machine learning-based fine-grained vulnerability detection is an important technique for locating vulnerable statements, which assists engineers in efficiently analyzing and fixing the vulnerabilities. However, due to insufficient code representations, code embeddings, and neural network design, current methods suffer low vulnerability localization performance. In this paper, we propose to address these shortcomings by presenting SlicedLocator, a novel fine-grained code vulnerability detection model that is trained in a dual-grained manner and can predict both program-level and statement-level vulnerabilities. We design the sliced dependence graph, a new code representation that not only preserves rich interprocedural relations but also eliminates vulnerability-irrelevant statements. We create attention-based code embedding networks that are trained with the entire model to extract vulnerability-aware code features. In addition, we present a new LSTM-GNN model as a fusion of semantic modeling and structural modeling. Experiment results on a large-scale C/C++ vulnerability dataset reveal that SlicedLocator outperforms state-of-the-art machine learning-based vulnerability detectors, especially in terms of localization metrics. © 2023 Elsevier Ltd,The explosive growth of vulnerabilities poses a significant threat to the security of software systems. While various deep-learning-based vulnerability detection methods have emerged, they primarily rely on semantic features extracted from a single code representation structure, which limits their ability to detect vulnerabilities hidden deep within the code. To address this limitation, we propose S (Formula presented.) FVD, short for Sequence and Structure Fusion-based Vulnerability Detector, which fuses vulnerability-indicative features learned from the multiple views of the code for more accurate vulnerability detection. Specifically, S (Formula presented.) FVD employs either well-matched or carefully extended neural network models to extract vulnerability-indicative semantic features from the token sequence, attributed control flow graph (ACFG) and abstract syntax tree (AST) representations of a function, respectively. These features capture different perspectives of the code, which are then fused to enable S (Formula presented.) FVD to accurately detect vulnerabilities that are well-hidden within a function. The experiments conducted on two large vulnerability datasets demonstrated the superior performance of S (Formula presented.) FVD against state-of-the-art approaches, with its accuracy and F1 scores reaching 98.07% and 98.14% respectively in detecting the presence of vulnerabilities, and 97.93% and 97.94%, respectively, in pinpointing specific vulnerability types. Furthermore, with regard to the real-world dataset D2A, S (Formula presented.) FVD achieved average performance gains of 6.86% and 14.84% in terms of accuracy and F1 metrics, respectively, over the state-of-the-art baselines. This ablation study also confirms the superiority of fusing the semantics implied in multiple distinct code views to further enhance vulnerability detection performance. © 2023 by the authors.,Vulnerability detection is essential to protect software systems. Various approaches based on deep learning have been proposed to learn the pattern of vulnerabilities and identify them. Although these approaches have shown vast potential in this task, they still suffer from the following issues: (1) It is difficult for them to distinguish vulnerability-related information from a large amount of irrelevant information, which hinders their effectiveness in capturing vulnerability features. (2) They are less effective in handling long code because many neural models would limit the input length, which hinders their ability to represent the long vulnerable code snippets. To mitigate these two issues, in this work, we proposed to decompose the syntax-based Control Flow Graph (CFG) of the code snippet into multiple execution paths to detect the vulnerability. Specifically, given a code snippet, we first build its CFG based on its Abstract Syntax Tree (AST), refer to such CFG as syntax-based CFG, and decompose the CFG into multiple paths from an entry node to its exit node. Next, we adopt a pre-trained code model and a convolutional neural network to learn the path representations with intra- and inter-path attention. The feature vectors of the paths are combined as the representation of the code snippet and fed into the classifier to detect the vulnerability. Decomposing the code snippet into multiple paths can filter out some redundant information unrelated to the vulnerability and help the model focus on the vulnerability features. Besides, since the decomposed paths are usually shorter than the code snippet, the information located in the tail of the long code is more likely to be processed and learned. To evaluate the effectiveness of our model, we build a dataset with over 231 k code snippets, in which there are 24 k vulnerabilities. Experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines by at least 22.30%, 42.92%, and 32.58% in terms of Precision, Recall, and F1-Score, respectively. Our further analysis investigates the reason for the proposed approach's superiority.  © 1976-2012 IEEE."
242,241,16,241_music_musical_melodies_melody,"music,musical,melodies,melody,songs,polyphonic,melodic,piano,melodydiffusion,audio","The last two decades have seen the emergence of a brand-new kind of music known as digital brain stimulant, also known as instrumental music or music without lyrics, which mostly comprises entrainment beats. While listening to it has the same ability to affect the brain as taking medication, it also has the risk of having a negative impact or encouraging unwanted behavior. This sparked the interest of a large number of studies in the psychological and physiological effects of music's brainwave entrainment beats on listeners. These studies started to categorize and examine how musical beats affected brainwave entrainment by looking at electroencephalogram (EEG) signals. Although this categorization represents a step forward for the early research efforts, it is constrained by the difficulty of having each musical track and conducting EEG tests on humans exposed to distortion due to noise in order to determine its influence. The work proposed in this article continues to explore this topic but in a novel, simple, accurate, and reliable categorization procedure based on the music signal elements themselves rather than dependent on EEG. VGGish and YAMNET based transfer deep learning models, are tuned to handle a straightforward, accurate real-time detector for the existence of the music beats inside music files with accuracy of 98.5 and 98.4, respectively. Despite the fact that they yield results that are equivalent, the YAMNET model is more suited for use with mobile devices due to its low power consumption and low latency. The article also proposes modified version of VGGish and YAMNET binary classifying models called BW-VGGish and BW-YAMNET respectively. The modification was to turn the binary classification into multi-classification. These multi-classifiers handle the classification of the influence of music beats (five different brain waves) on human brainwave entrainment with average accuracy of 94.5% and 94.5%, respectively. Since there was a lack of datasets addressing this kind of music, two datasets, the Brainwave Entrainment Beats (BWEB) dataset and the Brainwave Music Manipulation (BWMM) dataset, were generated for classification training and testing. The re-testing on a sample of music files that have their impact on brain waves (with their EEG) in an earlier study is done to strengthen the validity of the proposed work and to overcome the potential limitation of utilizing a music dataset that is not proved with its EEG. The success of the suggested models was demonstrated. © 2023 A. Sadek et al.,With the development of artificial intelligence and deep learning, a large number of music generation methods have been proposed. Recently, Transformer has been widely used in music generation. However, the structural complexity of music puts forward higher requirements for music generation. In this paper, we propose a new automatic music generation network which consists of a Recursive Skip Connection with Layer Normalization (RSCLN) model, a Transformer-XL model and a multi-head attention mechanism. Our method not only alleviates the gradient vanishing problem in the model training, but also increases the ability of the model to capture the correlation of music information before and after, so as to generate music works closer to the original music style. Effectiveness of the RSCLN_Transformer-XL music automatic generation method is verified through music similarity evaluation experiments using music structure similarity and listening test. The experimental results show that the RSCLN_Transformer-XL music automatic generation model can generate better music than the Transformer-XL model. © 2024, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,This paper utilizes deep learning algorithms to informally integrate modern vocal music teaching with traditional music culture and extracts audio time-domain features and frequency-domain features through neural network self-learning. Secondly, a large number of music tracks are decomposed into music patterns, which constitute a music pattern library, and a music training model is generated through the automatic music audio synthesis algorithm based on a recurrent neural network, and the GRU model is used for music training and model prediction. The strategy of integrating artificial intelligence and modern vocal music teaching mode through traditional music culture in modern vocal music teaching is informatized, and a controlled experiment is carried out with H Music Academy as an example. The results show that the average degree of completion of the learning objectives of the students in the two experimental classes is 89.32 and 87.16, respectively, which is 14.15 and 11.99 higher than the average degree of completion of the control class. This study demonstrates that the teaching mode of traditional music culture integration in modern vocal music teaching can enhance the student’s ability of vocal music skills and practically improve the students’ artistic literacy, which can improve the degree of completion of the student’s learning objectives and in turn, improve the overall level of vocal music teaching. © 2023 Ni Zhang, published by Sciendo."
243,242,16,242_wellbeing_happiness_depression_depressive,"wellbeing,happiness,depression,depressive,health,comorbidity,stressors,anxietydepression,comorbid,anxiety","This paper uses a large scale and nationally representative dataset, Chinese General Social Survey, to empirically examine the role of physical activity in reducing the negative effects of depression among people with mental disorders. Empirical results demonstrate that physical exercise could help to alleviate depression's adverse consequences on work and life for depressed individuals. The impact mechanism is that physical activity may decrease the severity of depression, enhance life satisfaction, improve mood, and make people have a better sense of purpose and meaning in life. Therefore, from the perspective of multidimensional subjective wellbeing, evaluative wellbeing, experienced wellbeing and eudaimonic wellbeing all play mediating roles in the reduction of depression's adverse effects. Heterogeneity analysis shows that there are no significant gender differences in the health benefits of physical exercise, but its impact tends to be more prominent for depressed individuals who are younger and higher educated, with better health status, and live in urban areas. It is also found that socioeconomic status may play an important moderating role. The health benefits of physical activity seem to be greater for depressed people who have lower income, work in the secondary labor market, and have lower levels of social capital and assets. In addition, the instrumental variable approach is used to identify the causal impact of physical activity, which further proves a significant effect of it based on tackling the endogeneity problem. Meanwhile, this paper uses different explanatory and explained variables, different statistical models, as well as machine learning and placebo techniques to conduct robustness tests, all of which lend credence to above findings. Copyright © 2022 Li, Ning, Xia and Liu.,This paper uses a nationally representative and large-scale dataset from China to empirically examine the relationship between exercise participation and happiness. To address the problem of reverse causality between the two factors, the instrumental variable (IV) approach is used to deal with endogeneity to some extent. It is demonstrated that higher frequencies of exercise participation are positively related to happiness. Findings also demonstrate that physical exercise could significantly decrease depressive disorders, improves self-rated health conditions and reduces the frequency of health problems affecting people's work and life. At the same time, all of above health factors significantly influence subjective wellbeing. When these health variables are included in regressions, the correlation between exercise participation and happiness declines. This confirms that physical activity helps to improve happiness by enhancing mental and overall health conditions. In addition, results show that physical activities are more prominently related to happiness for male, older and unmarried individuals and those living in rural areas, lacking social security and with higher levels of depression as well as lower socioeconomic status. Furthermore, a series of robustness checks are carried out and exercise participation's positive role in improving happiness is further confirmed using different happiness measures and instrumental variables, various IV models, as well as penalized machine learning methods and placebo tests. With the increasing emphasis of improving happiness as an important goal in the global public health policy, findings of this paper have important policy implications for enhancing subjective wellbeing. Copyright © 2023 Li, Ning and Xia.,Background: Comorbidity of psychiatric disorders such as depression and anxiety is very common among children and adolescents. Few studies have examined how comorbid anxiety and depression are associated with health risk behaviors (HRBs) in adolescents, which could inform preventative approaches for mental health. Objective: We evaluated the association between HRBs and comorbid anxiety and depression in a large adolescent cohort. Methods: We used data from 22,868 adolescents in the National Youth Cohort (China). Anxiety and depression symptoms were assessed using the 9-item Patient Health Questionnaire scale and the 7-item Generalized Anxiety Disorder scale, respectively. Comorbidity was determined by the coexistence of anxiety and depression. HRBs including poor diet, smoking, physical inactivity, and poor sleep, as well as the above HRB scores, were added to obtain the total HRB score (HRB risk index). Based on single and total HRB scores, we divided participants into low-, medium-, and high-risk groups. Potential confounders included gender, presence of siblings, regional economic level, educational status, self-rated health, parental education level, self-reported family income, number of friends, learning burden, and family history of psychosis. Correlation analysis was used to explore associations between single risk behaviors. Binary logistic regression estimated the association between HRBs and anxiety-depression comorbidity before and after adjusting for potential confounders. Results: The comorbidity rate of anxiety and depression among Chinese adolescents was 31.6% (7236/22,868). There was a statistically significant association between each HRB (P<.05), and HRBs were positively associated with comorbid anxiety and depression in the above population. For single HRBs, adolescents with poor diet, smoking, and poor sleep (medium-risk) were more prone to anxiety-depression comorbidity after adjusting for confounders compared to low-risk adolescents. However, adolescents with all high-risk HRBs were more likely to have comorbid anxiety and depression after adjusting for confounders (poor diet odds ratio [OR] 1.50, 95% CI 1.39-1.62; smoking OR 2.17, 95% CI 1.67-2.81; physical inactivity OR 1.16, 95% CI 1.06-1.28; poor sleep OR 1.84, 95% CI 1.70-2.01). Moreover, in both unadjusted (medium risk OR 1.79, 95% CI 1.56-2.05; high risk OR 3.09, 95% CI 2.72-3.52) and adjusted (medium risk OR 1.57, 95% CI 1.37-1.80; high risk OR 2.33, 95% CI 2.03-2.68) models, HRB risk index, like clustered HRBs, was positively associated with anxiety-depression comorbidity, and the strength of the association was stronger than for any single HRB. In addition, we found that compared to girls, the association between clustered HRBs and anxiety-depression comorbidity was stronger in boys after adjustment. Conclusions: We provide evidence that HRBs are related to comorbid anxiety and depression. Interventions that decrease HRBs may support mental health development in adolescence, with the potential to improve health and well-being through to adulthood. © Meng Wang, Xingyue Mou, Tingting Li, Yi Zhang, Yang Xie, Shuman Tao, Yuhui Wan, Fangbiao Tao, Xiaoyan Wu."
244,243,15,243_genomic_genotypes_breeding_genomewide,"genomic,genotypes,breeding,genomewide,traits,genes,genetic,phenotypic,gene,prediction","Background: Many studies have demonstrated the utility of machine learning (ML) methods for genomic prediction (GP) of various plant traits, but a clear rationale for choosing ML over conventionally used, often simpler parametric methods, is still lacking. Predictive performance of GP models might depend on a plethora of factors including sample size, number of markers, population structure and genetic architecture. Methods: Here, we investigate which problem and dataset characteristics are related to good performance of ML methods for genomic prediction. We compare the predictive performance of two frequently used ensemble ML methods (Random Forest and Extreme Gradient Boosting) with parametric methods including genomic best linear unbiased prediction (GBLUP), reproducing kernel Hilbert space regression (RKHS), BayesA and BayesB. To explore problem characteristics, we use simulated and real plant traits under different genetic complexity levels determined by the number of Quantitative Trait Loci (QTLs), heritability (h 2 and h 2e), population structure and linkage disequilibrium between causal nucleotides and other SNPs. Results: Decision tree based ensemble ML methods are a better choice for nonlinear phenotypes and are comparable to Bayesian methods for linear phenotypes in the case of large effect Quantitative Trait Nucleotides (QTNs). Furthermore, we find that ML methods are susceptible to confounding due to population structure but less sensitive to low linkage disequilibrium than linear parametric methods. Conclusions: Overall, this provides insights into the role of ML in GP as well as guidelines for practitioners. Copyright: © 2023 Farooq M et al.,Background: Genomewide prediction estimates the genomic breeding values of selection candidates which can be utilized for population improvement and cultivar development. Ridge regression and deep learning-based selection models were implemented for yield and agronomic traits of 204 chile pepper genotypes evaluated in multi-environment trials in New Mexico, USA. Results: Accuracy of prediction differed across different models under ten-fold cross-validations, where high prediction accuracy was observed for highly heritable traits such as plant height and plant width. No model was superior across traits using 14,922 SNP markers for genomewide selection. Bayesian ridge regression had the highest average accuracy for first pod date (0.77) and total yield per plant (0.33). Multilayer perceptron (MLP) was the most superior for flowering time (0.76) and plant height (0.73), whereas the genomic BLUP model had the highest accuracy for plant width (0.62). Using a subset of 7,690 SNP loci resulting from grouping markers based on linkage disequilibrium coefficients resulted in improved accuracy for first pod date, ten pod weight, and total yield per plant, even under a relatively small training population size for MLP and random forest models. Genomic and ridge regression BLUP models were sufficient for optimal prediction accuracies for small training population size. Combining phenotypic selection and genomewide selection resulted in improved selection response for yield-related traits, indicating that integrated approaches can result in improved gains achieved through selection. Conclusions: Accuracy values for ridge regression and deep learning prediction models demonstrate the potential of implementing genomewide selection for genetic improvement in chile pepper breeding programs. Ultimately, a large training data is relevant for improved genomic selection accuracy for the deep learning models. © 2023, The Author(s).,In modern plant breeding, genomic selection is becoming the gold standard to select superior genotypes in large breeding populations that are only partially phenotyped. Many breeding programs commonly rely on single-nucleotide polymorphism (SNP) markers to capture genome-wide data for selection candidates. For this purpose, SNP arrays with moderate to high marker density represent a robust and cost-effective tool to generate reproducible, easy-to-handle, high-throughput genotype data from large-scale breeding populations. However, SNP arrays are prone to technical errors that lead to failed allele calls. To overcome this problem, failed calls are often imputed, based on the assumption that failed SNP calls are purely technical. However, this ignores the biological causes for failed calls—for example: deletions—and there is increasing evidence that gene presence–absence and other kinds of genome structural variants can play a role in phenotypic expression. Because deletions are frequently not in linkage disequilibrium with their flanking SNPs, permutation of missing SNP calls can potentially obscure valuable marker–trait associations. In this study, we analyze published datasets for canola and maize using four parametric and two machine learning models and demonstrate that failed allele calls in genomic prediction are highly predictive for important agronomic traits. We present two statistical pipelines, based on population structure and linkage disequilibrium, that enable the filtering of failed SNP calls that are likely caused by biological reasons. For the population and trait examined, prediction accuracy based on these filtered failed allele calls was competitive to standard SNP-based prediction, underlying the potential value of missing data in genomic prediction approaches. The combination of SNPs with all failed allele calls or the filtered allele calls did not outperform predictions with only SNP-based prediction due to redundancy in genomic relationship estimates. Copyright © 2023 Weber, Chawla, Ehrig, Hickey, Frisch and Snowdon."
245,244,15,244_edgecomputing_iot_iotbased_edge,"edgecomputing,iot,iotbased,edge,edgeml,cloud,bandwidth,nodes,toolflows,ai","Internet-of-Things (IoT)-based cyber–physical systems are increasingly being adopted because of the recent technological advancements in sensor technology, edge computing, machine learning, and big data. Integrating machine learning into designing IoT-based cyber–physical systems is essential. However, it is considered a challenging problem. This stems from the fact that IoT devices generate extensive data that requires extensive processing to achieve adequate learning. Relying on local learning by each IoT device is not feasible in most cases due to its limited resources. On the contrary, relying on all cloud-based learning requires transmitting a large amount of data to the cloud to perform the learning process, which is inefficient in large-scale IoT deployments. Therefore, this paper proposes a novel edge-computing architecture that employs the concept of distributed multi-task learning over EC networks in large-scale IoT-based cyber–physical systems. The architecture develops multiple distributed learning algorithms, a data placement architecture, task allocation algorithms, and a network protocol. In addition, it considers the problem of learning model parameters from IoT data distributed over different edge nodes in a large geographical area without sending raw data to the cloud. The architecture supports several distributed machine models that are trained using a combination of machine learning algorithms and population-based search algorithms to optimize the learning process. Population-based search algorithms allow for maintaining a set of candidate solutions, with each solution corresponding to a unique point in the search space for an optimal solution. Having the dataset distributed over several edge nodes, with each node having its own unique set of candidate solutions, increases the chance of finding a solution that generalizes well for the overall dataset combined. Simulation experiments with real IoT datasets are conducted to evaluate the accuracy of the proposed learning models. Results show the ability to achieve high-accuracy results that are close to single-machine models but with significantly efficient edge computing resource utilization. © 2022 Elsevier B.V.,Due to Industry 4.0, machines can be connected to their manufacturing processes with the ability to react faster and smarter to changing conditions in a factory. Previously, Internet of Things (IoT) devices could only collect and send data to the cloud for analysis. However, the increasing computing capacity of today's devices allows them to perform complex computations on-device, resulting in edge computing. Edge devices are a fundamental component of modern, distributed real-world artificial intelligence (AI) systems in Industry 4.0 environments. As a result, edge computing extends cloud computing capabilities by bringing services near the edge of a network and thus supports a new variety of AI services and machine learning (ML) applications. However, there is a large difference between designing and training an ML model, potentially in the cloud, to create ML services that can be deployed and consumed on the edge. This article presents an ML workflow based on ML operations (MLOps) over the Thinger.io IoT platform to streamline the transition from model training to model deployment on edge devices. The proposed workflow is composed of different elements, such as the ML training pipeline, ML deployment pipeline, and ML workspace. Similarly, this article describes the ease of design and deployment of the proposed solution in a real environment, where an anomaly detection service is implemented for detecting outliers on temperature and humidity measurements. The performance tests performed over the ML pipeline steps and the ML service throughput on the edge indicate that this workflow adds minimum overhead to the process, providing a more reliable, reusable, and productive environment.  © 2014 IEEE.,An edge intelligence-aided Internet-of-Things (IoT) network has been proposed to accelerate the response of IoT services by deploying edge intelligence near IoT devices. The transmission of data from IoT devices to the edge nodes leads to large network traffic in the wireless connections. Federated Learning (FL) is proposed to solve the high computational complexity by training the model locally on IoT devices and sharing the model parameters in the edge nodes. This paper focuses on developing an efficient integration of joint edge intelligence nodes depending on investigating an energy-efficient bandwidth allocation, computing Central Processing Unit (CPU) frequency, optimization transmission power, and the desired level of learning accuracy to minimize the energy consumption and satisfy the FL time requirement for all IoT devices. The proposal efficiently optimized the computation frequency allocation and reduced energy consumption in IoT devices by solving the bandwidth optimization problem in closed form. The remaining computational frequency allocation, transmission power allocation, and loss could be resolved with an Alternative Direction Algorithm (ADA) to reduce energy consumption and complexity at every iteration of FL time from IoT devices to edge intelligence nodes. The simulation results indicated that the proposed ADA can adapt the central processing unit frequency and power transmission control to reduce energy consumption at the cost of a small growth of FL time.  © 2013 IEEE."
246,245,15,245_dementia_cognitive_alzheimers_impairment,"dementia,cognitive,alzheimers,impairment,cognition,aging,neuropsychological,fluency,prevalence,verbal","Background Post-traumatic stress disorder (PTSD) is associated with cognitive impairments. It is unclear whether problems persist after PTSD symptoms remit. Methods Data came from 12 270 trauma-exposed women in the Nurses' Health Study II. Trauma and PTSD symptoms were assessed using validated scales to determine PTSD status as of 2008 (trauma/no PTSD, remitted PTSD, unresolved PTSD) and symptom severity (lifetime and past-month). Starting in 2014, cognitive function was assessed using the Cogstate Brief Battery every 6 or 12 months for up to 24 months. PTSD associations with baseline cognition and longitudinal cognitive changes were estimated by covariate-adjusted linear regression and linear mixed-effects models, respectively. Results Compared to women with trauma/no PTSD, women with remitted PTSD symptoms had a similar cognitive function at baseline, while women with unresolved PTSD symptoms had worse psychomotor speed/attention and learning/working memory. In women with unresolved PTSD symptoms, past-month PTSD symptom severity was inversely associated with baseline cognition. Over follow-up, both women with remitted and unresolved PTSD symptoms in 2008, especially those with high levels of symptoms, had a faster decline in learning/working memory than women with trauma/no PTSD. In women with remitted PTSD symptoms, higher lifetime PTSD symptom severity was associated with a faster decline in learning/working memory. Results were robust to the adjustment for sociodemographic, biobehavioral, and health factors and were partially attenuated when adjusted for depression. Conclusion Unresolved but not remitted PTSD was associated with worse cognitive function assessed six years later. Accelerated cognitive decline was observed among women with either unresolved or remitted PTSD symptoms.  Copyright © The Author(s), 2023. Published by Cambridge University Press.,Background and aims: Exponential population aging has led to an increased prevalence of cognitive impairment worldwide. Hand grip strength, which may be associated with physical activity, could be a useful predictor of cognitive impairment. However, few studies have reported the association, if any, between hand grip strength and cognitive function. Methods: We used data obtained from the National Health and Nutrition Examination Survey between 2011–2012 and 2013–2014 to investigate the association between hand grip strength and cognitive impairment. Cognitive impairment was assessed using the Consortium to Establish a Registry for Alzheimer's Disease (CERAD), animal fluency (AF), and digit symbol substitution test (DSST) scores. Cutoff values of CERAD < 5, AF < 14, and DSST < 34 were used to define cognitive impairment. In this cross-sectional study, we used odds ratios to determine the potential usefulness of hand grip strength for the prediction of cognitive impairment. Results: This study included 2,623 participants aged ?60 years. The DSST results showed that hand grip strength was associated with a low risk of cognitive impairment and that subgroup analysis showed that male sex, 60–69 years of age, and the Non-Hispanic (NH)-White, NH Black, and Asian were associated with a significantly low risk of cognitive impairment. The CERAD test results showed that 70–79 years of age and the NH White were significantly associated with a low risk of cognitive impairment. By following full adjustment, we did not observe statistically significant differences between hand grip strength and cognitive impairment based on the CERAD test. The AF test results showed that >80 years of age, female sex, and the NH White were associated with a significantly low risk of cognitive impairment. The most important finding is that a linear association lies between grip strength and cognitive impairment, as well as a sex-based linear association. Machine learning of the XGBoost model suggests that grip strength is one of the top two most important negative predictor variables. Conclusion: We observed an inverse relationship between hand grip strength and cognitive impairment, which might suggest a shared underlying mechanism that needs to be further investigated using a large-scale prospective clinical trial to validate our findings. Copyright © 2022 Huang, Wang, Zhu, Huang, Li, Wang and Liu.,Importance: Understanding how socioeconomic factors are associated with cognitive aging is important for addressing health disparities in Alzheimer disease. Objective: To examine the association of neighborhood disadvantage with cognition among a multiethnic cohort of older adults. Design, Setting, and Participants: In this cross-sectional study, data were collected between September 1, 2017, and May 31, 2022. Participants were from the Health and Aging Brain Study-Health Disparities, which is a community-based single-center study in the Dallas/Fort Worth area of Texas. A total of 1614 Mexican American and non-Hispanic White adults 50 years and older were included. Exposure: Neighborhood disadvantage for participants' current residence was measured by the validated Area Deprivation Index (ADI); ADI Texas state deciles were converted to quintiles, with quintile 1 representing the least disadvantaged area and quintile 5 the most disadvantaged area. Covariates included age, sex, and educational level. Main Outcomes and Measures: Performance on cognitive tests assessing memory, language, attention, processing speed, and executive functioning; measures included the Spanish-English Verbal Learning Test (SEVLT) Learning and Delayed Recall subscales; Wechsler Memory Scale, third edition (WMS-III) Digit Span Forward, Digit Span Backward, and Logical Memory 1 and 2 subscales; Trail Making Test (TMT) parts A and B; Digit Symbol Substitution Test (DSST); Letter Fluency; and Animal Naming. Raw scores were used for analyses. Associations between neighborhood disadvantage and neuropsychological performance were examined via demographically adjusted linear regression models stratified by ethnic group. Results: Among 1614 older adults (mean [SD] age, 66.3 [8.7] years; 980 women [60.7%]), 853 were Mexican American (mean [SD] age, 63.9 [7.9] years; 566 women [66.4%]), and 761 were non-Hispanic White (mean [SD] age, 69.1 [8.7] years; 414 women [54.4%]). Older Mexican American adults were more likely to reside in the most disadvantaged areas (ADI quintiles 3-5), with 280 individuals (32.8%) living in ADI quintile 5, whereas a large proportion of older non-Hispanic White adults resided in ADI quintile 1 (296 individuals [38.9%]). Mexican American individuals living in more disadvantaged areas had worse performance than those living in ADI quintile 1 on 7 of 11 cognitive tests, including SEVLT Learning (ADI quintile 5: ? = -2.50; 95% CI, -4.46 to -0.54), SEVLT Delayed Recall (eg, ADI quintile 3: ? = -1.11; 95% CI, -1.97 to -0.24), WMS-III Digit Span Forward (eg, ADI quintile 4: ? = -1.14; 95% CI, -1.60 to -0.67), TMT part A (ADI quintile 5: ? = 7.85; 95% CI, 1.28-14.42), TMT part B (eg, ADI quintile 5: ? = 31.5; 95% CI, 12.16-51.35), Letter Fluency (ADI quintile 4: ? = -2.91; 95% CI, -5.39 to -0.43), and DSST (eg, ADI quintile 5: ? = -4.45; 95% CI, -6.77 to -2.14). In contrast, only non-Hispanic White individuals living in ADI quintile 4 had worse performance than those living in ADI quintile 1 on 4 of 11 cognitive tests, including SEVLT Learning (? = -2.35; 95% CI, -4.40 to -0.30), SEVLT Delayed Recall (? = -0.95; 95% CI, -1.73 to -0.17), TMT part B (? = 15.95; 95% CI, 2.47-29.44), and DSST (? = -3.96; 95% CI, -6.49 to -1.43). Conclusions and Relevance: In this cross-sectional study, aging in a disadvantaged area was associated with worse cognitive functioning, particularly for older Mexican American adults. Future studies examining the implications of exposure to neighborhood disadvantage across the life span will be important for improving cognitive outcomes in diverse populations. © 2023 American Medical Association. All rights reserved."
247,246,15,246_compression_cnns_compressed_compressing,"compression,cnns,compressed,compressing,tensorouter,compress,networks,quantization,networkofdatasets,mininet","In this paper, we propose an ultrafast automated model compression framework called SeerNet for flexible network deployment. Conventional non-differen-tiable methods discretely search the desirable compression policy based on the accuracy from exhaustively trained lightweight models, and existing differentiable methods optimize an extremely large supernet to obtain the required compressed model for deployment. They both cause heavy computational cost due to the complex compression policy search and evaluation process. On the contrary, we obtain the optimal efficient networks by directly optimizing the compression policy with an accurate performance predictor, where the ultrafast automated model compression for various computational cost constraint is achieved without complex compression policy search and evaluation. Specifically, we first train the performance predictor based on the accuracy from uncertain compression policies actively selected by efficient evolutionary search, so that informative supervision is provided to learn the accurate performance predictor with acceptable cost. Then we leverage the gradient that maximizes the predicted performance under the barrier complexity constraint for ultrafast acquisition of the desirable compression policy, where adaptive update stepsizes with momentum are employed to enhance optimality of the acquired pruning and quantization strategy. Compared with the state-of-the-art automated model compression methods, experimental results on image classification and object detection show that our method achieves competitive accuracy-complexity trade-offs with significant reduction of the search cost. Code is available at https://github.com/ZiweiWangTHU/SeerNet. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Deep learning has become the default solution for a plethora of problems nowadays. However, one drawback of such deep learning-based solutions is that the models are very large and cumbersome to process. As such, they are difficult to use in small or embedded devices and to transmit across the web. In light of this problem, this paper presents a novel method for converting large neural networks into lightweight, compressed models. Our method utilizes the dimensionality reduction algorithm known as Principal Component Analysis to decompose the network weights into smaller matrices to create a new, compressed architecture. This compressed model is further trained to overcome the error due to the lossy compression and then the parameters are finally stored after quantization. Experiments on benchmark datasets using standard models show that we achieve high compression, with compression rates between 5 to 35 depending on the complexity of the model, with little to no fall in model accuracy. Comparison with other state-of-the-art methods shows that the performance of our compression method is similar or even better in certain cases. This is the first work where dimensionality reduction and quantization are combined to create a new, compressed model. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Deep neural network (DNN) obtained satisfactory results on different vision tasks; however, they usually suffer from large models and massive parameters during model deployment. While DNN compression can reduce the memory footprint of deep model effectively, so that the deep model can be deployed on portable devices. However, most of the existing model compression methods cost lots of time, e.g., vector quantization or pruning, which makes them inept to the application that needs fast computation. In this paper, we therefore explore how to accelerate the model compression process by reducing the computation cost. Then, we propose a new model compression method, termed dictionary-pair-based fast data-free DNN compression, which aims at reducing the memory consumption of DNNs without extra training and can greatly improve the compression efficiency. Specifically, our method performs tensor decomposition of DNN model with a fast dictionary-pair learning-based reconstruction approach, which can be deployed on different weight layers (e.g., convolution and fully connected layers). Given a pre-trained DNN model, we first divide the parameters (i.e., weights) of each layer into a series of partitions for dictionary pair-driven fast reconstruction, which can potentially discover more fine-grained information and provide the possibility for parallel model compression. Then, dictionaries of less memory occupation are learned to reconstruct the weights. Moreover, automatic hyper-parameter tuning and shared-dictionary mechanism is proposed to improve the model performance and availability. Extensive experiments on popular DNN models (i.e., VGG-16, ResNet-18 and ResNet-50) showed that our proposed weight compression method can significantly reduce the memory footprint and speed up the compression process, with less performance loss. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
248,247,15,247_polymer_polymers_polymergnn_polymersolvent,"polymer,polymers,polymergnn,polymersolvent,polymeric,solvents,molecular,solvent,predicting,prediction","Artificial intelligence-based methods are becoming increasingly effective at screening libraries of polymers down to a selection that is manageable for experimental inquiry. The vast majority of presently adopted approaches for polymer screening rely on handcrafted chemostructural features extracted from polymer repeat units?a burdensome task as polymer libraries, which approximate the polymer chemical search space, progressively grow over time. Here, we demonstrate that directly “machine learning” important features from a polymer repeat unit is a cheap and viable alternative to extracting expensive features by hand. Our approach?based on graph neural networks, multitask learning, and other advanced deep learning techniques?speeds up feature extraction by 1-2 orders of magnitude relative to presently adopted handcrafted methods without compromising model accuracy for a variety of polymer property prediction tasks. We anticipate that our approach, which unlocks the screening of truly massive polymer libraries at scale, will enable more sophisticated and large scale screening technologies in the field of polymer informatics. © 2023 The Authors. Published by American Chemical Society.,Predicting and understanding the phase equilibria or phase separation in polymer-solvent solutions represent unresolved fundamental problems in polymer science. The phase behavior and thermodynamics of polymer miscibility depend on the inter- and intramolecular interactions of a polymer with a certain molecular weight distribution mixed with a solvent. Here, we develop a machine-learning framework to achieve highly generalized and robust prediction of Flory-Huggins ? parameters for polymer-solvent solutions. The model was trained using experimentally observed temperature-dependent ? parameters for 1190 samples, comprising 46 unique polymers and 140 solvent species. However, the difficulty was that the data set was quantitatively limited and qualitatively biased owing to technical issues in determining the Flory-Huggins ? parameters. To overcome these limitations, we produced an in-house data set of ? parameters obtained from quantum chemical calculations for thousands of polymer-solvent pairs and a large list of soluble and insoluble polymer-solvent pairs. Using these three data sets, we conducted multitask machine learning that simultaneously performed the “soluble/insoluble” classification and quantitative evaluation of both experimental and calculated ? parameters. Consequently, we obtained a highly generalized model applicable to a wide range of polymer solution spaces. In this paper, the predictive power and physicochemical implications of the model are demonstrated, along with quantitative comparisons with existing methods. © 2023 The Authors. Published by American Chemical Society,Abstract: Polymers are diverse and versatile materials that have met a wide range of material application demands. They come in several flavors and architectures (e.g., homopolymers, copolymers, polymer blends, and polymers with additives). Searching this enormous space for suitable materials with a specific set of property/performance targets is thus nontrivial, painstaking, and expensive. Such a search process can be made effective by the creation of rapid and accurate property predictors. In this article, we present a machine learning framework to predict the thermal properties of homopolymers, copolymers, and polymer blends. A universal fingerprinting scheme capable of handling this entire polymer chemical class has been developed and a multitask deep learning algorithm is trained simultaneously on a large data set of glass-transition, melting, and degradation temperatures. The trained models demonstrate precision and scalability to other properties when relevant data becomes accessible. Impact statement: The chemical and structural variations that can be achieved with the polymers is staggering. Such extraordinary and diverse possibilities translate to attractive combinations of physical properties impacting several application spaces, making the polymeric class of materials ubiquitous in our modern society. This chemistry–structure–property diversity is accompanied by a major challenge. Searching the chemo-structural space to identify suitable application-relevant candidates with the right set of target properties is nontrivial, requiring advanced rapid property prediction and search schemes. In this article, we present a data-driven machine learning framework to instantaneously predict the thermal properties (an important property class) of a dizzyingly large class of polymer archetypes, namely, homopolymers, copolymers, and polymer blends. A state-of-the-art machine learning algorithm has been developed and trained on a large data set of glass-transition, melting, and degradation temperatures, to make instantaneous predictions of these properties for any new-to-the-world polymer that falls in this large important polymer chemical class. This prediction scheme paves the way for discovering polymers with unprecedented thermal stability by allowing searches of enormous chemical spaces at scale. Graphical abstract: [Figure not available: see fulltext.] © 2023, The Author(s), under exclusive License to the Materials Research Society."
249,248,15,248_entities_entity_labeling_labelling,"entities,entity,labeling,labelling,semantic,text,classification,texts,learning,deep","Named Entity Recognition is an information extraction task that serves as a pre-processing step for other natural language processing tasks, such as machine translation, information retrieval, and question answering. Named entity recognition enables the identification of proper names as well as temporal and numeric expressions in an open domain text. For Semitic languages such as Arabic, Amharic, and Hebrew, the named entity recognition task is more challenging due to the heavily inflected structure of these languages. In this study, we annotate a new comparatively large Amharic named entity recognition dataset and make it publicly available. Using this new dataset, we build multiple Amharic named entity recognition systems based on recent deep learning approaches including transfer learning (RoBERTa), and bidirectional long short-term memory coupled with a conditional random fields layer. By applying the Synthetic Minority Over-sampling Technique to mitigate the imbalanced classification problem, our best performing RoBERTa based named entity recognition system achieves an f1-score of 93%, which is the new state-of-the-art result for Amharic named entity recognition.  © 2013 IEEE.,The Chinese named entity recognition (NER) is a critical task in natural language processing, aiming at identifying and classifying named entities in text. However, the specificity of domain texts and the lack of large-scale labelled datasets have led to the poor performance of NER methods trained on public domain corpora on domain texts. In this paper, a named entity recognition method incorporating sentence semantic information is proposed, mainly by adaptively incorporating sentence semantic information into character semantic information through an attention mechanism and a gating mechanism to enhance entity feature representation while attenuating the noise generated by irrelevant character information. In addition, to address the lack of large-scale labelled samples, we used data self-augmentation methods to expand the training samples. Furthermore, we introduced a Weighted Strategy considering that the low-quality samples generated by the data self-augmentation process can have a negative impact on the model. Experiments on the TCM prescriptions corpus showed that the F1 values of our method outperformed the comparison methods. © 2023, Strojarski Facultet. All rights reserved.,With the rapid growth of Internet penetration, more and more people choose the Internet to express their views on topics of interest. In recent years, named entity recognition (NER) is becoming a popular task for the public to obtain structured information from public opinion text. At present, NER models with good results, such as deep learning model, need a lot of labeled data for training. However, this will give rise to a problem: labeling a large amount of data requires a lot of human resources, which is thankless in some areas. Therefore, in this paper, we propose a NER model combining active learning and deep learning methods. Firstly, the active learning method can solve the above problem. The strategy combines uncertainty-based sampling and diversity-based sampling to estimate the information of data. We use highly informative data as the initial training dataset. Secondly, this paper uses a deep learning model combining bidirectional encoder representations from Transformers, bidirectional long–short-term memory and conditional random field (BERT-BiLSTM-CRF). BERT extracts the semantic features of data, and BiLSTM predicts the probability distribution of entity labels. We use the CRF for decoding the probability distribution into corresponding entity labels. Finally, we use the initial training dataset for training BERT-BiLSTM-CRF. This model predicts the entity labels of the unlabeled data. Then, we judge if the machine-labeled data is highly reliable and expand the highly reliable data to the initial training dataset. The updated dataset retrains the NER model, so that the trained model has higher precision than the previous model. The results show that our model performs well without a large number of labeled datasets. The model achieves a precision value of 70.31%, recall rate of 74.93% and F1 score of 72.55% in the named entity recognition task, which proves the effectiveness of our model. Besides, the F1 score of BERT-BiLSTM-CRF with uncertainty-based sampling and diversity-based sampling (UD_BBC) is higher than the BiLSTM-CRF based on maximum normalized log-probability (MNLP_BiLSTM-CRF) by 9.00%, when recognizing overall entity categories. It provides a solution to the problem of named entity recognition in educational public opinion. © 2022 Elsevier Ltd"
250,249,15,249_adversarial_faults_fault_feature,"adversarial,faults,fault,feature,generative,features,domaininvariant,detection,diagnostic,sinclstm","Diagnosis of compound faults remains a challenge owing to the coupling of fault characteristics and the exponential increment of the number of possible fault types. Current compound faults diagnostic methods often require a large number of training data for each type of compound fault. In real-world scenarios, training data of compound faults are usually difficult to acquire and sometimes even inaccessible. In contrast, single fault samples are much easier to obtain. Thus in this paper inspired by the idea of zero-shot learning, we present a novel label information vector generative zero-shot model to identify unknown compound faults, using only single fault samples as the training set. This model comprises several modules, namely label information vector (LIV) definition, feature extractor, and generative adversarial modules, respectively responsible for representing the prior knowledge of specific class labels for the single fault and compound fault, extraction of fault features, and mapping the relationship between the fault features and the fault LIVs. By adversarial training between the samples and LIVs of single faults, the model can generate compound fault features using the compound fault LIVs. Thus the unknown compound faults are identified by measuring the distance between the features extracted from the testing compound fault samples and the generated features from LIVs. The proposed method is evaluated on a self-built experimental platform. The results demonstrate that without any compound fault samples in the training set, the compound fault classification accuracy of the model reaches 78.10%. © 2023 Elsevier Ltd,In recent years, supervised deep learning-based methods have achieved significant results in fuel cell system fault diagnosis. However, most existing deep learning-based fault prediction methods suffer from missing fault labels and data limitations due to the difficulty in obtaining fault and degradation data in real fuel cell systems. To address the above challenges, this paper proposes a fault diagnosis method based on digital twin and unsupervised domain adaptive learning. The method has two key features: First, a maximum-relevance minimum-redundancy algorithm is used to select the input signals. Then a high-order fuel cell system model is developed to obtain a large amount of digital domain fault data at low cost by simulating fault injection. Second, domain-invariant features are extracted using a domain-adaptive adversarial learning approach to reduce the distribution differences between the digital and real domains. The method successfully diagnosed nine typical faults in the fuel cell air, hydrogen, and thermal subsystems without real data fault labels. Under dynamic load conditions, the diagnostic accuracy reached 92.5 %. In addition, the method achieves a diagnostic accuracy of over 90 % under domain adversarial training using only normal real data. The experimental results show that the proposed method can achieve fuel cell system fault diagnosis without fault labels and significantly reduce the dependence on fault data. © 2023 Hydrogen Energy Publications LLC,Data-driven fault diagnosis techniques utilizing deep learning have achieved widespread success. However, their diagnostic capability and application possibility are significantly reduced in real-world scenarios where fault modes are not fully covered and labels are lacking. Owing to potential conflicts of interest and legal risks, industrial equipment fault data usually exist in the form of isolated islands, making it difficult to carry out large-scale centralized model training. This paper proposes open-set federated adversarial domain adaptation (OS-FADA) to achieve collaborative evolution of fault diagnosis capabilities among cross-domain data owners while protecting privacy. The OS-FADA is a general fault diagnosis framework that employs two-phase adversarial learning. First, faced with the data distribution shift caused by variable working conditions, a generative adversarial feature extractor training strategy is designed to achieve domain-invariant fault feature extraction by approximating the feature distributions of clients to a unified generated distribution. Second, considering the label distribution shift of unknown faults occurring in the target client, an adversarial learning method is proposed to establish decision boundaries between known and unknown faults. Ultimately, the co-evolution of fault diagnosis models between clients is achieved by combining two-phase adversarial learning and federated aggregation. Results from an industrial gearbox case demonstrate that our proposed method achieves over 20% diagnostic accuracy improvement and has excellent potential for cross-domain fault diagnosis tasks with unknown faults when the data silos problem cannot be ignored. © 2023 IOP Publishing Ltd."
251,250,15,250_angiography_coronary_artery_arteries,"angiography,coronary,artery,arteries,atherosclerosis,cardiovascular,stroke,tomography,aorta,plaque","Background: Vascular calcification is recognized as the advanced stage of atherosclerosis burden. We hypothesized that vascular calcium quantification in CT angiography (CTA) would be helpful to differentiate large artery atherosclerosis (LAA) from other stroke etiology in patients with ischemic stroke. Methods: We studied 375 acute ischemic stroke patients (200 males, mean age 69.9 years) who underwent complete CTA images of the aortic arch, neck, and head. The automatic artery and calcification segmentation method measured calcification volumes in the intracranial internal carotid artery (ICA), cervical carotid artery, and aortic arch using deep-learning U-net model and region-grow algorithms. We investigated the correlations and patterns of vascular calcification in the different vessel beds among stroke etiology by age category (young: <65 years, intermediate: 65–74 years, older ?75 years). Results: Ninety-five (25.3%) were diagnosed with LAA according to TOAST criteria. Median calcification volumes were higher by increasing the age category in each vessel bed. One-way ANOVA with Bonferroni correction showed calcification volumes in all vessel beds were significantly higher in LAA compared with other stroke subtypes in the younger subgroup. Calcification volumes were independently associated with LAA in intracranial ICA (OR; 2.89, 95% CI 1.56–5.34, P = .001), cervical carotid artery (OR; 3.40, 95% CI 1.94–5.94, P < .001) and aorta (OR; 1.69, 95%CI 1.01–2.80, P = .044) in younger subsets. By contrast, the intermediate and older subsets did not show a significant relationship between calcification volumes and stroke subtypes. Conclusion: Atherosclerosis calcium volumes in major vessels were significantly higher in LAA compared to non-LAA stroke in younger age. © 2023 Elsevier B.V.,Purpose: Although SPECT myocardial perfusion imaging (MPI) is susceptible to artifacts from soft tissue attenuation, most scans are performed without attenuation correction. Deep learning-based attenuation corrected (DLAC) polar maps improved diagnostic accuracy for detection of coronary artery disease (CAD) beyond non-attenuation-corrected (NAC) polar maps in a large single center study. However, the generalizability of this approach to other institutions with different scanner models and protocols is uncertain. In this study, we evaluated the diagnostic performance of DLAC compared to NAC for detection of CAD as defined by invasive coronary angiography (ICA) in a large multi-center trial. Methods: During the phase 3 flurpiridaz multi-center diagnostic clinical trial, conducted over 74 international sites, patients with known or suspected CAD who were referred for a clinically indicated ICA were enrolled. Using receiver operating characteristic (ROC) analysis, we evaluated the detectability of obstructive CAD, defined by quantitative coronary angiography by a core laboratory, using total perfusion deficit (TPD) as an integrated measure of defect extent and severity on DLAC polar maps compared to NAC polar maps. This was also compared against the visual scoring of three expert core lab readers. Results: Out of 755 patients, 722 (69% male) had evaluable SPECT and ICA for this study. ROC analysis demonstrated significant improvement in detecting per-patient obstructive CAD with DLAC over NAC with area under the curve (AUC) of 0.752 (95% CI: 0.711–0.792) for DLAC compared to 0.717 (0.675–0.759) for NAC (p value = 0.016). Compared to the consensus of expert readers AUC = 0.743 (0.701–0.784), DLAC was comparable (p value = 0.913), whereas NAC underperformed (p value = 0.051). Conclusion: DL-based attenuation correction improves diagnostic performance of SPECT MPI for detecting CAD in data from a large multi-center clinical trial regardless of SPECT camera model or protocol. Trial registration: A Phase 3 Multi-center Study to Assess PET Imaging of Flurpiridaz F 18 Injection in Patients With CAD, ClinicalTrials.gov Identifier: NCT01347710, registered on 4 May 2011. https://clinicaltrials.gov/ct2/show/study/NCT01347710 © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Background: Plaque analysis with coronary computed tomography angiography (CCTA) is a promising tool to identify high risk of future coronary events. The analysis process is time-consuming, and requires highly trained readers. Deep learning models have proved to excel at similar tasks, however, training these models requires large sets of expert-annotated training data. The aims of this study were to generate a large, high-quality annotated CCTA dataset derived from Swedish CArdioPulmonary BioImage Study (SCAPIS), report the reproducibility of the annotation core lab and describe the plaque characteristics and their association with established risk factors. Methods and results: The coronary artery tree was manually segmented using semi-automatic software by four primary and one senior secondary reader. A randomly selected sample of 469 subjects, all with coronary plaques and stratified for cardiovascular risk using the Systematic Coronary Risk Evaluation (SCORE), were analyzed. The reproducibility study (n = 78) showed an agreement for plaque detection of 0.91 (0.84–0.97). The mean percentage difference for plaque volumes was ?0.6% the mean absolute percentage difference 19.4% (CV 13.7%, ICC 0.94). There was a positive correlation between SCORE and total plaque volume (rho = 0.30, p < 0.001) and total low attenuation plaque volume (rho = 0.29, p < 0.001). Conclusions: We have generated a CCTA dataset with high-quality plaque annotations showing good reproducibility and an expected correlation between plaque features and cardiovascular risk. The stratified data sampling has enriched high-risk plaques making the data well suited as training, validation and test data for a fully automatic analysis tool based on deep learning. © 2023 The Authors"
252,251,14,251_dialogue_dialog_dialoguespecific_conversational,"dialogue,dialog,dialoguespecific,conversational,conversation,dialogueadaptive,dailydialog,utterancelevel,corpus,contextual","Task-oriented dialogue systems continue to face significant challenges as they require not only an understanding of dialogue history but also domain-specific knowledge. However, knowledge is often dynamic, making it difficult to effectively integrate into the learning process. Existing large language model approaches primarily treat knowledge bases as textual resources, neglecting to capture the underlying relationships between facts within the knowledge base. To address this limitation, we propose a novel dialogue system called PluDG. We regard the knowledge as a knowledge graph and propose a knowledge extraction plug-in, Kg-Plug, to capture the features of the graph and generate prompt entities to assist the system's dialogue generation. Besides, we propose Unified Memory Integration, a module that enhances the comprehension of the sentence's internal structure and optimizes the knowledge base's encoding location. We conduct experiments on three public datasets and compare PluDG with several state-of-the-art dialogue models. The experimental results indicate that PluDG achieves significant improvements in both accuracy and diversity, outperforming the current state-of-the-art dialogue system models and achieving state-of-the-art performance. © 2023 Dong and Chen,An effective dialogue system needs amount of training data, but the existing training data is insufficient. Although the pre-trained model has made great progress in recent years, which can alleviate the problem of low resource dialogue to a certain extent, the pre-trained model is large and difficult to deploy. How to improve the performance of dialogue model without additional annotation data and decreasing the model volume has become a new challenge. We propose a multi-source data augmentation method for low-resource dialogue generation by utilizing inverse curriculum learning (inverse CL). Firstly, we adopt three data augmentation methods, including round-trip translation, paraphrasing and pre-trained model, to generate augmentation data. Next, we propose a new training strategy based on inverse CL to utilize different augmentation data. Comparing with the baselines, our method comprehensively outperform the baselines on all evaluation metrics, which shows the effectiveness of our proposed training strategy for dialogue generation. To the best of our knowledge, this is the first systematic investigation of data augmentation in the dialogue generation. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Achieving high performance in a multi-domain dialogue system with low computation is undoubtedly challenging. Previous works applying an end-To-end approach have been very successful. However, the computational cost remains a major issue since the large-sized language model using GPT-2 is required. Meanwhile, the optimization for individual components in the dialogue system has not shown promising result, especially for the component of dialogue management due to the complexity of multi-domain state and action representation. To cope with these issues, this article presents an efficient guidance learning where the imitation learning and the hierarchical reinforcement learning (HRL) with human-in-The-loop are performed to achieve high performance via an inexpensive dialogue agent. The behavior cloning with auxiliary tasks is exploited to identify the important features in latent representation. In particular, the proposed HRL is designed to treat each goal of a dialogue with the corresponding sub-policy so as to provide efficient dialogue policy learning by utilizing the guidance from human through action pruning and action evaluation, as well as the reward obtained from the interaction with the simulated user in the environment. Experimental results on ConvLab-2 framework show that the proposed method achieves state-of-The-Art performance in dialogue policy optimization and outperforms the GPT-2 based solutions in end-To-end system evaluation. © 2014 IEEE."
253,252,14,252_intrusion_federated_iot_attacks,"intrusion,federated,iot,attacks,privacy,security,sdn,malicious,distributed,detection","Distributed denial-of-service (DDoS) attacks continue to grow at a rapid rate plaguing Internet Service Providers (ISPs) and individuals in a stealthy way. Thus, intrusion detection systems (IDSs) must evolve to cope with these increasingly sophisticated and challenging security threats. Traditional IDSs are prone to zero-day attacks since they are usually signature-based detection systems. The recent advent of machine learning and deep learning (ML/DL) techniques can help strengthen these IDSs. However, the lack of up-to-date labeled training datasets makes these ML/DL based IDSs inefficient. The privacy nature of these datasets and widespread emergence of adversarial attacks make it difficult for major organizations to share their sensitive data. Federated Learning (FL) is gaining momentum from both academia and industry as a new sub-field of ML that aims to train a global statistical model across multiple distributed users, referred to as collaborators, without sharing their private data. Due to its privacy-preserving nature, FL has the potential to enable privacy-aware learning between a large number of collaborators. This paper presents a novel framework, called MiTFed, that allows multiple software defined networks (SDN) domains (i.e., collaborators) to collaboratively build a global intrusion detection model without sharing their sensitive datasets. In particular, MiTFed consists of: (1) a novel distributed architecture that allows multiple SDN based domains to securely collaborate in order to cope with sophisticated security threats while preserving the privacy of each SDN domain; (2) a novel Secure Multiparty Computation (SMPC) scheme to securely aggregate local model updates; and (3) a blockchain based scheme that uses Ethereum smart contracts to maintain the collaboration in a fully decentralized, trustworthy, flexible, and efficient manner. To the best of our knowledge, MiTFed is the first framework that leverages FL, blockchain and SDN technologies to mitigate the new emerging security threats in large scale. To evaluate MiTFed, we conduct several experiments using real-world network attacks; the experimental results using the well-known public network security dataset NSL-KDD show that MiTFed achieves efficiency and high accuracy in detecting the new emerging security threats in both binary and multi-class classification while preserving the privacy of collaborators, making it a promising framework to cope with the new emerging security threats in SDN.  © 2013 IEEE.,Federated Learning (FL) is a promising distributed learning approach to enable intelligent Internet of Things (IoT) applications. However, FL is vulnerable to model poisoning attacks in which malicious clients abate the accuracy of the global model by committing crafted local model updates to the server. Existing defense methods either rely on a validation dataset or simply remove the detected malicious clients from the subsequent training process to handle attacks from a large number of malicious clients. Thus, the performance of existing methods deteriorates drastically in many scenarios where the data distributions of clients are unpredictable. To address these deficiencies, we propose a framework called FL overwatch (FLOW) to efficiently defend against model poisoning attacks taking advantages of the local model updates in current and historical training iterations. On one hand, FLOW detects malicious clients in each iteration by measuring the cosine distances between the local model updates of clients, such that malicious updates are eliminated from the current aggregation. On the other hand, FLOW gracefully punishes the previously identified malicious clients rather than removes them from the whole training process. As a result, FLOW can embrace a richer reliable set of local model updates than existing methods in aggregation. Extensive experiments on widely-used benchmark datasets show that FLOW can achieve higher success defending ratio and higher accuracy of global models over existing Byzantine-robust FL methods under typical untargeted attacks and targeted attacks. Furthermore, FLOW also shows significant effectiveness in defending against adaptive attacks tailored to FLOW. IEEE,The Internet of Things (IoT) is a network of electrical devices that are connected to the Internet wirelessly. This group of devices generates a large amount of data with information about users, which makes the whole system sensitive and prone to malicious attacks eventually. The rapidly growing IoT-connected devices under a centralized ML system could threaten data privacy. The popular centralized machine learning (ML)-assisted approaches are difficult to apply due to their requirement of enormous amounts of data in a central entity. Owing to the growing distribution of data over numerous networks of connected devices, decentralized ML solutions are needed. In this paper, we propose a Federated Learning (FL) method for detecting unwanted intrusions to guarantee the protection of IoT networks. This method ensures privacy and security by federated training of local IoT device data. Local IoT clients share only parameter updates with a central global server, which aggregates them and distributes an improved detection algorithm. After each round of FL training, each of the IoT clients receives an updated model from the global server and trains their local dataset, where IoT devices can keep their own privacy intact while optimizing the overall model. To evaluate the efficiency of the proposed method, we conducted exhaustive experiments on a new dataset named Edge-IIoTset. The performance evaluation demonstrates the reliability and effectiveness of the proposed intrusion detection model by achieving an accuracy (92.49%) close to that offered by the conventional centralized ML models’ accuracy (93.92%) using the FL method. © 2023 by the authors."
254,253,14,253_gaussian_sparse_priors_nonparametric,"gaussian,sparse,priors,nonparametric,bayesian,kernels,gpbo,multivariate,modeling,posterior","In a traditional Gaussian graphical model, data homogeneity is routinely assumed with no extra variables affecting the conditional independence. In modern genomic datasets, there is an abundance of auxiliary information, which often gets under-utilized in determining the joint dependency structure. In this article, we consider a Bayesian approach to model undirected graphs underlying heterogeneous multivariate observations with additional assistance from covariates. Building on product partition models, we propose a novel covariate-dependent Gaussian graphical model that allows graphs to vary with covariates so that observations whose covariates are similar share a similar undirected graph. To efficiently embed Gaussian graphical models into our proposed framework, we explore both Gaussian likelihood and pseudo-likelihood functions. For Gaussian likelihood, a G-Wishart distribution is used as a natural conjugate prior, and for the pseudo-likelihood, a product of Gaussian-conditionals is used. Moreover, the proposed model has large prior support and is flexible to approximate any ?-Hölder conditional variance-covariance matrices with (Formula presented.). We further show that based on the theory of fractional likelihood, the rate of posterior contraction is minimax optimal assuming the true density to be a Gaussian mixture with a known number of components. The efficacy of the approach is demonstrated via simulation studies and an analysis of a protein network for a breast cancer dataset assisted by mRNA gene expression as covariates. Supplementary materials for this article are available online. © 2023 American Statistical Association.,Gaussian process-based Bayesian optimization (GPBO) is used to search parameters in machine learning, material design, etc. It is a method for finding optimal solutions in a search space through the following four procedures. (1) Develop a Gaussian process regression (GPR) model using observed data. (2) The GPR model is used to obtain the estimated mean and estimated variance for the search space. (3) The point where the sum of the estimated mean and the weighted estimated variance (upper confidence bound, UCB) is largest is the next search point (in the case of a maximum search). (4) Repeat the above procedures. Thus, the generalization performance of the GPR is directly related to the search performance of the GPBO. In procedure (1), the kernel parameters (KPs) of the GPR are tuned via gradient descent (GD) using the log-likelihood as the objective function. However, if the number of iterations of the GD is too high, there is a risk that the KPs will overfit the observed data. In this case, because the estimated mean and variance output by the GPR model are inappropriate, the next search point cannot be properly determined. Therefore, overtuned KPs degrade the GPBO search performance. However, this negative effect can be mitigated by changing the parameters of the GPBO. We focus on the weight of the estimated variances (exploration weight) of the UCB as one of these parameters. In a GPBO with a large exploration weight, the observed data appear in various regions in the search space. If the KP is tuned using such data, the GPR model can estimate the diverse regions somewhat correctly, even if the KP overfits the observed data, i.e., the negative effect of overtuned KPs on the GPR is mitigated by setting a larger exploration weight for the UCB. This suggests that the negative effect of overtuned KPs on the GPBO search performance may be related to the UCB exploration weight. In the present study, this hypothesis was tested using simple numerical simulations. Specifically, GPBO was applied to a simple black-box function with two optimal solutions. As parameters of GPBO, we set the number of KP iterations of GD in the range of 0–500 and the exploration weight as (Formula presented.). The number of KP iterations expresses the degree of overtuning, and the exploration weight expresses the strength of the GPBO search. The results indicate that, in the overtuned KP situation, GPBO with a larger exploration weight has better search performance. This suggests that, when searching for solutions with a small GPBO exploration weight, one must be careful about overtuning KPs. The findings of this study are useful for successful exploration with GPBO in all situations where it is used, e.g., machine learning hyperparameter tuning. © 2023 by the author.,Gaussian process regression (GPR) is a kernel-based learning model, which unfortunately suffers from computational intractability for irregular domain and large datasets due to the full kernel matrix. In this paper, we propose a novel method to produce a sparse kernel matrix using the compact support radial kernels (CSRKs) to efficiently learn the GPR from large datasets. The CSRKs can effectively avoid the ill-conditioned and full kernel matrix during GPR training and prediction, consequently reducing computational costs and memory requirements. In practice, the interest in CSRKs waned slightly as it became evident that, there is a trade-off principle (conflict between accuracy and sparsity) for compactly supported kernels. Hence, when using kernels with compact support, during GPR training, the main focus will be on providing a high level of accuracy. In this case, the advantage of achieving a sparse covariance matrix for CSRKs will almost disappear, as we will see in the numerical results. This trade-off has led authors to search for an “optimal” value of the scale parameter. Accordingly, by selecting the suitable priors on the kernel hyperparameters, and simply estimating the hyperparameters using a modified version of the maximum likelihood estimation (MLE), the GPR model derived from the CSRKs yields maximal accuracy while still maintaining a sparse covariance matrix. In fact, in GPR training, modified version of the MLE will be proportional to the product of MLE and a given suitable prior distribution for the hyperparameters that provides an efficient method for learning. The misspecification of prior distributions and their impact on the predictability of the sparse GPR models are also comprehensively investigated using several empirical studies. The proposed new approach is applied to some irregular domains with noisy test functions in 2D data sets in a comparative study. We finally investigate the effect of prior on the predictability of GPR models based on the real dataset. The derived results suggest the proposed method leads to more sparsity and well-conditioned kernel matrices in all cases. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
255,254,14,254_aging_metabolic_agingrelated_aerobic,"aging,metabolic,agingrelated,aerobic,metabolomics,biomarkers,biomarker,lifespan,longevity,exercise","Aging is a complicated process characterized by progressive and extensive changes in physiological homeostasis at the organismal, tissue, and cellular levels. In modern society, age estimation is essential in a large variety of legal rights and duties. Accumulating evidence suggests roles for microRNAs (miRNAs) and circular RNAs (circRNAs) in regulating numerous processes during aging. Here, we performed circRNA sequencing in two age groups and analyzed microarray data of 171 healthy subjects (17–104 years old) downloaded from Gene Expression Omnibus (GEO) and ArrayExpress databases with integrated bioinformatics methods. A total of 1,403 circular RNAs were differentially expressed between young and old groups, and 141 circular RNAs were expressed exclusively in elderly samples while 10 circular RNAs were expressed only in young subjects. Based on their expression pattern in these two groups, the circular RNAs were categorized into three classes: age-related expression between young and old, age-limited expression-young only, and age-limited expression-old only. Top five expressed circular RNAs among three classes and a total of 18 differentially expressed microRNAs screened from online databases were selected to validate using RT-qPCR tests. An independent set of 200 blood samples (20–80 years old) was used to develop age prediction models based on 15 age-related noncoding RNAs (11 microRNAs and 4 circular RNAs). Different machine learning algorithms for age prediction were applied, including regression tree, bagging, support vector regression (SVR), random forest regression (RFR), and XGBoost. Among them, random forest regression model performed best in both training set (mean absolute error = 3.68 years, r = 0.96) and testing set (MAE = 6.840 years, r = 0.77). Models using one single type of predictors, circular RNAs-only or microRNAs-only, result in bigger errors. Smaller prediction errors were shown in males than females when constructing models according to different-sex separately. Putative microRNA targets (430 genes) were enriched in the cellular senescence pathway and cell homeostasis and cell differentiation regulation, indirectly indicating that the microRNAs screened in our study were correlated with development and aging. This study demonstrates that the noncoding RNA aging clock has potential in predicting chronological age and will be an available biological marker in routine forensic investigation to predict the age of biological samples. Copyright © 2022 Wang, Zhang, Wang, Fu, Wang, Li and Cong.,Ageing, a sedentary lifestyle, and obesity are associated with increased oxidative stress, while regular exercise is associated with an increased antioxidant capacity in trained skeletal muscles. Whether a higher aerobic fitness is associated with increased expression of antioxidant enzymes and their regulatory factors in skeletal muscle remains unknown. Although oestrogens could promote a higher antioxidant capacity in females, it remains unknown whether a sex dimorphism exists in humans regarding the antioxidant capacity of skeletal muscle. Thus, the aim was to determine the protein expression levels of the antioxidant enzymes SOD1, SOD2, catalase and glutathione reductase (GR) and their regulatory factors Nrf2 and Keap1 in 189 volunteers (120 males and 69 females) to establish whether sex differences exist and how age, VO2max and adiposity influence these. For this purpose, vastus lateralis muscle biopsies were obtained in all participants under resting and unstressed conditions. No significant sex differences in Nrf2, Keap1, SOD1, SOD2, catalase and GR protein expression levels were observed after accounting for VO2max, age and adiposity differences. Multiple regression analysis indicates that the VO2max in mL.kg LLM?1.min?1can be predicted from the levels of SOD2, Total Nrf2 and Keap1 (R = 0.58, P < 0.001), with SOD2 being the main predictor explaining 28 % of variance in VO2max, while Nrf2 and Keap1 explained each around 3 % of the variance. SOD1 protein expression increased with ageing in the whole group after accounting for differences in VO2max and body fat percentage. Overweight and obesity were associated with increased pSer40-Nrf2, pSer40-Nrf2/Total Nrf2 ratio and SOD1 protein expression levels after accounting for differences in age and VO2max. Overall, at the population level, higher aerobic fitness is associated with increased basal expression of muscle antioxidant enzymes, which may explain some of the benefits of regular exercise. © 2023 The Authors,Background: People age at different rates. Biological age is a risk factor for many chronic diseases independent of chronological age. A good lifestyle is known to improve overall health, but its association with biological age is unclear. Methods: This study included participants from the UK Biobank who had undergone 12-lead resting electrocardiography (ECG). Biological age was estimated by a deep learning model (defined as ECG-age), and the difference between ECG-age and chronological age was defined as ?age. Participants were further categorized into an ideal (score 4), intermediate (scores 2 and 3) or unfavorable lifestyle (score 0 or 1). Four lifestyle factors were investigated, including diet, alcohol consumption, physical activity, and smoking. Linear regression models were used to examine the association between lifestyle factors and ?age, and the models were adjusted for sex and chronological age. Results: This study included 44,094 individuals (mean age 64 ± 8, 51.4% females). A significant correlation was observed between predicted biological age and chronological age (correlation coefficient = 0.54, P < 0.001) and the mean ?age (absolute error of biological age and chronological age) was 9.8 ± 7.4 years. ?age was significantly associated with all of the four lifestyle factors, with the effect size ranging from 0.41 ± 0.11 for the healthy diet to 2.37 ± 0.30 for non-smoking. Compared with an ideal lifestyle, an unfavorable lifestyle was associated with an average of 2.50 ± 0.29 years of older predicted ECG-age. Conclusion: In this large contemporary population, a strong association was observed between all four studied healthy lifestyle factors and deaccelerated aging. Our study underscores the importance of a healthy lifestyle to reduce the burden of aging-related diseases. 2023 Zhang, Miao, Wang, Thomas, Ribeiro, Brant, Ribeiro and Lin."
256,255,14,255_cardiovascular_predicting_datamart_mortality,"cardiovascular,predicting,datamart,mortality,hospitalized,hospital,atrial,clinical,health,hypertension","A heart failure (HF) condition is a type of chronic cardiovascular disease that affects millions of people globally. It can lead to various symptoms and has a significant impact on the quality of life. Despite the advancements that have been made in treati ng this condition, it remains a major public health issue. One of the biggest challenges that HF management faces is the high number of readmissions. This issue contributes to the increasing of patients' outcomes and costs the healthcare system. Implementing effective interventions and identifying those at high risk of returning to the hospital can help lower the financial burden on the system. Through the use of machine learning techniques, researchers can now predict the likelihood of HF readmissions. These tools can analyze large datasets and provide a personalized diagnosis and treatment plan. There have been various studies that have examined the use of ML for predicting HF readmissions. The goal of this study is to analyze the various techniques used in predicting HF readmissions and provide a comprehensive analysis of their performance. Through a combination of data collected from various sources, including a diverse set of patients, we will be able to explore the performance of various ML algorithms. In addition to the algorithms' performance, we will also look into their impact on various parameters, such as model evaluation metrics, optimization techniques, and feature selection. The findings of this study will be used to inform policymakers and healthcare providers about the use of ML techniques to identify patients at high risk of HF readmissions. These insights can help them improve the quality of care for those with this condition and develop effective interventions. The objective of this study is to use the power of ML to improve the management of HF and reduce the burden of readmissions on both the patients and the healthcare systems. © 2024, Ismail Saritas. All rights reserved.,Background: Heart failure (HF) is a multifaceted clinical syndrome characterized by different etiologies, risk factors, comorbidities, and a heterogeneous clinical course. The current model, based on data from clinical trials, is limited by the biases related to a highly-selected sample in a protected environment, constraining the applicability of evidence in the real-world scenario. If properly leveraged, the enormous amount of data from real-world may have a groundbreaking impact on clinical care pathways. We present, here, the development of an HF DataMart framework for the management of clinical and research processes. Methods: Within our institution, Fondazione Policlinico Universitario A. Gemelli in Rome (Italy), a digital platform dedicated to HF patients has been envisioned (GENERATOR HF DataMart), based on two building blocks: 1. All retrospective information has been integrated into a multimodal, longitudinal data repository, providing in one single place the description of individual patients with drill-down functionalities in multiple dimensions. This functionality might allow investigators to dynamically filter subsets of patient populations characterized by demographic characteristics, biomarkers, comorbidities, and clinical events (e.g., re-hospitalization), enabling agile analyses of the outcomes by subsets of patients. 2. With respect to expected long-term health status and response to treatments, the use of the disease trajectory toolset and predictive models for the evolution of HF has been implemented. The methodological scaffolding has been constructed in respect of a set of the preferred standards recommended by the CODE-EHR framework. Results: Several examples of GENERATOR HF DataMart utilization are presented as follows: to select a specific retrospective cohort of HF patients within a particular period, along with their clinical and laboratory data, to explore multiple associations between clinical and laboratory data, as well as to identify a potential cohort for enrollment in future studies; to create a multi-parametric predictive models of early re-hospitalization after discharge; to cluster patients according to their ejection fraction (EF) variation, investigating its potential impact on hospital admissions. Conclusion: The GENERATOR HF DataMart has been developed to exploit a large amount of data from patients with HF from our institution and generate evidence from real-world data. The two components of the HF platform might provide the infrastructural basis for a combined patient support program dedicated to continuous monitoring and remote care, assisting patients, caregivers, and healthcare professionals. 2023 D'Amario, Laborante, Delvinoti, Lenkowicz, Iacomini, Masciocchi, Luraschi, Damiani, Rodolico, Restivo, Ciliberti, Paglianiti, Canonico, Paternello, Cesario, Valentini, Scambia and Crea.,Background: Most risk prediction models are confined to specific medical conditions, thus limiting their application to general medical populations. Objectives: The MARKER-HF (Machine learning Assessment of RisK and EaRly mortality in Heart Failure) risk model was developed in heart failure (HF) patients. We assessed the ability of MARKER-HF to predict 1-year mortality in a large community-based hospital registry database including patients with and without HF. Methods: This study included 41,749 consecutive patients who underwent echocardiography in a tertiary referral hospital (4,640 patients with and 37,109 without HF). Patients without HF were further subdivided into those with (n = 22,946) and without cardiovascular disease (n = 14,163) and also into cohorts based on recent acute coronary syndrome or history of atrial fibrillation, chronic obstructive pulmonary disease, chronic kidney disease, diabetes mellitus, hypertension, or malignancy. Results: The median age of the 41,749 patients was 65 years, and 56.2% were male. The receiver operated area under the curves for MARKER-HF prediction of 1-year mortality of patients with HF was 0.729 (95% CI: 0.706-0.752) and for patients without HF was 0.770 (95% CI: 0.760-0.780). MARKER-HF prediction of mortality was consistent across subgroups with and without cardiovascular disease and in patients diagnosed with acute coronary syndrome, atrial fibrillation, chronic obstructive pulmonary disease, chronic kidney disease, diabetes mellitus, or hypertension. Patients with malignancy demonstrated higher mortality at a given MARKER-HF score than did patients in the other groups. Conclusions: MARKER-HF predicts mortality for patients with HF as well as for patients suffering from a variety of diseases. © 2023 The Authors"
257,256,13,256_bayesian_mcmc_models_probabilistic,"bayesian,mcmc,models,probabilistic,posterior,metropolishastings,inference,sampling,stochastic,monte","We consider the use of Gaussian process (GP) priors for solving inverse problems in a Bayesian framework. As is well known, the computational complexity of GPs scales cubically in the number of datapoints. Here we show that in the context of inverse problems involving integral operators, one faces additional difficulties that hinder inversion on large grids. Furthermore, in that context, covariance matrices can become too large to be stored. By leveraging recent results about sequential disintegrations of Gaussian measures, we are able to introduce an implicit representation of posterior covariance matrices that reduces the memory footprint by only storing low rank intermediate matrices, while allowing individual elements to be accessed on-the-fly without needing to build full posterior covariance matrices. Moreover, it allows for fast sequential inclusion of new observations. These features are crucial when considering sequential experimental design tasks. We demonstrate our approach by computing sequential data collection plans for excursion set recovery for a gravimetric inverse problem, where the goal is to provide fine resolution estimates of high density regions inside the Stromboli volcano, Italy. Sequential data collection plans are computed by extending the weighted integrated variance reduction (wIVR) criterion to inverse problems. Our results show that this criterion is able to significantly reduce the uncertainty on the excursion volume, reaching close to minimal levels of residual uncertainty. Overall, our techniques allow the advantages of probabilistic models to be brought to bear on large-scale inverse problems arising in the natural sciences. Particularly, applying the latest developments in Bayesian sequential experimental design on realistic large-scale problems opens new venues of research at a crossroads between mathematical modelling of natural phenomena, statistical data science, and active learning. © 2023 Society for Industrial and Applied Mathematics and American Statistical Association.,Due to the importance of uncertainty quantification (UQ), the Bayesian approach to inverse problems has recently gained popularity in applied mathematics, physics, and engineering. However, traditional Bayesian inference methods based on Markov chain Monte Carlo (MCMC) tend to be computationally intensive and inefficient for such high-dimensional problems. To address this issue, several methods based on surrogate models have been proposed to speed up the inference process. More specifically, the calibration-emulation-sampling (CES) scheme has been proven to be successful in large dimensional UQ problems. In this work, we propose a novel CES approach for Bayesian inference based on deep neural network models for the emulation phase. The resulting algorithm is computationally more efficient and more robust against variations in the training set. Further, by using an autoencoder (AE) for dimension reduction, we have been able to speed up our Bayesian inference method up to three orders of magnitude. Overall, our method, henceforth called the dimension-reduced emulative autoencoder Monte Carlo (DREAMC) algorithm, is able to scale Bayesian UQ up to thousands of dimensions for inverse problems. Using two low-dimensional (linear and nonlinear) inverse problems, we illustrate the validity of this approach. Next, we apply our method to two high-dimensional numerical examples (elliptic and advection-diffusion) to demonstrate its computational advantages over existing algorithms. © 2022 Society for Industrial and Applied Mathematics and American Statistical Association.,Bayesian neural networks harness the power of Bayesian inference which provides an approach to neural learning that not only focuses on accuracy but also uncertainty quantification. Markov Chain Monte Carlo (MCMC) methods implement Bayesian inference by sampling from the posterior distribution of the model parameters. In the case of Bayesian neural networks, the model parameters refer to weights and biases. MCMC methods suffer from scalability issues in large models, such as deep neural networks with thousands to millions of parameters. In this paper, we present a Bayesian ensemble learning framework that utilizes gradient boosting by combining multiple shallow neural networks (base learners) that are trained by MCMC sampling. We present two Bayesian gradient boosting strategies that employ simple neural networks as base learners with Langevin MCMC sampling. We evaluate the performance of these methods on various classification and time-series prediction problems. We demonstrate that the proposed framework improves the prediction accuracy of canonical gradient boosting while providing uncertainty quantification via Bayesian inference. Furthermore, we demonstrate that the respective methods scale well when the size of the dataset and model increases. © 2023 The Author(s)"
258,257,13,257_monitoring_industrial_processes_autoencoder,"monitoring,industrial,processes,autoencoder,sensor,features,dynamic,process,pdm,pcselm","Reliable monitoring of mineral process systems is key to more efficient plant operation. Multivariate statistical process control based on principal component analysis is well-established in industry but may not be effective when dealing with dynamic nonlinear or transient processes, where process behavior may change rapidly from time to time. Although a large variety of nonlinear models have been proposed to address these problems, the monitoring of complex dynamic process systems remains challenging. Isolation forests are unsupervised machine learning models that provide an interesting approach to process monitoring that has not been explored extensively yet. In this investigation, this approach is compared with traditional multivariate statistical process monitoring based on principal component models. Three real-world case studies are considered. The first case study is based on coal flotation, the second is based on features extracted from a platinum group metal flotation froth; and the third is based on data from an industrial semi-autogenous grinding circuit. In each case, the models were trained on data representing normal operating conditions and then tested on new process data that were generally different from the training data to test their ability to identify these data as out-of-control. The isolation forest models performed better than the principal component models when the data were nonlinear, but not when the data associated with normal operation and faulty conditions were linearly separable, as was the case with the flotation data. © 2024 by the authors.,With the development of information and communication technologies, industrial cyber-physical systems (ICPSs) have accumulated a large amount of data, which enables us to convert data into industrial insight. However, since the industrial process of ICPS is always complicated and large scale, the raw data only contain a few operation condition information, which brings challenges to process monitoring and control. Thus, an efficient operation condition division method for ICPS is necessary. Although many operation condition division methods have been proposed, they were mainly relying on the static characteristics and ignored how the industrial process varies dynamically. Meanwhile, with the industrial process running, there may exist some new operation conditions that make the operation condition division task even more difficult. In order to grasp the static and dynamic features simultaneously of the industrial process and divide operation conditions accurately, we proposed an operation condition division method based on joint static and dynamic analysis with incremental learning. In detail, the slow feature analysis (SFA) and self-organizing map (SOM) network were proposed to extract the static and dynamic features jointly. Then, a division strategy was proposed to distinguish the operation condition changing points. For the new operating condition, we designed an incremental learning method based on the SOM network, which can update the operation condition model in real time. Extensive experiments, including a numerical simulation, two benchmark processes, and an industrial roasting process demonstrate that the proposed method can identify the operation conditions of the raw data in ICPS accurately and efficiently.  © 2014 IEEE.,This paper introduces a novel sparse dynamic inner principal component analysis (SDiPCA) based monitoring for multimode dynamic processes. Different from traditional multimode monitoring algorithms, a model is updated for sequential modes by memorizing the significant features of existing modes. By adopting the concept of intelligent synapses in continual learning, a loss of quadratic term is introduced to penalize the changes of mode-relevant parameters, where modified synaptic intelligence (MSI) is proposed to estimate the parameter importance. Thus, the proposed algorithm is referred to as SDiPCA-MSI. When a new mode arrives, a set of normal samples should be collected. The previous significant features are consolidated without explicitly storing training samples, while extracting new information from the current mode. Consequently, SDiPCA-MSI can provide outstanding performance for successive modes. Characteristics of the proposed approach are discussed, including the computational complexity, advantages and potential limitations. Compared with several state-of-the-art monitoring methods, the effectiveness and superiorities of the proposed method are demonstrated by a continuous stirred tank heater case and a practical industrial system. Note to Practitioners - Multimode process monitoring is increasingly significant as industrial systems generally operate in varying operating conditions. However, most researches focus on multiple local monitoring models for complex multimode processes and assume that data of all possible modes are available and stored before learning. When similar or new modes arrive, local models are rebuilt corresponding to each mode and the model's capacity would increase with the continuous emergence of modes. Adaptive methods are a branch of multimode monitoring algorithms, but they strive to extract information of the current mode to ensure the monitoring performance while forgetting the previously learned knowledge gradually. This paper proposes a novel sparse dynamic inner principal component analysis with continual learning ability for multimode dynamic process monitoring, where modified synaptic intelligence is developed to measure the parameter importance accurately. It requires limited computation and storage resources for successive modes, which is convenient for practical applications. Similar to current multimode process monitoring algorithms, a set of data should be collected before learning a new mode, which may bring difficulties to real-time monitoring. For industrial systems, such as large-scale power plants and chemical systems, the proposed method has outstanding ability to monitor successive dynamic modes.  © 2004-2012 IEEE."
259,258,13,258_tokamak_disruption_tokamaks_disruptions,"tokamak,disruption,tokamaks,disruptions,fusion,predictor,prediction,plasma,future,plasmas","This paper reports on advances in the state-of-the-art deep learning disruption prediction models based on the Fusion Recurrent Neural Network (FRNN) originally introduced in a 2019 NATURE publication [https://doi.org/10.1038/s41586-019-1116-4]. In particular, the predictor now features not only the “disruption score,” as an indicator of the probability of an imminent disruption, but also a “sensitivity score” in real time to indicate the underlying reasons for the imminent disruption. This adds valuable physics interpretability for the deep learning model and can provide helpful guidance for control actuators now implemented into a modern plasma control system (PCS). The advance is a significant step forward in moving from modern deep learning disruption prediction to real-time control and brings novel AI-enabled capabilities relevant for application to the future burning plasma ITER system. Our analyses use large amounts of data from JET and DIII-D vetted in the earlier NATURE publication. In addition to “when” a shot is predicted to disrupt, this paper addresses reasons “why” by carrying out sensitivity studies. FRNN is accordingly extended to use more channels of information, including measured DIII-D signals such as (i) the “n1rms” signal that is correlated with the n = 1 modes with finite frequency, including neoclassical tearing mode and sawtooth dynamics; (ii) the bolometer data indicative of plasma impurity control; and (iii) “q-min”—the minimum value of the safety factor relevant to the key physics of kink modes. The additional channels and interpretability features expand the ability of the deep learning FRNN software to provide information about disruption subcategories as well as more precise and direct guidance for the actuators in a PCS. © 2023 The Authors. Contributions to Plasma Physics published by Wiley-VCH GmbH.,Predicting disruptions across different tokamaks is necessary for next generation device. Future large-scale tokamaks can hardly tolerate disruptions at high performance discharge, which makes it difficult for current data-driven methods to obtain an acceptable result. A machine learning method capable of transferring a disruption prediction model trained on one tokamak to another is required to solve the problem. The key is a feature extractor which is able to extract common disruption precursor traces in tokamak diagnostic data, and can be easily transferred to other tokamaks. Based on the concerns above, this paper presents a deep feature extractor, namely, the fusion feature extractor (FFE), which is designed specifically for extracting disruption precursor features from common diagnostics on tokamaks. Furthermore, an FFE-based disruption predictor on J-TEXT is demonstrated. The feature extractor is aimed to extracting disruption-related precursors and is designed according to the precursors of disruption and their representations in common tokamak diagnostics. Strong inductive bias on tokamak diagnostics data is introduced. The paper presents the evolution of the neural network feature extractor and its comparison against general deep neural networks, as well as a physics-based feature extraction with a traditional machine learning method. Results demonstrate that the FFE may reach a similar effect with physics-guided manual feature extraction, and obtain a better result compared with other deep learning methods. © 2023 Chinese Physical Society and IOP Publishing Ltd.,The ability to identify underlying disruption precursors is key to disruption avoidance. In this paper, we present an integrated deep learning (DL) based model that combines disruption prediction with the identification of several disruption precursors like rotating modes, locked modes, H-to-L back transitions and radiative collapses. The first part of our study demonstrates that the DL-based unstable event identifier trained on 160 manually labeled DIII-D shots can achieve, on average, 84% event identification rate of various frequent unstable events (like H-L back transition, locked mode, radiative collapse, rotating MHD mode, large sawtooth crash), and the trained identifier can be adapted to label unseen discharges, thus expanding the original manually labeled database. Based on these results, the integrated DL-based framework is developed using a combined database of manually labeled and automatically labeled DIII-D data, and it shows state-of-the-art (AUC = 0.940) disruption prediction and event identification abilities on DIII-D. Through cross-machine numerical disruption prediction studies using this new integrated model and leveraging the C-Mod, DIII-D, and EAST disruption warning databases, we demonstrate the improved cross-machine disruption prediction ability and extended warning time of the new model compared with a baseline predictor. In addition, the trained integrated model shows qualitatively good cross-machine event identification ability. Given a labeled dataset, the strategy presented in this paper, i.e. one that combines a disruption predictor with an event identifier module, can be applied to upgrade any neural network based disruption predictor. The results presented here inform possible development strategies of machine learning based disruption avoidance algorithms for future tokamaks and highlight the importance of building comprehensive databases with unstable event information on current machines. © 2023 The Author(s). Published on behalf of IAEA by IOP Publishing Ltd."
260,259,13,259_vegetation_ecosystem_ecosystems_co2,"vegetation,ecosystem,ecosystems,co2,forest,photosynthesis,climate,soil,carbon,models","Estimating gross primary productivity (GPP) is important for simulating the subsequent carbon cycle elements and assessing the capacity of terrestrial ecosystems to support the sustainable development of human society. Light use efficiency (LUE) models were widely used to estimate GPP due to their concise model structures. However, quantifying LUEmax (maximum light use efficiency) and representing the responses of photosynthesis to environmental factors are still subject to large uncertainties, which lead to substantial errors in GPP simulations. In this study, we developed a hybrid model based on machine learning and a LUE model for GPP estimates. This hybrid model was built by targeting LUE with a machine learning approach, namely multi-layer perceptron (MLP), and then, estimating GPP within a LUE model framework with the MLP-based LUE and other required inputs. We trained the hybrid LUE (H-LUE) model and then, compared it against two conventional LUE models, the vegetation photosynthesis model (VPM) and vegetation photosynthesis and respiration model (VPRM), regarding GPP estimation, using tower-based daily-scale observations from 180 flux sites that cover nine different plant function types (PFTs). The results revealed better performance (R2 = 0.86 and RMSE = 1.79 gC m?2 d?1 on the test dataset) of the H-LUE model compared to the VPM and VPRM. Evaluations of the three models under four different extreme conditions consistently revealed better performance of the H-LUE model, indicating greater adaptability of the model to varied environments in the context of climate change. Furthermore, we also found that the H-LUE model can reasonably represent the responses of the LUE to meteorological variables. Our study revealed the reliable and robust performance of the developed hybrid LUE when simulating GPP across global biomes, providing references for developing better hybrid GPP models. © 2023 by the authors.,China's forests play a vital role in the global carbon cycle through the absorption of atmospheric CO2 to mitigate climate change caused by the increase of anthropogenic CO2. It is essential to evaluate the carbon sink potential (CSP) of China's forest ecosystem. Combining NDVI, field-investigated, and vegetation and soil carbon density data modeled by process-based models, we developed the state-of-the-art learning ensembles model of process-based models (the multi-model random forest ensemble (MMRFE) model) to evaluate the carbon stocks of China's forest ecosystem in historical (1982–2021) and future (2022–2081, without NDVI-driven data) periods. Meanwhile, we proposed a new carbon sink index (CSindex) to scientifically and accurately evaluate carbon sink status and identify carbon sink intensity zones, reducing the probability of random misjudgments as a carbon sink. The new MMRFE models showed good simulation results in simulating forest vegetation and soil carbon density in China (significant positive correlation with the observed values, r = 0.94, P < 0.001). The modeled results show that a cumulative increase of 1.33 Pg C in historical carbon stocks of forest ecosystem is equivalent to 48.62 Bt CO2, which is approximately 52.03% of the cumulative increased CO2 emissions in China from 1959 to 2018. In the next 60 years, China's forest ecosystem will absorb annually 1.69 (RCP45 scenario) to 1.85 (RCP85 scenario) Bt CO2. Compared with the carbon stock in the historical period, the cumulative absorption of CO2 by China's forest ecosystem in 2032–2036, 2062–2066, and 2077–2081 are approximately 11.25–39.68, 110.66–121.49 and 101.31–111.11 Bt CO2, respectively. In historical and future periods, the medium and strong carbon sink intensity regions identified by the historical CSindex covered 65% of the total forest area, cumulative absorbing approximately 31.60 and 65.83–72.22 Bt CO2, respectively. In the future, China's forest ecosystem has a large CSP with a non-continuous increasing trend. However, the CSP should not be underestimated. Notably, the medium carbon sink intensity region should be the priority for natural carbon sequestration action. This study not only provides an important methodological basis for accurately estimating the future CSP of forest ecosystem but also provides important decision support for future forest ecosystem carbon sequestration action. © 2023 The Authors,Terrestrial gross primary production (GPP) represents the magnitude of CO2 uptake through vegetation photosynthesis, and is a key variable for carbon cycles between the biosphere and atmosphere. Light use efficiency (LUE) models have been widely used to estimate GPP for its physiological mechanisms and availability of data acquisition and implementation, yet each individual GPP model has exhibited large uncertainties due to input errors and model structure, and further studies of systematic validation, comparison, and fusion of those models with eddy covariance (EC) site data across diverse ecosystem types are still needed in order to further improve GPP estimation. We here compared and fused five GPP models (VPM, EC-LUE, GOL-PEM, CHJ, and C-Fix) across eight ecosystems based on FLUXNET2015 data set using the ensemble methods of Bayesian Model Averaging (BMA), Support Vector Machine (SVM), and Random Forest (RF) separately. Our results showed that for individual models, EC-LUE gave a better performance to capture interannual variability of GPP than other models, followed by VPM and GLO-PEM, while CHJ and C-Fix were more limited in their estimation performance. We found RF and SVM were superior to BMA on merging individual models at various plant functional types (PFTs) and at the scale of individual sites. On the basis of individual models, the fusion methods of BMA, SVM, and RF were examined by a five-fold cross validation for each ecosystem type, and each method successfully improved the average accuracy of estimation by 8%, 18%, and 19%, respectively. © 2023. American Geophysical Union. All Rights Reserved."
261,260,13,260_microrna_micrornas_mirnas_mirna,"microrna,micrornas,mirnas,mirna,rnas,pirna,lncrna,lncrnas,mirnagene,ncrna","Determining microRNA (miRNA)-disease associations (MDAs) is an integral part in the prevention, diagnosis, and treatment of complex diseases. However, wet experiments to discern MDAs are inefficient and expensive. Hence, the development of reliable and efficient data integrative models for predicting MDAs is of significant meaning. In the present work, a novel deep learning method for predicting MDAs through deep autoencoder with multiple kernel learning (DAEMKL) is presented. Above all, DAEMKL applies multiple kernel learning (MKL) in miRNA space and disease space to construct miRNA similarity network and disease similarity network, respectively. Then, for each disease or miRNA, its feature representation is learned from the miRNA similarity network and disease similarity network via the regression model. After that, the integrated miRNA feature representation and disease feature representation are input into deep autoencoder (DAE). Furthermore, the novel MDAs are predicted through reconstruction error. Ultimately, the AUC results show that DAEMKL achieves outstanding performance. In addition, case studies of three complex diseases further prove that DAEMKL has excellent predictive performance and can discover a large number of underlying MDAs. On the whole, our method DAEMKL is an effective method to identify MDAs. © 2021 IEEE.,Background: A large number of experiments show that the abnormal expression of miRNA is closely related to the occurrence, diagnosis and treatment of diseases. Identifying associations between miRNAs and diseases is important for clinical applications of complex human diseases. However, traditional biological experimental methods and calculation-based methods have many limitations, which lead to the development of more efficient and accurate deep learning methods for predicting miRNA-disease associations. Results: In this paper, we propose a novel model on the basis of adaptive deep propagation graph neural network to predict miRNA-disease associations (ADPMDA). We first construct the miRNA-disease heterogeneous graph based on known miRNA-disease pairs, miRNA integrated similarity information, miRNA sequence information and disease similarity information. Then, we project the features of miRNAs and diseases into a low-dimensional space. After that, attention mechanism is utilized to aggregate the local features of central nodes. In particular, an adaptive deep propagation graph neural network is employed to learn the embedding of nodes, which can adaptively adjust the local and global information of nodes. Finally, the multi-layer perceptron is leveraged to score miRNA-disease pairs. Conclusion: Experiments on human microRNA disease database v3.0 dataset show that ADPMDA achieves the mean AUC value of 94.75% under 5-fold cross-validation. We further conduct case studies on the esophageal neoplasm, lung neoplasms and lymphoma to confirm the effectiveness of our proposed model, and 49, 49, 47 of the top 50 predicted miRNAs associated with these diseases are confirmed, respectively. These results demonstrate the effectiveness and superiority of our model in predicting miRNA-disease associations.  © 2023 The Author(s). Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.,MicroRNAs (miRNAs) are an important class of non-coding RNAs that play an essential role in the occurrence and development of various diseases. Identifying the potential miRNA-disease associations (MDAs) can be beneficial in understanding disease pathogenesis. Traditional laboratory experiments are expensive and time-consuming. Computational models have enabled systematic large-scale prediction of potential MDAs, greatly improving the research efficiency. With recent advances in deep learning, it has become an attractive and powerful technique for uncovering novel MDAs. Consequently, numerous MDA prediction methods based on deep learning have emerged. In this review, we first summarize publicly available databases related to miRNAs and diseases for MDA prediction. Next, we outline commonly used miRNA and disease similarity calculation and integration methods. Then, we comprehensively review the 48 existing deep learning-based MDA computation methods, categorizing them into classical deep learning and graph neural network-based techniques. Subsequently, we investigate the evaluation methods and metrics that are frequently used to assess MDA prediction performance. Finally, we discuss the performance trends of different computational methods, point out some problems in current research, and propose 9 potential future research directions. Data resources and recent advances in MDA prediction methods are summarized in the GitHub repository <uri>https://github.com/sheng-n/DL-miRNA-disease-association-methods</uri>. IEEE"
262,261,13,261_neuron_neuronal_synapses_spiking,"neuron,neuronal,synapses,spiking,neurons,synaptic,neural,spiketiming,spiketimingdependent,memory","Neural coding and memory formation depend on temporal spiking sequences that span high-dimensional neural ensembles. The unsupervised discovery and characterization of these spiking sequences requires a suitable dissimilarity measure to spiking patterns, which can then be used for clustering and decoding. Here, we present a new dissimilarity measure based on optimal transport theory called SpikeShip, which compares multi-neuron spiking patterns based on all the relative spike-timing relationships among neurons. SpikeShip computes the optimal transport cost to make all the relative spike-timing relationships (across neurons) identical between two spiking patterns. We show that this transport cost can be decomposed into a temporal rigid translation term, which captures global latency shifts, and a vector of neuron-specific transport flows, which reflect inter-neuronal spike timing differences. SpikeShip can be effectively computed for high-dimensional neuronal ensembles, has a low (linear) computational cost that has the same order as the spike count, and is sensitive to higher-order correlations. Furthermore, SpikeShip is binless, can handle any form of spike time distributions, is not affected by firing rate fluctuations, can detect patterns with a low signal-to-noise ratio, and can be effectively combined with a sliding window approach. We compare the advantages and differences between SpikeShip and other measures like SPIKE and Victor-Purpura distance. We applied SpikeShip to large-scale Neuropixel recordings during spontaneous activity and visual encoding. We show that high-dimensional spiking sequences detected via SpikeShip reliably distinguish between different natural images and different behavioral states. These spiking sequences carried complementary information to conventional firing rate codes. SpikeShip opens new avenues for studying neural coding and memory consolidation by rapid and unsupervised detection of temporal spiking patterns in high-dimensional neural ensembles. Copyright: © 2023 Sotomayor-Gómez et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.,Attractor networks are an influential theory for memory storage in brain systems. This theory has recently been challenged by the observation of strong temporal variability in neuronal recordings during memory tasks. In this work, we study a sparsely connected attractor network where memories are learned according to a Hebbian synaptic plasticity rule. After recapitulating known results for the continuous, sparsely connected Hopfield model, we investigate a model in which new memories are learned continuously and old memories are forgotten, using an online synaptic plasticity rule. We show that for a forgetting timescale that optimizes storage capacity, the qualitative features of the network's memory retrieval dynamics are age dependent: most recent memories are retrieved as fixed-point attractors while older memories are retrieved as chaotic attractors characterized by strong heterogeneity and temporal fluctuations. Therefore, fixed-point and chaotic attractors coexist in the network phase space. The network presents a continuum of statistically distinguishable memory states, where chaotic fluctuations appear abruptly above a critical age and then increase gradually until the memory disappears. We develop a dynamical mean field theory to analyze the age-dependent dynamics and compare the theory with simulations of large networks. We compute the optimal forgetting timescale for which the number of stored memories is maximized. We found that the maximum age at which memories can be retrieved is given by an instability at which old memories destabilize and the network converges instead to a more recent one. Our numerical simulations show that a high degree of sparsity is necessary for the dynamical mean field theory to accurately predict the network capacity. To test the robustness and biological plausibility of our results, we study numerically the dynamics of a network with learning rules and transfer function inferred from in vivo data in the online learning scenario. We found that all aspects of the network's dynamics characterized analytically in the simpler model also hold in this model. These results are highly robust to noise. Finally, our theory provides specific predictions for delay response tasks with aging memoranda. In particular, it predicts a higher degree of temporal fluctuations in retrieval states associated with older memories, and it also predicts fluctuations should be faster in older memories. Overall, our theory of attractor networks that continuously learn new information at the price of forgetting old memories can account for the observed diversity of retrieval states in the cortex, and in particular, the strong temporal fluctuations of cortical activity.  © 2023 authors. Published by the American Physical Society.,Memories may be encoded in the brain via strongly interconnected groups of neurons, called assemblies. The concept of Hebbian plasticity suggests that these assemblies are generated through synaptic plasticity, strengthening the recurrent connections within select groups of neurons that receive correlated stimulation. To remain stable in absence of such stimulation, the assemblies need to be self-reinforcing under the plasticity rule. Previous models of such assembly maintenance require additional mechanisms of fast homeostatic plasticity often with biologically implausible timescales. Here we provide a model of neuronal assembly generation and maintenance purely based on spike-timing-dependent plasticity (STDP) between excitatory neurons. It uses irregularly and stochastically spiking neurons and STDP that depresses connections of uncorrelated neurons. We find that assemblies do not grow beyond a certain size, because temporally imprecisely correlated spikes dominate the plasticity in large assemblies. Assemblies in the model can be learned or spontaneously emerge. The model allows for prominent, stable overlap structures between static assemblies. Further, assemblies can drift, particularly according to a novel, transient overlap-based mechanism. Finally the model indicates that assemblies grow in the aging brain, where connectivity decreases. © 2023 Manz, Memmesheimer. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
263,262,13,262_logistics_optimization_scheduling_warehouses,"logistics,optimization,scheduling,warehouses,routes,transportation,algorithm,heuristic,cargo,routing","With the development of the logistics economy, problems such as the timeliness of logistics distribution and the high cost of distribution have emerged. A new adaptive genetic algorithm is proposed to solve these problems. The pc and pm values of the algorithm are related to the number of iterations and the individual fitness values. To improve the local optimization ability of the algorithm, a large neighborhood search algorithm is proposed. In addition, this study establishes a soft time window town logistics distribution model with constraints. The model considers the optimal cost as the objective function and customer satisfaction as the influencing factor. In the experiment, the proposed adaptive genetic algorithm is compared with the traditional genetic algorithm, validating the effectiveness of the proposed algorithm. © 2022,To meet the demands of green logistics while considering the time-dependent effects caused by traffic congestion, we establish a time-dependent green vehicle routing problem with time windows model for cold chain logistics. This model aims to minimize the total cost, including the transportation cost, refrigeration cost, carbon emission cost, and labor cost. Vehicles are allowed to wait to avoid a bad traffic environment after completing their services to customers. To solve the model, we develop a two-stage hybrid search algorithm. In the first stage of this algorithm, an adaptive large neighborhood search technique is used to determine the vehicle route, while in the second stage, a shortest-path algorithm is used to determine the departure time of the vehicles from customer's node. Finally, numerical experiments are performed to verify the effectiveness and superiority of our model and the proposed hybrid search algorithm by comparing with the standard instances and large-scale instances. © 2022 Elsevier Ltd,Featured Application: This paper studies the joint optimization of the threshold setting of large parcels, the location selection of multi-functional transit centers, and the allocation of terminal demand points under the implementation of multi-product (parcel) distribution by urban logistics enterprises, which is a new extension of the urban logistics site selection-path problem. Through this article, constructive suggestions are put forward for the future development of large-scale businesses in the logistics field, and other logistics companies can make a comprehensive assessment according to their own needs and conditions. Aiming at the problems of low resource utilization and high distribution cost of urban logistics enterprises, this paper introduces the threshold setting of large parcels, comprehensively considers the processing links of large parcels and standard parcels in loading, unloading, sorting, and other processing links, and constructs a logistics planning model with the type of multi-functional transit center as the variable and the total cost of the logistics system as the goal. Aiming at the shortcomings of the honey badger algorithm, three optimization strategies are used to improve the logistics model, and the effectiveness of the improved algorithm is verified by comparing with the CPLEX operation results. Based on the operation data of SF Jinzhou, this paper obtains the optimization results of large parcel threshold, multi-function transit center location layout, and terminal demand point allocation. From the results, the introduction of the threshold setting for large parcels has played a significant role in the joint optimization of multi-functional center location selection and terminal demand point allocation under multi-parcel distribution and provides theoretical data support for the existing urban logistics location problem. © 2023 by the authors."
264,263,13,263_cnn_imagenet_convolutional_convolution,"cnn,imagenet,convolutional,convolution,recognition,neural,classification,vision,taskimage,underfitting","Underfitting, model complexity, and resource optimization restrict machine learning's effectiveness on large datasets are hindered by these issues. Deep learning networks can be used to huge volumes of data to uncover new information, anticipate the future, and apply that knowledge. Many deep learning architectures have been created to outperform earlier machine learning approaches as data rises. In computer vision (CNN), the convolutional neural network is a popular deep neural network design. The convolutional layer, pooling layer, activation layer, and linked convolutional layer make up a CNN. To complete operations, a deep CNN uses several connected convolutional layers, 3 × 3 pixels of the input image are permitted past the filter at the convolutional layer. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Image recognition and classification is a significant research topic in computational vision and widely used computer technology. The methods often used in image classification and recognition tasks are based on deep learning, like Convolutional Neural Networks (CNNs), LeNet, and Long Short-Term Memory networks (LSTM). Unfortunately, the classification accuracy of these methods is unsatisfactory. In recent years, using large-scale deep learning networks to achieve image recognition and classification can improve classification accuracy, such as VGG16 and Residual Network (ResNet). However, due to the deep network hierarchy and complex parameter settings, these models take more time in the training phase, especially when the sample number is small, which can easily lead to overfitting. This paper suggested a deep learningbased image classification technique based on a CNN model and improved convolutional and pooling layers. Furthermore, the study adopted the approximate dynamic learning rate update algorithm in the model training to realize the learning rate’s self-adaptation, ensure the model’s rapid convergence, and shorten the training time. Using the proposed model, an experiment was conducted on the Fashion-MNIST dataset, taking 6,000 images as the training dataset and 1,000 images as the testing dataset. In actual experiments, the classification accuracy of the suggested method was 93 percent, 4.6 percent higher than that of the basic CNN model. Simultaneously, the study compared the influence of the batch size of model training on classification accuracy. Experimental outcomes showed this model is very generalized in fashion clothing image classification tasks. © 2023,Journal of Information and Communication Technology.All Rights Reserved.,In recent years, artificial intelligence has gradually become the core driving force of a new round of scientific and technological revolution and industrial transformation, and is exerting a profound impact on all aspects of human life. With the rapid development of Internet big data and high-performance parallel computing, relevant research in computer vision has made significant progress in the past few years, becoming one of the important application branches in the field of artificial intelligence. The exercise of image classification forming part of computer vision tasks involves a large amount of computation, and training based on traditional deep learning (DL) classification models typically involves slow training and low accuracy in many parameters. Thus, in order to solve these problems, an image classification model based on DL and SAE network was proposed. Firstly, the main research of computer vision task-image classification is introduced in detail. Then, the combination framework of deep neural network and SAE network is built. At the same time, the deep neural network was used to carry out convolution operation of the parameters learned by SAE and extract each feature of the image with neurons, so as to improve the training accuracy of the deep neural network. Finally, the traditional deep neural network and SAE network were used for comparative experiment and analysis. Experimental results show that the proposed method has a certain degree of improvement in image classification accuracy compared with traditional deep neural network and SAE network, and the accuracy reaches 97.13%.  © 2023 Shijia Ling et al., published by Sciendo."
265,264,13,264_courts_court_law_documents,"courts,court,law,documents,corpus,document,prosecutors,nlp,retrieval,cases","Legal case retrieval has received increasing attention in recent years. However, compared to ad hoc retrieval tasks, legal case retrieval has its unique challenges. First, case documents are rather lengthy and contain complex legal structures. Therefore, it is difficult for most existing dense retrieval models to encode an entire document and capture its inherent complex structure information. Most existing methods simply truncate part of the document content to meet the input length limit of PLMs, which will lead to information loss. Additionally, the definition of relevance in the legal domain differs from that in the general domain. Previous semantic-based or lexical-based methods fail to provide a comprehensive understanding of the relevance of legal cases. In this article, we propose a Structured Legal case Retrieval (SLR) framework, which incorporates internal and external structural information to address the above two challenges. Specifically, to avoid the truncation of long legal documents, the internal structural information, which is the organization pattern of legal documents, can be utilized to split a case document into segments. By dividing the document-level semantic matching task into segment-level subtasks, SLR can separately process segments using different methods based on the characteristic of each segment. In this way, the key elements of a case document can be highlighted without losing other content information. Second, toward a better understanding of relevance in the legal domain, we investigate the connections between criminal charges appearing in large-scale case corpus to generate a chargewise relation graph. Then, the similarity between criminal charges can be pre-computed as the external structural information to enhance the recognition of relevant cases. Finally, a learning-to-rank algorithm integrates the features collected from internal and external structures to output the final retrieval results. Experimental results on public legal case retrieval benchmarks demonstrate the superior effectiveness of SLR over existing state-of-the-art baselines, including traditional bag-of-words and neural-based methods. Furthermore, we conduct a case study to visualize how the proposed model focuses on key elements and improves retrieval performance. © 2023 Association for Computing Machinery. All rights reserved.,The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal NLP). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models (LLMs), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study (SMS) to bridge this gap.We aim to provide a descriptive statistical analysis of the Legal NLP research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal NLP field.We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing (NLP), understanding the capabilities and limitations of Legal NLP becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal NLP researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns. Authors,Technology has substantially transformed the way legal services operate in many different countries. With a large and complex collection of digitized legal documents, the judiciary system worldwide presents a promising scenario for the development of intelligent tools. In this work, we tackle the challenging task of organizing and summarizing the constantly growing collection of legal documents, uncovering hidden topics, or themes that later can support tasks such as legal case retrieval and legal judgment prediction. Our approach to this problem relies on topic discovery techniques combined with a variety of preprocessing techniques and learning-based vector representations of words, such as Doc2Vec and BERT-like models. The proposed method was validated using four different datasets composed of short and long legal documents in Brazilian Portuguese, from legal decisions to chapters in legal books. Analysis conducted by a team of legal specialists revealed the effectiveness of the proposed approach to uncover unique and relevant topics from large collections of legal documents, serving many purposes, such as giving support to legal case retrieval tools and also providing the team of legal specialists with a tool that can accelerate their work of labeling/tagging legal documents. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
266,265,13,265_sonar_cnn_recognition_underwater,"sonar,cnn,recognition,underwater,acoustic,deep,neural,classification,sea,convolutional","Object detection of continuous sonar image sequences has become an efficient way for underwater environment exploration. However, the task always suffers from the influence of th e complex underwater environment. In particular, the existing algorithms mainly focus on image-based detection and can not balance the detection speed and accuracy in continuous sonar image sequences. To solve the problem, this paper proposes a novel automatic detection algorithm based on deep learning for continuous sonar image sequences. Firstly, the convLSTM (convolution Long Short-Term Memory) is improved to fuse sonar features obtained from the cross-detection model, which are from three aspects: 1) The original convolution is replaced by depthwise separable convolution; 2) The original network input is divided into G groups and processed by group convolution; 3) A connection layer between the Bottleneck convolution layer and output is added to further capture feature information between frames. Then, to fully extract sonar features, a cross-detection network is established by fusing two different feature extraction networks MobileNetV3-Large and MobileNetV3-Small. Finally, we combine thecross-detection network with the improved convLSTM to establish the whole model, which can fully extract and utilize temporal information in continuous sonar image sequences. The experimental results show that the proposed model has effectively improved the detection speed in sonar image sequences at 150 FPS, simultaneously keeping an 85.8% mAP. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The research of underwater sonar image classification is beneficial to the development of marine resources. In recent years, deep neural networks have achieved great success in the fields of image classification and image recognition. However, deep neural networks require a large amount of training data. Due to the difficulty of obtaining sonar image datasets, the existing public sonar datasets are generally small, and sonar image classification can be regarded as a small sample problem. To solve this problem, we designed a deep adaptive sonar image classification network (DASCN) based on deep learning and domain adaptation. The feature extraction module in DASCN extracts multiscale features of images; the attention module learns the importance of different channel features; and the domain adaptation module reduces the difference between the source domain and the target domain. It is worth noting that our DASCN does not require a large number of training samples, and sonar images used for training do not need to be labeled. As demonstrated in comprehensive experiments, the classification accuracy of the DASCN model reached 89.4% on the sonar image dataset. Our DASCN achieves unsupervised accurate classification of small sample sonar images. In addition, our DASCN has good classification results on the Office-31 dataset and the Office Home dataset and has good generalization performance.  © 2023 SPIE and IS&T.,The importance of active sonar is increasing due to the quieting of submarines and the increase in maritime traffic. However, the multipath propagation of sound waves and the low signal-to-noise ratio due to multiple clutter make it difficult to detect, track, and identify underwater targets using active sonar. To solve this problem, machine learning and deep learning techniques that have recently been in the spotlight are being applied, but these techniques require a large amount of data. In order to supplement insufficient active sonar data, methods based on mathematical modeling are primarily utilized. However, mathematical modeling-based methods have limitations in accurately simulating complicated underwater phenomena. Therefore, an artificial intelligence-based sonar signal synthesis technique is proposed in this paper. The proposed method modified the major modules of the Tacotron model, which is widely used in the field of speech synthesis, in order to apply the Tacotron model to the field of sonar signal synthesis. To prove the validity of the proposed method, spectrograms of synthesized sonar signals are analyzed and the mean opinion score was measured. Through the evaluation, we confirmed that the proposed method can synthesize active sonar data similar to the trained one. © 2022 by the authors."
267,266,13,266_svm_svms_classifiers_classifier,"svm,svms,classifiers,classifier,classification,svc,datasets,vector,vectors,labeling","In practical applications, supervised learning algorithms, including support vector machine (SVM), heavily rely on precise labeling to train predictive models. Nonetheless, real-world datasets often comprise mislabeled samples, which can have considerable influence on the performance of these algorithms. On the other hand, SVM suffers from computational costs when facing large-scale datasets. Twin support vector machine (TWSVM) tackles this issue and finds two nonparallel hyperplanes by solving two smaller models compared to SVM such that each one is closer to one of the two classes and is at least a unit distance far away from the samples of the other class. In this paper, to address label noise in datasets, we propose a TWSVM-based mixed-integer programming model for relabeling instances directly, while inheriting the advantages of TWSVM. Each model decides whether the samples of one class should be considered among instances that are as close as possible to its corresponding hyperplane. Therefore, each model exhibits the ability to recognize instances bearing close resemblance to one class while their assigned labels belong to the other one, prompting their reclassification. Conversely, instances demonstrating lower similarities to the other class retain their original labels. To show the efficiency of proposed models experiments are conducted on 12 UCI datasets. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.,Support vector machine (SVM) is a popular supervised learning algorithm based on margin maximization. It has a high training cost and does not scale well to a large number of data points. We propose a multiresolution algorithm MRH-SVM that trains SVM on a hierarchical data aggregation structure, which also serves as a common data input to other learning algorithms. The proposed algorithm learns SVM models using high-level data aggregates and only visits data aggregates at more detailed levels where support vectors reside. In addition to performance improvements, the algorithm has advantages such as the ability to handle data streams and datasets with imbalanced classes. Experimental results show significant performance improvements in comparison with existing SVM algorithms. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,In recent years, various studies have been conducted on SVMs and their applications in different area. They have been developed significantly in many areas. SVM is one of the most robust classification and regression algorithms that plays a significant role in pattern recognition. However, SVM has not been developed significantly in some areas like large-scale datasets, unbalanced datasets, and multiclass classification. Efficient SVM training in large-scale datasets is of great importance in the big data era. However, as the number of samples increases, the time and memory required to train SVM increase, making SVM impractical even for a medium-sized problem. With the emergence of big data, this problem becomes more significant. This paper presents a novel distributed method for SVM training in which a very small subset of training samples is used for classification, which reduces the problem size and thus the required memory and computational resources. The solution of this problem almost converges to standard SVM. This method includes three steps: first, detecting a subset of distributed training samples, second, creating local models of SVM and obtaining partial vectors, and finally combining the partial vectors and obtaining the global vector and the final model. In addition, the datasets which suffer from unbalanced number of samples and tend to the majority class, the proposed method balances the samples of the two classes and it can be used in unbalanced datasets. The empirical results show that using this method is efficient for large-scale problems. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
268,267,13,267_landform_landforms_land_classification,"landform,landforms,land,classification,deeplabv3,imagery,dataset,mlctnet,photographs,maps","Spatially explicit information on land cover (LC) is commonly derived using remote sensing, but the lack of training data still remains a major challenge for producing accurate LC products. Here, we develop a computer vision methodology to extract LC information from photos from the Land Use-Land Cover Area Frame Survey (LUCAS). Given the large number of photographs available and the comprehensive spatial coverage, the objective is to show how the automatic classification of photos could be used to develop reference data sets for training and validation of LC products as well as other purposes. We first selected a representative sample of 1120 photos covering eight major LC types across the European Union. We then applied semantic segmentation to these photos using a neural network (Deeplabv3+) trained with the ADE20k dataset. For each photo, we extracted the original LC identified by the LUCAS surveyor, the segmented objects, and the pixel count for each ADE20k class. Using the latter as input features, we then trained a Random Forest model to classify the LC of the photo. Examining the relationship between the objects/features extracted by Deeplabv3+ and the LC labels provided by the LUCAS surveyors demonstrated how the LC classes can be decomposed into multiple objects, highlighting the complexity of LC classification from photographs. The results of the classification show a mean F1 Score of 89%, increasing to 93% when the Wetland class is not considered. Based on these results, this approach holds promise for the automated retrieval of LC information from the rich source of LUCAS photographs as well as the increasing number of geo-referenced photos now becoming available through social media and sites like Mapillary or Google Street View. © 2023 The Authors,Global land cover map provides fundamental information for understanding the relationship between global environmental change and human settlement. With the development of data-driven deep learning theory, semantic segmentation network has largely facilitated the global land cover mapping activity. However, the performance of semantic segmentation network is closely related to the number and quality of training data, and the existing annotation data are usually insufficient in quantity, quality, and spatial resolution, and are usually sampled at local region and lack diversity and variability, making data-driven model difficult to extend to global scale. Therefore, we proposed a large-scale annotation dataset (Globe230k) for semantic segmentation of remote sensing image, which has 3 superiorities: (a) large scale: the Globe230k dataset includes 232,819 annotated images with a size of 512 × 512 and a spatial resolution of 1 m, including 10 first-level categories; (b) rich diversity: the annotated images are sampled from worldwide regions, with coverage area of over 60,000 km2, indicating a high variability and diversity; (c) multimodal: the Globe230k dataset not only contains RGB bands but also includes other important features for Earth system research, such as normalized differential vegetation index (NDVI), digital elevation model (DEM), vertical-vertical polarization (VV) bands, and vertical-horizontal polarization (VH) bands, which can facilitate the multimodal data fusion research. We used the Globe230k dataset to test several state-of-the-art semantic segmentation algorithms and found that it is able to evaluate algorithms in multiple aspects that are crucial for characterizing land covers, including multiscale modeling, detail reconstruction, and generalization ability. The dataset has been made public and can be used as a benchmark to promote further development of global land cover mapping and semantic segmentation algorithm development. Copyright © 2023 Qian Shi et al. Exclusive licensee Aerospace Information Research Institute, Chinese Academy of Sciences. Distributed under a Creative Commons Attribution License 4.0 (CC BY 4.0).,The application of deep learning methods to remote sensing data has produced good results in recent studies. A promising application area is automatic land cover classification (semantic segmentation) from very high-resolution satellite imagery. However, the deep learning methods require large, labelled training datasets that are suitable for the study area. Map data can be used as training data, but it is often insufficiently detailed for very high-resolution satellite imagery. National airborne laser scanner (lidar) datasets provide additional details and are available in many countries. Successful land cover classifications from lidar datasets have been reached, e.g., by object-based image analysis. In the present study, we investigated the feasibility of using airborne laser scanner data and object-based image analysis to automatically generate labelled training data for a deep neural network -based land cover classification of a VHR satellite image. Input data for the object-based classification included digital surface models, intensity and pulse information derived from the lidar data. The resulting land cover classification was then utilized as training data for deep learning. A state-of-the-art deep learning architecture, UnetFormer, was trained and applied to the land cover classification of a WorldView-3 stereo dataset. For the semantic segmentation, three different input data composites were produced using the red, green, blue, NIR and digital surface model bands derived from the satellite data. The quality of the generated training data and the semantic segmentation results was estimated using an independent test set of ground truth points. The results show that final satellite image classification accuracy (94–96%) close to the training data accuracy (97%) was obtained. It was also demonstrated that the resulting maps could be used for land cover change detection. © 2023 The Authors"
269,268,13,268_sensorimotor_stimuli_inattention_saccade,"sensorimotor,stimuli,inattention,saccade,adaptation,perceptual,cortex,movements,memory,attention","Motor adaptation maintains movement accuracy. To evaluate movement accuracy, motor adaptation relies on an error signal, generated by the movement target, while suppressing error signals from irrelevant objects in the vicinity. Previous work used static testing environments, where all information required to evaluate movement accuracy was available simultaneously. Using saccadic eye movements as a model for motor adaptation, we tested how movement accuracy is maintained in dynamic environments, where the availability of conflicting error signals varied over time. Participants made a vertical saccade toward a target (either a small square or a large ring). Upon saccade detection, two candidate stimuli were shown left and right of the target, and participants were instructed to discriminate a feature on one of the candidates. Critically, candidate stimuli were presented sequentially, and saccade adaptation, thus, had to resolve a conflict between a task-relevant and a task-irrelevant error signal that were separated in space and time. We found that the saccade target influenced several aspects of oculomotor learning. In presence of a small target, saccade adaptation evaluated movement accuracy based on the first available error signal after the saccade, irrespective of its task relevance. However, a large target not only allowed for greater flexibility when evaluating movement accuracy, but it also promoted a stronger contribution of strategic behavior when compensating inaccurate saccades. Our results demonstrate how motor adaptation maintains movement accuracy in dynamic environments, and how properties of the visual environment modulate the relative contribution of different learning processes. ©2023 The Authors.,We examined the extent to which intentionally underperforming a goal-directed reaching task impacts how memories of recent performance contribute to sensorimotor adaptation. Healthy human subjects performed computerized cognition testing and an assessment of sensorimotor adaptation, wherein they grasped the handle of a horizontal planar robot while making goal-directed out-and-back reaching movements. The robot exerted forces that resisted hand motion with a spring-like load that changed unpredictably between movements. The robotic test assessed how implicit and explicit memories of sensorimotor performance contribute to the compensation for the unpredictable changes in the hand-held load. After each movement, subjects were to recall and report how far the hand moved on the previous trial (peak extent of the out-and-back movement). Subjects performed the tests under two counter-balanced conditions: one where they performed with their best effort, and one where they intentionally sabotaged (i.e., suppressed) kinematic performance. Results from the computerized cognition tests confirmed that subjects understood and complied with task instructions. When suppressing performance during the robotic assessment, subjects demonstrated marked changes in reach precision, time to capture the target, and reaction time. We fit a set of limited memory models to the data to identify how subjects used implicit and explicit memories of recent performance to compensate for the changing loads. In both sessions, subjects used implicit, but not explicit, memories from the most recent trial to adapt reaches to unpredictable spring-like loads. Subjects did not “give up” on large errors, nor did they discount small errors deemed “good enough”. Although subjects clearly suppressed kinematic performance (response timing, movement variability, and self-reporting of reach error), the relative contributions of sensorimotor memories to trial-by-trial variations in task performance did not differ significantly between the two testing conditions. We conclude that intentional performance suppression had minimal impact on how implicit sensorimotor memories contribute to adaptation of unpredictable mechanical loads applied to the hand. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Integrating sensory information during movement and adapting motor plans over successive movements are both essential for accurate, flexible motor behaviour. When an ongoing movement is off target, feedback control mechanisms update the descending motor commands to counter the sensed error. Over longer timescales, errors induce adaptation in feedforward planning so that future movements become more accurate and require less online adjustment from feedback control processes. Both the degree to which sensory feedback is integrated into an ongoing movement and the degree to which movement errors drive adaptive changes in feedforward motor plans have been shown to scale inversely with sensory uncertainty. However, since these processes have only been studied in isolation from one another, little is known about how they are influenced by sensory uncertainty in real-world movement contexts where they co-occur. Here, we show that sensory uncertainty may impact feedforward adaptation of reaching movements differently when feedback integration is present versus when it is absent. In particular, participants gradually adjust their movements from trial-to-trial in a manner that is well characterised by a slow and consistent envelope of error reduction. Riding on top of this slow envelope, participants exhibit large and abrupt changes in their initial movement vectors that are strongly correlated with the degree of sensory uncertainty present on the previous trial. However, these abrupt changes are insensitive to the magnitude and direction of the sensed movement error. These results prompt important questions for current models of sensorimotor learning under uncertainty and open up new avenues for future exploration in the field. © 2023 Hewitson et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
270,269,13,269_apps_recommender_deepapp_app,"apps,recommender,deepapp,app,mobile,prediction,predict,recommendation,recommendations,browsing","This paper aims to predict a set of apps a user will open on her mobile device in the next time slot. Such an information is essential for many smartphone operations, e.g., app pre-loading and content pre-caching, to improve user experience. However, it is hard to build an explicit model that accurately captures the complex environment context and predicts a set of apps at one time. This paper presents a deep reinforcement learning framework, named as DeepAPP, which learns a model-free predictive neural network from historical app usage data. Meanwhile, an online updating strategy is designed to adapt the predictive network to the time-varying app usage behavior. To transform DeepAPP into a practical deep reinforcement learning system, several challenges are addressed by developing a context representation method for complex contextual environment, a general agent for overcoming data sparsity and a lightweight personalized agent for minimizing the prediction time. Extensive experiments on a large-scale anonymized app usage dataset reveal that DeepAPP provides high accuracy (precision 70.6 percent and recall of 62.4 percent) and reduces the prediction time of the state-of-the-art by 6.58×. A field experiment of 29 participants demonstrates DeepAPP can effectively reduce launch time of apps.  © 2002-2012 IEEE.,While thousands of new mobile applications (i.e., apps) are being added to the major app markets daily, only a small portion of them attain their financial goals and survive in these competitive marketplaces. A key to the quick growth and success of relatively less popular apps is that they should make their way to the limited list of apps recommended to users of already popular apps; however, the focus of the current literature on consumers has created a void of design principles for app developers. In this study, employing a predictive network analytics approach combined with deep learning-based natural language processing and explainable artificial intelligence techniques, we shift the focus from consumers and propose a developer-oriented recommender model. We employ a set of app-specific and network-driven variables to present a novel approach for predicting potential recommendation relationships among apps, which enables app developers and marketers to characterize and target appropriate consumers. We validate the proposed model using a large (>23,000), longitudinal dataset of medical apps collected from the iOS App Store at two time points. From a total of 10,234 network links (recommendations) formed between the two data collection points, the proposed approach was able to correctly predict 8,780 links (i.e., 85.8 %). We perform Shapley Additive exPlanation (SHAP) analysis to identify the most important determinants of link formations and provide insights for the app developers about the factors and design principles they can incorporate into their development process to maximize the chances of success for their apps. © 2023 Elsevier Inc.,It is extremely difficult to find one app in app stores that exactly meets the needs of users with the boom in mobile applications nowadays. Although numerous app recommendation services are available, they mainly employ static data (e.g., information of installed apps) and rarely consider the dynamics of user interests. In this paper, we assume that user interest consists of two components: short-term temporal interests and long-term preferences, and we propose one general framework, namely DeepApp, to enhance app recommendation performance. In DeepApp, on the one hand, we use one linear model to characterize the stable user-app associations; on the other hand, we employ the Long Short-Term Memory (LSTM) model to capture the evolution of interests based on the usage patterns of mobile apps. Finally, the Wide &Deep model is applied to fuse the effects of these two types of interests, long-term preferences and short-term temporal interests, by learning the latent interaction between linear and nonlinear features. DeepApp was evaluated on a large-scale dataset, with 4,775,293 users and 238,206 mobile apps. DeepApp achieves a significant performance gain compared with baselines (more than 6% in terms of NDCG@6 over probability matrix factorization (PMF), neural collaborative filtering (NCF), and neural tensor factorization (NTF)). This demonstrates that the integration of dynamic user interests is beneficial for mobile app recommendations. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
271,270,13,270_turbine_turbines_wake_wakes,"turbine,turbines,wake,wakes,wind,prediction,flow,turbulent,turbulence,learningbased","Complete and clear global wind turbine wake data is very important for the study of wind turbine wake characteristics in increasingly large offshore wind farms. Existing wake measurement techniques can only obtain local high-resolution (HR) wake flow field, or sacrifice accuracy to obtain larger measurement area, which is insufficient for accurate modeling of wake effect. To overcome this challenge, this paper proposes a novel super-resolution (SR) reconstruction approach that can reconstruct the global HR wake flow field from low-resolution (LR) wake flow field measurement data effectively. The proposed approach utilizes a deep learning framework called down-sampled skip-connection and multi-scale network. The performance of the SR approach is evaluated by enhancing the resolution of the wake flow field at different scale factors, and its potential application is demonstrated by assessing the prediction accuracy of three typical wake models. The results indicate that the resolution of the global wind turbine wake can be improved by 16 times using the SR model, and the reconstructed global SR wake flow fields are consistent with the ground truth in terms of both the spatial distribution and the temporal variation. By comparing the prediction results of three different wake models with the LR or SR wake data, it is shown that the SR flow reconstruction method can be applied to more accurately evaluate the wake model prediction performance, which has the potential to improve wake models. Overall, this study presents an innovative solution to the problem of incomplete and inaccurate wake flow measurement in the wind energy industry, which could reduce the workload of experimental measurements and the cost burden of accurate measuring equipment for engineering applications. © 2023 Elsevier Ltd,As wind energy continues to be a crucial part of sustainable power generation, the need for precise and efficient modeling of wind turbines, especially under yawed conditions, becomes increasingly significant. Addressing this, the current study introduces a machine learning-based symbolic regression approach for elucidating wake dynamics. Utilizing WindSE’s actuator line method (ALM) and Large Eddy Simulation (LES), we model an NREL 5-MW wind turbine under yaw conditions ranging from no yaw to 40 degrees. Leveraging a hold-out validation strategy, the model achieves robust hyper-parameter optimization, resulting in high predictive accuracy. While the model demonstrates remarkable precision in predicting wake deflection and velocity deficit at both the wake center and hub height, it shows a slight deviation at low downstream distances, which is less critical to our focus on large wind farm design. Nonetheless, our approach sets the stage for advancements in academic research and practical applications in the wind energy sector by providing an accurate and computationally efficient tool for wind farm optimization. This study establishes a new standard, filling a significant gap in the literature on the application of machine learning-based wake models for wind turbine yaw wake prediction. © 2023 by the authors.,Wind turbine wake prediction considering yawed condition is of great importance for wind farm applications. Tremendous efforts have been made on yawed wind turbine wake prediction by using analytical wake models and numerical simulations, while their applications are still limited due to a lack of balanced consideration on both efficiency and precision. In this work, a deep learning-based wake prediction model is developed by integrating the transformer module into the conditional generative adversarial network. The developed model takes the inflow velocity and turbulence field as the input to predict the three-dimensional wake flow under different yawed conditions. The data generation approach by both analytical and numerical ways is used to build a large wake database. Subsequently, a pretraining-finetuning strategy is adopted to improve the model training efficiency and enhance the prediction performance. The validation results show that the proposed model can achieve a good agreement with numerical simulations at different streamwise distances under various inflow conditions, with the mean absolute relative error of 5.0 % and 7.27 % for wake velocity and turbulence intensity, respectively. The model parameters are also investigated to illustrate the wake prediction improvement by transformer-mixed modelling method. The wake prediction performance of the proposed model is validated by a comparison with analytical wake model and other popular machine learning-based methods. Moreover, the power calculation of multiple wind turbines is conducted to demonstrate the easy implementation and good performance in wind farm applications. © 2024 Elsevier Ltd"
272,271,12,271_forecast_forecasting_predictability_neural,"forecast,forecasting,predictability,neural,chaotic,reservoir,chaos,reservoirs,recurrent,dynamical","Cross-Validation (CV) is still uncommon in time series modeling. Echo State Networks (ESNs), as a prime example of Reservoir Computing (RC) models, are known for their fast and precise one-shot learning, that often benefit from good hyper-parameter tuning. This makes them ideal to change the status quo. We discuss CV of time series for predicting a concrete time interval of interest, suggest several schemes for cross-validating ESNs and introduce an efficient algorithm for implementing them. This algorithm is presented as two levels of optimizations of doing k-fold CV. Training an RC model typically consists of two stages: (i) running the reservoir with the data and (ii) computing the optimal readouts. The first level of our optimization addresses the most computationally expensive part (i) and makes it remain constant irrespective of k. It dramatically reduces reservoir computations in any type of RC system and is enough if k is small. The second level of optimization also makes the (ii) part remain constant irrespective of large k, as long as the dimension of the output is low. We discuss when the proposed validation schemes for ESNs could be beneficial, three options for producing the final model and empirically investigate them on six different real-world datasets, as well as do empirical computation time experiments. We provide the code in an online repository. Proposed CV schemes give better and more stable test performance in all the six different real-world datasets, three task types. Empirical run times confirm our complexity analysis. In most situations, k-fold CV of ESNs and many other RC models can be done for virtually the same time and space complexity as a simple single-split validation. This enables CV to become a standard practice in RC. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.,Echo state network (ESN), a type of special recurrent neural network with a large-scale randomly fixed hidden layer (called a reservoir) and an adaptable linear output layer, has been widely employed in the field of time series analysis and modeling. However, when tackling the problem of multidimensional chaotic time series prediction, due to the randomly generated rules for input and reservoir weights, not only the representation of valuable variables is enriched but also redundant and irrelevant information is accumulated inevitably. To remove the redundant components, reduce the approximate collinearity among echo-state information, and improve the generalization and stability, a new method called hierarchical ESN with sparse learning (HESN-SL) is proposed. The HESN-SL mines and captures the latent evolution patterns hidden from the dynamic system by means of layer-by-layer processing in stacked reservoirs, and leverage monotone accelerated proximal gradient algorithm to train a sparse output layer with variable selection capability. Meanwhile, we further prove that the HESN-SL satisfies the echo state property, which guarantees the stability and convergence of the proposed model when applied to time series prediction. Experimental results on two synthetic chaotic systems and a real-world meteorological dataset illustrate the proposed HESN-SL outperforms both original ESN and existing hierarchical ESN-based models for multidimensional chaotic time series prediction.  © 2012 IEEE.,From one side, Evolutionary Algorithms have enabled enormous progress over the last years in the optimization field. They have been applied to a variety of problems, including optimization of Neural Networks’ architectures. On the other side, the Echo State Network (ESN) model has become increasingly popular in time series prediction, for instance when modeling chaotic sequences. The network has numerous hidden neurons forming a recurrent topology, so-called reservoir, which is fixed during the learning process. Initial reservoir design has mostly been made by human experts; as a consequence, it is prone to errors and bias, and it is a time consuming task. In this paper, we introduce an automatic general neuroevolutionary framework for ESNs, on which we develop a computational tool for evolving reservoirs, called EVOlutionary Echo State Network (EvoESN). To increase efficiency, we represent the large matrix of reservoir weights in the Fourier space, where we perform the evolutionary search strategy. This frequency space has major advantages compared with the original weight space. After updating the Fourier coefficients, we go back to the weight space and perform a conventional training phase for full setting the reservoir architecture. We analyze the evolutionary search employing genetic algorithms and particle swarm optimization, obtaining promising results with the latter over three well-known chaotic time series. The proposed framework leads fast to very good results compared with modern ESN models. Hence, this contribution positions an important family of recurrent systems in the promising neuroevolutionary domain. © 2023 Elsevier B.V."
273,272,12,272_imagenet_cnns_supervised_learning,"imagenet,cnns,supervised,learning,machinelearning,selflearning,deep,imaging,images,classification","Self-supervised representation learning has been extremely successful in medical image analysis, as it requires no human annotations to provide transferable representations for downstream tasks. Recent self-supervised learning methods are dominated by noise-contrastive estimation (NCE, also known as contrastive learning), which aims to learn invariant visual representations by contrasting one homogeneous image pair with a large number of heterogeneous image pairs in each training step. Nonetheless, NCE-based approaches still suffer from one major problem that is one homogeneous pair is not enough to extract robust and invariant semantic information. Inspired by the archetypical triplet loss, we propose GraVIS, which is specifically optimized for learning self-supervised features from dermatology images, to group homogeneous dermatology images while separating heterogeneous ones. In addition, a hardness-aware attention is introduced and incorporated to address the importance of homogeneous image views with similar appearance instead of those dissimilar homogeneous ones. GraVIS significantly outperforms its transfer learning and self-supervised learning counterparts in both lesion segmentation and disease classification tasks, sometimes by 5 percents under extremely limited supervision. More importantly, when equipped with the pre-trained weights provided by GraVIS, a single model could achieve better results than winners that heavily rely on ensemble strategies in the well-known ISIC 2017 challenge.  © 2022 IEEE.,The locality and spatial field of view of image operators have played a major role in image analysis, from hand-crafted to deep learning methods. In Convolutional Neural Networks (CNNs), the field of view is traditionally set to very small values (e.g. 3 × 3 pixels) for individual kernels and grown throughout the network by cascading layers. Automatically learning or adapting the best spatial support of the kernels can be done by using large kernels. Due to the computation requirements of standard CNN architectures, this has been little investigated in the literature. However, if large receptive fields are needed to capture wider contextual information on a given task, it could be learned from the data. Obtaining an optimal receptive field with few layers is very relevant in applications with a limited amount of annotated training data, e.g. in medical imaging. We show that CNNs (2D U-Nets) with large kernels outperform similar models with standard small kernels on the task of nuclei segmentation in histopathology images. We observe that the large kernels mostly capture low-frequency information, which motivates the need for large kernels and their efficient compression via the Discrete Cosine Transform (DCT). Following this idea, we develop a U-Net model with wide and compressed DCT kernels that leads to similar performance and trends to the standard U-Net, with reduced complexity. Visualizations of the kernels in the spatial and frequency domains, as well as the effective receptive fields, provide insights into the models’ behaviors and the learned features. © 2023,Machine-learning models for medical tasks can match or surpass the performance of clinical experts. However, in settings differing from those of the training dataset, the performance of a model can deteriorate substantially. Here we report a representation-learning strategy for machine-learning models applied to medical-imaging tasks that mitigates such ‘out of distribution’ performance problem and that improves model robustness and training efficiency. The strategy, which we named REMEDIS (for ‘Robust and Efficient Medical Imaging with Self-supervision’), combines large-scale supervised transfer learning on natural images and intermediate contrastive self-supervised learning on medical images and requires minimal task-specific customization. We show the utility of REMEDIS in a range of diagnostic-imaging tasks covering six imaging domains and 15 test datasets, and by simulating three realistic out-of-distribution scenarios. REMEDIS improved in-distribution diagnostic accuracies up to 11.5% with respect to strong supervised baseline models, and in out-of-distribution settings required only 1–33% of the data for retraining to match the performance of supervised models retrained using all available data. REMEDIS may accelerate the development lifecycle of machine-learning models for medical imaging. © 2023, The Author(s), under exclusive licence to Springer Nature Limited."
274,273,12,273_pharmacokinetic_pharmacodynamic_physicochemical_chemotypes,"pharmacokinetic,pharmacodynamic,physicochemical,chemotypes,chemicals,drug,chemical,models,qsar,vivo","Pulmonary absorption is an important route for drug delivery and chemical exposure. To streamline the chemical assessment process for the reduction of animal experiments, several animal-free models were developed for pulmonary absorption research. While Calu-3 and Caco-2 cells and their derived computational models were used in estimating pulmonary permeability, the ex vivo isolated perfused lung (IPL) models are considered more clinically relevant measurements. However, the IPL experiments are resource-consuming making it infeasible for the large-scale screening of potential inhaled toxicants and drugs. In silico models are desirable for estimating pulmonary absorption. This study presented a novel machine learning method that employed an extratrees-based multitask learning approach to predict the IPL absorption rate constant (kaIPL) of various chemicals. The shared permeability knowledge was extracted by simultaneously learning three relevant tasks of Caco-2 and Calu-3 cell permeability and IPL absorption rate. Seven informative physicochemical descriptors were identified. A rigorous evaluation of the developed prediction model showed good performance with a high correlation between predictions and observations (r = 0.84) in the independent test dataset. Two case studies of inhalation drugs and respiratory sensitizers revealed the potential application of this model, which may serve as a valuable tool for predicting pulmonary absorption of chemicals. © 2024 Elsevier Ltd,Background: Despite their large numbers and widespread use, very little is known about the extent to which per- and polyfluoroalkyl substances (PFAS) can cross the placenta and expose the developing fetus. Objective: The aim of our study is to develop a computational approach that can be used to evaluate the of extend to which small molecules, and in particular PFAS, can cross to cross the placenta and partition to cord blood. Methods: We collected experimental values of the concentration ratio between cord and maternal blood (RCM) for 260 chemical compounds and calculated their physicochemical descriptors using the cheminformatics package Mordred. We used the compiled database to, train and test an artificial neural network (ANN). And then applied the best performing model to predict RCM for a large dataset of PFAS chemicals (n = 7982). We, finally, examined the calculated physicochemical descriptors of the chemicals to identify which properties correlated significantly with RCM. Results: We determined that 7855 compounds were within the applicability domain and 127 compounds are outside the applicability domain of our model. Our predictions of RCM for PFAS suggested that 3623 compounds had a log RCM > 0 indicating preferable partitioning to cord blood. Some examples of these compounds were bisphenol AF, 2,2-bis(4-aminophenyl)hexafluoropropane, and nonafluoro-tert-butyl 3-methylbutyrate. Significance: These observations have important public health implications as many PFAS have been shown to interfere with fetal development. In addition, as these compounds are highly persistent and many of them can readily cross the placenta, they are expected to remain in the population for a long time as they are being passed from parent to offspring. Impact: Understanding the behavior of chemicals in the human body during pregnancy is critical in preventing harmful exposures during critical periods of development. Many chemicals can cross the placenta and expose the fetus, however, the mechanism by which this transport occurs is not well understood. In our study, we developed a machine learning model that describes the transplacental transfer of chemicals as a function of their physicochemical properties. The model was then used to make predictions for a set of 7982 per- and polyfluorinated alkyl substances that are listed on EPA’s CompTox Chemicals Dashboard. The model can be applied to make predictions for other chemical categories of interest, such as plasticizers and pesticides. Accurate predictions of RCM can help scientists and regulators to prioritize chemicals that have the potential to cause harm by exposing the fetus. © 2022, The Author(s), under exclusive licence to Springer Nature America, Inc.,Accurate prediction of human pharmacokinetics (PK) remains one of the key objectives of drug metabolism and PK (DMPK) scientists in drug discovery projects. This is typically performed by using in vitro-in vivo extrapolation (IVIVE) based on mechanistic PK models. In recent years, machine learning (ML), with its ability to harness patterns from previous outcomes to predict future events, has gained increased popularity in application to absorption, distribution, metabolism, and excretion (ADME) sciences. This study compares the performance of various ML and mechanistic models for the prediction of human IV clearance for a large (645) set of diverse compounds with literature human IV PK data, as well as measured relevant in vitro end points. ML models were built using multiple approaches for the descriptors: (1) calculated physical properties and structural descriptors based on chemical structure alone (classical QSAR/QSPR); (2) in vitro measured inputs only with no structure-based descriptors (ML IVIVE); and (3) in silico ML IVIVE using in silico model predictions for the in vitro inputs. For the mechanistic models, well-stirred and parallel-tube liver models were considered with and without the use of empirical scaling factors and with and without renal clearance. The best ML model for the prediction of in vivo human intrinsic clearance (CLint) was an in vitro ML IVIVE model using only six in vitro inputs with an average absolute fold error (AAFE) of 2.5. The best mechanistic model used the parallel-tube liver model, with empirical scaling factors resulting in an AAFE of 2.8. The corresponding mechanistic model with full in silico inputs achieved an AAFE of 3.3. These relative performances of the models were confirmed with the prediction of 16 Pfizer drug candidates that were not part of the original data set. Results show that ML IVIVE models are comparable to or superior to their best mechanistic counterparts. We also show that ML IVIVE models can be used to derive insights into factors for the improvement of mechanistic PK prediction. © 2023 American Chemical Society."
275,274,12,274_segmentation_cardiac_autosegmentation_cardiovascular,"segmentation,cardiac,autosegmentation,cardiovascular,endocardium,ventricle,segmented,myocardial,tomography,myocardium","Cardiac computed tomography angiography (CTA) is an emerging imaging modality for assessing coronary artery as well as various cardiovascular structures. Recently, deep learning (DL) methods have been successfully applied to many applications of medical image analysis including cardiac CTA structure segmentation. However, DL requires a large amounts of data and high-quality labels for training which can be burdensome to obtain due to its labor-intensive nature. In this study, we aim to develop a fully automatic artificial intelligence (AI) system, named DeepHeartCT, for accurate and rapid cardiac CTA segmentation based on DL. The proposed system was trained using a large clinical dataset with computer-generated labels to segment various cardiovascular structures including left and right ventricles (LV, RV), left and right atria (LA, RA), and LV myocardium (LVM). This new system was trained directly using high-quality computer labels generated from our previously developed multi-atlas based AI system. In addition, a reverse ranking strategy was proposed to assess the segmentation quality in the absence of manual reference labels. This strategy allowed the new framework to assemble optimal computer-generated labels from a large dataset for effective training of a deep convolutional neural network (CNN). A large clinical cardiac CTA studies (n = 1,064) were used to train and validate our framework. The trained model was then tested on another independent dataset with manual labels (n = 60). The Dice score, Hausdorff distance and mean surface distance were used to quantify the segmentation accuracy. The proposed DeepHeartCT framework yields a high median Dice score of 0.90 [interquartile range (IQR), 0.90–0.91], a low median Hausdorff distance of 7 mm (IQR, 4–15 mm) and a low mean surface distance of 0.80 mm (IQR, 0.57–1.29 mm) across all segmented structures. An additional experiment was conducted to evaluate the proposed DL-based AI framework trained with a small vs. large dataset. The results show our framework also performed well when trained on a small optimal training dataset (n = 110) with a significantly reduced training time. These results demonstrated that the proposed DeepHeartCT framework provides accurate and rapid cardiac CTA segmentation that can be readily generalized for handling large-scale medical imaging applications. Copyright © 2022 Bui, Hsu, Chang, Sun, Tran, Shanbhag, Zhou, Mehta and Chen.,Cardiac substructure segmentation is a prerequisite for cardiac diagnosis and treatment, providing a basis for accurate calculation, modeling, and analysis of the entire cardiac structure. CT (computed tomography) imaging can be used for a noninvasive qualitative and quantitative evaluation of the cardiac anatomy and function. Cardiac substructures have diverse grayscales, fuzzy boundaries, irregular shapes, and variable locations. We designed a deep learning-based framework to improve the accuracy of the automatic segmentation of cardiac substructures. This framework integrates cardiac anatomical knowledge; it uses prior knowledge of the location, shape, and scale of cardiac substructures and separately processes the structures of different scales. Through two successive segmentation steps with a coarse-to-fine cascaded network, the more easily segmented substructures were coarsely segmented first; then, the more difficult substructures were finely segmented. The coarse segmentation result was used as prior information and combined with the original image as the input for the model. Anatomical knowledge of the large-scale substructures was embedded into the fine segmentation network to guide and train the small-scale substructures, achieving efficient and accurate segmentation of ten cardiac substructures. Sixty cardiac CT images and ten substructures manually delineated by experienced radiologists were retrospectively collected; the model was evaluated using the DSC (Dice similarity coefficient), Recall, Precision, and the Hausdorff distance. Compared with current mainstream segmentation models, our approach demonstrated significantly higher segmentation accuracy, with accurate segmentation of ten substructures of different shapes and sizes, indicating that the segmentation framework fused with prior anatomical knowledge has superior segmentation performance and can better segment small targets in multi-target segmentation tasks. © 2023 by the authors.,Background and objective: The sheer volume of data generated by population imaging studies is unparalleled by current capabilities to extract objective and quantitative cardiac phenotypes; subjective and time-consuming manual image analysis remains the gold standard. Automated image analytics to compute quantitative imaging biomarkers of cardiac function are desperately needed. Data volumes and their variability pose a challenge to most state-of-the-art methods for endo- and epicardial contours, which lack robustness when applied to very large datasets. Our aim is to develop an analysis pipeline for the automatic quantification of cardiac function from cine magnetic resonance imaging data. Method: This work adopt 4,638 cardiac MRI cases coming from UK Biobank with ground truth available for left and RV contours. A hybrid and robust algorithm is proposed to improve the accuracy of automatic left and right ventricle segmentation by harnessing the localization accuracy of deep learning and the morphological accuracy of 3D-ASM (three-dimensional active shape models). The contributions of this paper are three-fold. First, a fully automatic method is proposed for left and right ventricle initialization and cardiac MRI segmentation by taking full advantage of spatiotemporal constraint. Second, a deeply supervised network is introduced to train and segment the heart. Third, the 3D-ASM image search procedure is improved by combining image intensity models with convolutional neural network (CNN) derived distance maps improving endo- and epicardial edge localization. Results: The proposed architecture outperformed the state of the art for cardiac MRI segmentation from UK Biobank. The statistics of RV landmarks detection errors for Triscuspid valve and RV apex are 4.17 mm and 5.58 mm separately. The overlap metric, mean contour distance, Hausdorff distance and cardiac functional parameters are calculated for the LV (Left Ventricle) and RV (Right Ventricle) contour segmentation. Bland–Altman analysis for clinical parameters shows that the results from our automated image analysis pipelines are in good agreement with results from expert manual analysis. Conclusions: Our hybrid scheme combines deep learning and statistical shape modeling for automatic segmentation of the LV/RV from cardiac MRI datasets is effective and robust and can compute cardiac functional indexes from population imaging. © 2023 Elsevier B.V."
276,275,12,275_evacuation_planning_rescue_reinforcement,"evacuation,planning,rescue,reinforcement,emergencies,emergency,disasters,disaster,scheduling,reinforcerouting","Hospitals play a key role in providing medical assistance in a post-disaster phase. However, depending on the nature, scale, and severity of disasters, they may also be affected and may need to evacuate their patients. Current evacuation response plans for hospitals are predominantly based on the use of land vehicles, whereas little methodological advancements have been reported for when aerial hospital evacuations become necessary. To address this problem, this study develops a model that facilitates evacuation planning for moving patients from at-risk hospitals to remote locations using limited aerial transportation resources. A subset of the model parameters is treated as stochastic in order to reflect the uncertainties involved in real-world emergency conditions. The complexities of the model are handled via the use of a metaheuristic approach, the Variable neighbourhood search (VNS) algorithm, to provide a suitable solution within a reasonable amount of time. A relaxed mathematical model is used in the proposed VNS to generate an initial solution, which is then transformed into a feasible solution through a try-and-error procedure. The proposed method applies a reinforcement learning procedure that uses different local search strategies within the loop of the VNS. A hypothetical evacuation scenario of real-life scale, from an island state in Australia to the mainland states, is used to evaluate the ability of the model to generate efficient plans. In this case study, Tasmania is considered as an evacuation point, while hospitals located in other major cities host evacuated patients. Comparing the proposed method with a conventional method shows superior performance based on all relevant metrics. Emergency response agencies across many countries that can adopt the proposed methodology, when facing large-scale emergencies caused by wars, epidemics, or natural hazards such as wildfires and earthquakes. © 2023 Elsevier Ltd,A high-intensity storm surge hazard exposes residents and facilities to inundation danger. A continuously operating robust inundation emergency logistics system is vital to guarantee lives and support the control of hazardous materials. A massive inundation area, large number of affected facilities, and instantly changing flooding situation bring tremendous challenges to rescue resource allocation. In this article, a rescue resource distribution scheduling of storm surge inundation logistics is proposed to quantitatively formulate the rescue time minimization problem in emergency logistics. The mixed-integer linear programming (MILP) method is proposed for the emergency logistics scheduling model validation and optimality comparison. To enhance the efficiency of creating a good quality allocation strategy when facing large-scale problems, a deep reinforcement learning algorithm - deep deterministic policy gradient (DDPG) - is utilized to search the solutions. Based on a rescue resource scheduling model of storm surge inundation logistics targeting storm surge Mangkhut in September, 2018, a case study of the Futian District, Shenzhen, China, was conducted to verify the correctness and efficiency of the MILP and the DDPG. The optimal schedule solution designed by the MILP had a duration of 3.5138 h, while the solution calculated by DDPG had a duration of 5.065 h for the rescue. The execution time of DDPG was stable and under a second, while the execution time of the MILP was over two hours.  © 2005-2012 IEEE.,Evacuation planning and emergency routing systems are crucial in saving lives during disasters. Traditional emergency routing systems, despite their best efforts, often struggle to accurately capture the dynamic nature of flood conditions, road closures, and other real-time changes inherent in urban disaster logistics. This paper introduces the ReinforceRouting model, a novel approach to optimizing evacuation routes using reinforcement learning (RL). The model incorporates a unique RL environment that considers multiple criteria, such as traffic conditions, hazardous situations, and the availability of safe routes. The RL agent in this model learns optimal actions through interaction with the environment, receiving feedback in the form of rewards or penalties. The ReinforceRouting model excels in executing prompt and accurate route planning on large road networks, outperforming traditional RL algorithms and shortest-path-based algorithms. A higher safety score and episode reward of the model are demonstrated when compared to these classical methods. This innovative approach to disaster evacuation planning offers a promising avenue for enhancing the efficiency, safety, and reliability of emergency responses in dynamic urban environments. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group."
277,276,12,276_ranking_retrieval_reranker_ranker,"ranking,retrieval,reranker,ranker,rankers,rank,queries,search,overparameterize,relevance","While learning to rank (LTR) is widely employed in web searches to prioritize pertinent webpages from the retrieved contents based on input queries, traditional LTR models stumble over two principal stumbling blocks leading to subpar performance: (1) the lack of well-annotated query-webpage pairs with ranking scores to cover search queries of various popularity, debilitating their coverage of search queries across the popularity spectrum, and (2) ill-trained models that are incapable of inducing generalized representations for LTR, culminating in overfitting. To tackle above challenges, we proposed a G?enerativeS?emi - S?upervisedP?re -trained (GS 2 P) Learning to Rank model. Specifically, GS 2 P first generates pseudo-labels for the unlabeled samples using tree-based LTR models after a series of co-training procedures, then learns the representations of query-webpage pairs with self-attentive transformers via both discriminative (LTR) and generative (denoising autoencoding for reconstruction) losses. Finally, GS 2 P boosts the performance of LTR through incorporating Random Fourier Features to over-parameterize the models into “interpolating regime”, so as to enjoy the further descent of generalization errors with learned representations. We conduct extensive offline experiments on a publicly available dataset and a real-world dataset collected from a large-scale search engine. The results show that GS 2 P can achieve the best performance on both datasets, compared to baselines. We also deploy GS 2 P at a large-scale web search engine with realistic traffic, where we can still observe significant improvement in real-world applications. GS 2 P performs consistently in both online and offline experiments. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.,While learning to rank (LTR) has been widely used in web search to prioritize most relevant webpages among the retrieved contents subject to the input queries, the traditional LTR models fail to deliver decent performance due to two main reasons: 1) the lack of well-annotated query-webpage pairs with ranking scores to cover search queries of various popularity, and 2) ill-trained models based on a limited number of training samples with poor generalization performance. To improve the performance of LTR models, tremendous efforts have been done from above two aspects, such as enlarging training sets with pseudo-labels of ranking scores by self-training, or refining the features used for LTR through feature extraction and dimension reduction. Though LTR performance has been marginally increased, we still believe these methods could be further improved in the newly-fashioned 'interpolating regime'. Specifically, instead of lowering the number of features used for LTR models, our work proposes to transform original data with random Fourier feature, so as to over-parameterize the downstream LTR models (e.g., GBRank or LightGBM) with features in ultra-high dimensionality and achieve superb generalization performance. Furthermore, rather than self-training with pseudo-labels produced by the same LTR model in a 'self-tuned' fashion, the proposed method incorporates the diversity of prediction results between the listwise and pointwise LTR models while co-training both models with a cyclic labeling-prediction pipeline in a 'ping-pong' manner. We deploy the proposed Co-trained and Over-parameterized LTR system COLTR at Baidu search and evaluate COLTR with a large number of baseline methods. The results show that COLTR could achieve ? NDCG 4 ?NDCG4 = 3.64% ?4.92%, compared to baselines, under various ratios of labeled samples. We also conduct a 7-day A/B Test using the realistic web traffics of Baidu Search, where we can still observe significant performance improvement around ?NDCG4 = 0.17%?0.92% in real-world applications. COLTR performs consistently both in online and offline experiments.  © 1989-2012 IEEE.,While China has become the largest online market in the world with approximately 1 billion internet users, Baidu runs the world’s largest Chinese search engine serving more than hundreds of millions of daily active users and responding to billions of queries per day. To handle the diverse query requests from users at the web-scale, Baidu has made tremendous efforts in understanding users’ queries, retrieving relevant content from a pool of trillions of webpages, and ranking the most relevant webpages on the top of the results. Among the components used in Baidu search, learning to rank (LTR) plays a critical role and we need to timely label an extremely large number of queries together with relevant webpages to train and update the online LTR models. To reduce the costs and time consumption of query/webpage labelling, we study the problem of active learning to rank (active LTR) that selects unlabeled queries for annotation and training in this work. Specifically, we first investigate the criterion–Ranking entropy (RE) characterizing the entropy of relevant webpages under a query produced by a sequence of online LTR models updated by different checkpoints, using a query-by-committee (QBC) method. Then, we explore a new criterion namely prediction variances (PV) that measures the variance of prediction results for all relevant webpages under a query. Our empirical studies find that RE may favor low-frequency queries from the pool for labelling while PV prioritizes high-frequency queries more. Finally, we combine these two complementary criteria as the sample selection strategies for active learning. Extensive experiments with comparisons to baseline algorithms show that the proposed approach could train LTR models to achieve higher discounted cumulative gain (i.e., the relative improvement ?DCG 4 = 1.38%) with the same budgeted labelling efforts. © 2024, Institute of Automation, Chinese Academy of Sciences and Springer-Verlag GmbH Germany, part of Springer Nature."
278,277,12,277_prostatectomy_prostate_prostatex_biopsy,"prostatectomy,prostate,prostatex,biopsy,prostatic,ultrasound,cancer,tumour,pathologists,mri","Background: Accurate delineations of regions of interest (ROIs) on multi-parametric magnetic resonance imaging (mpMRI) are crucial for development of automated, machine learning-based prostate cancer (PCa) detection and segmentation models. However, manual ROI delineations are labor-intensive and susceptible to inter-reader variability. Histopathology images from radical prostatectomy (RP) represent the “gold standard” in terms of the delineation of disease extents, for example, PCa, prostatitis, and benign prostatic hyperplasia (BPH). Co-registering digitized histopathology images onto pre-operative mpMRI enables automated mapping of the ground truth disease extents onto mpMRI, thus enabling the development of machine learning tools for PCa detection and risk stratification. Still, MRI-histopathology co-registration is challenging due to various artifacts and large deformation between in vivo MRI and ex vivo whole-mount histopathology images (WMHs). Furthermore, the artifacts on WMHs, such as tissue loss, may introduce unrealistic deformation during co-registration. Purpose: This study presents a new registration pipeline, MSERgSDM, a multi-scale feature-based registration (MSERg) with a statistical deformation (SDM) constraint, which aims to improve accuracy of MRI-histopathology co-registration. Methods: In this study, we collected 85 pairs of MRI and WMHs from 48 patients across three cohorts. Cohort 1 (D1), comprised of a unique set of 3D printed mold data from six patients, facilitated the generation of ground truth deformations between ex vivo WMHs and in vivo MRI. The other two clinically acquired cohorts (D2 and D3) included 42 patients. Affine and nonrigid registrations were employed to minimize the deformation between ex vivo WMH and ex vivo T2-weighted MRI (T2WI) in D1. Subsequently, ground truth deformation between in vivo T2WI and ex vivo WMH was approximated as the deformation between in vivo T2WI and ex vivo T2WI. In D2 and D3, the prostate anatomical annotations, for example, tumor and urethra, were made by a pathologist and a radiologist in collaboration. These annotations included ROI boundary contours and landmark points. Before applying the registration, manual corrections were made for flipping and rotation of WMHs. MSERgSDM comprises two main components: (1) multi-scale representation construction, and (2) SDM construction. For the SDM construction, we collected N = 200 reasonable deformation fields generated using MSERg, verified through visual inspection. Three additional methods, including intensity-based registration, ProsRegNet, and MSERg, were also employed for comparison against MSERgSDM. Results: Our results suggest that MSERgSDM performed comparably to the ground truth (p > 0.05). Additionally, MSERgSDM (ROI Dice ratio = 0.61, landmark distance = 3.26 mm) exhibited significant improvement over MSERg (ROI Dice ratio = 0.59, landmark distance = 3.69 mm) and ProsRegNet (ROI Dice ratio = 0.56, landmark distance = 4.00 mm) in local alignment. Conclusions: This study presents a novel registration method, MSERgSDM, for mapping ex vivo WMH onto in vivo prostate MRI. Our preliminary results demonstrate that MSERgSDM can serve as a valuable tool to map ground truth disease annotations from histopathology images onto MRI, thereby assisting in the development of machine learning models for PCa detection on MRI. © 2023 The Authors. Medical Physics published by Wiley Periodicals LLC on behalf of American Association of Physicists in Medicine.,The Gleason scoring system is a reliable method for quantifying the aggressiveness of prostate cancer, which provides an important reference value for clinical assessment on therapeutic strategies. However, to the best of our knowledge, no study has been done on the pathological grading of prostate cancer from single ultrasound images. In this work, a novel Automatic Region-based Gleason Grading (ARGG) network for prostate cancer based on deep learning is proposed. ARGG consists of two stages: (1) a region labeling object detection (RLOD) network is designed to label the prostate cancer lesion region; (2) a Gleason grading network (GNet) is proposed for pathological grading of prostate ultrasound images. In RLOD, a new feature fusion structure Skip-connected Feature Pyramid Network (CFPN) is proposed as an auxiliary branch for extracting features and enhancing the fusion of high-level features and low-level features, which helps to detect the small lesion and extract the image detail information. In GNet, we designed a synchronized pulse enhancement module (SPEM) based on pulse-coupled neural networks for enhancing the results of RLOD detection and used as training samples, and then fed the enhanced results and the original ones into the channel attention classification network (CACN), which introduces an attention mechanism to benefit the prediction of cancer grading. Experimental performance on the dataset of prostate ultrasound images collected from hospitals shows that the proposed Gleason grading model outperforms the manual diagnosis by physicians with a precision of 0.830. In addition, we have evaluated the lesions detection performance of RLOD, which achieves a mean Dice metric of 0.815. © 2022 Elsevier Ltd,Prostate cancer, the most common cancer in men, is influenced by age, family history, genetics, and lifestyle factors. Early detection of prostate cancer using screening methods improves outcomes, but the balance between overdiagnosis and early detection remains debated. Using Deep Learning (DL) algorithms for prostate cancer detection offers a promising solution for accurate and efficient diagnosis, particularly in cases where prostate imaging is challenging. In this paper, we propose a Prostate Cancer Detection Model (PCDM) model for the automatic diagnosis of prostate cancer. It proves its clinical applicability to aid in the early detection and management of prostate cancer in real-world healthcare environments. The PCDM model is a modified ResNet50-based architecture that integrates faster R-CNN and dual optimizers to improve the performance of the detection process. The model is trained on a large dataset of annotated medical images, and the experimental results show that the proposed model outperforms both ResNet50 and VGG19 architectures. Specifically, the proposed model achieves high sensitivity, specificity, precision, and accuracy rates of 97.40%, 97.09%, 97.56%, and 95.24%, respectively. © 2024, The Author(s)."
279,278,12,278_inpainting_inpaints_gan_generative,"inpainting,inpaints,gan,generative,pixels,images,convolution,mask,murals,image","With the development of image generation and processing techniques, image inpainting techniques based on deep learning have achieved impressive results. Especially the emphasis on global context during inpainting enables the network to generate reasonably coarse inpainting results at low resolutions. However, how to better achieve high-quality texture filling at high resolutions is still a challenging problem. To address this problem, most methods design two-stage networks to achieve structure and texture restoration separately. But in the face of large-scale masks, the generated textures still suffer from blurring and artifacts. Therefore, in order to achieve inpainting of images with large-scale masks and generate fine textures, this paper proposes an end-to-end generative adversarial model for large mask inpainting, called Panoramic Feature Aggregation Network (PFAN). First, this paper designs a Euclidean Attention Mechanism (EAM) which exploits encoder features to generate low-resolution structure restoration. Then a Feature Aggregation Synthesis Block (FASB) is proposed in the decoder to achieve high-resolution complex texture filling. With the global receptive fields of these two modules, texture filling results with satisfactory performance even under large-scale masks. Experiments on CelebA-HQ, Paris Street View and FFHQ datasets show that the proposed method has superior performance. © 2024 Elsevier B.V.,Image inpainting is the process to fill missing pixels in the damaged image and this process has drawn more attraction and gained active and expensive research topic in recent decades, because the high quality in the image inpainting benefits a greater range of applications, like object removal, photo restoration, and so on. Inpainting of larger quality of the image needs to fill the empty regions with plausible content in the damaged image. The existing inpainting methods either fill image regions by stealing the image patches or semantically create coherent patches from the regional context. Most of the traditional models perform well on small holes images, but restoring the image with large holes still results a challenging task. To overcome such issues and to generate effective inpainting results, a proposed method named the hybrid context deep learning approach is designed in order to fill empty regions of crack images. Moreover, the proposed method is more effective by employing a hybrid optimization algorithm for training of classifier to generate a more robust and accurate inpainted result. The developed model includes two different deep learning classifiers to accomplish the process of image inpainting in such a way that the results are fused through the probabilistic model. Moreover, the proposed approach attains higher performance by the metrics such as Peak signal-to-noise ratio (PSNR), Structural Similarity Index (SSIM), Second Derivative like Measure of Enhancement (SDME), and Universal Quality Index (UQI) with the values of 38.02 db, 0.867, 54.32 db, and 0.864, respectively. © 2023 World Scientific Publishing Company.,As the only underground mural in the collection, the tomb murals are subject to damage due to temperature, humidity, and foundation settlement changes. Traditional mural inpainting takes a long time and requires experts to draw it manually. Therefore, the need for digital inpainting is increasing to save time and costs. Due to the scarcity of samples and the variety of damage, the image features are scattered and partially sparse, and the colors are less vivid than in other images. Traditional deep learning inpainting causes information loss and generates irrational structures. The generative adversarial network is, recently, a more effective method. Therefore, this paper presents an inpainting model based on dual-attention multiscale feature aggregation and an improved generator. Firstly, an improved residual prior and attention mechanism is added to the generator module to preserve the image structure. Secondly, the model combines spatial and channel attention with multiscale feature aggregation to change the mapping network structure and improve the inpainting accuracy. Finally, the segmental loss function and its training method are improved.The experimental results show that the results of using signal-to-noise ratio (PSNR), structural similarity (SSIM), and mean square error (MSE) on epitaxial mask, crack mask, random small mask, and random large mask are better than other methods. It demonstrates the performance of this paper in inpainting different diseases of murals. It can be used as a reference for experts in manual inpainting, saving the cost and time of manual inpainting. © 2023 by the authors."
280,279,11,279_gan_transistors_transistor_ghz,"gan,transistors,transistor,ghz,mhz,inductance,model,devices,millimeterwave,circuits","The development of SiC and GaN power devices to achieve high-speed switching operations in power converter circuits is underway. The stray inductance caused by the bus bar geometries between DC capacitors and power devices influences high-speed switching circuits, such as surge voltages and switching losses. Therefore, the evaluation of the parasitic parameters is essential in designing power converter circuits. Currently, parasitic parameters that consider various bus bar geometries are calculated using finite element analysis (FEA) each time, which requires a large calculation time. This article proposes a prediction procedure for the parasitic parameters that easily and quickly consider complex bus bar geometries by performing online machine learning of FEA-based datasets. A laminated bus bar with two apertures to connect DC capacitors or power modules is analyzed, and the parasitic resistance, inductance, and capacitance are predicted. This article describes how large datasets can be obtained from multiple installments and perform online machine learning using XGBoost. To discuss the benefits of online machine learning, the prediction accuracy using test data in multiple machine learning models with different amounts of training data is compared. The mean relative error (MRE) between the predicted and analyzed values improves from 250% to 8% when parasitic parameters are predicted in the frequency range of 50 kHz to 100 MHz.  © 2020 IEEE.,We present an artificial neural network (ANN) model that predicts high-frequency, large-signal hetero-junction bipolar transistor (HBT) performance as a function of the load reflection coefficient and input power trained on a set of load&#x2014;pull (LP) data that include delivered source power, output power, power-added efficiency (PAE), gain compression, input voltage, and output current. The ANN models are trained with data measured using a novel <inline-formula> <tex-math notation=""LaTeX"">$D$</tex-math> </inline-formula>-band vector LP test bench at 130, 135, and 140 GHz. We analyze common-base (CB) and common-emitter (CE) indium phosphide (InP) HBTs and show that the CE HBT provides the highest impedance margin that maximizes PAE across a 10% bandwidth centered at 140 GHz. We fabricate CE HBT power cells and demonstrate state-of-the-art measured performance (36.2% PAE at 135 GHz at 4-dB compressed gain and 13.6 dBm). Using the ANN model trained at 130 GHz, transfer learning is accomplished for 135 GHz, 140 GHz, and prematched device datasets. The size of the ANN training sets is varied from 25% to 1% of the data collected, showing that our models are capable of predicting performance with an average rms error below 1.5% across sets. To our knowledge, this is the first demonstration of transfer learning within ANNs for transistor nonlinear modeling. IEEE,The gallium-nitride (GaN) high electron-mobility transistor (HEMT) technology has emerged as an attractive candidate for high-frequency, high-power, and high-temperature applications due to the unique physical characteristics of the GaN material. Over the years, much effort has been spent on measurement-based modeling since accurate models are essential for allowing the use of this advanced transistor technology at its best. The present analysis is focused on the modeling of the scattering (S-) parameter measurements for a 0.25 ?m GaN HEMT on silicon carbide (SiC) substrate at extreme operating conditions: a large gate width (i.e., the transistor is based on an interdigitated layout consisting of ten fingers, each with a length of 150 ?m, resulting in a total gate periphery of 1.5 mm), a high ambient temperature (i.e., from 35 °C up to 200 °C with a step of 55 °C), a high dissipated power (i.e., 5.1 W at 35 °C), and a high frequency in the millimeter-wave range (i.e., from 200 MHz up to 65 GHz with a step of 200 MHz). Three different modeling approaches are investigated: the equivalent-circuit model, artificial neural networks (ANNs), and gated recurrent units (GRUs). As is shown, each modeling approach has its pros and cons that need to be considered, depending on the target performance and their specifications. This implies that an appropriate selection of the transistor modeling approach should be based on discerning and prioritizing the key features that are indeed the most important for a given application. © 2023 by the authors."
281,280,11,280_kinase_kinases_kinasespecific_kinasephos,"kinase,kinases,kinasespecific,kinasephos,bioinformatics,kinasesubstrate,datasets,predicting,proteins,inhibitors","Kinase plays a significant role in various disease signaling pathways. Due to the highly conserved sequence of kinase family members, understanding the selectivity profile of kinase inhibitors remains a priority for drug discovery. Previous methods for kinase selectivity identification use biochemical assays, which are very useful but limited by the protein available. The lack of kinase selectivity can exert benefits but also can cause adverse effects. With the explosion of the dataset for kinase activities, current computational methods can achieve accuracy for large-scale selectivity predictions. Here, we present a multimodal multi-task deep neural network model for kinase selectivity prediction by calculating the fingerprint and physiochemical descriptors. With the multimodal inputs of structure and physiochemical properties information, the multi-task framework could accurately predict the kinome map for selectivity analysis. The proposed model displays better performance for kinase–target prediction based on system evaluations. Graphic abstract: [Figure not available: see fulltext.]. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.,Conventional machine learning (ML) and deep learning (DL) play a key role in the selectivity prediction of kinase inhibitors. A number of models based on available datasets can be used to predict the kinase profile of compounds, but there is still controversy about the advantages and disadvantages of ML and DL for such tasks. In this study, we constructed a comprehensive benchmark dataset of kinase inhibitors, involving in 141,086 unique compounds and 216,823 well-defined bioassay data points for 354 kinases. We then systematically compared the performance of 12 ML and DL methods on the kinase profiling prediction task. Extensive experimental results reveal that (1) Descriptor-based ML models generally slightly outperform fingerprint-based ML models in terms of predictive performance. RF as an ensemble learning approach displays the overall best predictive performance. (2) Single-task graph-based DL models are generally inferior to conventional descriptor- and fingerprint-based ML models, however, the corresponding multi-task models generally improves the average accuracy of kinase profile prediction. For example, the multi-task FP-GNN model outperforms the conventional descriptor- and fingerprint-based ML models with an average AUC of 0.807. (3) Fusion models based on voting and stacking methods can further improve the performance of the kinase profiling prediction task, specifically, RF::AtomPairs + FP2 + RDKitDes fusion model performs best with the highest average AUC value of 0.825 on the test sets. These findings provide useful information for guiding choices of the ML and DL methods for the kinase profiling prediction tasks. Finally, an online platform called KIPP (https://kipp.idruglab.cn) and python software are developed based on the best models to support the kinase profiling prediction, as well as various kinase inhibitor identification tasks including virtual screening, compound repositioning and target fishing. © 2024, The Author(s).,Deep learning-based in silico alternatives have been demonstrated to be of significant importance in the acceleration of the drug discovery process and enhancement of success rates. Cyclin-dependent kinase 12 (CDK12) is a transcription-related cyclin-dependent kinase that may act as a biomarker and therapeutic target for cancers. However, currently, there is no high selective CDK12 inhibitor in clinical development and the identification of new specific CDK12 inhibitors has become increasingly challenging due to their similarity with CDK13. In this study, we developed a virtual screening workflow that combines deep learning with virtual screening tools and can be applied rapidly to millions of molecules. We designed a Transformer architecture Drug-Target Interaction (DTI) model with dual-branched self-supervised pre-trained molecular graph models and protein sequence models. Our predictive model produced satisfactory predictions for various targets, including CDK12, with several novel hits. We screened a large compound library consisting of 4.5 million drug-like molecules and recommended a list of potential CDK12 inhibitors for further experimental testing. In kinase assay, compared to the positive CDK12 inhibitor THZ531, the compounds CICAMPA-01, 02, 03 displayed more effective inhibition of CDK12, up to three times as much as THZ531. The compounds CICAMPA-03, 05, 04, 07 showed less inhibition of CDK13 compare to THZ531. In vitro, the IC50 of CICAMPA-01, 04, 05, 06, 09 was less than 3 ?M in the HER2 positive CDK12 amplification breast cancer cell line BT-474. Overall, this study provides a highly efficient and end-to-end deep learning protocol, in conjunction with molecular docking, for discovering CDK12 inhibitors in cancers. Additionally, we disclose five novel CDK12 inhibitors. These results may accelerate the discovery of novel chemical-class drugs for cancer treatment. © 2023 The Authors"
282,281,11,281_chatbots_chatbot_patientprovider_clinicians,"chatbots,chatbot,patientprovider,clinicians,chatgpt,chatbotgenerated,patients,clinical,medical,ai","Introduction: Innovative large language model (LLM)-powered chatbots, which are extremely popular nowadays, represent potential sources of information on resuscitation for the general public. For instance, the chatbot-generated advice could be used for purposes of community resuscitation education or for just-in-time informational support of untrained lay rescuers in a real-life emergency. Study Objective: This study focused on assessing performance of two prominent LLM-based chatbots, particularly in terms of quality of the chatbot-generated advice on how to give help to a non-breathing victim. Methods: In May 2023, the new Bing (Microsoft Corporation, USA) and Bard (Google LLC, USA) chatbots were inquired (n = 20 each): What to do if someone is not breathing? Content of the chatbots' responses was evaluated for compliance with the 2021 Resuscitation Council United Kingdom guidelines using a pre-developed checklist. Results: Both chatbots provided context-dependent textual responses to the query. However, coverage of the guideline-consistent instructions on help to a non-breathing victim within the responses was poor: mean percentage of the responses completely satisfying the checklist criteria was 9.5% for Bing and 11.4% for Bard (P >.05). Essential elements of the bystander action, including early start and uninterrupted performance of chest compressions with adequate depth, rate, and chest recoil, as well as request for and use of an automated external defibrillator (AED), were missing as a rule. Moreover, 55.0% of Bard's responses contained plausible sounding, but nonsensical guidance, called artificial hallucinations, that create risk for inadequate care and harm to a victim. Conclusion: The LLM-powered chatbots' advice on help to a non-breathing victim omits essential details of resuscitation technique and occasionally contains deceptive, potentially harmful directives. Further research and regulatory measures are required to mitigate risks related to the chatbot-generated misinformation of public on resuscitation.  © 2023 The Author(s).,Background: Chatbots are being piloted to draft responses to patient questions, but patients' ability to distinguish between provider and chatbot responses and patients' trust in chatbots' functions are not well established. Objective: This study aimed to assess the feasibility of using ChatGPT (Chat Generative Pre-trained Transformer) or a similar artificial intelligence-based chatbot for patient-provider communication. Methods: A survey study was conducted in January 2023. Ten representative, nonadministrative patient-provider interactions were extracted from the electronic health record. Patients' questions were entered into ChatGPT with a request for the chatbot to respond using approximately the same word count as the human provider's response. In the survey, each patient question was followed by a provider- or ChatGPT-generated response. Participants were informed that 5 responses were provider generated and 5 were chatbot generated. Participants were asked-and incentivized financially-to correctly identify the response source. Participants were also asked about their trust in chatbots' functions in patient-provider communication, using a Likert scale from 1-5. Results: A US-representative sample of 430 study participants aged 18 and older were recruited on Prolific, a crowdsourcing platform for academic studies. In all, 426 participants filled out the full survey. After removing participants who spent less than 3 minutes on the survey, 392 respondents remained. Overall, 53.3% (209/392) of respondents analyzed were women, and the average age was 47.1 (range 18-91) years. The correct classification of responses ranged between 49% (192/392) to 85.7% (336/392) for different questions. On average, chatbot responses were identified correctly in 65.5% (1284/1960) of the cases, and human provider responses were identified correctly in 65.1% (1276/1960) of the cases. On average, responses toward patients' trust in chatbots' functions were weakly positive (mean Likert score 3.4 out of 5), with lower trust as the health-related complexity of the task in the questions increased. Conclusions: ChatGPT responses to patient questions were weakly distinguishable from provider responses. Laypeople appear to trust the use of chatbots to answer lower-risk health questions. It is important to continue studying patient-chatbot interaction as chatbots move from administrative to more clinical roles in health care. © 2023 The Author(s).,IMPORTANCE Informed consent is a critical component of patient care before invasive procedures, yet it is frequently inadequate. Electronic consent forms have the potential to facilitate patient comprehension if they provide information that is readable, accurate, and complete; it is not known if large language model (LLM)-based chatbots may improve informed consent documentation by generating accurate and complete information that is easily understood by patients. OBJECTIVE To compare the readability, accuracy, and completeness of LLM-based chatbot- vs surgeon-generated information on the risks, benefits, and alternatives (RBAs) of common surgical procedures. DESIGN, SETTING, AND PARTICIPANTS This cross-sectional study compared randomly selected surgeon-generated RBAs used in signed electronic consent forms at an academic referral center in San Francisco with LLM-based chatbot-generated (ChatGPT-3.5, OpenAI) RBAs for 6 surgical procedures (colectomy, coronary artery bypass graft, laparoscopic cholecystectomy, inguinal hernia repair, knee arthroplasty, and spinal fusion). MAIN OUTCOMES AND MEASURES Readability was measured using previously validated scales (Flesh-Kincaid grade level, Gunning Fog index, the Simple Measure of Gobbledygook, and the Coleman-Liau index). Scores range from 0 to greater than 20 to indicate the years of education required to understand a text. Accuracy and completeness were assessed using a rubric developed with recommendations from LeapFrog, the Joint Commission, and the American College of Surgeons. Both composite and RBA subgroup scores were compared. RESULTS The total sample consisted of 36 RBAs, with 1 RBA generated by the LLM-based chatbot and 5 RBAs generated by a surgeon for each of the 6 surgical procedures. The mean (SD) readability score for the LLM-based chatbot RBAs was 12.9 (2.0) vs 15.7 (4.0) for surgeon-generated RBAs (P = .10). The mean (SD) composite completeness and accuracy score was lower for surgeons’ RBAs at 1.6 (0.5) than for LLM-based chatbot RBAs at 2.2 (0.4) (P < .001). The LLM-based chatbot scores were higher than the surgeon-generated scores for descriptions of the benefits of surgery (2.3 [0.7] vs 1.4 [0.7]; P < .001) and alternatives to surgery (2.7 [0.5] vs 1.4 [0.7]; P < .001). There was no significant difference in chatbot vs surgeon RBA scores for risks of surgery (1.7 [0.5] vs 1.7 [0.4]; P = .38). CONCLUSIONS AND RELEVANCE The findings of this cross-sectional study suggest that despite not being perfect, LLM-based chatbots have the potential to enhance informed consent documentation. If an LLM were embedded in electronic health records in a manner compliant with the Health Insurance Portability and Accountability Act, it could be used to provide personalized risk information while easing documentation burden for physicians. © 2023 American Medical Association. All rights reserved."
283,282,10,282_climate_urbanization_weather_urban,"climate,urbanization,weather,urban,temperature,daytime,summer,spatiotemporal,highrise,thermal","Large-scale long-period urban heat island (UHI) intensity (UHII) prediction with high spatiotemporal resolutions, satisfactory accuracy, and calculation efficiency is crucial and challenging for UHI mitigation studies. This study proposes a framework combining an urban weather generator (UWG), local climate zone (LCZ), deep-learning method, and Python automatic cyclical calculation to obtain hourly UHII throughout one year and over a total of 1920 blocks in Hangzhou City. The spatial-averaged hourly UHII are between ?2 °C and 6 °C in more than 96 % time, and those at 0 °C–1 °C contribute to 43.80 % of the total time. Spring gives the most intense nocturnal UHII, while winter has the weakest one. A significant diurnal UCI phenomenon could accompany the strong nocturnal UHI phenomenon. From the synthetic (based on seasonal data) 24-h curves, mean UHII and UCII (average of the positive and negative values, respectively) drop by approximately 25 % in winter compared to spring. Spatially, UHII is higher in the central regions with compact buildings but significantly lower in the high-vegetation-coverage regions. Based on LCZ framework, LCZ 1 (compact high-rise configurations) has the highest UHII, independent of examined periods. UHII relations to building coverage, vegetation ratio, and building height are individually quantified. The building coverage has the highest influence on annual UHII, with a correlation coefficient as high as 0.76. Results indicate that, for UHI mitigation purposes, the percentage of compact high-rise configurations (LCZ 1) in the urban area shall be limited, and the vegetation ratio is better to be greater than 20 %. © 2023 Elsevier Ltd,Driven by ongoing urbanization and accelerated global warming, urban heat stress develops into a major threat for a large part of the world population. While the literature is clear about the relevance of urban form in modifying urban heat, the spatially explicit context of heat modifications by the built environment is insufficiently explored across cities. Here we develop a consistent methodology based on a machine learning model to determine the predictive features of urban form that shape urban ambient temperature (AT) under different urban and climate conditions. For this, we combine urban climate data at 100 m resolution from the UrbClim model with urban form features computed from different open datasets. We exemplify our innovation by evaluating summer daytime and nighttime temperature variation within three European cities that are representative of distinct geographies and climates: Berlin, Zürich, and Sevilla. Mean AT at dense urban spots was higher by 3 °C (on average) during summer compared to sub-urban sites across all cities. We find that urban form features explain around 2/3 of within-city temperature variations. Vegetation cover and water bodies are most significant in explaining spatial temperature patterns during the daytime, while the impervious land cover is most critical at nighttime. Cooling effect of water areas can accumulate to ?1.5 °C, while heating effects of roads can induce +0.5 °C during daytime. The magnitude of effects varies across cities, primarily based on the climate type, altitude, and dominant land-cover. However, further testing should be conducted to develop a better understanding of impact variations. The findings of this research will provide urban planners and policymakers with crucial insights towards heat mitigation and sustainable urban developments through region-tailored modifications and replacement of land-cover. © 2023 Elsevier B.V.,This study presents a fully reproducible clustering-based methodology for the assessment of the urban heat island intensity (UHII) at the territory scale, using parametric microclimate models and limited computational resources. In large-scale climate modeling, a common preliminary operation is to utilize the well-established Local Climate Zone classification to characterize the thermal response of urban areas based on morphology. With the increasing availability of urban datasets, data-driven approaches can be implemented to quantitatively derive meaningful urban features without relying on a standardized classification. The proposed methodology employs a Gaussian Mixture Model clustering algorithm to partition the urban territory into a suitable number of homogeneous microclimate zones, enabling the calculation and mapping of the UHII for each zone through the Urban Weather Generator (UWG) tool. The developed approach is applied to the Canton of Geneva, Switzerland, identifying ten microclimatic areas and analyzing the spatiotemporal variation of UHII. Results show yearly average values of UHII ranging from 1.7 °C to 2.2 °C, depending on urban morphology. The simulated values are partially validated by comparison with on-site measurements from two urban weather stations, yielding a satisfactory agreement. The methodology can support urban planning with the goal of avoid overheating through a large-scale mapping. © 2023"
284,283,10,283_adversarial_attackdefend_attacks_attack,"adversarial,attackdefend,attacks,attack,security,vulnerabilities,exploitability,cyberattacks,attackerx2019s,vulnerability","False data injection attack (FDIA) is a deliberate modification of measurement data collected by the power grid using vulnerabilities in power grid state estimation, resulting in erroneous judgments made by the power grid control center. As a symmetrical defense scheme, FDIA detection usually uses machine learning methods to detect attack samples. However, existing detection models for FDIA typically require large-scale training samples, which are difficult to obtain in practical scenarios, making it difficult for detection models to achieve effective detection performance. In light of this, this paper proposes a novel FDIA sample generation method to construct large-scale attack samples by introducing a hybrid Laplacian model capable of accurately fitting the distribution of data changes. First, we analyze the large-scale power system sensing measurement data and establish the data distribution model of symmetric Laplace distribution. Furthermore, a hybrid Laplace-domain symmetric distribution model with multi-dimensional component parameters is constructed, which can induce a deliberate deviation in the state estimation from its safe value by injecting into the power system measurement. Due to the influence of the multivariate parameters of the hybrid Laplace-domain distribution model, the sample deviation generated by this model can not only obtain an efficient attack effect, but also effectively avoid the recognition of the FDIA detection model. Extensive experiments are carried out over IEEE 14-bus and IEEE 118-bus test systems. The corresponding results unequivocally demonstrate that our proposed attack method can quickly construct large-scale FDIA attack samples and exhibit significantly higher resistance to detection by state-of-the-art detection models, while also offering superior concealment capabilities compared to traditional FDIA approaches. © 2023 by the authors.,Machine learning (ML) sees an increasing prevalence of being used in the internet-of-things (IoT)-based smart grid. However, the trustworthiness of ML is a severe issue that must be addressed to accommodate the trend of ML-based smart grid applications (MLsgAPPs). The adversarial distortion injected into the power signal will greatly affect the system&#x2019;s normal control and operation. Therefore, it is imperative to conduct vulnerability assessment for MLsgAPPs applied in the safety-critical power systems. In this paper, we provide a comprehensive review of the recent progress in designing attack and defense methods for MLsgAPPs. Unlike the traditional survey about ML security, this is the first review work about the security of MLsgAPPs that focuses on the characteristics of power systems. We first highlight the specifics for constructing adversarial attacks on MLsgAPPs. Then, the vulnerability of MLsgAPP is analyzed from the perspective of the power system and ML model, respectively. Afterward, a comprehensive survey is conducted to review and compare existing studies about the adversarial attacks on MLsgAPPs in scenarios of generation, transmission, distribution, and consumption, and the countermeasures are reviewed according to the attacks that they defend against. Finally, the future research directions are discussed on the attacker&#x2019;s and defender&#x2019;s side, respectively. We also analyze the potential vulnerability of large language model-based (e.g., ChatGPT) smart grid applications. Overall, our purpose is to encourage more researchers to contribute to investigating the adversarial issues of MLsgAPPs. IEEE,Penetration testing (PT) is a method for assessing and evaluating the security of digital assets by planning, generating, and executing possible attacks that aim to discover and exploit vulnerabilities. In large networks, penetration testing becomes repetitive, complex and resource consuming despite the use of automated tools. This paper investigates reinforcement learning (RL) to make penetration testing more intelligent, targeted, and efficient. The proposed approach called Intelligent Automated Penetration Testing Framework (IAPTF) utilizes model-based RL to automate sequential decision making. Penetration testing tasks are treated as a partially observed Markov decision process (POMDP) which is solved with an external POMDP-solver using different algorithms to identify the most efficient options. A major difficulty encountered was solving large POMDPs resulting from large networks. This was overcome by representing networks hierarchically as a group of clusters and treating each cluster separately. This approach is tested through simulations of networks of various sizes. The results show that IAPTF with hierarchical network modeling outperforms previous approaches as well as human performance in terms of time, number of tested vectors and accuracy, and the advantage increases with the network size. Another advantage of IAPTF is the ease of repetition for retesting similar networks, which is often encountered in real PT. The results suggest that IAPTF is a promising approach to offload work from and ultimately replace human pen testing. © 2022, The Author(s)."
