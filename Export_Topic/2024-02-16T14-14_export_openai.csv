,Topic,Count,Name,Representation,Representative_Docs
0,-1,6999,-1_Construction safety using computer vision and teacher-student learning,Construction safety using computer vision and teacher-student learning,"Due to the recent advancements in the Internet of Things (IoT) and cloud computing technologies, the detection and prevention of intrusions in enterprise networks have become a crucial and challenging task. Real-time monitoring of network traffic and resources is required to protect those networks from intrusions. An Intrusion Detection System (IDS) analyses the data packets from the network and the system-level applications to detect any malicious activity. However, existing IDSs require all the data, collected at different network nodes, to be collated at one central location to perform the analysis for any model development. This approach hampers the data privacy at the network nodes as the data needs to be shared with other nodes. Furthermore, many of the existing IDSs are unable to adapt to evolving attack patterns, which may result in poor network vulnerability detection and significant degradation in the performance of the systems. To address these limitations, we present a Federated Deep Reinforcement Learning-based IDS in which multiple agents are deployed on the network in a distributed fashion, and each of these agents runs a Deep Q-Network logic. We considered the data privacy concerns of each agent while designing the system. In our system, the data at each agent node is not shared with any other nodes. At the same time, however, all the agents in the system benefit, via the attention weighted model aggregation process, from the distribution and pattern of the data available at all the other agents. We have also developed an attention mechanism that dynamically determines attention value of an agent, which is used in the model aggregation process. Our model can be scaled to large networks and is resistant to hardware or network failures at any agent node. We have tested and evaluated our proposed system on the cloud-based ISOT-CID dataset and the standard benchmark NSL-KDD dataset. The experimental findings demonstrate the performance and robustness of our proposed model in terms of metrics like accuracy, precision, false-positive rate, and area under the ROC curve. © 2023 Elsevier Ltd,Load prediction is one of the basic tasks in energy system operation and management. With the development of data mining and artificial intelligence, data driven-based prediction models have been widely used. However, the accuracy of prediction model depends on large amounts of high-quality training data, that is difficult to obtain in practice, especially for new buildings. In order to solve the problem of data shortage, a building load data transfer model based on one dimensional convolutional neural network is proposed in this paper. The data transfer model can narrow the special features gap between the source domain and target domain, so that the source building data after transferring can be used to complete the prediction task on the target building. To verify the effectiveness of this strategy, the proposed model is applied in four scenes to transfer source building data to target building and then the prediction models are built using transferred data. The results show that the mean absolute percentage errors are reduced by 7.52%, 4.96%, 6.59% and 2.34%, respectively, compared with the model trained on the limited data in the four scenes. This work can provide guidance for the effective use of existing data resources. © 2023,Machine learning now is used across many sectors and provides consistently precise predictions. The machine learning system is able to learn effectively because the training dataset contains examples of previously completed tasks. After learning how to process the necessary data, researchers have proven that machine learning algorithms can carry out the whole work autonomously. In recent years, cancer has become a major cause of the worldwide increase in mortality. Therefore, early detection of cancer improves the chance of a complete recovery, and Machine Learning (ML) plays a significant role in this perspective. Cancer diagnostic and prognosis microarray dataset is available with the biopsy dataset. Because of its importance in making diagnoses and classifying cancer diseases, the microarray data represents a massive amount. It may be challenging to do an analysis on a large number of datasets, though. As a result, feature selection is crucial, and machine learning provides classification techniques. These algorithms choose the relevant features that help build a more precise categorization model. Accurately classifying diseases is facilitated as a result, which aids in disease prevention. This work aims to synthesize existing knowledge on cancer diagnosis using machine learning techniques into a compact report. Current research work aims to propose an ensemble-based machine learning model En-PaFlower using Particle Swarm Optimization (PSO) as the feature selection algorithm, Flower Pollination algorithm (FPA) as the optimization algorithm with the majority voting algorithm. Finally, the performance of the proposed algorithm is evaluated over three different types of cancer disease datasets with accuracy, precision, recall, specificity, and F-1 Score etc as the evaluation parameters. The empirical analysis shows that the proposed methodology shows highest accuracy as 95.65%. © 2023 The Author(s)."
1,0,311,0_Impact of School Closures on Students' Academic Performance and the Role of Teaching Strategies,Impact of School Closures on Students' Academic Performance and the Role of Teaching Strategies,"Recent years have seen a sharp increase in the transition from traditional teaching methods to online distance teaching and learning in all educational institutions including schools and higher education institutions. This change means that student-teachers need to have online learning experience and acquire professional skills needed to become online distance teachers. In the last two decades, research has investigated various aspects of online teaching but a main subject which has not been studied in depth is the learners’ feeling of belonging in an online distance course, especially among student-teachers. The research investigated the contribution of student-teachers’ self-perceived readiness for learning online and a collaborative online learning environment to the learners’ sense of belonging. For this purpose, quantitative and qualitative data were gathered from pre- and post-tests and personal and collaborative blogs from 172 student-teachers’ studying in two large teacher-education colleges in the center of Israel, to measure their self-perceived readiness for online collaborative learning, their sense of belonging and the learning experiences and changes they underwent along the course. It was found that a student-teacher, who had previously enjoyed a good experience in a collaborative online course, and had strong technological literacy for online learning, performed better in an online course, operated optimally in collaborative task performance, and felt a strong sense of belonging to the group at the course’s end. These findings can inform future teacher training, the formation of new pedagogic models and teaching methods for collaborative online environments and their implementation in teaching-learning. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,This study aimed to explore the integration of metacognition in online science education for college students and tested the feasibility of the learning model on students’ high order thinking skills (HOTS). The analyze, design, develop, implement, and evaluate (ADDIE) model was employed in this study. A needs analysis was conducted through interviews and questionnaire surveys to 21 science lecturers from primary school teacher education study programs at seven state universities and 14 private universities in Indonesia. In the development phase, the effectiveness of the model was examined through an experimental study involving three groups of students: experimental group (41 students), control group 1 (39 students), and control group 2 (39 students). The experimental study was performed using the randomized pretest-posttest comparison group design. The research hypothesis was investigated using a general linear model and multivariate analysis of variance. Through awareness-building, essential questioning, planning, monitoring, evaluating, and reflecting, this study successfully integrated metacognition into online science education. The model's learning syntax incorporated both synchronous and asynchronous learning activities. Virtual and contextual projects are critical components of this approach because they demonstrate how metacognition is regulated. Expert judgment indicated that the model under development was highly feasible. The experimental study established that the learning model had a considerable effect on students’ HOTS, which rose by 75% (a large effect) due to the model’s implementation. © 2022, Institute of Advanced Engineering and Science. All rights reserved.,Students with low family socioeconomic status (SES) often have lower academic achievement than their peers with high family SES, as has been widely demonstrated. Nevertheless, there is a group of students beating the odds and achieving academic excellence despite the socio-economic background of their families. The students who have the capacity to overcome adversities and achieve successful educational achievements are referred to as academically resilient students. This study’s purpose was to identify the protective factors among academically resilient students. A total of 46,089 students from 303 primary schools in grade 6, 55,477 students from 256 junior high schools in grade 9, and 37,856 students from 66 high schools in grade 11 in a city in northeast China participated in the large-scale investigation. Students completed a structured questionnaire to report their demographic information, psychological characteristics, and three academic tests. A causal comparative research model was applied to determine significant protective factors associated with resilient students (referring to students are resilient if they are among the 25% most socio-economically disadvantaged students in their city but are able to achieve the top 25% or above in all three academic domains). Multivariable logistic regression analyses found that the intrinsic protective factors for resilient students included higher proportion of academic importance identity, higher proportion of achievement approaching motivation, longer-term future educational expectation, and more positive academic emotion compared with non-resilient students; the extrinsic protective factors included parents’ higher proportion of positive expectations for their children’ future development, as well as more harmonious peer and teacher–student relationships. The results of this study provide important targets for psychological intervention of disadvantaged students, and future intervention studies can increase their likelihood of becoming resilient students by improving their recognition of the importance of learning, stronger motivation for achievement approaching, longer-term expectations for future academic careers, and positive academic emotions and harmonious teacher–student relationships. © 2022 by the authors."
2,1,292,1_Plant disease detection and classification using deep learning techniques,Plant disease detection and classification using deep learning techniques,"The detection of diseases in rice plants is an essential step in ensuring healthy crop growth and maximizing yields. A real-time and accurate plant disease detection technique can assist in the development of mitigation strategies to ensure food security on a large scale and economical rice crop protection. An accurate classification of rice plant diseases using DL and computer vision could create a foundation to achieve a site-specific application of agrochemicals. Image investigation tools are efficient for the early diagnosis of plant diseases and the continuous monitoring of plant health status. This article presents an Enhanced Sea Horse Optimization with Deep Learning-based Multimodal Fusion for Rice Plant Disease Detection and Classification (ESHODL-MFRPDC) technique. The proposed technique employed a DL-based fusion process with a hyperparameter tuning strategy to achieve an improved rice plant disease detection performance. The ESHODL-MFRPDC approach used Bilateral Filtering (BF)-based noise removal and contrast enhancement as a preprocessing step. Furthermore, Mayfly Optimization (MFO) with a Multi-Level Thresholding (MLT) based segmentation process was used to recognize the diseased portions in the leaf image. A fusion of three DL models was used for feature extraction, namely Residual Network (ResNet50), Xception, and NASNet. The Quasi-Recurrent Neural Network (QRNN) was used for the recognition of rice plant diseases, and its hyperparameters were set using the ESHO method. The performance of the ESHODL-MFRPDC method was validated using the rice leaf disease dataset from the UCI database. An extensive comparison study demonstrated the promising performance of the proposed method over others. © 2023, Dr D. Pylarinos. All rights reserved.,Plant health is an important factor in agricultural production as it mostly affected by plant diseases. Due to plant diseases, the growth and crop yield gets affected which results in negative impact on agriculture in terms of economic loss to farmers. In plant disease management, early and accurate disease detection can control its spreading and avoid unnecessary loss to farmers. Traditionally, plant disease detection has been carried out through visual inspection by human experts. This method is based on subjective perception hence it has risk for error in detecting accurate disease. In recent past, researchers have proposes numerous machine learning approaches to detect the plant diseases. Due to advancement in artificial intelligence and electronic gadgets technology, there is large scope for improvement in neural network algorithms for detecting plant diseases early and accurately by extracting leaves features efficiently. To detect tomato plant diseases, the novel convolutional neural network (CNN) model has been proposed in this paper. The hierarchical mixed pooling technique for smoothing to sharpening approach has been used in proposed CNN model. The system uses tomato plant leaf images obtained from Kaggle dataset. The system has been trained with 1000 images of healthy leaf and 1000 images each for nine different diseases frequently occurs in tomato plant. The different training models has been framed and experimented to identify efficient hierarchy of pooling techniques. The CNN training model 3 exhibit smoothing to sharpening approach with “Average-Max-GlobalMax” mixed pooling hierarchy and depicts better performance with a training loss 28.88%, a validation loss 12.61%, a training accuracy 96.46%, and a validation accuracy 95.41% at 20 epochs. Also, the performance of designed system have been evaluated with different state-of-art deep learning algorithms and compared with proposed CNN model. © 2023 University of Bahrain. All rights reserved.,Crop diseases cause a substantial loss in the quantum and quality of agricultural production. Regular monitoring may help in early stage disease detection an d thereby reduction in crop loss. An automatic plant disease identification system based on visual symptoms can provide a smart agriculture solution to such problems. Various solutions for plant disease identification have been provided by researchers using image processing, machine learning and deep learning techniques. In this paper a lightweight Convolutional Neural Network ‘VGG-ICNN’ is introduced for the identification of crop diseases using plant-leaf images. VGG-ICNN consists of around 6 million parameters that are substantially fewer than most of the available high performing deep learning models. The performance of the model is evaluated on five different public datasets covering a large number of crop varieties. These include multiple crop species datasets: PlantVillage and Embrapa with 38 and 93 categories, respectively, and single crop datasets: Apple, Maize, and Rice, each with four, four, and five categories, respectively. Experimental results demonstrate that the method outperforms some of the recent deep learning approaches on crop disease identification, with 99.16% accuracy on the PlantVillage dataset. The model is also shown to perform consistently well on all the five datasets, as compared with some recent lightweight CNN models. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
3,2,220,2_Resource offloading for edge computing and 5G using DRL-based algorithms,Resource offloading for edge computing and 5G using DRL-based algorithms,"Due to the performance and resource limitations of wireless devices at the edge of the network, the intrusion detection system deployed on the mobile edge network will cause severe packet loss when faced with large traffic. Based on this, a collaborative intrusion detection system (CIDS) architecture applied to mobile edge computing is proposed, which can offload part of the detection tasks to an intrusion detection system with better performance and resources on the edge server. On this basis, a task offloading scheduling algorithm based on Deep Q Network (DQN) is proposed. First, the time delay, energy consumption, and offloading decision models are established. Then, the task scheduling process is described as a Markov decision process and the relevant space and value function are established. Finally, the problem of excessive state and action space in Q-learning is solved by the Deep Q Network. Experiments have shown that our proposed scheme enables the system to have advantages over the comparative algorithms in terms of response time, energy consumption, and packet loss rate, etc. © 2022 Elsevier Ltd,The cloud-based resources can accommodate massive energy sources and the main challenge arises when processing the terminal nodes associated with the Internet of Things (IoT). The main challenge the cloud faces when satisfying the user requests is the long delay, large bandwidth, and resource-constrained devices not capable of processing computational needs. These challenges are overcome in this work by using fog computing to process a wide range of IoT requests and workloads near the end user. The main motivation of this work is to enhance the intelligent task offloading decision by achieving a tradeoff between Quality of Service (QoS) and power consumption for a large number of mobile and fog nodes. This paper proposes an Adaptive Deep Belief Network (ADBN) that uses the Hybrid Bayesian Search and Lichtenberg Optimization (BSI-LO) technique to solve this complexity. The hybrid BSI-LO optimized ADBN architecture minimizes the energy consumption of fog devices operating on the network edge by taking different parameters such as battery lifetime, delay, workload, and power consumption into consideration and offers an effective intelligent task allocation decision. In this way, the proposed model handles a large number of requests raised in the IoT-Fog cloud network by efficient resource allocation. The hybrid BSI-LO algorithm is implemented to mutually optimize the resource allocation and offloading strategy. The experimental results confirm that the proposed methodology is effective in terms of improving the bandwidth, task weight cost, energy consumption, delay, and system utility of edge devices resulting in an efficient energy-saving strategy for the IoT-Fog-cloud architecture. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,With the surge of intelligent devices, applications of Internet of Things (IoT) are growing at a rapid pace. As a result, a massive amount of raw data is generated, which must be processed and stored. IoT devices standalone are not enough to handle large amount of data. Hence, to improve the performance, users started to push some jobs to far-situated cloud data centers, which would lead to more complications such as high bandwidth usage, service latency, and energy consumption. Fog computing emerges as a key enabling technology that brings cloud services closer to the end-user. However, owing to the unpredictability of tasks and Quality of Service (QoS) requirements of users, efficient task scheduling and resource allocation mechanisms are needed to balance the demand. To handle the problem efficiently, we have designed the task offloading problem as Markov Decision Process (MDP) by considering various user QoS factors including end-to-end latency, energy consumption, task deadline, and priority. Three different model-free off-policy Deep Reinforcement Learning (DRL) based solutions are outlined to maximize the reward in terms of resource utilization. Finally, extensive experimentation is conducted to validate and compare the efficiency and effectiveness of proposed mechanisms. Results show that with the proposed method, on average 96.23% of tasks can satisfy the deadline with an 8.25% increase. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
4,3,207,3_Climate Change Impacts on Hydrological Forecasting and Precipitation Downscaling,Climate Change Impacts on Hydrological Forecasting and Precipitation Downscaling,"Climate change is expected to alter the magnitude and spatiotemporal patterns of hydro-climate variables such as precipitation, which has significant impacts on the ecosystem, human societies and water security. Global Climate Models are the major tools to simulate historical as well as future precipitation. However, due to imperfect model structures, parameters and boundary conditions, direct model outputs are subject to large uncertainty, which needs serious evaluation and bias correction before usage. In this study, seasonal precipitation predictions from 30 Coupled Model Inter-comparison Project Phase 6 (CMIP6) models and Climate Research Unit observations are used to evaluate historical precipitation climatology in global continents during 1901–2014. A grid based model heterogeneity oriented Convolutional Neural Network (CNN) is proposed to correct the ensemble mean precipitation bias ratio. Besides, regression based Linear Scaling (LS), distribution based Quantile Mapping (QM) and spatial correlation CNN bias correction approaches are employed for comparison. Results of model performance evaluation indicate that generally precipitation prediction is more reliable in JJA than DJF on the global scale. Most models tend to have larger bias ratio for extreme precipitation. In addition, current CMIP6 models still have certain issues in accurate simulation of precipitation in mountainous regions and the regions affected by complex climate systems. Moreover, the proposed grid based model heterogeneity oriented CNN has better performance in ensemble mean bias correction than LS, QM, and spatial correlation CNN, which could consider the relative model performance and capture the features similar to actual climate dynamics. © 2023. American Geophysical Union. All Rights Reserved.,To examine the characteristics of future precipitation under climate change is of great significance to urban water security. In this paper, multiple machine learning techniques, i.e., statistical downscaling model (SDSM), support vector machine (SVM), and multilayer perceptron (MLP), were used to downscale large-scale climatic variables simulated by the General Circulation Models (GCMs) to precipitation on a local scale. It was demonstrated in Shenzhen city, China, through multisite downscaling schemes based on projections from the Max Planck Institute Earth System Model (MPI-ESM1.2-HR), Meteorological Research Institute Earth System Model Version 2.0 (MRI-ESM2.0), and Beijing Climate Center Climate System Model (BCC-CSM2-MR). The obtained results showed that the downscaled precipitation would provide good monthly simulations against observations at 10 discrete stations. Regardless of superior performance of SVM and MLP over SDSM, the daily precipitation simulations should be further improved, and downscaling of heavy daily precipitations would be promoted by quantile mapping corrections. Due to the relatively poor simulation performance of BCC-CSM2-MR, the other two climate models were considered under the Shared Socioeconomic Pathways (SSP1-2.6, SSP2-4.5, and SSP5-8.5 scenarios) for ensemble precipitation projections for 2015-2100. Under the SSP1-2.6 scenario, the amounts of annual average precipitation for 10 stations were estimated to be higher relative to the historical period (2.7%-17%), and 9 out of 10 stations presented an increasing trend. However, downward trends also existed at three stations when it comes to scenarios SSP2-4.5 and SSP5-8.5. Moreover, a significantly positive trend was found to dominate the trend changes of annual extreme daily precipitation during 2015-2050, but the detected trends at stations were greatly dependent on the downscaling techniques and climate models. Besides, the increase in daily extreme precipitations for various return periods as well as statistically different precipitation characteristics for discrete stations would further shed light on urgent demands on urban resilient strategies for climate change adaptation. © 2022 American Society of Civil Engineers.,Rainfall-runoff modeling is a complex nonlinear spatiotemporal prediction problem. However, few studies have considered the spatial characteristics of rainfall-runoff relationship in runoff forecasts based on machine learning. With the emergence of high-resolution Satellite-based Precipitation Products (SPPs) and the continuous improvement of rainfall estimation accuracy, the shortcoming of sparse spatial information for in-situ rainfall monitoring has been made up. Therefore, this study developed a large scale spatiotemporal deep learning rainfall-runoff (SDLRR) forecasting model for hydrological stations in the upper Yangtze River, and evaluated the positive impact of utilizing spatial information of three SPPs on reducing errors of runoff forecasts. The adopted remote sensing precipitation products are bias-corrected Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS), Integrated Multi-satellite Retrievals for Global Precipitation Measurement data (IMERG) and Tropical Rainfall Measuring Mission Multi-satellite Precipitation Analysis data (TMPA). For runoff forecasting at the Luoduxi (LDX) hydrological station, compared to regular Long Short Term Memory Network (LSTM) model, the proposed SDLRR model that utilizing IMERG data as precipitation input (IMERG_SDLRR) improved 15% in terms of Coefficient of Determination (R2) and improved 25% in terms of Root Mean Squared Error (RMSE). Compared to the best performance model among models using area-averaged precipitation as input, IMERG_SDLRR improved 5% in terms of R2 and 11% in terms of RMSE. Good performance was also acquired in the other hydrological stations. For extreme flood forecasts, IMERG_SDLRR decreased Mean Relative Error (MRE) by 0.29 and increased Qualified Rate (QR) by 53% compared to LSTM, and decreased MRE by 0.08 and increased QR by 6% compared to the best performance model using area-averaged precipitation as input. The utilization of IMERG or TMPA spatial information improved the accuracy of runoff forecasting. The accuracy evaluation of SPPs based on the results of spatiotemporal rainfall-runoff forecasts method was also demonstrated. The research is of great significance for developing runoff forecasting methods and optimizing water resources management. © 2022"
5,4,196,4_Using ChatGPT for Fostering Critical Thinking Skills in Higher Education,Using ChatGPT for Fostering Critical Thinking Skills in Higher Education,"Large Language Models (LLMs) and conversational-style generative artificial intelligence (AI) are causing major disruption to higher education pedagogy. The emergence of tools like ChatGPT has raised concerns about plagiarism detection but also presents opportunities for educators to leverage AI to build supportive learning environments. In this commentary, we explore the potential of AI-augmented teaching and learning practice in higher education, discussing both the productive affordances and challenges associated with these technologies. We offer instructional advice for writing instructional text to guide the generation of quality outputs from AI models, as well as a case study to illustrate using AI for assessment design. Ultimately, we suggest that AI should be seen as one tool among many that can be used to enhance teaching and learning outcomes in higher education. Practitioner Notes 1. Learning to write effective instructional prompts for AI models will help augment learning and teaching practice. 2. AI models offer the potential for significant productive affordances, including personalised feedback, adaptive learning pathways, and enhanced student engagement. 3. To successfully integrate AI into higher education, institutions must prioritise faculty development programs that provide training and support for educators to effectively use these technologies in the classroom. 4. Institutions must ensure that AI is used in a way that aligns with their values and mission and that students are informed about how their data is being used. 5. It is important to recognise that AI is not a panacea for all of the challenges facing higher education. Rather, it should be seen as one tool among many that can be used to enhance teaching and learning outcomes. © 2023, University of Wollongong. All rights reserved.,Introduction: The emergence of artificial intelligence (AI) has presented several opportunities to ease human work. AI applications are available for almost every domain of life. A new technology, Chat Generative Pre-Trained Transformer (ChatGPT), was introduced by OpenAI in November 2022, and has become a topic of discussion across the world. ChatGPT-3 has brought many opportunities, as well as ethical and privacy considerations. ChatGPT is a large language model (LLM) which has been trained on the events that happened until 2021. The use of AI and its assisted technologies in scientific writing is against research and publication ethics. Therefore, policies and guidelines need to be developed over the use of such tools in scientific writing. The main objective of the present study was to highlight the use of AI and AI assisted technologies such as the ChatGPT and other chatbots in the scientific writing and in the research domain resulting in bias, spread of inaccurate information and plagiarism. Methodology: Experiments were designed to test the accuracy of ChatGPT when used in research and academic writing. Results: The information provided by ChatGPT was inaccurate and may have far-reaching implications in the field of medical science and engineering. Critical thinking should be encouraged among researchers to raise awareness about the associated privacy and ethical risks. Conclusions: Regulations for ethical and privacy concerns related to the use of ChatGPT in academics and research need to be developed. Copyright © 2023 Guleria et al.,The artificial intelligence (AI) tool ChatGPT, which is based on a large language model (LLM), is gaining popularity in academic institutions, notably in the medical field. This article provides a brief overview of the capabilities of ChatGPT for medical writing and its implications for academic integrity. It provides a list of AI generative tools, common use of AI generative tools for medical writing, and provides a list of AI generative text detection tools. It also provides recommendations for policymakers, information professionals, and medical faculty for the constructive use of AI generative tools and related technology. It also highlights the role of health sciences librarians and educators in protecting students from generating text through ChatGPT in their academic work. © 2023 Health Libraries Group."
6,5,194,5_Machine Learning Applications in Astronomical Surveys,Machine Learning Applications in Astronomical Surveys,"In the era of huge astronomical surveys, machine learning offers promising solutions for the efficient estimation of galaxy properties. The traditional, ‘supervised’ paradigm for the application of machine learning involves training a model on labelled data, and using this model to predict the labels of previously unlabelled data. The semi-supervised ‘pseudo-labelling’ technique offers an alternative paradigm, allowing the model training algorithm to learn from both labelled data and as-yet unlabelled data. We test the pseudo-labelling method on the problems of estimating redshift, stellar mass, and star formation rate, using COSMOS2015 broad band photometry and one of several publicly available machine learning algorithms, and we obtain significant improvements compared to purely supervised learning. We find that the gradient-boosting tree methods CatBoost, XGBoost, and LightGBM benefit the most, with reductions of up to ?15 per cent in metrics of absolute error. We also find similar improvements in the photometric redshift catastrophic outlier fraction. We argue that the pseudo-labelling technique will be useful for the estimation of redshift and physical properties of galaxies in upcoming large imaging surveys such as Euclid and LSST, which will provide photometric data for billions of sources. © 2022 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.,We introduce the Virgo Consortium's FLAMINGO suite of hydrodynamical simulations for cosmology and galaxy cluster physics. To ensure the simulations are sufficiently realistic for studies of large-scale structure, the subgrid prescriptions for stellar and AGN feedback are calibrated to the observed low-redshift galaxy stellar mass function and cluster gas fractions. The calibration is performed using machine learning, separately for each of FLAMINGO's three resolutions. This approach enables specification of the model by the observables to which they are calibrated. The calibration accounts for a number of potential observational biases and for random errors in the observed stellar masses. The two most demanding simulations have box sizes of 1.0 and 2.8 Gpc on a side and baryonic particle masses of 1 × 108 and, respectively. For the latter resolution, the suite includes 12 model variations in a 1 Gpc box. There are 8 variations at fixed cosmology, including shifts in the stellar mass function and/or the cluster gas fractions to which we calibrate, and two alternative implementations of AGN feedback (thermal or jets). The remaining 4 variations use the unmodified calibration data but different cosmologies, including different neutrino masses. The 2.8 Gpc simulation follows 3 × 1011 particles, making it the largest ever hydrodynamical simulation run to z = 0. Light-cone output is produced on-The-fly for up to 8 different observers. We investigate numerical convergence, show that the simulations reproduce the calibration data, and compare with a number of galaxy, cluster, and large-scale structure observations, finding very good agreement with the data for converged predictions. Finally, by comparing hydrodynamical and 'dark-matter-only' simulations, we confirm that baryonic effects can suppress the halo mass function and the matter power spectrum by up to ?20 per cent.  © 2023 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.,Emission-line galaxy classification plays an important role in comprehending the formation and evolution of galaxies. The widely used optical spectral classification method for galaxies is the BPT diagram, which classifies emission-line galaxies on the basis of precise spectral line measurements. Various classical machine learning methods have been utilized to classify galaxy spectra. Deep learning (DL) is more feasible for a huge amount of data, as it can learn patterns autonomously from the original data. This study aims to explore the possibility of applying DL to classify galaxy spectra and improve classification efficiency. A one-dimensional convolutional neural network model called GalSpecNet was constructed to classify emission-line galaxy spectra, which recognizes star-forming, composite, active galactic nucleus (AGN), and normal galaxies with an accuracy of over 93 per cent. This study employs the Gradient-weighted Class Activation Mapping to elucidate the decision-making process of the model by inspecting spectral features that the model prioritizes for each type of galaxy. The findings suggest that the model considers features highly consistent with the conventional BPT method. Subsequently, we applied the model to the cross-matched galaxies of Sloan Digital Sky Survey Data Release 16 (DR16) and Large Sky Area Multi-Object Fiber Spectroscopic Telescope DR8 and present a catalogue comprising of 41 699 star-forming candidates and 55 103 AGN candidates. The catalogue is publicly available. © 2023 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society."
7,6,168,6_Medical Natural Language Processing (NLP) for Clinical Notes,Medical Natural Language Processing (NLP) for Clinical Notes,"Introduction: In the medical field, electronic medical records contain a large amount of textual information, and the unstructured nature of this information makes data extraction and analysis challenging. Therefore, automatic extraction of entity information from electronic medical records has become a significant issue in the healthcare domain. Methods: To address this problem, this paper proposes a deep learning-based entity information extraction model called Entity-BERT. The model aims to leverage the powerful feature extraction capabilities of deep learning and the pre-training language representation learning of BERT(Bidirectional Encoder Representations from Transformers), enabling it to automatically learn and recognize various entity types in medical electronic records, including medical terminologies, disease names, drug information, and more, providing more effective support for medical research and clinical practices. The Entity-BERT model utilizes a multi-layer neural network and cross-attention mechanism to process and fuse information at different levels and types, resembling the hierarchical and distributed processing of the human brain. Additionally, the model employs pre-trained language and sequence models to process and learn textual data, sharing similarities with the language processing and semantic understanding of the human brain. Furthermore, the Entity-BERT model can capture contextual information and long-term dependencies, combining the cross-attention mechanism to handle the complex and diverse language expressions in electronic medical records, resembling the information processing method of the human brain in many aspects. Additionally, exploring how to utilize competitive learning, adaptive regulation, and synaptic plasticity to optimize the model's prediction results, automatically adjust its parameters, and achieve adaptive learning and dynamic adjustments from the perspective of neuroscience and brain-like cognition is of interest. Results and discussion: Experimental results demonstrate that the Entity-BERT model achieves outstanding performance in entity recognition tasks within electronic medical records, surpassing other existing entity recognition models. This research not only provides more efficient and accurate natural language processing technology for the medical and health field but also introduces new ideas and directions for the design and optimization of deep learning models. Copyright © 2023 Lu, Jiang, Shi, Zhong, Gu, Huangfu and Gong.,Background: Longitudinal data on key cancer outcomes for clinical research, such as response to treatment and disease progression, are not captured in standard cancer registry reporting. Manual extraction of such outcomes from unstructured electronic health records is a slow, resource-intensive process. Natural language processing (NLP) methods can accelerate outcome annotation, but they require substantial labeled data. Transfer learning based on language modeling, particularly using the Transformer architecture, has achieved improvements in NLP performance. However, there has been no systematic evaluation of NLP model training strategies on the extraction of cancer outcomes from unstructured text. Results: We evaluated the performance of nine NLP models at the two tasks of identifying cancer response and cancer progression within imaging reports at a single academic center among patients with non-small cell lung cancer. We trained the classification models under different conditions, including training sample size, classification architecture, and language model pre-training. The training involved a labeled dataset of 14,218 imaging reports for 1112 patients with lung cancer. A subset of models was based on a pre-trained language model, DFCI-ImagingBERT, created by further pre-training a BERT-based model using an unlabeled dataset of 662,579 reports from 27,483 patients with cancer from our center. A classifier based on our DFCI-ImagingBERT, trained on more than 200 patients, achieved the best results in most experiments; however, these results were marginally better than simpler “bag of words” or convolutional neural network models. Conclusion: When developing AI models to extract outcomes from imaging reports for clinical cancer research, if computational resources are plentiful but labeled training data are limited, large language models can be used for zero- or few-shot learning to achieve reasonable performance. When computational resources are more limited but labeled training data are readily available, even simple machine learning architectures can achieve good performance for such tasks. © 2023, BioMed Central Ltd., part of Springer Nature.,OBJECTIVE: This work aims to explore the value of Dutch unstructured data, in combination with structured data, for the development of prognostic prediction models in a general practitioner (GP) setting. MATERIALS AND METHODS: We trained and validated prediction models for 4 common clinical prediction problems using various sparse text representations, common prediction algorithms, and observational GP electronic health record (EHR) data. We trained and validated 84 models internally and externally on data from different EHR systems. RESULTS: On average, over all the different text representations and prediction algorithms, models only using text data performed better or similar to models using structured data alone in 2 prediction tasks. Additionally, in these 2 tasks, the combination of structured and text data outperformed models using structured or text data alone. No large performance differences were found between the different text representations and prediction algorithms. DISCUSSION: Our findings indicate that the use of unstructured data alone can result in well-performing prediction models for some clinical prediction problems. Furthermore, the performance improvement achieved by combining structured and text data highlights the added value. Additionally, we demonstrate the significance of clinical natural language processing research in languages other than English and the possibility of validating text-based prediction models across various EHR systems. CONCLUSION: Our study highlights the potential benefits of incorporating unstructured data in clinical prediction models in a GP setting. Although the added value of unstructured data may vary depending on the specific prediction task, our findings suggest that it has the potential to enhance patient care. © The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association."
8,7,160,7_Smart Transportation and Traffic Prediction,Smart Transportation and Traffic Prediction,"Intelligent Transportation Systems (ITS) are becoming increasingly important as traditional traffic management systems struggle to handle the rapid growth of vehicles on the road. Accurate traffic prediction is a critical component of ITS, as it can help improve traffic management, avoid congested roads, and allocate resources more efficiently for connected vehicles. However, modeling traffic in a large and interconnected road network is challenging because of its complex spatio-temporal data. While classical statistics and machine learning methods have been used for traffic prediction, they have limited ability to handle complex traffic data, leading to unsatisfactory accuracy. In recent years, deep learning methods, such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), have shown superior capabilities for traffic prediction. However, most CNN-based models are built for Euclidean grid-structured data, while traffic road network data are irregular and better formatted as graph-structured data. Graph Convolutional Neural Networks (GCNs) have emerged to extend convolution operations to more general graph-structured data. This paper reviews recent developments in traffic prediction using deep learning, focusing on GCNs as a promising technique for handling irregular, graph-structured traffic data. We also propose a novel GCN-based method that leverages attention mechanisms to capture both local and long-range dependencies in traffic data with Kalman Filter, and we demonstrate its effectiveness through experiments on real-world datasets where the model achieved around 5% higher accuracy compared to the original model. © (2023). All Rights Reserved.,Purpose: The study aims to propose an intelligent real-time traffic model to address the traffic congestion problem. The proposed model assists the urban population in their everyday lives by assessing the probability of road accidents and accurate traffic information prediction. It also helps in reducing overall carbon dioxide emissions in the environment and assists the urban population in their everyday lives by increasing overall transportation quality. Design/methodology/approach: This study offered a real-time traffic model based on the analysis of numerous sensor data. Real-time traffic prediction systems can identify and visualize current traffic conditions on a particular lane. The proposed model incorporated data from road sensors as well as a variety of other sources. It is difficult to capture and process large amounts of sensor data in real time. Sensor data is consumed by streaming analytics platforms that use big data technologies, which is then processed using a range of deep learning and machine learning techniques. Findings: The study provided in this paper would fill a gap in the data analytics sector by delivering a more accurate and trustworthy model that uses internet of things sensor data and other data sources. This method can also assist organizations such as transit agencies and public safety departments in making strategic decisions by incorporating it into their platforms. Research limitations/implications: The model has a big flaw in that it makes predictions for the period following January 2020 that are not particularly accurate. This, however, is not a flaw in the model; rather, it is a flaw in Covid-19, the global epidemic. The global pandemic has impacted the traffic scenario, resulting in erratic data for the period after February 2020. However, once the circumstance returns to normal, the authors are confident in their model’s ability to produce accurate forecasts. Practical implications: To help users choose when to go, this study intended to pinpoint the causes of traffic congestion on the highways in the Bay Area as well as forecast real-time traffic speeds. To determine the best attributes that influence traffic speed in this study, the authors obtained data from the Caltrans performance measurement system (PeMS), reviewed it and used multiple models. The authors developed a model that can forecast traffic speed while accounting for outside variables like weather and incident data, with decent accuracy and generalizability. To assist users in determining traffic congestion at a certain location on a specific day, the forecast method uses a graphical user interface. This user interface has been designed to be readily expanded in the future as the project’s scope and usefulness increase. The authors’ Web-based traffic speed prediction platform is useful for both municipal planners and individual travellers. The authors were able to get excellent results by using five years of data (2015–2019) to train the models and forecast outcomes for 2020 data. The authors’ algorithm produced highly accurate predictions when tested using data from January 2020. The benefits of this model include accurate traffic speed forecasts for California’s four main freeways (Freeway 101, I-680, 880 and 280) for a specific place on a certain date. The scalable model performs better than the vast majority of earlier models created by other scholars in the field. The government would benefit from better planning and execution of new transportation projects if this programme were to be extended across the entire state of California. This initiative could be expanded to include the full state of California, assisting the government in better planning and implementing new transportation projects. Social implications: To estimate traffic congestion, the proposed model takes into account a variety of data sources, including weather and incident data. According to traffic congestion statistics, “bottlenecks” account for 40% of traffic congestion, “traffic incidents” account for 25% and “work zones” account for 10% (Traffic Congestion Statistics). As a result, incident data must be considered for analysis. The study uses traffic, weather and event data from the previous five years to estimate traffic congestion in any given area. As a result, the results predicted by the proposed model would be more accurate, and commuters who need to schedule ahead of time for work would benefit greatly. Originality/value: The proposed work allows the user to choose the optimum time and mode of transportation for them. The underlying idea behind this model is that if a car spends more time on the road, it will cause traffic congestion. The proposed system encourages users to arrive at their location in a short period of time. Congestion is an indicator that public transportation needs to be expanded. The optimum route is compared to other kinds of public transit using this methodology (Greenfield, 2014). If the commute time is comparable to that of private car transportation during peak hours, consumers should take public transportation. © 2022, Emerald Publishing Limited.,Large-scale and diversified traffic data resources strongly support research into estimating urban traffic states and predicting traffic flow. There are many studies on traffic prediction, but there is still not a universally applicable real-world traffic flow prediction method. This paper regards urban road sections as a microscopic traffic system. Based on a deep understanding of the traffic state of road sections, it proposes a pertinent traffic flow prediction framework based on the traffic factor state network (TFSN) framework by combining model-driven methods with machine learning to identify traffic patterns in road sections. For different road traffic patterns, it proves mathematically that the state of traffic flow in each period tends to be the state of the corresponding period with greater probability. According to different road patterns and traffic states, suitable traffic flow modeling and prediction methods were selected. The case shows that this method can improve the accuracy of traffic flow predictions. The research results demonstrate that the average absolute percentage error of traffic flow predictions in urban sections selected with different characteristics and models is reduced by 7.51% compared with the direct prediction error method, verifying the effectiveness and usability of the proposed prediction framework. © 2023 Elsevier B.V."
9,8,149,8_Diagnosis of lung disorders using AI and medical imaging,Diagnosis of lung disorders using AI and medical imaging,"The rapid outbreak of coronavirus threatens humans’ life all around the world. Due to the insufficient diagnostic infrastructures, developing an accurate, efficient, inexpensive, and quick diagnostic tool is of great importance. To date, researchers have proposed several detection models based on chest imaging analysis, primarily based on deep neural networks; however, none of which could achieve a reliable and highly sensitive performance yet. Therefore, the nature of this study is primary epidemiological research that aims to overcome the limitations mentioned above by proposing a large-scale publicly available dataset of chest computed tomography scan (CT-scan) images consisting of more than 13k samples. Secondly, we propose a more sensitive deep neural networks model for CT-scan images of the lungs, providing a pixel-wise attention layer on top of the high-level features extracted from the network. Moreover, the proposed model is extended through a transfer learning approach for being applicable in the case of chest X-Ray (CXR) images. The proposed model and its extension have been trained and evaluated through several experiments. The inclusion criteria were patients with suspected PE and positive real-time reverse-transcription polymerase chain reaction (RT-PCR) for SARS-CoV-2. The exclusion criteria were negative or inconclusive RT-PCR and other chest CT indications. Our model achieves an AUC score of 0.886, significantly better than its closest competitor, whose AUC is 0.843. Moreover, the obtained results on another commonly-used benchmark show an AUC of 0.899, outperforming related models. Additionally, the sensitivity of our model is 0.858, while that of its closest competitor is 0.81, explaining the efficiency of pixel-wise attention strategy in detecting coronavirus. Our promising results and the efficiency of the models imply that the proposed models can be considered reliable tools for assisting doctors in detecting coronavirus. © 2022, The Author(s).,Combating the covid19 scourge is a prime concern for the human race today. Rapid diagnosis is critical to identify the infection accurately. Due to the prevalence of public health crisis, reaction-based blood tests are the customary approach for identifying covid19. As a result, scientists are analyzing screening methods like deep layered machine learning on chest radiographs. Despite their usefulness, these approaches have large computational costs, rendering them unworkable in practice. This study's main goal is to establish an accurate yet efficient method for predicting SARS-CoV-2 infection (Severe Acute Respiratory Syndrome CoronaVirus 2) using chest radiography pictures. We utilized and enhanced the graph-based family of neural networks to achieve the stated goal. The IsoCore algorithm is trained on a collection of X-ray images separated into four categories: healthy, Covid19, viral pneumonia, and bacterial pneumonia. The IsoCore model has 5 to 10 times fewer parameters than the other tested designs. It attains an overall accuracy of 99.79%. We believe the acquired results are the most ideal in the deep inference domain at this time. This proposed model might be employed by doctors via phones. © The Authors.,Pneumonia has been directly responsible for a huge number of deaths all across the globe. Pneumonia shares visual features with other respiratory diseases, such as tuberculosis, which can make it difficult to distinguish between them. Moreover, there is significant variability in the way chest X-ray images are acquired and processed, which can impact the quality and consistency of the images. This can make it challenging to develop robust algorithms that can accurately identify pneumonia in all types of images. Hence, there is a need to develop robust, data-driven algorithms that are trained on large, high-quality datasets and validated using a range of imaging techniques and expert radiological analysis. In this research, a deep-learning-based model is demonstrated for differentiating between normal and severe cases of pneumonia. This complete proposed system has a total of eight pre-trained models, namely, ResNet50, ResNet152V2, DenseNet121, DenseNet201, Xception, VGG16, EfficientNet, and MobileNet. These eight pre-trained models were simulated on two datasets having 5856 images and 112,120 images of chest X-rays. The best accuracy is obtained on the MobileNet model with values of 94.23% and 93.75% on two different datasets. Key hyperparameters including batch sizes, number of epochs, and different optimizers have all been considered during comparative interpretation of these models to determine the most appropriate model. © 2023 by the authors."
10,9,147,9_Graph Embedding and Representation Learning,Graph Embedding and Representation Learning,"Unsupervised graph representation learning aims to learn low-dimensional node embeddings without supervision while preserving graph topological structures and node attributive features. Previous Graph Neural Networks (GNN) require a large number of labeled nodes, which may not be accessible in real-world applications. To this end, we present a novel unsupervised graph neural network model with Cluster-aware Self-training and Refining (CLEAR). Specifically, in the proposed CLEAR model, we perform clustering on the node embeddings and update the model parameters by predicting the cluster assignments. To avoid degenerate solutions of clustering, we formulate the graph clustering problem as an optimal transport problem and leverage a balanced clustering strategy. Moreover, we observe that graphs often contain inter-class edges, which mislead the GNN model to aggregate noisy information from neighborhood nodes. Therefore, we propose to refine the graph topology by strengthening intra-class edges and reducing node connections between different classes based on cluster labels, which better preserves cluster structures in the embedding space. We conduct comprehensive experiments on two benchmark tasks using real-world datasets. The results demonstrate the superior performance of the proposed model over baseline methods. Notably, our model gains over 7% improvements in terms of accuracy on node clustering over state-of-the-arts. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.,Information aggregation and propagation over networks via graph neural networks (GNNs) plays an important role in node or graph representation learning, which currently depend on the calculation with a fixed adjacency matrix, facing over-smoothing problem, and difficulty to stack multiple layers for high-level representations. In contrast, Transformer calculates an importance score for each node to learn its embedding via the attention mechanism and has achieved great successes in many natural language processing (NLP) and computer vision (CV) tasks. However, Transformer is inflexible to extend to graphs, as its input and output must have the same dimension. It will also become intractable to allocate attention over a large-scale graph due to distractions. Moreover, most graph Transformers are trained in supervised ways, which consume additional resources to annotate samples with potentially wrong labels and have limited generalization of representations. Therefore, this article attempts to build a new Sparse Graph Transformer with Contrastive learning for graph representation learning, called SGTC. Specifically, we first employ centrality measures to remove the redundant topological information from input graph according to the influences of nodes and edges, then disturb the pruned graph to get two different augmentation views, and learn node representations in a contrastive manner. Besides, a novel sparse attention mechanism is also proposed to capture structural features of graphs, which effectively save memory and training time. SGTC can produce low-dimensional and high-order node representations, which have better generalization for multiple tasks. The proposed model is evaluated on three downstream tasks over six networks, and experimental results confirm its superior performance against the state-of-the-art baselines. © 2014 IEEE.,Graph embedding is an important technique used for representing graph structure data that preserves intrinsic features in a low-dimensional space suitable for graph-based applications. Graphs containing node attributes and weighted links are commonly employed to model various real-world problems and issues in computer science. In recent years, a hot research topic has been the exploitation of diverse information, including node attributes and topological semantic information, in graph embedding. However, due to limitations in deep learning based on neural networks, such information has not been fully utilized nor adequately integrated in existing models, leaving graph embedding unsatisfactory, especially for large resource graphs (e.g., knowledge graphs and task interaction graphs). In this study, we introduce a resource-centric graph embedding approach based on deep random forests learning, which reconstructs graphs using a deep autoencoder to achieve high effectiveness. To accomplish this, our approach employs three key components. The first component is a preprocessor driven by graph similarity, alongside modularity and self-attention modules, to comprehensively integrate graph representation. The second component utilizes local graph information structures to enhance the raw graph. Finally, we integrate diverse information using multi-grained scanning and dual-level cascade forests in the deep learning extractor and generator, ultimately producing the final graph embedding. Experimental results on seven real-world scenarios show that our approach outperforms state-of-the-art embedding methods. © 2023 Elsevier Ltd"
11,10,147,10_Hardware Accelerators for Sparse Transformer Models,Hardware Accelerators for Sparse Transformer Models,"Graph optimization problems (such as minimum vertex cover, maximum cut, traveling salesman problems) appear in many fields including social sciences, power systems, chemistry, and bioinformatics. Recently, deep reinforcement learning (DRL) has shown success in automatically learning good heuristics to solve graph optimization problems. However, the existing RL systems either do not support graph RL environments or do not support multiple or many GPUs in a distributed setting. This has compromised the ability of reinforcement learning in solving large-scale graph optimization problems due to lack of parallelization and high scalability. To address the challenges of parallelization and scalability, we develop RL4GO, a high-performance distributed-GPU DRL framework for solving graph optimization problems. RL4GO focuses on a class of computationally demanding RL problems, where both the RL environment and policy model are highly computation intensive. Traditional reinforcement learning systems often assume either the RL environment is of low time complexity or the policy model is small. In this work, we distribute large-scale graphs across distributed GPUs and use the spatial parallelism and data parallelism to achieve scalable performance. We compare and analyze the performance of the spatial parallelism and data parallelism and show their differences. To support graph neural network (GNN) layers that take as input data samples partitioned across distributed GPUs, we design parallel mathematical kernels to perform operations on distributed 3D sparse and 3D dense tensors. To handle costly RL environments, we design a parallel graph environment to scale up all RL-environment-related operations. By combining the scalable GNN layers with the scalable RL environment, we are able to develop high-performance RL4GO training and inference algorithms in parallel. Furthermore, we propose two optimization techniques - replay buffer on-the-fly graph generation and adaptive multiple-node selection - to minimize the spatial cost and accelerate reinforcement learning. This work also conducts in-depth analyses of parallel efficiency and memory cost and shows that the designed RL4GO algorithms are scalable on numerous distributed GPUs. Evaluations on large-scale graphs show that (1) RL4GO training and inference can achieve good parallel efficiency on 192 GPUs, (2) its training time can be 18 times faster than the state-of-the-art Gorila distributed RL framework [34], and (3) its inference performance achieves a 26 times improvement over Gorila.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.,Deep learning is employed in many applications, such as computer vision, natural language processing, robotics, and recommender systems. Large and complex neural networks lead to high accuracy; however, they adversely affect many aspects of deep learning performance, such as training time, latency, throughput, energy consumption, and memory usage in the training and inference stages. To solve these challenges, various optimization techniques and frameworks have been developed for the efficient performance of deep learning models in the training and inference stages. Although optimization techniques such as quantization have been studied thoroughly in the past, less work has been done to study the performance of frameworks that provide quantization techniques. In this paper, we have used different performance metrics to study the performance of various quantization frameworks, including TensorFlow automatic mixed precision and TensorRT. These performance metrics include training time and memory utilization in the training stage along with latency and throughput for graphics processing units (GPUs) in the inference stage. We have applied the automatic mixed precision (AMP) technique during the training stage using the TensorFlow framework, while for inference we have utilized the TensorRT framework for the post-training quantization technique using the TensorFlow TensorRT (TF-TRT) application programming interface (API).We performed model profiling for different deep learning models, datasets, image sizes, and batch sizes for both the training and inference stages, the results of which can help developers and researchers to devise and deploy efficient deep learning models for GPUs. © 2023 by the authors.,Deep neural network (DNN) obtained satisfactory results on different vision tasks; however, they usually suffer from large models and massive parameters during model deployment. While DNN compression can reduce the memory footprint of deep model effectively, so that the deep model can be deployed on portable devices. However, most of the existing model compression methods cost lots of time, e.g., vector quantization or pruning, which makes them inept to the application that needs fast computation. In this paper, we therefore explore how to accelerate the model compression process by reducing the computation cost. Then, we propose a new model compression method, termed dictionary-pair-based fast data-free DNN compression, which aims at reducing the memory consumption of DNNs without extra training and can greatly improve the compression efficiency. Specifically, our method performs tensor decomposition of DNN model with a fast dictionary-pair learning-based reconstruction approach, which can be deployed on different weight layers (e.g., convolution and fully connected layers). Given a pre-trained DNN model, we first divide the parameters (i.e., weights) of each layer into a series of partitions for dictionary pair-driven fast reconstruction, which can potentially discover more fine-grained information and provide the possibility for parallel model compression. Then, dictionaries of less memory occupation are learned to reconstruct the weights. Moreover, automatic hyper-parameter tuning and shared-dictionary mechanism is proposed to improve the model performance and availability. Extensive experiments on popular DNN models (i.e., VGG-16, ResNet-18 and ResNet-50) showed that our proposed weight compression method can significantly reduce the memory footprint and speed up the compression process, with less performance loss. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
12,11,143,11_Medical Image Segmentation Methods with Semi-Supervised Learning,Medical Image Segmentation Methods with Semi-Supervised Learning,"Accurate and massive medical image annotation data is crucial for diagnosis, surgical planning, and deep learning in the development of medical images. However, creating large annotated datasets is challenging because labeling medical images is complicated, laborious, and time-consuming and requires expensive and professional medical skills. To significantly reduce the cost of labeling, an interactive image annotation framework based on composite geodesic distance is proposed, and medical images are labeled through segmentation. This framework uses Attention U-net to obtain initial segmentation based on adding user interaction to indicate incorrect segmentation. Another Attention U-net takes the user's interaction with the initial segmentation as input. It uses a composite geodesic distance transform to convert the user's interaction into constraints, giving accurate segmentation results. To further improve the labeling efficiency for large datasets, this paper validates the proposed framework against the segmentation background of a self-built prostate MRI image datasets. Experimental results show that the proposed method achieves higher accuracy in less interactive annotation and less time than traditional interactive annotation methods with better Dice and Jaccard results. This has important implications for improving medical diagnosis, surgical planning, and the development of deep-learning models in medical imaging. © 2023 Elsevier Ltd,Background and objective: Deep learning-based methods for fast target segmentation of magnetic resonance imaging (MRI) have become increasingly popular in recent years. Generally, the success of deep learning methods in medical image segmentation tasks relies on a large amount of labeled data. The time-consuming and labor-intensive problem of data annotation is a major challenge in medical image segmentation tasks. The aim of this work is to enhance the segmentation of MR images using a semi-supervised learning-based method using a small amount of labeled data and a large amount of unlabeled data. Methods: To utilize the effective information of the unlabeled data, we designed the method of guiding the Student segmentation model simultaneously by the Dual-Teacher structure of CNN and transformer forming the subject network. Both Teacher A and Student models are CNNs, and the TA-S module they form is a mean teacher structure with added data noise. In the TB-S module formed by the combination of Student and Teacher B models, their backbone networks CNN and transformer capture the local and global information of the image at the same time, respectively, to create pseudo labels for each other and perform cross-supervision. The Dual-Teacher guides the Student through synchronous training and performs knowledge rectification and communication with each other through consistent regular constraints, which better utilizes the valid information in the unlabeled data. In addition, the segmentation predictions of Teacher A and Student and Teacher A and Teacher B are screened for uncertainty assessment during the training process to enhance the prediction accuracy and generalization of the model. This method uses the mechanism of simultaneous training of the synthetic structure composed of TA-S and TB-S modules to jointly guide the optimization of the Student model to obtain better segmentation ability. Results: We evaluated the proposed method on a publicly available MRI dataset from a cardiac segmentation competition organized by MICCAI in 2017. Compared with several existing state-of-the-art semi-supervised segmentation methods, the method achieves better segmentation results in terms of Dice coefficient and HD distance evaluation metrics of 0.878 and 4.9 mm and 0.886 and 5.0 mm, respectively, using a training set containing only 10% and 20% of labeled data. Conclusion: This method fuses CNN and transformer to design a new Teacher-Student semi-supervised learning optimization strategy, which greatly improves the utilization of a large number of unlabeled medical images and the effectiveness of model segmentation results. © 2022 Elsevier B.V.,The availability of large, high-quality annotated datasets in the medical domain poses a substantial challenge in segmentation tasks. To mitigate the reliance on annotated training data, self-supervised pre-training strategies have emerged, particularly employing contrastive learning methods on dense pixel-level representations. In this work, we proposed to capitalize on intrinsic anatomical similarities within medical image data and develop a semantic segmentation framework through a self-supervised fusion network, where the availability of annotated volumes is limited. In a unified training phase, we combine segmentation loss with contrastive loss, enhancing the distinction between significant anatomical regions that adhere to the available annotations. To further improve the segmentation performance, we introduce an efficient parallel transformer module that leverages Multiview multiscale feature fusion and depth-wise features. The proposed transformer architecture, based on multiple encoders, is trained in a self-supervised manner using contrastive loss. Initially, the transformer is trained using an unlabeled dataset. We then fine-tune one encoder using data from the first stage and another encoder using a small set of annotated segmentation masks. These encoder features are subsequently concatenated for the purpose of brain tumor segmentation. The multiencoder-based transformer model yields significantly better outcomes across three medical image segmentation tasks. We validated our proposed solution by fusing images across diverse medical image segmentation challenge datasets, demonstrating its efficacy by outperforming state-of-the-art methodologies. IEEE"
13,12,141,12_IoT Intrusion Detection System,IoT Intrusion Detection System,"Intrusion detection systems (IDS) play a critical role in safeguarding computer networks against unauthorized access and malicious activities. However, traditional IDS approaches face challenges in accurately detecting complex and evolving cyber threats. The proposed framework leverages the power of deep learning to automatically extract meaningful features from network traffic data, enabling more accurate and robust intrusion detection. The proposed deep convolutional neural network (DCNN) has been trained on large-scale datasets, incorporating both normal and malicious network traffic, to enable effective discrimination between normal and anomalous behavior. To evaluate the performance of the framework, a comprehensive performance evaluation approach is developed, considering key metrics such as detection accuracy, false positive rate, and computational efficiency. Additionally, GPU has been utilized for boosting the performance of the model, demonstrating the effectiveness and superiority of the deep CNN-based intrusion detection system over traditional methods. The novelty of this study lies in the development of a dependable intrusion detection system that harnesses the potential of DCNN for network traffic analysis. The proposed framework is evaluated with four publicly available IDS datasets, namely ISCX-IDS 2012, DDoS (Kaggle), CICIDS2017, and CICIDS2018. Our results demonstrate the effectiveness of the optimized DCNN model in improving IDS performance and accuracy. With detection accuracy levels ranging from 99.79% to 100%, our results underscore the model's efficacy, offering a dependable and efficient approach for the detection of cyber threats. The outcomes of this study have significant implications for network security, providing valuable insights for practitioners and researchers working towards building robust and intelligent intrusion detection systems. © 2023 The Author(s),Deep learning (DL) techniques are being widely researched for their effectiveness in detecting cyber intrusions against the Internet of Things (IoT). Time sensitive Critical Infrastructures (CIs) that rely on IoT require rapid detection of cyber intrusions close to the constrained devices in order to prevent service delays. Deep learning techniques perform better in detecting attacks compared to shallow machine learning algorithms and can be used for intrusion detection. However, communication overheads due to large volume of IoT data and computation requirements for deep learning models prevents effective application of deep learning models closer to the constrained devices. Existing IDS techniques are either based on shallow learning algorithms or not trained on relevant IoT datasets and furthermore not designed for distributed fog-cloud deployment. To counter these issues, we propose a novel fog-cloud based IoT intrusion detection framework which incorporates a distributed processing by splitting the dataset according to attack class and a feature selection step on time-series IoT data. This is followed by a deep learning Recurrent Neural Network (SimpleRNN and Bi-directional Long Short-Term Memory (LSTM)) for attack detection. The effectiveness of the proposed approach was evaluated using the high-dimensional BoT-IoT dataset which contains large volumes of realistic IoT attack traffic. Results show that feature selection methods significantly reduced the dataset size by 90% under the computation requirements without compromising on the attack detection ability. The models built on reduced dataset achieved higher recall rate compared to models trained on full feature set without loosing class differentiation ability. The SimpleRNN and Bi-LSTM models also did not suffer any underfitting or overfitting with the reduced feature space. The proposed deep learning based IoT intrusion detection framework is suitable for fog-cloud based deployment and can scale well even with large volumes of IoT data. © 2023 Elsevier B.V.,With the rapid development of the Internet of Things (IoT), there are several challenges pertaining to security in IoT applications. Compared with the characteristics of the traditional Internet, the IoT has many problems, such as large assets, complex and diverse structures, and lack of computing resources. Traditional network intrusion detection systems cannot meet the security needs of IoT applications. In view of this situation, this study applies cloud computing and machine learning to the intrusion detection system of IoT to improve detection performance. Usually, traditional intrusion detection algorithms require considerable time for training, and these intrusion detection algorithms are not suitable for cloud computing due to the limited computing power and storage capacity of cloud nodes; therefore, it is necessary to study intrusion detection algorithms with low weights, short training time, and high detection accuracy for deployment and application on cloud nodes. An appropriate classification algorithm is a primary factor for deploying cloud computing intrusion prevention systems and a prerequisite for the system to respond to intrusion and reduce intrusion threats. This paper discusses the problems related to IoT intrusion prevention in cloud computing environments. Based on the analysis of cloud computing security threats, this study extensively explores IoT intrusion detection, cloud node monitoring, and intrusion response in cloud computing environments by using cloud computing, an improved extreme learning machine, and other methods. We use the Multi-Feature Extraction Extreme Learning Machine (MFE-ELM) algorithm for cloud computing, which adds a multi-feature extraction process to cloud servers, and use the deployed MFE-ELM algorithm on cloud nodes to detect and discover network intrusions to cloud nodes. In our simulation experiments, a classical dataset for intrusion detection is selected as a test, and test steps such as data preprocessing, feature engineering, model training, and result analysis are performed. The experimental results show that the proposed algorithm can effectively detect and identify most network data packets with good model performance and achieve efficient intrusion detection for heterogeneous data of the IoT from cloud nodes. Furthermore, it can enable the cloud server to discover nodes with serious security threats in the cloud cluster in real time, so that further security protection measures can be taken to obtain the optimal intrusion response strategy for the cloud cluster. © 2022 Chongqing University of Posts and Telecommunications"
14,13,140,13_Sentiment analysis of text reviews in social media and online platforms.,Sentiment analysis of text reviews in social media and online platforms.,"Technological advancements in e-commerce and Web 2.0 have revolutionized the way customers express their opinions about services and features through reviews on various websites. This trend is especially prevalent in the travel industry, where online sources provide valuable insights into the food and shelter aspects of destinations. However, the sheer volume of reviews available online makes it challenging for travellers to filter relevant information. To address this issue, aspect-based sentiment analysis (ABSA) is proposed as a technique to extract feature-based opinions. Topic modelling and sentiment analysis are two significant techniques used to assist with this analysis. Topic modelling is the process of identifying thematic relationships among documents, while sentiment analysis identifies the opinions expressed in the text. In this study, one of the leading travel websites, Tripadvisor, is used to collect customer reviews of various restaurants, which are then subjected to aspect-based sentiment analysis using latent dirichlet allocation (LDA) and ensemble bagging support vector machine (EBSVM) classifier techniques. The aim is to identify the most relevant aspect for the restaurant domain and improve sentiment analysis performance. To address the issue of class imbalances in the datasets, synthetic minority over-sampling technique (SMOTE) is implemented. The performance of the LDA is evaluated using the coherence score, which produces quality topics for restaurant reviews. The effectiveness of the EBSVM classifier is measured using accuracy, precision, recall, and F1 score. The proposed model achieves an accuracy of 96.1%, which outperforms other techniques discussed in the existing literature. Overall, this study demonstrates the effectiveness of aspect-based sentiment analysis in extracting relevant opinions from a large volume of reviews and highlights the potential of machine learning techniques in improving sentiment analysis performance. The suggested approach enhances overall performance when compared to other techniques discussed in the existing literature. © 2023 Shini George and V. Srividhya.,With the recent expansion of social media in the form of social networks, online portals, and microblogs, users have generated a vast number of opinions, reviews, ratings, and feedback. Businesses, governments, and individuals benefit greatly from this information. While this information is intended to be informative, a large portion of it necessitates the use of text mining and sentiment analysis models. It is a matter of concern that reviews on social media lack text context semantics. A model for sentiment classification for customer reviews based on manifold dimensions and manifold modeling is presented to fully exploit the sentiment data provided in reviews and handle the issue of the absence of text context semantics. This paper uses a deep learning framework to model review texts using two dimensions of language texts and ideogrammatic icons and three levels of documents, sentences, and words for a text context semantic analysis review that enhances the precision of the sentiment categorization process. Observations from the experiments show that the proposed model outperforms the current sentiment categorization techniques by more than 8.86%, with an average accuracy rate of 97.30%. © 2023 by the authors.,Sentiment Analysis is a method to identify, extract, and quantify people’s feelings, opinions, or attitudes. The wealth of online data motivates organizations to keep tabs on customers’ opinions and feelings by turning to sentiment analysis tasks. Along with the sentiment analysis, the emotion analysis of written reviews is also essential to improve customer satisfaction with restaurant service. Due to the availability of massive online data, various computerized methods are proposed in the literature to decipher text sentiments. The majority of current methods rely on machine learning, which necessitates the pre-training of large datasets and incurs substantial space and time complexity. To address this issue, we propose a novel unsupervised sentiment classification model. This study presents an unsupervised mathematical optimization framework to perform sentiment and emotion analysis of reviews. The proposed model performs two tasks. First, it identifies a review’s positive and negative sentiment polarities, and second, it determines customer satisfaction as either satisfactory or unsatisfactory based on a review. The framework consists of two stages. In the first stage, each review’s context, rating, and emotion scores are combined to generate performance scores. In the second stage, we apply a non-cooperative game on performance scores and achieve Nash Equilibrium. The output from this step is the deduced sentiment of the review and the customer’s satisfaction feedback. The experiments were performed on two restaurant review datasets and achieved state-of-the-art results. We validated and established the significance of the results through statistical analysis. The proposed model is domain and language-independent. The proposed model ensures rational and consistent results. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
15,14,135,14_Medical Applications of ChatGPT and Language Models,Medical Applications of ChatGPT and Language Models,"Background: Large language models (LLMs) are garnering wide interest due to their human-like and contextually relevant responses. However, LLMs’ accuracy across specific medical domains has yet been thoroughly evaluated. Myopia is a frequent topic which patients and parents commonly seek information online. Our study evaluated the performance of three LLMs namely ChatGPT-3.5, ChatGPT-4.0, and Google Bard, in delivering accurate responses to common myopia-related queries. Methods: We curated thirty-one commonly asked myopia care-related questions, which were categorised into six domains—pathogenesis, risk factors, clinical presentation, diagnosis, treatment and prevention, and prognosis. Each question was posed to the LLMs, and their responses were independently graded by three consultant-level paediatric ophthalmologists on a three-point accuracy scale (poor, borderline, good). A majority consensus approach was used to determine the final rating for each response. ‘Good’ rated responses were further evaluated for comprehensiveness on a five-point scale. Conversely, ‘poor’ rated responses were further prompted for self-correction and then re-evaluated for accuracy. Findings: ChatGPT-4.0 demonstrated superior accuracy, with 80.6% of responses rated as ‘good’, compared to 61.3% in ChatGPT-3.5 and 54.8% in Google Bard (Pearson's chi-squared test, all p ? 0.009). All three LLM-Chatbots showed high mean comprehensiveness scores (Google Bard: 4.35; ChatGPT-4.0: 4.23; ChatGPT-3.5: 4.11, out of a maximum score of 5). All LLM-Chatbots also demonstrated substantial self-correction capabilities: 66.7% (2 in 3) of ChatGPT-4.0's, 40% (2 in 5) of ChatGPT-3.5's, and 60% (3 in 5) of Google Bard's responses improved after self-correction. The LLM-Chatbots performed consistently across domains, except for ‘treatment and prevention’. However, ChatGPT-4.0 still performed superiorly in this domain, receiving 70% ‘good’ ratings, compared to 40% in ChatGPT-3.5 and 45% in Google Bard (Pearson's chi-squared test, all p ? 0.001). Interpretation: Our findings underscore the potential of LLMs, particularly ChatGPT-4.0, for delivering accurate and comprehensive responses to myopia-related queries. Continuous strategies and evaluations to improve LLMs’ accuracy remain crucial. Funding: Dr Yih-Chung Tham was supported by the National Medical Research Council of Singapore (NMRC/MOH/HCSAINV21nov-0001). © 2023 The Author(s),BACKGROUND: This study aimed to assess the performance of ChatGPT, a large language model (LLM), on the Italian State Exam for Medical Residency (SSM) test to determine its potential as a tool for medical education and clinical decision-making support. MATERIALS AND METHODS: A total of 136 questions were obtained from the official SSM test. ChatGPT responses were analyzed and compared to the performance of medical doctors who took the test in 2022. Questions were classified into clinical cases (CC) and notional questions (NQ). RESULTS: ChatGPT achieved an overall accuracy of 90.44%, with higher performance on clinical cases (92.45%) than on notional questions (89.15%). Compared to medical doctors' scores, ChatGPT performance was higher than 99.6% of the participants. CONCLUSIONS: These results suggest that ChatGPT holds promise as a valuable tool in clinical decision-making, particularly in the context of clinical reasoning. Further research is needed to explore the potential applications and implementation of large language models (LLMs) in medical education and medical practice.,Background: Chat Generative Pre-Trained Transformer (ChatGPT) is an artificial learning and large language model tool developed by OpenAI in 2022. It utilizes deep learning algorithms to process natural language and generate responses, which renders it suitable for conversational interfaces. ChatGPT’s potential to transform medical education and clinical practice is currently being explored, but its capabilities and limitations in this domain remain incompletely investigated. The present study aimed to assess ChatGPT’s performance in medical knowledge competency for problem assessment in obstetrics and gynecology (OB/GYN). Methods: Two datasets were established for analysis: questions (1) from OB/GYN course exams at a German university hospital and (2) from the German medical state licensing exams. In order to assess ChatGPT’s performance, questions were entered into the chat interface, and responses were documented. A quantitative analysis compared ChatGPT’s accuracy with that of medical students for different levels of difficulty and types of questions. Additionally, a qualitative analysis assessed the quality of ChatGPT’s responses regarding ease of understanding, conciseness, accuracy, completeness, and relevance. Non-obvious insights generated by ChatGPT were evaluated, and a density index of insights was established in order to quantify the tool’s ability to provide students with relevant and concise medical knowledge. Results: ChatGPT demonstrated consistent and comparable performance across both datasets. It provided correct responses at a rate comparable with that of medical students, thereby indicating its ability to handle a diverse spectrum of questions ranging from general knowledge to complex clinical case presentations. The tool’s accuracy was partly affected by question difficulty in the medical state exam dataset. Our qualitative assessment revealed that ChatGPT provided mostly accurate, complete, and relevant answers. ChatGPT additionally provided many non-obvious insights, especially in correctly answered questions, which indicates its potential for enhancing autonomous medical learning. Conclusion: ChatGPT has promise as a supplementary tool in medical education and clinical practice. Its ability to provide accurate and insightful responses showcases its adaptability to complex clinical scenarios. As AI technologies continue to evolve, ChatGPT and similar tools may contribute to more efficient and personalized learning experiences and assistance for health care providers. Copyright © 2023 Riedel, Kaefinger, Stuehrenberg, Ritter, Amann, Graf, Recker, Klein, Kiechle, Riedel and Meyer."
16,15,135,"15_EEG-based Sleep Staging, Seizure Detection, and Motor Imagery Classification","EEG-based Sleep Staging, Seizure Detection, and Motor Imagery Classification","Introduction: Motor imagery electroencephalography (MI-EEG) has significant application value in the field of rehabilitation, and is a research hotspot in the brain-computer interface (BCI) field. Due to the small training sample size of MI-EEG of a single subject and the large individual differences among different subjects, existing classification models have low accuracy and poor generalization ability in MI classification tasks. Methods: To solve this problem, this paper proposes a electroencephalography (EEG) joint feature classification algorithm based on instance transfer and ensemble learning. Firstly, the source domain and target domain data are preprocessed, and then common space mode (CSP) and power spectral density (PSD) are used to extract spatial and frequency domain features respectively, which are combined into EEG joint features. Finally, an ensemble learning algorithm based on kernel mean matching (KMM) and transfer learning adaptive boosting (TrAdaBoost) is used to classify MI-EEG. Results: To validate the effectiveness of the algorithm, this paper compared and analyzed different algorithms on the BCI Competition IV Dataset 2a, and further verified the stability and effectiveness of the algorithm on the BCI Competition IV Dataset 2b. The experimental results show that the algorithm has an average accuracy of 91.5% and 83.7% on Dataset 2a and Dataset 2b, respectively, which is significantly better than other algorithms. Discussion: The statement explains that the algorithm fully exploits EEG signals and enriches EEG features, improves the recognition of the MI signals, and provides a new approach to solving the above problem. Copyright © 2023 Wang, Dai, Liu, Chen, Hu, Hu and Li.,Electroencephalogram (EEG) signals have been widely studied in human emotion recognition. The majority of existing EEG emotion recognition algorithms utilize dozens or hundreds of electrodes covering the whole scalp region (denoted as full-channel EEG devices in this paper). Nowadays, more and more portable and miniature EEG devices with only a few electrodes (denoted as few-channel EEG devices in this paper) are emerging. However, emotion recognition from few-channel EEG data is challenging because the device can only capture EEG signals from a portion of the brain area. Moreover, existing full-channel algorithms cannot be directly adapted to few-channel EEG signals due to the significant inter-variation between full-channel and few-channel EEG devices. To address these challenges, we propose a novel few-channel EEG emotion recognition framework from the perspective of knowledge transfer. We leverage full-channel EEG signals to provide supplementary information for few-channel signals via a transfer learning-based model CD-EmotionNet, which consists of a base emotion model for efficient emotional feature extraction and a cross-device transfer learning strategy. This strategy helps to enhance emotion recognition performance on few-channel EEG data by utilizing knowledge learned from full-channel EEG data. To evaluate our cross-device EEG emotion transfer learning framework, we construct an emotion dataset containing paired 18-channel and 5-channel EEG signals from 25 subjects, as well as 5-channel EEG signals from 13 other subjects. Extensive experiments show that our framework outperforms state-of-the-art EEG emotion recognition methods by a large margin. IEEE,Electroencephalography (EEG) is often used to evaluate several types of neurological brain disorders because of its noninvasive and high temporal resolution. In contrast to electrocardiography (ECG), EEG can be uncomfortable and inconvenient for patients. Moreover, deep-learning techniques require a large dataset and a long time for training from scratch. Therefore, in this study, EEG–EEG or EEG–ECG transfer learning strategies were applied to explore their effectiveness for the training of simple cross-domain convolutional neural networks (CNNs) used in seizure prediction and sleep staging systems, respectively. The seizure model detected interictal and preictal periods, whereas the sleep staging model classified signals into five stages. The patient-specific seizure prediction model with six frozen layers achieved 100% accuracy for seven out of nine patients and required only 40 s of training time for personalization. Moreover, the cross-signal transfer learning EEG–ECG model for sleep staging achieved an accuracy approximately 2.5% higher than that of the ECG model; additionally, the training time was reduced by >50%. In summary, transfer learning from an EEG model to produce personalized models for a more convenient signal can both reduce the training time and increase the accuracy; moreover, challenges such as data insufficiency, variability, and inefficiency can be effectively overcome. © 2023 by the authors."
17,16,132,16_Session-based recommendation models based on graph neural networks and the challenges of incorporating higher-order features and item position information.,Session-based recommendation models based on graph neural networks and the challenges of incorporating higher-order features and item position information.,"The Recommendation is used to exploit the relations among known features and content that describe items (content-based filtering) or the overlap of similar users who interacted with or rated the target item (collaborative filtering). To combine these two filtering approaches, current model-based hybrid recommendation systems typically require extensive feature engineering to construct a user profile. Name Entity Recognition (NER) provides a straightforward way to identifies one item from a set of other items that have similar attributes of the related objects. However, due to the large scale of the data used in real world recommendation systems, little research exists on applying NER models to hybrid recommendation systems in job vacancy environment. This paper is proposed a way to adapt the name entity recognition approaches to construct a real hybrid job recommendation system. Furthermore, in order to satisfy a common requirement in recommendation systems the approach of accuracy, precision, recall and F-measure is using in this recommendation system in a principled way. The experimental results demonstrate the efficiency of our proposed approach as well as its improved performance on recommendation precision. © 2023 Little Lion Scientific.,Recommendation systems have become more important since the widespread use of the Internet. The large amount of information means that it is difficult for users to discover what they really need. However, users' preferences can change over time because of the age and the impact of social networks. Recommendation systems must capture users' preferences and recommend suitable items, which is a difficult challenge. This study proposes a novel recommendation system that adapts to the changing preferences of users. By learning and adapting to users' changing preferences better recommendations are provided. This study uses context factors as additional information to model users' preferences more accurately. For the proposed recommendation system, the context is based on users' most recent interaction items. This study proposes two novel attention mechanisms for the recommendation system. The contextual item attention module captures contextual information, changing pattern in users' preference and the importance of items. The multi-head attention module extends the diversity of users' preferences and adapts to changing preferences. The recommendation performance is improved using additional item's temporal information to model the contextual item's representation. Experiments compare the proposed algorithm with several state-of-the-art recommendation methods using three real-world datasets. The experimental results demonstrate that the proposed context-aware recommendation model outperforms traditional methods and demonstrate the effectiveness with which contextual information is captured by the attention mechanism.  © 1989-2012 IEEE.,Traditional recommendation systems can only estimate user preferences and model user preferences on items from past interaction history, thus suffering from the limitations of obtaining real-time and fine-grained user preferences. Conversational recommendation systems (CRS), which introduce conversational technology into recommendation systems, provide a new solution to this problem in recent years. Unlike traditional recommendation systems which, due to their static way of working, cannot answer what the user's current preferences are and what the user's reasons are for buying an item, CRS are interactive recommendation systems, which have the advantage of dynamically interacting with users and obtaining their real-time preferences in the process of interaction, understanding what they currently like and what their reasons are for liking an item, thus allowing the recommendation system to quickly understand the user's intent and make recommendations that the user desires, enhancing the user's experience and trust in the system. However, current CRS studies more often uses positive feedback on the granularity of attributes given by the user to model the user's real-time preferences, ignoring the impact of negative user feedback on modeling the user's real-time preferences, but negative feedback is an important part of user feedback and can also indicate real-time user preferences, making it difficult to make fine-grained corrections to the user preference representation, which means it is difficult to effectively balance the relationship between users' long-term preference and real-time preference. At the same time, current state-of-the-art work in the field only takes advantage of the natural advantages of graph structure to limit the size of the attribute candidate set, which suffers from the problem of too many interaction rounds due to the large attribute candidate set. To address the problems mentioned above, we propose a conversational recommendation model NCPR based on the classical conversational recommendation framework CPR, which can make full use of all feedback given by the user during the interaction to correct the user's preference representation, including positive feedback at the attribute granularity, negative feedback at the attribute granularity, and negative feedback at the item granularity. In addition, CPR models conversation recommendation as a path inference problem on a graph, i. e. CRS can only select attribute nodes adjacent to the current node to ask the user. This approach helps CRS to limit the size of the attribute candidate set in a single round of decision making. NCPR uses an attribute-based collaborative filtering algorithm to reorder the attribute candidate set based on negative feedback from the attribute granularity, i. e. removing those attributes in the attribute candidate set that are similar to those rejected by the user, which can further reduce the candidate attribute space size while taking advantage of the natural advantages of graph structure to limit the size of the attribute candidate set. Experimental results on four benchmark CRS datasets show that the proposed method significantly outperforms state-of-the-art baselines in terms of both evaluation metrics. In addition, we design and implement a web-side conversational recommendation system that interacts with online users to generate recommendation results, demonstrating the effectiveness of NCPR in a real-world conversational recommendation scenario. © 2023 Science Press. All rights reserved."
18,17,128,"17_Retinal image analysis and automated diagnosis for ocular diseases, including diabetic retinopathy (DR), glaucoma, and macular degeneration","Retinal image analysis and automated diagnosis for ocular diseases, including diabetic retinopathy (DR), glaucoma, and macular degeneration","Diabetic retinopathy (DR) is a leading cause of blindness, and early detection is crucial for effectively managing and preventing vision loss. This paper proposes a deep learning-based model for the early detection of diabetic retinopathy (DR) using retinal images. The proposed model uses a convolutional neural network (CNN) architecture and transfer learning-based EfficientNet architecture for multiclass classification (0- No DR, 1- Low, 2- Medium, 3- High, 4- Proliferative) of DR, on a large dataset of annotated retinal images. The performance of the model is evaluated on an independent test set and compared with CNN and EfficientNet methods. Results show that the efficient model achieves high accuracy and outperforms existing methods for DR detection. Moreover, the model can detect DR at an early stage, enabling timely interventions and preventing vision loss. The results show that we achieved a training accuracy of 94.42% after 20 epochs and a testing accuracy of 81.81%. The model's accuracy and early detection capability make it a promising tool for enhancing DR screening programs and enabling timely interventions to prevent vision loss. © 2023Penerbit UTM Press. All rights reserved.,Recently, there has been a considerable rise in the number of diabetic patients suffering from diabetic retinopathy (DR). DR is one of the most chronic diseases and makes the key cause of vision loss in middle-aged people in the developed world. Initial detection of DR becomes necessary for decreasing the disease severity by making use of retinal fundus images. This article introduces a Deep Learning Enabled Large Scale Healthcare Decision Making for Diabetic Retinopathy (DLLSHDM-DR) on Retinal Fundus Images. The proposed DLLSHDM-DR technique intends to assist physicians with the DR decision-making method. In the DLLSHDM-DR technique, image preprocessing is initially performed to improve the quality of the fundus image. Besides, the DLLSHDM-DR applies HybridNet for producing a collection of feature vectors. For retinal image classification, the DLLSHDM-DR technique exploits the Emperor Penguin Optimizer (EPO) with a Deep Recurrent Neural Network (DRNN). The application of the EPO algorithm assists in the optimal adjustment of the hyperparameters related to the DRNN model for DR detection showing the novelty of our work. To assuring the improved performance of the DLLSHDM-DR model, a wide range of experiments was tested on the EyePACS dataset. The comparison outcomes assured the better performance of the DLLSHDM-DR approach over other DL models. © 2023 CRL Publishing. All rights reserved.,Diabetic retinopathy (DR) is the damage to the micro-vascular system in the retina, due to prolonged diabetes mellitus. Diagnosis and treatment of DR entail screening of retinal fundus images of diabetic patients. The manual inspection of pathological changes in retinal images is a skill-based task that involves lots of effort and time. Therefore, computer-aided detection and diagnosis of DR have been extensively explored for the past few decades. In recent years with the development of different benchmark deep convolutional neural networks (CNN), deep learning and machine learning have been efficiently and effectively adapted to different DR classification tasks. The success of CNNs largely relies on how good they are in extracting discriminative features from the fundus images. However, to the best of our knowledge, till date no study has been conducted to evaluate the feature extraction capabilities of all the benchmark CNNs to support the DR classification tasks and to find the best training-hyper-parameters for each of them in fundus retinal image-based DR classification tasks. In this work, we try to find the best benchmark CNN, which can be used as the backbone feature extractor for the DR classification tasks using fundus retinal images. We also aim to find the optimal hyper-parameters for training each of the benchmark CNN family, particularly when they are applied to the DR gradation tasks using retinal image datasets with huge class-imbalance and limited samples of higher severity classes. To address the cause, we conduct a detailed comprehensive comparative study on the performances of almost all the benchmark CNNs and their variants proposed during 2014 to 2019, for the DR gradation tasks on common standard retinal datasets. We have also conducted a comprehensive optimal training hyper-parameter search for each of the benchmark CNN family for the fundus image-based DR classification tasks. The benchmark CNNs are transfer learned and end-to-end trained in an incremental fashion on a class-balanced dataset curated from the train set of the EyePACS dataset. The benchmark models are evaluated on APTOS, MESSIDOR-1, and MESSIDOR-2 datasets to test their cross-dataset generalization. Experimental results show that features extracted by EfficientNetB1 have outperformed features of all the other CNN models in DR classification tasks on all three test datasets. MobileNet-V3-Large also shows promising performance on MESSIDOR-1 dataset. The success of EfficientNetB1 and MobileNet-V3-Large indicates that comparatively shallower and light-weighted CNNs tend to extract more discriminative and expressive features from fundus images for DR stage detection. In future, researchers can explore different preprocessing and post-processing techniques and incorporate novel architectural components on these networks to further improve the classification accuracy and robustness. © 2022, King Fahd University of Petroleum & Minerals."
19,18,120,18_Machine Learning Models for Atomic Density Predictions and Molecular Simulations,Machine Learning Models for Atomic Density Predictions and Molecular Simulations,"Quantum chemistry provides chemists with invaluable information, but the high computational cost limits the size and type of systems that can be studied. Machine learning (ML) has emerged as a means to dramatically lower the cost while maintaining high accuracy. However, ML models often sacrifice interpretability by using components such as the artificial neural networks of deep learning that function as black boxes. These components impart the flexibility needed to learn from large volumes of data but make it difficult to gain insight into the physical or chemical basis for the predictions. Here, we demonstrate that semiempirical quantum chemical (SEQC) models can learn from large volumes of data without sacrificing interpretability. The SEQC model is that of density-functional-based tight binding (DFTB) with fixed atomic orbital energies and interactions that are one-dimensional functions of the interatomic distance. This model is trained to ab initio data in a manner that is analogous to that used to train deep learning models. Using benchmarks that reflect the accuracy of the training data, we show that the resulting model maintains a physically reasonable functional form while achieving an accuracy, relative to coupled cluster energies with a complete basis set extrapolation (CCSD(T)*/CBS), that is comparable to that of density functional theory (DFT). This suggests that trained SEQC models can achieve a low computational cost and high accuracy without sacrificing interpretability. Use of a physically motivated model form also substantially reduces the amount of ab initio data needed to train the model compared to that required for deep learning models. © 2023 The Authors. Published by American Chemical Society.,According to density functional theory, any chemical property can be inferred from the electron density, making it the most informative attribute of an atomic structure. In this work, we demonstrate the use of established physical methods to obtain important chemical properties from model-predicted electron densities. We introduce graph neural network architectural choices that provide physically relevant and useful electron density predictions. Despite not being trained to predict atomic charges, the model is able to predict atomic charges with an error of an order of magnitude lower than that of a sum of atomic charge densities. Similarly, the model predicts dipole moments with half the error of the sum of the atomic charge densities method. We demonstrate that larger data sets lead to more useful predictions for these tasks. These results pave the way for an alternative path in atomistic machine learning where data-driven approaches and existing physical methods are used in tandem to obtain a variety of chemical properties in an explainable and self-consistent manner. © 2023 The Authors. Published by American Chemical Society.,Electronic properties and absorption spectra are the grounds to investigate molecular electronic states and their interactions with the environment. Modeling and computations are required for the molecular understanding and design strategies of photo-active materials and sensors. However, the interpretation of such properties demands expensive computations and dealing with the interplay of electronic excited states with the conformational freedom of the chromophores in complex matrices (i.e., solvents, biomolecules, crystals) at finite temperature. Computational protocols combining time dependent density functional theory and ab initio molecular dynamics (MD) have become very powerful in this field, although they require still a large number of computations for a detailed reproduction of electronic properties, such as band shapes. Besides the ongoing research in more traditional computational chemistry fields, data analysis and machine learning methods have been increasingly employed as complementary approaches for efficient data exploration, prediction and model development, starting from the data resulting from MD simulations and electronic structure calculations. In this work, dataset reduction capabilities by unsupervised clustering techniques applied to MD trajectories are proposed and tested for the ab initio modeling of electronic absorption spectra of two challenging case studies: a non-covalent charge-transfer dimer and a ruthenium complex in solution at room temperature. The K-medoids clustering technique is applied and is proven to be able to reduce by ?100 times the total cost of excited state calculations on an MD sampling with no loss in the accuracy and it also provides an easier understanding of the representative structures (medoids) to be analyzed on the molecular scale. © 2023 by the authors."
20,19,120,19_ECG-based Diagnosis and Classification of Cardiovascular Diseases,ECG-based Diagnosis and Classification of Cardiovascular Diseases,"The proliferation of wearable devices has allowed the collection of electrocardiogram (ECG) recordings daily to monitor heart rhythm and rate. For example, 24-hour Holter monitors, cardiac patches, and smartwatches are widely used for ECG gathering and application. An automatic atrial fibrillation (AF) detector is required for timely ECG interpretation. Deep learning models can accurately identify AFs if large amounts of annotated data are available for model training. However, it is impractical to request sufficient labels for ECG recordings for an individual patient to train a personalized model. We propose a Siamese-network-based approach for transfer learning to address this issue. A pre-trained Siamese convolutional neural network is created by comparing two labeled ECG segments from the same patient. We sampled 30-second ECG segments with a 50% overlapping window from the ECG recordings of patients in the MIT-BIH Atrial Fibrillation Database. Subsequently, we independently detected the occurrence of AF in each patient in the Long-Term AF Database. By fine-tuning the model with the 1, 3, 5, 7, 9, or 11 ECG segments ranging from 30 to 180 s, our method achieved macro-F1 scores of 96.84%, 96.91%, 96.97%, 97.02%, 97.05%, and 97.07%, respectively. © 2023 Elsevier B.V.,This paper presents a new spline-based modeling method of electrocardiogram (ECG) signal that can reproduce normal as well as abnormal ECG beats. Large volume ECG data is required for automatic machine learning diagnostic systems, medical education, research and testing purposes but due to privacy issues, access to this medical data is very difficult. Given this, modeling an ECG signal is a very challenging task in the field of biomedical signal processing. Spline-based modeling is the latest and one of the most efficient methods with very low computational complexity in the domain of ECG signal generation. In this paper, healthy ECG and arrhythmia conditions have been considered for the synthetic generation, (namely Atrial fibrillation and Congestive heart failure ECG beats) because these are the leading causes of death globally. To validate the performance of the presented modeling method, it is tested on 100 signals, also the percentage root mean square difference (PRD) and the root mean square error (RMSE) have been determined. These calculated values are analyzed and the results are found to be very promising and show that the presented method is one of the best methods in the field of synthetic ECG signal generation. A comparison amongst relevant existing techniques and the proposed method is also presented. The performance merit values PRD and RMSE, for the proposed method obtained are 38.99 and 0.10092, respectively, which are lower than the values obtained in other compared methods. To ensure fidelity of the proposed modeling technique with respect to IEC60601 standard, few Conformance Testing Services (CTS)database signals have also been modelled with a very close resemblance with the standard signals. © 2023 IOP Publishing Ltd.,Cardiovascular diseases are serious threats to human health. Electrocardiogram (ECG) is of great significance for clinical diagnosis and follow-up treatment of arrhythmias. However, the ECG signal is inevitably contaminated by a large amount of noise during ECG acquisition and transmission, which affects the results of arrhythmia detection. Noise reduction was often applied before classification in previous studies, which may cause the loss of some heart beats. Therefore, the preprocessing of noise reduction is omitted and noisy signals are directly classified in this paper. A novel deep learning model, deep shrinkage network, is developed in this paper to improve feature extraction ability and arrhythmia detection accuracy of noisy ECG recordings. Soft thresholding is introduced into deep fully convolutional neural network (DFCNN) to eliminate noise. ECG spectrograms are used as the input of the proposed network, and the Focal Loss function is employed to solve the problem of data imbalance. By training the original data from the MIT-BIH Arrhythmia Database, an overall accuracy of 99.74% is achieved. Significant advantages are also shown in the detection task of noisy ECG signals with different SNRs, demonstrating the effectiveness of the proposed network. ICIC International ©2023."
21,20,120,20_Medical Imaging Reconstruction and Denoising,Medical Imaging Reconstruction and Denoising,"Background: Volumetric reconstruction of magnetic resonance imaging (MRI) from sparse samples is desirable for 3D motion tracking and promises to improve magnetic resonance (MR)-guided radiation treatment precision. Data-driven sparse MRI reconstruction, however, requires large-scale training datasets for prior learning, which is time-consuming and challenging to acquire in clinical settings. Purpose: To investigate volumetric reconstruction of MRI from sparse samples of two orthogonal slices aided by sparse priors of two static 3D MRI through implicit neural representation (NeRP) learning, in support of 3D motion tracking during MR-guided radiotherapy. Methods: A multi-layer perceptron network was trained to parameterize the NeRP model of a patient-specific MRI dataset, where the network takes 4D data coordinates of voxel locations and motion states as inputs and outputs corresponding voxel intensities. By first training the network to learn the NeRP of two static 3D MRI with different breathing motion states, prior information of patient breathing motion was embedded into network weights through optimization. The prior information was then augmented from two motion states to 31 motion states by querying the optimized network at interpolated and extrapolated motion state coordinates. Starting from the prior-augmented NeRP model as an initialization point, we further trained the network to fit sparse samples of two orthogonal MRI slices and the final volumetric reconstruction was obtained by querying the trained network at 3D spatial locations. We evaluated the proposed method using 5-min volumetric MRI time series with 340 ms temporal resolution for seven abdominal patients with hepatocellular carcinoma, acquired using golden-angle radial MRI sequence and reconstructed through retrospective sorting. Two volumetric MRI with inhale and exhale states respectively were selected from the first 30 s of the time series for prior embedding and augmentation. The remaining 4.5-min time series was used for volumetric reconstruction evaluation, where we retrospectively subsampled each MRI to two orthogonal slices and compared model-reconstructed images to ground truth images in terms of image quality and the capability of supporting 3D target motion tracking. Results: Across the seven patients evaluated, the peak signal-to-noise-ratio between model-reconstructed and ground truth MR images was 38.02 ± 2.60 dB and the structure similarity index measure was 0.98 ± 0.01. Throughout the 4.5-min time period, gross tumor volume (GTV) motion estimated by deforming a reference state MRI to model-reconstructed and ground truth MRI showed good consistency. The 95-percentile Hausdorff distance between GTV contours was 2.41 ± 0.77 mm, which is less than the voxel dimension. The mean GTV centroid position difference between ground truth and model estimation was less than 1 mm in all three orthogonal directions. Conclusion: A prior-augmented NeRP model has been developed to reconstruct volumetric MRI from sparse samples of orthogonal cine slices. Only one exhale and one inhale 3D MRI were needed to train the model to learn prior information of patient breathing motion for sparse image reconstruction. The proposed model has the potential of supporting 3D motion tracking during MR-guided radiotherapy for improved treatment precision and promises a major simplification of the workflow by eliminating the need for large-scale training datasets. © 2023 American Association of Physicists in Medicine.,Acceleration in MRI has garnered much attention from the deep-learning community in recent years, particularly for imaging large anatomical volumes such as the abdomen or moving targets such as the heart. A variety of deep learning approaches have been investigated, with most existing works using convolutional neural network (CNN)-based architectures as the reconstruction backbone, paired with fixed, rather than learned, k-space undersampling patterns. In both image domain and k-space, CNN-based architectures may not be optimal for reconstruction due to its limited ability to capture long-range dependencies. Furthermore, fixed undersampling patterns, despite ease of implementation, may not lead to optimal reconstruction. Lastly, few deep learning models to date have leveraged temporal correlation across dynamic MRI data to improve reconstruction. To address these gaps, we present a dual-domain (image and k-space), transformer-based reconstruction network, paired with learning-based undersampling that accepts temporally correlated sequences of MRI images for dynamic reconstruction. We call our model DuDReTLU-net. We train the network end-to-end against fully sampled ground truth dataset. Human cardiac CINE images undersampled at different factors (5?100) were tested. Reconstructed images were assessed both visually and quantitatively via the structural similarity index, mean squared error, and peak signal-to-noise. Experimental results show superior performance of DuDReTLU-net over state-of-the-art methods (LOUPE, k-t SLR, BM3D-MRI) in accelerated MRI reconstruction; ablation studies show that transformer-based reconstruction outperformed CNN-based reconstruction in both image domain and k-space; dual-domain reconstruction architectures outperformed single-domain reconstruction architectures regardless of reconstruction backbone (CNN or transformer); and dynamic sequence input leads to more accurate reconstructions than single frame input. We expect our results to encourage further research in the use of dual-domain architectures, transformer-based architectures, and learning-based undersampling, in the setting of accelerated MRI reconstruction. The code for this project is made freely available at https://github.com/william2343/dual-domain-mri-recon-nets (Hong et al., 2022). © 2023 Elsevier Ltd,Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable self-supervised learning for MR image reconstruction by combining a self-supervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of Noise2Noise in MR reconstruction by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the proposed method requires a reduced amount of training data to achieve high reconstruction quality among the state-of-art of MR reconstruction utilizing the Noise2Noise method. IEEE"
22,21,119,21_Defect detection in industrial manufacturing using deep learning algorithms,Defect detection in industrial manufacturing using deep learning algorithms,"Defect detection is one of the key factors in fabric quality control. To improve the speed and accuracy of denim fabric defect detection, this paper proposes a defect detection algorithm based on cascading feature extraction architecture. Firstly, this paper extracts these weight parameters of the pre-trained VGG16 model on the large dataset ImageNet and uses its portability to train the defect detection classifier and the defect recognition classifier respectively. Secondly, retraining and adjusting partial weight parameters of the convolution layer were retrained and adjusted from of these two training models on the high-definition fabric defect dataset. The last step is merging these two models to get the defect detection algorithm based on cascading architecture. Then there are two comparative experiments between this improved defect detection algorithm and other feature extraction methods, such as VGG16, ResNet-50, and Xception. The results of experiments show that the defect detection accuracy of this defect detection algorithm can reach 94.3% and the speed is also increased by 1–3 percentage points. © 2023 KIPS,Industrial defect detection methods based on deep learning can reduce the cost of traditional manual quality inspection, improve the accuracy and efficiency of detection, and are widely used in industrial fields. Traditional computer defect detection methods focus on manual features and require a large amount of defect data, which has some limitations. This paper proposes a texture surface defect detection method based on convolutional neural network and wavelet analysis: TSDNet. The approach combines wavelet analysis with patch extraction, which can detect and locate many defects in a complex texture background; a patch extraction method based on random windows is proposed, which can quickly and effectively extract defective patches; and a judgment strategy based on a sliding window is proposed to improve the robustness of CNN. Our method can achieve excellent detection accuracy on DAGM 2007, a micro-surface defect database and KolektorSDD dataset, and can find the defect location accurately. The results show that in the complex texture background, the method can obtain high defect detection accuracy with only a small amount of training data and can accurately locate the defect position. © 2023 by the authors.,The occurrence of anomalies on the surface of industrial products can lead to issues such as decreased product quality, reduced production efficiency, and safety hazards. Early detection and resolution of these problems are crucial for ensuring the quality and efficiency of production. The key challenge in applying deep learning to surface defect detection of industrial products is the scarcity of defect samples, which will make supervised learning methods unsuitable for surface defect detection problems. Therefore, it is a reasonable solution to use anomaly detection methods to deal with surface defect detection. Among image-based anomaly detection, reconstruction-based methods are the most commonly used. However, reconstruction-based approaches lack the involvement of defect samples in the training process, posing the risk of a perfect reconstruction of defects by the reconstruction network. In this paper, we propose a reconstruction-based defect detection algorithm that addresses these challenges by utilizing more realistic synthetic anomalies for training. Our model focuses on creating authentic synthetic defects and introduces an auto-encoder image reconstruction network with deep feature consistency constraints, as well as a defect separation network with a large receptive field. We conducted experiments on the challenging MVTec anomaly detection dataset and our trained model achieved an AUROC score of 99.70% and an average precision (AP) score of 99.87%. Our method surpasses recently proposed defect detection algorithms, thereby enhancing the accuracy of surface defect detection in industrial products. © 2024 by the authors."
23,22,117,22_Federated Learning with Communication Efficiency and Heterogeneous Clients,Federated Learning with Communication Efficiency and Heterogeneous Clients,"The Federated learning (FL) technique resolves the issue of training machine learning (ML) techniques on distributed networks, including the huge volume of modern smart devices. FL clients frequently use Wi-Fi and have to interact in unstable network surroundings. However, as the present FL aggregation approaches receive and send a large number of weights, accuracy can be decreased considerably in unstable network surroundings. Therefore, this study presents a Quantum with Metaheuristics Algorithm Based Minimization of Communication Costs in Federated Learning (QMAMCC-FL) technique. The presented QMAMCC-FL technique is designed a federated hybrid convolutional neural network with a gated recurrent unit (HCNN-GRU) model with a quantum Aquila optimization (QAO) algorithm. The QMAMCC-FL technique upgrades the global model via weight collection of the learned model, which is commonly used in FL. The proposed model can be employed to increase the performance of network communication and reduce the size of data transmitted from clients to servers such as smartphones and tablets. The experimental analysis of the QMAMCC-FL approach is tested, and the outcomes show better performance over other existing models.  © 2013 IEEE.,Federated Learning (FL) is a very effective distributed machine learning framework that enables a large number of devices to jointly train models without sharing raw data. However, the process of iterative learning and data communication in FL can be time-consuming, depending heavily on the number of clients participating in training and the number of local iterations between two consecutive global aggregations (communication period). In this paper, we analyze the runtime of the FL framework and propose a joint optimization problem, which considers both the number of local clients involved in global training and the communication period to minimize the error-runtime convergence of FedAvg. By theoretical analysis, we obtain the optimal solutions of this optimization problem in closed forms under the assumption of different probability distributions of the client's local computation time. In addition, we design an adaptive control algorithm that can dynamically select the number of clients and communication period during FL training to speed up the convergence of the model. Experimental results validate our theoretical analysis and show that the proposed algorithm has outstanding performance compared with other related FL control algorithms. © 2023 Elsevier Ltd,The recent advances in 5G and mobile edge computing facilitate the rapid development of the Internet of Things (IoT), enabling collective intelligence with data support from a massive number of IoT devices. Meanwhile, federated learning (FL) has emerged as a promising solution for collaborative training while preserving user privacy, which, however, is prone to poor learning performance in large-scale IoT scenarios. On the one hand, due to the task heterogeneity in various IoT scenarios and data heterogeneity (non-IID) across different IoT clients, more than traditional FL models derived from a Uniform FL (UFL) learning architecture are required to satisfy the diversified demands of each client. On the other hand, with the proliferation of IoT devices, achieving low latency and low communication costs with traditional FL architectures becomes even more challenging. In this article, we argue that customizing a specific model for each client is an urgent requirement, calling for the development of UFL to Personalized FL (PFL). In addition, the confluence of PFL and edge computing further provides opportunities for practical implementation in the 5G IoT environment. Accordingly, we propose EdgeFHN, an edge computing-based personalized federated learning framework, which can strike a balance between collaborative FL training among different clients and efficient model personalization for each client. Experiments on image classification demonstrate the superiority of our framework in improving accuracy and reducing communication overhead compared with other state-of-the-art solutions.  © 1986-2012 IEEE."
24,23,117,23_Interprofessional Education and Teamwork in Healthcare Settings,Interprofessional Education and Teamwork in Healthcare Settings,"Background: There are still concerns about the effectiveness of clinical education models which are done with the aim of reducing the theoretical-practical gap in nursing. In this article, we intend to describe an innovative model to create an integration and structured relationship between educational and healthcare provider institutions. The basis of this work is the full-time presence of nursing teacher in the clinical settings and the development of their role to improve the education of students and nurses and the quality of nursing services. Methods: This was a participatory action research. This action research was implemented in four steps of problem identification, planning, action and reflection. Interviews, focus groups and observation were used for the qualitative part. Clinical Learning Environment Inventory (CLEI), Job Satisfaction in Nursing Instrument questionnaires and Patient Satisfaction with Nursing Care Quality Questionnaire were completed before and after the study. Qualitative content analysis, paired and independent t test were used for data analysis. Results: The academic-practice integration Model of TPSN is a dynamic and interactive model for accountability in nursing Discipline. Unlike the medical education model that includes patients, students, and physicians as the three points of a triangle, this model, which is shaped like a large triangle, places the person in need of care and treatment (patient, client, family, or society) in the center of the triangle, aiming to focus on the healthcare receiver. The model consists of three components (Mentoring component, Preceptorship component, and integrated clinical education component). Each of the components of this model alone will not be able to eliminate the ultimate goal of bridging the theory-practice gap. Conclusions: A new and innovative model was proposed to reduce the theory-practice gap in the present study. This model increases the collaboration between educational institutions and healthcare settings compared with the previous models. The TPSN model helps students, nurses, and nursing instructors integrate theoretical knowledge with clinical practice and act as professional nurses. © 2022, The Author(s).,Interprofessional education and collaborative practice requirements of professional health programs have resulted in collaborative learning programming in nearly all health professions. Due to recent initiatives for contemporary education of health professionals on pain science education, an academic health science center at a large public southeastern university created a focused interprofessional learning activity on pain science education within its curriculum. The learning objectives for this session aligned with both the core competencies from the Interprofessional Education Collaborative and the International Association for the Study of Pain. In a two-year period, nearly 750 students participated in this interprofessional pain science education session from medicine, nursing, pharmacy, physical therapy, and behavioral health. The session involved a standardized patient encounter for a patient presenting with chronic pain. Both faculty observers and the standardized patients assessed student teams on their interprofessional competencies during the activity, who then discussed findings prior to faculty-student debriefings. Faculty and standardized patient raters agreed that student groups demonstrated very strong skills in communication, information sharing, and solicitation of patient/client input during the event. Other skills were met with some level of disagreement. This session identifies the potential impact of interprofessional learning that can transpire through a standardized patient experience with modeling of activities in association with contemporary pain science education competencies. Future models of this program should include further assessment of interprofessional learning from students of all health care disciplines as well as continued solicitation of input from the patient or standardized patients. © 2022 Elsevier Inc.,Understanding the effects of racism on public health is necessary to stimulate structural and systemic change to improve the health outcomes of patients. As a healthcare team, racism is best addressed through interprofessional collaboration to develop equitable and sustainable solutions that transform the health and wellbeing of patients and communities. As a part of the collective effort to properly educate our health professional students about the declarations by local and county health departments that “racism as a public health crisis,” we sought to utilize an interprofessional collaboration model. A steering committee of faculty, staff, and students at The Ohio State University created a new longitudinal interprofessional education (IPE) exercise titled Personal and Collective Responsibility for Health Equity: Anti-Racism in Action (ARIA). The participating Colleges/Schools of Dentistry, Medicine, Nursing, Optometry, Pharmacy, Public Health, School of Health and Rehabilitation Sciences (SHRS), Social Work, and Veterinary Medicine worked with leaders from the University's Office of Interprofessional Practice and Education to develop this 5-week program to engage a cohort of approximately 1300 students in a virtual learning experience during the Spring of 2021, when COVID-19 restricted in-person instruction. Ultimately, 200-interprofessional teams of 5–7 students were involved in this learning experience. Students individually selected resources (readings or videos) from a comprehensive resource kit provided by the steering team, to learn about and reflect on the differential types of racism, how it impacts well-being, health care delivery, and how health professionals can collaborate to advance health equity. The 200 interprofessional teams met twice virtually during the 5-week module and were provided with discussion questions and short assignments. Each team then contextualized, designed and submitted a final poster of a project to describe how an interprofessional approach to racism could further racial equity in health and healthcare delivery as applied to a selected perceived health equity issue, problem or dilemma. Student survey data was used to describe the effect of this module on student learning and attitudes. Students generally agreed that the module helped them to achieve the learning objectives. A thematic analysis of open-ended responses revealed that students generally had a positive response to the content on racism and the opportunity to learn interprofessionally, and they had specific suggestions for how to improve the experience. The results were utilized to re-design the activity for the following year and may be useful to other institutions wishing to address racism through interprofessional education. © 2023 Elsevier Inc."
25,24,106,24_Code Summarization and Bug Severity Prediction,Code Summarization and Bug Severity Prediction,"Code smell is an indicator of potential problems in a software design that have a negative impact on readability and maintainability. Hence, detecting code smells in a timely and effective manner can provide guides for developers in refactoring. Fortunately, many approaches like metric-based, heuristic-based, machine-learning-based and deep-learning-based have been proposed to detect code smells. However, existing methods, using the simple code representation to describe different code smells unilaterally, cannot efficiently extract enough rich information from source code. In addition, one code snippet often has several code smells at the same time and there is a lack of multi-label code smell detection based on deep learning. In this paper, we present a large-scale dataset for the multi-label code smell detection task since there is still no publicly sufficient dataset for this task. The release of this dataset would push forward the research in this field. Based on it, we propose a hybrid model with multi-level code representation to further optimize the code smell detection. First, we parse the code into the abstract syntax tree (AST) with control and data flow edges and the graph convolution network is applied to get the prediction at the syntactic and semantic level. Then we use the bidirectional long-short term memory network with attention mechanism to analyze the code tokens at the token-level in the meanwhile. Finally, we get the fusion prediction result of the models. Experimental results illustrate that our proposed model outperforms the state-of-the-art methods not only in single code smell detection but also in multi-label code smell detection.  © 2022 World Scientific Publishing Company.,Software defects not only reduce operational reliability but also significantly increase overall maintenance costs. Consequently, it is necessary to predict software defects at an early stage. Existing software defect prediction studies work with artificially designed metrics or features extracted from source code by machine learning-based approaches to perform classification. However, these methods fail to make full use of the defect-related information other than code, such as comments in codes and commit messages. Therefore, in this paper, additional information extracted from natural language text is combined with the programming language codes to enrich the semantic features. A novel model based on Transformer architecture and multi-channel CNN, PM2-CNN, is proposed for software defect prediction. Pretrained language model and CNN-based classifier are utilized in the model to obtain context-sensitive representations and capture the local correlation of sequences. A large and widely used dataset is utilized to verify the effectiveness of the proposed method. The results show that the proposed method has improvements in generic evaluation metrics compared with the optimal baseline method. Accordingly, external information can have a positive impact on software defect prediction, and our model effectively incorporates such information to improve detection performance. © 2023 Elsevier Inc.,In software engineering, software personnel faced many large-scale software and complex systems, these need programmers to quickly and accurately read and understand the code, and efficiently complete the tasks of software change or maintenance tasks. Code-NN is the first model to use deep learning to accomplish the task of code summary generation, but it is not used the structural information in the code itself. In the past five years, researchers have designed different code summarization systems based on neural networks. They generally use the end-to-end neural machine translation framework, but many current research methods do not make full use of the structural information of the code. This paper raises a new model called G-DCS to automatically generate a summary of java code; the generated summary is designed to help programmers quickly comprehend the effect of java methods. G-DCS uses natural language processing technology, and training the model uses a code corpus. This model could generate code summaries directly from the code files in the coded corpus. Compared with the traditional method, it uses the information of structural on the code. Through Graph Convolutional Neural Network (GCN) extracts the structural information on the code to generate the code sequence, which makes the generated code summary more accurate. The corpus used for training was obtained from GitHub. Evaluation criteria using BLEU-n. Experimental results show that our approach outperforms models that do not utilize code structure information. © 2023 Taiwan Academic Network Management Committee. All rights reserved."
26,25,104,25_Air Pollution Monitoring and Prediction in China,Air Pollution Monitoring and Prediction in China,"Large-scale restrictions on anthropogenic activities in China in 2020 due to the Corona Virus Disease 2019 (COVID-19) indirectly led to improvements in air quality. Previous studies have paid little attention to the changes in nitrogen dioxide (NO2), fine particulate matter (PM2.5) and ozone (O3) concentrations at different levels of anthropogenic activity limitation and their interactions. In this study, machine learning models were used to simulate the concentrations of three pollutants during periods of different levels of lockdown, and compare them with observations during the same period. The results show that the difference between the simulated and observed values of NO2 concentrations varies at different stages of the lockdown. Variation between simulated and observed O3 and PM2.5 concentrations were less distinct at different stages of lockdowns. During the most severe period of the lockdowns, NO2 concentrations decreased significantly with a maximum decrease of 65.28 %, and O3 concentrations increased with a maximum increase of 75.69 %. During the first two weeks of the lockdown, the titration reaction in the atmosphere was disrupted due to the rapid decrease in NO2 concentrations, leading to the redistribution of Ox (NO2 + O3) in the atmosphere and eventually to the production of O3 and secondary PM2.5. The effect of traffic restrictions on the reduction of NO2 concentrations is significant. However, it is also important to consider the increase in O3 due to the constant volatile organic compounds (VOCs) and the decrease in NOx (NO+NO2). Traffic restrictions had a limited effect on improving PM2.5 pollution, so other beneficial measures were needed to sustainably reduce particulate matter pollution. Research on COVID-19 could provide new insights into future clean air action. © 2023,The spatial distribution characteristics of multi-air pollutants and their impacts are difficult to quantify effectively. As PM2.5 and NO2 are the main air pollutants, it is of great significance to explore the spatial causes of their pollution and their interaction mechanism. This study used machine learning (LightGBM) and hot spot analysis to map the spatial distribution of PM2.5 and NO2 in Southwest Fujian (SWFJ) in 2018 and their key pollution areas. Then, the factors and interactive detection of geographical detectors were used to conduct a detailed analysis of the quantitative impact of potential factors such as human activities, terrain, air pollutants, and meteorology on PM2.5 and NO2 pollution. From this we can learn that 1. LightGBM has good stability for drawing the spatial distribution of PM2.5 and NO2. 2. The spatial mechanism of PM2.5 and NO2 can be effectively interpreted from a massive data and macro perspective. 3. A large amount of evidence shows that potential factors such as human activities, topography, air pollutants and meteorology have direct or indirect effects on PM2.5 and NO2 pollution in the SWFJ area. This includes the direct impact of local road traffic emissions on the distribution of PM2.5 and NO2 pollution, the digestion of both by vegetation, the mutual transformation of atmospheric pollutants themselves, and the impact of meteorological conditions. This study not only confirms the effectiveness of machine learning combined with geographical detectors to promote the study of regional air pollution mechanisms, but also confirms the feasibility of exploring the spatial distribution mechanisms of various air pollutants. Therefore, this study is of great significance for explaining the spatial distribution of PM2.5 and NO2, and can also provide reference for policy formulation to reduce regional PM2.5 and NO2 concentrations. © 2023 Elsevier Ltd,Poor air quality has various detrimental physical and mental effects on human health and quality of life. In particular, PM2.5 air pollution has been associated with cardiovascular and respiratory problems. Therefore, air quality management is an essential issue for densely populated cities to reduce or prevent the adverse effects of air pollution. Considering this, reliable models for predicting pollution levels for pollutants like PM2.5 are critical tools for decision-making. For this purpose, this study presents three kinds of deep learning (DL) algorithms (LSTM, RNN, and GRU) that utilize a time-windowing strategy to predict the hourly concentration of PM2.5 in the Istanbul metropolitan. The models were trained and tested using large data sets that envelope air quality parameters (PM2.5, SO2, NO, NO2, NOX, and O3) and meteorological factors (temperature, wind speed, relative humidity, and air pressure) for about five years. The experimental results demonstrate that the LSTM+LSTM model performs significantly better with an R2 of 0.98 and 0.97 at the significance level (p < 0.05) for training and test sets compared to other deep learning algorithms. In addition, data for one year from several stations located in nine different districts of Istanbul were used to evaluate the proposed model's generalization ability. As a result, the proposed LSTM+LSTM model has a good generalization ability with an R2 accuracy rate of 0.90 (p < 0.05) and above for all stations and can be used for non-linear, non-stationary multidimensional time series data. Furthermore, the results were compared to other studies in the literature; it was found that the proposed LSTM+LSTM model performed better in predicting PM2.5 concentrations. © 2023 Elsevier B.V."
27,26,104,26_Privacy-Preserving Federated Learning with Secure Encryption and Communication,Privacy-Preserving Federated Learning with Secure Encryption and Communication,"Federated learning (FL) is a distributed approach to developing collaborative learning models from decentralized data. This is relevant to many real applications, such as in the field of the Internet of Things, since the models can be used in edge computing devices. FL approaches are motivated by and designed to protect privacy, a highly relevant issue given current data protection regulations. Although FL methods are privacy-preserving by design, recently published papers show that privacy leaks do occur, caused by attacks designed to extract private data from information interchanged during learning. In this work, we present an FL method based on a neural network without hidden layers that incorporates homomorphic encryption (HE) to enhance robustness against the above-mentioned attacks. Unlike traditional FL methods that require multiple rounds of training for convergence, our method obtains the collaborative global model in a single training round, yielding an effective and efficient model that simplifies management of the FL training process. In addition, since our method includes HE, it is also robust against model inversion attacks. In experiments with big data sets and a large number of clients in a federated scenario, we demonstrate that use of HE does not affect the accuracy of the model, whose results are competitive with state-of-the-art machine learning models. We also show that behavior in terms of accuracy is the same for identically and non-identically distributed data scenarios. © 2023 The Author(s),There are several unsolved problems in federated learning, such as the security concerns and communication costs associated with it. Differential privacy (DP) offers effective privacy protection by introducing noise to parameters based on rigorous privacy definitions. However, excessive noise addition can potentially compromise the accuracy of the model. Another challenge in federated learning is the issue of high communication costs. Training large-scale federated models can be slow and expensive in terms of communication resources. To address this, various model pruning algorithms have been proposed. To address these challenges, this paper introduces a communication-efficient, privacy-preserving FL algorithm based on two-stage gradient pruning and differentiated differential privacy, named IsmDP-FL. The algorithm leverages a two-stage approach, incorporating gradient pruning and differentiated differential privacy. In the first stage, the trained model is subject to gradient pruning, followed by the addition of differential privacy to the important parameters selected after pruning. Non-important parameters are pruned by a certain ratio, and differentiated differential privacy is applied to the remaining parameters in each network layer. In the second stage, gradient pruning is performed during the upload to the server for aggregation, and the final result is returned to the client to complete the federated learning process. Extensive experiments demonstrate that the proposed method ensures a high communication efficiency, maintains the model privacy, and reduces the unnecessary use of the privacy budget. © 2023 by the authors.,Federated learning (FL) enables multiple worker devices share local models trained on their private data to collaboratively train a machine learning model. However, local models are proved to imply the information about the private data and, thus, introduce much vulnerabilities to inference attacks where the adversary reconstructs or infers the sensitive information about the private data (e.g., labels, memberships, etc.) from the local models. To address this issue, existing works proposed homomorphic encryption, secure multiparty computation (SMC), and differential privacy methods. Nevertheless, the homomorphic encryption and SMC-based approaches are not applicable to large-scale FL scenarios as they incur substantial additional communication and computation costs and require secure channels to delivery keys. Moreover, differential privacy brings a substantial tradeoff between privacy budget and model performance. In this article, we propose a novel FL framework, which can protect the data privacy of worker devices against the inference attacks with minimal accuracy cost and low computation and communication cost, and does not rely on the secure pairwise communication channels. The main idea is to generate the lightweight keys based on computational Diffie-Hellman (CDH) problem to encrypt the local models, and the FL server can only get the sum of the local models of all worker devices without knowing the exact local model of any specific worker device. The extensive experimental results on three real-world data sets validate that the proposed FL framework can protect the data privacy of worker devices, and only incurs a small constant of computation and communication cost and a drop in test accuracy of no more than 1%.  © 2014 IEEE."
28,27,101,"27_Cognitive Impairment in Rodents: Hippocampus, Synaptic Dysfunction, and PSD95","Cognitive Impairment in Rodents: Hippocampus, Synaptic Dysfunction, and PSD95","Long-term high-fat diet (HFD) in adolescents leads to impaired hippocampal function and increases the risk of cognitive impairment. Studies have shown that HFD activates hippocampal microglia and induces hippocampal inflammation, which is an important factor for cognitive impairment. Electroacupuncture stimulation (ES), a nerve stimulation therapy, is anti-inflammatory. This study explored its therapeutic potential and mechanism of action in obesity-related cognitive impairment. 4-week-old C57 mice were given either normal or HFD for 22 weeks. At 19 weeks, some of the HFD mice were treated with ES and nigericin sodium salt. The cognitive behavior was assessed through Morris water maze test at 23 weeks. Western blotting was used to detect the expression levels of pro-inflammatory molecules IL-1? and IL-1R, synaptic plasticity related proteins synaptophysin and Postsynaptic Density-95 (PSD-95), and apoptotic molecules (Caspase-3 and Bcl-2), in the hippocampus. The number, morphology, and status of microglia, along with the brain-derived neurotrophic factor?BDNF? content, were analyzed using immunofluorescence. ES treatment improved cognitive deficits in HFD model mice, and decreased the expressions of microglial activation marker, CD68, and microglial BDNF. Inhibition of proinflammatory cytokine, IL-1?, and IL-1R promoted PSD-95 and synaptophysin expressions. Peripheral NLRP3 inflammasome agonist injections exacerbated the cognitive deficits in HFD mice and promoted the expressions of IL-1? and IL-1R in the hippocampus. The microglia showed obvious morphological damage and apoptosis. Collectively, our findings suggest that ES inhibits inflammation, regulates microglial BDNF, and causes remodeling of hippocampal function in mice to counteract obesity-like induced cognitive impairment. Overexcitation of peripheral inflammasome complexes induces hippocampal microglia apoptosis, which hinders the effects of ES. © 2023 Elsevier B.V.,Ethnopharmacological relevance: Epimedium brevicornu Maxim. is a traditional medicinal Chinese herb that is enriched with flavonoids, which have remarkably high medicinal value. Icariin (ICA) is a marker compound isolated from the total flavonoids of Epimedium brevicornu Maxim. It has been shown to improve Neurodegenerative disease, therefore, ICA is probably a potential drug for treating AD. Materials and methods: The 6–8-week-old SPF-class male ICR mice were randomly divided into 8 groups for modeling, and then the mice were administered orally with ICA for 21 days. The behavioral experiments were conducted to evaluate if learning and memory behavior were absent in mice, confirming that infusion of Amyloid ?-protein (A?)1-42 caused significant memory impairment. The morphological changes and damage of neurons in the mice's brains were observed by HE and Nissl staining. The spinous protrusions (dendritic spines) on neuronal dendrites were investigated by Golgi-Cox staining. The molecular mechanism of ICA was examined by Western Blot. The protein docking of ICA and Donepezil with BDNF were analyzed to determine their interaction. Results: The behavioral experimental results showed that in A?1-42-induced AD mice, the learning and memory abilities were improved after using ICA. At the same time, the low, medium, and high doses of ICA could reduce the content of A?1-42 in the hippocampus of AD mice, repair neuronal damage, enhance synaptic plasticity, as well as increase the expression of BDNF, Tr?B, CREB, Akt, GAP43, PSD95, and SYN proteins in the hippocampus of mice. However, the effect with high doses of ICA is more pronounced. The high-dose administration of ICA has the best therapeutic effect on AD mice. After administering the inhibitor k252a, the therapeutic effect of ICA was reversed. The macromolecular docking results of ICA and BDNF protein demonstrated a strong interaction of ?7.8 kcal/mol, which indicates that ICA plays a therapeutic role in AD mice by regulating the BDNF-Tr?B signaling pathway. Conclusions: The results confirm that ICA can repair neuronal damage, enhance synaptic plasticity, as well as ultimately improve learning and memory impairment through the regulation of the BDNF-Tr?B signaling pathway. © 2023 Elsevier B.V.,Purpose: To elucidate the potential mechanisms of QFY for the treatment of Alzheimer’s Disease (AD), and explore the effective substances of QFY. Materials and Methods: UPLC-LTQ-Orbitrap-MS was used to identify the chemical constituents of the serum samples and the cerebrospinal fluid samples of rats after QFY administration. Network pharmacology was used to predict potential targets and pathways of QFY against AD. The AD mice model was established by subcutaneous injection of D-gal for 8 consecutive weeks. New object recognition (NOR) and Morris water maze test (MWM) were used to evaluate the learning and memory abilities of mice. Moreover, the levels of TNF-?, IL-1?, and IL-18 in the brain hippocampus of mice were determined by ELISA. The expression of Bax, Bcl-2, Caspase-1, PSD95, SYP, ICAM-1 and MCP-1 proteins in the hippocampus was detected by Western blotting. Furthermore, qRT-PCR was used to detect the gene expressions of PSD95, SYP, M1 and M2 polarization markers of microglia, including iNOS, CD16, ARG-1, and IL-10 in the hippocampus. Results: A total of 51 prototype compounds were detected in rat serum and 15 prototype components were identified in rat cerebrospinal fluid. Behavioral experiments revealed that QFY significantly increased the recognition index, decreased the escape latency, increased the platform crossing times and increased the residence time in the target quadrant. QFY also could alleviate the ultrastructural pathological changes in the hippocampus of AD mice. Meanwhile, QFY treatment suppressed the expression of inflammatory factors, such as TNF-?, IL-1?, and IL-18. QFY improved the synaptic plasticity of the hippocampus in D-gal model mice by significantly increasing the expression of proteins and mRNAs of PSD95 and SYP. Conclusion: QFY could effectively improve the learning and memory impairment of D-gal-induced AD mice by inhibiting the excessive activation of microglia, enhancing the expression of M2 microglia, inhibiting the increase of inflammatory factors, cell adhesion factors and chemokines, anti-apoptosis, and improving synaptic plasticity. © 2023 Lei et al."
29,28,97,28_Brain functional connectivity in neurodevelopmental and neuropsychiatric disorders,Brain functional connectivity in neurodevelopmental and neuropsychiatric disorders,"Brain functional connectivity (FC) networks inferred from functional magnetic resonance imaging (fMRI) have shown altered or aberrant brain functional connectome in various neuropsychiatric disorders. Recent application of deep neural networks to connectome-based classification mostly relies on traditional convolutional neural networks (CNNs) using input FCs on a regular Euclidean grid to learn spatial maps of brain networks neglecting the topological information of the brain networks, leading to potentially sub-optimal performance in brain disorder identification. We propose a novel graph deep learning framework that leverages non-Euclidean information inherent in the graph structure for classifying brain networks in major depressive disorder (MDD). We introduce a novel graph autoencoder (GAE) architecture, built upon graph convolutional networks (GCNs), to embed the topological structure and node content of large fMRI networks into low-dimensional representations. For constructing the brain networks, we employ the Ledoit-Wolf (LDW) shrinkage method to efficiently estimate high-dimensional FC metrics from fMRI data. We explore both supervised and unsupervised techniques for graph embedding learning. The resulting embeddings serve as feature inputs for a deep fully-connected neural network (FCNN) to distinguish MDD from healthy controls (HCs). Evaluating our model on resting-state fMRI MDD dataset, we observe that the GAE-FCNN outperforms several state-of-the-art methods for brain connectome classification, achieving the highest accuracy when using LDW-FC edges as node features. The graph embeddings of fMRI FC networks also reveal significant group differences between MDD and HCs. Our framework demonstrates the feasibility of learning graph embeddings from brain networks, providing valuable discriminative information for diagnosing brain disorders. IEEE,Background: Although stratifying autism spectrum disorder (ASD) into different subtypes is a common effort in the research field, few papers have characterized the functional connectivity alterations of ASD subgroups classified by their clinical presentations. Methods: This is a case-control rs-fMRI study, based on large samples of open database (Autism Brain Imaging Data Exchange, ABIDE). The rs-MRI data from n = 415 ASD patients (males n = 357), and n = 574 typical development (TD) controls (males n = 410) were included. Clinical features of ASD were extracted and classified using data from each patient's Autism Diagnostic Interview-Revised (ADI-R) evaluation. Each subtype of ASD was characterized by local functional connectivity using regional homogeneity (ReHo) for assessment, remote functional connectivity using voxel-mirrored homotopic connectivity (VMHC) for assessment, the whole-brain functional connectivity, and graph theoretical features. These identified imaging properties from each subtype were integrated to create a machine learning model for classifying ASD patients into the subtypes based on their rs-fMRI data, and an independent dataset was used to validate the model. Results: All ASD participants were classified into Cluster-1 (patients with more severe impairment) and Cluster-2 (patients with moderate impairment) according to the dimensional scores of ADI-R. When compared to the TD group, Cluster-1 demonstrated increased local connection and decreased remote connectivity, and widespread hyper- and hypo-connectivity variations in the whole-brain functional connectivity. Cluster-2 was quite similar to the TD group in both local and remote connectivity. But at the level of whole-brain functional connectivity, the MCC-related connections were specifically impaired in Cluster-2. These properties of functional connectivity were fused to build a machine learning model, which achieved ?75% for identifying ASD subtypes (Cluster-1 accuracy = 81.75%; Cluster-2 accuracy = 76.48%). Conclusions: The stratification of ASD by clinical presentations can help to minimize disease heterogeneity and highlight the distinguished properties of brain connectivity in ASD subtypes. © 2023 The Authors,To learn multiscale functional connectivity patterns of the aging brain, we built a brain age prediction model of functional connectivity measures at seven scales on a large fMRI dataset, consisting of resting-state fMRI scans of 4186 individuals with a wide age range (22 to 97 years, with an average of 63) from five cohorts. We computed multiscale functional connectivity measures of individual subjects using a personalized functional network computational method, harmonized the functional connectivity measures of subjects from multiple datasets in order to build a functional brain age model, and finally evaluated how functional brain age gap correlated with cognitive measures of individual subjects. Our study has revealed that functional connectivity measures at multiple scales were more informative than those at any single scale for the brain age prediction, the data harmonization significantly improved the brain age prediction performance, and the data harmonization in the functional connectivity measures' tangent space worked better than in their original space. Moreover, brain age gap scores of individual subjects derived from the brain age prediction model were significantly correlated with clinical and cognitive measures. Overall, these results demonstrated that multiscale functional connectivity patterns learned from a large-scale multi-site rsfMRI dataset were informative for characterizing the aging brain and the derived brain age gap was associated with cognitive and clinical measures. © 2023"
30,29,97,29_Stock price prediction using machine learning algorithms and deep learning models,Stock price prediction using machine learning algorithms and deep learning models,"Predicting stock market fluctuations is a difficult task due to its intricate and ever-changing nature. To address this challenge, we propose an approach to minimize forecasting errors by utilizing a classification-based technique, which is a widely used set of algorithms in the field of machine learning. Our study focuses on the potential effectiveness of this approach in improving stock market predictions. Specifically, we introduce a new method to predict stock returns using an Extra Trees Classifier. Technical indicators are used as inputs to train our model while the target is the percentage difference between the closing price and the closing price after 10 trading days for 120 companies from various industries. The 10-day time frame strikes a good balance between accuracy and practicality for traders, avoiding the low accuracy of short time frames and the impracticality of longer ones. The Extra Trees Classifier algorithm is ideal for stock market predictions because of its ability to handle large data sets with a high number of input features and improve model robustness by reducing overfitting. Our results show that our Extra Trees Classifier model outperforms the more traditional Random Forest method, achieving an accuracy of 86.1%. These findings suggest that our model can effectively predict significant price changes in the stock market with high precision. Overall, our study provides valuable insights into the potential of classification-based techniques in enhancing stock market predictions. © 2023 by the author.,As the center of the financial market, the stock market is popular with the public attention of investors. It is of great significance for investors that an effective analytic method of stock public opinion is proposed. As the main communication platform, the forum not only provides the investors with investment information but also comments related to the stock market. In view of the defects of text emotions and investment problems, this paper proposes a framework based on web crawler and deep learning technologies including one-dimensional convolutional neural networks (1DCNN) and long short-term memory (LSTM), to evaluate the stock market volatility. Among them, the extracted features include not only the stock price but also the text information. Firstly, we develop the crawler technology to grab large-scale text data from the internet and they are manually labeled their emotions by analyzing the relevant financial knowledge. Secondly, as the character-level text classification method, the 1DCNN is designed for text sentiment classification to detect the reliability of text annotation. Finally, considering the time sequence of price and the continuity of post influence, the emotional and technical features are combined to estimate the fluctuation of the stock market in different industries by the LSTM model. We test four evaluation indexes, the classification accuracy of the model is 74.38%, the accuracy rate is 76.83%, the recall rate is 70%, and the F1 value is 72.8%. The results show that the combination of characteristics of internet public opinion more effectively evaluates the changes in the stock market. © 2022, King Fahd University of Petroleum & Minerals.,Using machine learning coupled with stock price data to predict stock price trends has attracted increasing attention from data mining and machine learning communities. An accurate prediction results can help investors reduce investment risks and improve investment returns. The research on correlation stocks is one of the most important directions among many studies. Due to the high volatility and randomness of stock data, the correlation between stocks changes over time, which makes the stock correlation in static correlation stock sets often inconsistent with reality. Furthermore, various raw data related to stocks contain sufficient stock history information to analyze the future trend of stocks, but traditional prediction models cannot make good use of this information, which restricts the learning ability of the model and reduces the prediction accuracy. In this paper, we propose a stock prediction model combining multi-view stock data features with dynamic market correlation information (MDF-DMC). The model extracts stock trend features by combining multi-view raw data of a single stock with a Multi-layer Perceptron Mixer (MLP-Mixer); The improved Transformer encoder learns the correlation between the stock to be predicted and all the selected stocks in the stock market dynamically and extracts the features of the market correlation. We have conducted a large number of experiments on a total of 578 stocks in the stock markets of China and the United States, and the results show that our model has achieved excellent accuracy and returns across all data sets. © 2023 Elsevier Ltd"
31,30,95,30_Vibration-based Fault Diagnosis of Bearings,Vibration-based Fault Diagnosis of Bearings,"Ensuring safe machine operation in industrial environments requires accurate bearing fault diagnosis. However, maintaining consistent data distribution between input training and test sets in practical online engineering tests based on depth models can be challenging. Inconsistencies in data distribution can hinder internal feature extraction and significantly affect the accuracy of online diagnosis. Additionally, different types of fault data are strongly nonlinearly coupled in space, requiring a deep model with powerful feature learning capabilities to reduce coupling effects between data. However, in practical engineering, deep models face difficulties in learning-rich fault features when few labeled training samples are available, leading to overfitting and poor generalization issues. To overcome these challenges, this study proposes an intelligent diagnosis method based on the Mel-scale frequency cepstrum coefficient (MFCC) and deep convolutional neural network with spatial adjacent region dropout (DCNN-SARD). Firstly, MFCC is utilized to extract sign fault features from vibration signals in different frequency bands, thus reducing the effect of inconsistent data distribution on fault diagnosis accuracy. Secondly, according to the different fault types, the obtained signature fault features are organized into one-dimensional vectors to construct block sample datasets. Finally, the fault features and input to the next convolution are enriched by creating a spatial descent region on the output feature map of the DCNN and randomly dropping the activation of neighboring kernels. This improves the generalizability of the model without increasing its structural complexity. The effectiveness of the proposed algorithm is experimentally validated on two common bearing datasets, and its reliability is verified on a large gas turbine bearing dataset. © 2023, The Author(s), under exclusive licence to The Brazilian Society of Mechanical Sciences and Engineering.,Fault diagnosis is an essential process for the health maintenance of rotating machinery. With the development of AI technology, many deep learning-based methods have been applied to fault diagnosis to enhance the intelligence level of equipment maintenance. Such methods normally need a large amount of labeled data for model training. However, label acquisition is a difficult task that requires extensive human labor. To address these issues, a fault diagnosis method based on feature extraction via an unsupervised graph neural network is proposed in this paper. In the proposed method, the K-nearest neighbor approach is adopted to construct a fault graph from the collected signals, thereby providing extra relationship information for fine feature mining. Then, the GraphSAGE model is trained on the constructed graph in an unsupervised way, that is, it does not need labeled data, to extract features of each signal sample. Based on the extracted features, some traditional classifiers are adopted to identify the fault types. The proposed model is evaluated on a rolling bearing dataset provided by the University of Paderborn and a motor rotor dataset collected by a constructed motor rotor system. Compared with some traditional deep learning-based fault diagnosis methods, the proposed model can achieve more accurate diagnoses even when there are only a few labeled samples. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,In recent years, rolling bearing fault diagnosis technology based on deep learning (DL) has provided a more intelligent and reliable method for the safe operation of mechanical systems due to its powerful feature learning ability. However, in real industrial scenarios, the acquisition of fault samples is limited and knotty, which makes it difficult for DL methods that require a large number of fault samples to be successful. To overcome the above problems, in this article, a novel meta-learning network with adaptive input (AI) and attention mechanism is proposed for rolling bearing fault diagnosis with small samples under different working conditions. First, inspired by the envelope demodulation signal processing method, an AI length selection strategy considering the different working conditions is proposed, which improves the disadvantages of Gram angle field (GAF) 2-D coding method with the traditional fixed input. Second, the residual structure and attention mechanism are introduced to make the network have stronger feature extraction and generalization performance and further improve the classification accuracy. Finally, the effectiveness of the method is verified on the Case Western Reserve University (CWRU) bearing fault datasets and the high-speed train axle box bearing fault datasets conducted by us. The results show that the proposed improved model-agnostic meta-learning (MAML) network is superior to the other four mainstream meta-learning methods under the same conditions, and satisfactory fault diagnosis results can be obtained on the bearing fault datasets under different working conditions.  © 2023 IEEE."
32,31,92,31_Multimodal Video Retrieval and Captioning with CLIP and Language-Image Models,Multimodal Video Retrieval and Captioning with CLIP and Language-Image Models,"Video moment retrieval aims to locate the timestamps best matching the query description within an untrimmed video. However, existing video moment retrieval approaches typically suffer from two major limitations: (1)Utilize only negative moment-sentence pairs sampled from intra-videos, which may overfit the bias of the dataset and not have an excellent understanding of the video and query due to the dataset size and annotation biases. (2)Decouple the video and the query, perform unimodal learning separately, and then concatenate them together as multimodal fusion features. In this paper, we propose a novel approach named Momentum Contrastive Matching Network(MCMN). Inspired by MoCo, we propose the Momentum Cross-modal Contrast for cross-modal learning to enable large-scale negative sample interactions, which contributes to the generation of more precise and discriminative representations, and use temporal decay to model key attenuation in the memory queue when computing the contrastive loss. In addition, we use an attention module to adaptively generate clip-specific word embeddings to achieve semantic alignment from a temporal perspective, which are considered to be more important for finding relevant video contents with large boundary ambiguities. Experimental results on the three major video moment retrieval benchmark datasets, including TACoS, Charades-STA, and ActivityNet Captions demonstrate that MCMN surpasses previous methods and reaches state-of-the-art with disparate visual features. IEEE,The explosive growth of videos on the Internet makes it a great challenge to use texts to retrieve the videos we need. The general method of text-video retrieval is to project them into a common semantic space to calculate the similarity score. The key technologies of a retrieval model are how to get strong feature representations of text and video and bridge the semantic gap between the two modalities. Moreover, most existing methods do not consider the strong consistency of text-video positive sample pairs. Considering the above problems, we proposed a text-video retrieval method based on enhanced self-attention and multi-task learning in this paper. Firstly, while encoding, the extracted text feature vectors and the extracted video feature vectors are input into Transformer based on enhanced self-attention mechanism for encoding and fusion. Then the text representations and video representations are projected into a common semantic space. Finally, by introducing multi-task learning in the common semantic space, our proposed approach combines the semantic similarity measurement task and the semantic consistency judgement task to optimize the common space through semantic consistency constraints. Our method obtains better retrieval performance on the MSR-Video to Text (MSRVTT), Large Scale Movie Description Challenge (LSMDC), and ActivityNet datasets than some existing approaches, which proves the effectiveness of our proposed strategies. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Video captioning requires that the model has the abilities of video understanding, video-Text alignment, and text generation. Due to the semantic gap between vision and language, conducting video-Text alignment is a crucial step to reduce the semantic gap, which maps the representations from the visual to the language domain. However, the existing methods often overlook this step, so the decoder has to directly take the visual representations as input, which increases the decoder's workload and limits its ability to generate semantically correct captions. In this paper, we propose a video-Text alignment module with a retrieval unit and an alignment unit to learn video-Text aligned representations for video captioning. Specifically, we firstly propose a retrieval unit to retrieve sentences as additional input which is used as the semantic anchor between visual scene and language description. Then, we employ an alignment unit with the input of the video and retrieved sentences to conduct the video-Text alignment. The representations of two modal inputs are aligned in a shared semantic space. The obtained video-Text aligned representations are used to generate semantically correct captions. Moreover, retrieved sentences provide rich semantic concepts which are helpful for generating distinctive captions. Experiments on two public benchmarks, i.e., VATEX and MSR-VTT, demonstrate that our method outperforms state-of-The-Art performances by a large margin. The qualitative analysis shows that our method generates correct and distinctive captions.  © 2023 Association for Computing Machinery."
33,32,92,32_Few-shot learning for fine-grained classification and object recognition.,Few-shot learning for fine-grained classification and object recognition.,"Deep neural networks have achieved promising progress in remote sensing (RS) image classification, for which the training process requires abundant samples for each class. However, it is time-consuming and unrealistic to annotate labels for each RS category, given the fact that the RS target database is increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel classes that are not seen during training, which provides a promising solution for the aforementioned problem. However, previous ZSL models mainly depend on manually-labeled attributes or word embeddings extracted from language models to transfer knowledge from seen classes to novel classes. Those class embeddings may not be visually detectable and the annotation process is time-consuming and labor-intensive. Besides, pioneer ZSL models use convolutional neural networks pre-trained on ImageNet, which focus on the main objects appearing in each image, neglecting the background context that also matters in RS scene classification. To address the above problems, we propose to collect visually detectable attributes automatically. We predict attributes for each class by depicting the semantic-visual similarity between attributes and images. In this way, the attribute annotation process is accomplished by machine instead of human as in other methods. Moreover, we propose a Deep Semantic-Visual Alignment (DSVA) that take advantage of the self-attention mechanism in the transformer to associate local image regions together, integrating the background context information for prediction. The DSVA model further utilizes the attribute attention maps to focus on the informative image regions that are essential for knowledge transfer in ZSL, and maps the visual images into attribute space to perform ZSL classification. With extensive experiments, we show that our model outperforms other state-of-the-art models by a large margin on a challenging large-scale RS scene classification benchmark. Moreover, we qualitatively verify that the attributes annotated by our network are both class discriminative and semantic related, which benefits the zero-shot knowledge transfer. © 2023 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS),With the advent of large-scale datasets, significant advancements have been made in image semantic segmentation. However, the annotation of these datasets necessitates substantial human and financial resources. Therefore, the focus of research has shifted towards few-shot semantic segmentation, which leverages a small number of labeled samples to effectively segment unknown categories. The current mainstream methods are to use the meta-learning framework to achieve model generalization, and the main challenges are as follows. (1) The trained model will be biased towards the seen class, so the model will misactivate the seen class when segmenting the unseen class, which makes it difficult to achieve the idealized class agnostic effect. (2) When the sample size is limited, there exists an intra-class gap between the provided support images and the query images, significantly impacting the model’s generalization capability. To solve the above two problems, we propose a network with prototype complementarity characteristics (PCNet). Specifically, we first generate a self-support query prototype based on the query image. Through the self-distillation, the query prototype and the support prototype perform feature complementary learning, which effectively reduces the influence of the intra-class gap on the model generalization. A standard semantic segmentation model is introduced to segment the seen classes during the training process to achieve accurate irrelevant class shielding. After that, we use the rough prediction map to extract its background prototype and shield the background in the query image by the background prototype. In this way, we obtain more accurate fine-grained segmentation results. The proposed method exhibits superiority in extensive experiments conducted on the PASCAL- (Formula presented.) and COCO- (Formula presented.) datasets. We achieve new state-of-the-art results in the few-shot semantic segmentation task, with an mIoU of 71.27% and 51.71% in the 5-shot setting, respectively. Comprehensive ablation experiments and visualization studies show that the proposed method has a significant effect on small-sample semantic segmentation. © 2023 by the authors.,Few-shot object detection (FSOD) aims to detect objects belonging to novel classes with few training samples. With the small number of novel class samples, the visual information extracted is insufficient to accurately represent the object itself, presenting significant intra-class variance and confusion between classes of similar samples, resulting in large errors in the detection results of the novel class samples. We propose a few-shot object detection framework to achieve effective classification and detection by embedding semantic information and contrastive learning. Firstly, we introduced a semantic fusion (SF) module, which projects semantic spatial information into visual space for interaction, to compensate for the lack of visual information and further enhance the representation of feature information. To further improve the classification performance, we embed the memory contrastive proposal (MCP) module to adjust the distribution of the feature space by calculating the contrastive loss between the class-centered features of previous samples and the current input features to obtain a more discriminative embedding space for better intra-class aggregation and inter-class separation for subsequent classification and detection. Extensive experiments on the PASCAL VOC and MS-COCO datasets show that the performance of our proposed method is effectively improved. Our proposed method improves nAP50 over the baseline model by 4.5% and 3.5%. © 2023 by the authors."
34,33,91,33_Speech and Speaker Recognition,Speech and Speaker Recognition,"With the popularization of computers, artificial intelligence technology has become more and more mature, among which speech recognition technology in artificial intelligence is favored by people. In the past few years, the acoustic model combined with the Gaussian mixture model and the hidden Markov model has always been in the leading position. In the field of speech recognition technology, because the development of speech data has gradually expanded, and the complexity of the data has also increased. The larger the data, the traditional data network model is slowly showing inadequacy. However, the deep neural network model is easy to deal with large and complex data modeling. This article combines the advantages of basic learning theory and speech recognition technology, and launches in-depth research on embedding learning theoretical knowledge into the field of speech recognition. Nowadays, large-scale text information databases relying on computers are becoming more and more important in linguistic research, and a large-scale corpus that fully reflects language facts and contains rich language information has been constructed. The establishment of the text information database system is long, from word segmentation, part-of-speech tagging to syntactic tagging to semantic tagging. Therefore, the characteristic of information system processing is that the systematic description depends on the application environment of understanding vocabulary and reasoning. According to different scenarios, the realization methods of semantic description roles are also different, and the description of semantic roles in correct scenarios is clearer, more detailed and systematic. Therefore, this article is of great significance for solving the semantic problem of using Chinese frame network for Chinese information processing in the context of speech recognition. © 2023, The Author(s) under exclusive licence to The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.,With the popularization of computers, artificial intelligence technology has become more and more mature, among which speech recognition technology in artificial intelligence is favored by people. In the past few years, the acoustic model combined with the Gaussian mixture model and the hidden Markov model has always been in the leading position. In the field of speech recognition technology, because the development of speech data has gradually expanded, and the complexity of the data has also increased. The larger the data, the traditional data network model is slowly showing inadequacy. However, the deep neural network model is easy to deal with large and complex data modeling. This article combines the advantages of basic learning theory and speech recognition technology, and launches in-depth research on embedding learning theoretical knowledge into the field of speech recognition. Nowadays, large-scale text information databases relying on computers are becoming more and more important in linguistic research, and a large-scale corpus that fully reflects language facts and contains rich language information has been constructed. The establishment of the text information database system is long, from word segmentation, part-of-speech tagging to syntactic tagging to semantic tagging. Therefore, the characteristic of information system processing is that the systematic description depends on the application environment of understanding vocabulary and reasoning. According to different scenarios, the realization methods of semantic description roles are also different, and the description of semantic roles in correct scenarios is clearer, more detailed and systematic. Therefore, this article is of great significance for solving the semantic problem of using Chinese frame network for Chinese information processing in the context of speech recognition. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Recently, more and more personalized speech enhancement (PSE) systems with excellent performance have been proposed. Compared with traditional speech enhancement systems, PSE systems have a wider range of application scenarios, which can simultaneously remove background noise and interfering speaker's speech. However, two issues still limit the performance and generalization ability of the model: 1) Acoustic environment mismatch between the test noisy speech and clean enrollment speech of target speaker, which limits the performance of personalized speech enhancement system; 2) Hard sample mining and learning. How to improve the performance of hard samples determines the practicality of a personalized speech enhancement system in complex real-world scenarios. In this paper, a dynamic acoustic compensation (DAC) is proposed to alleviate the environment mismatch, by intercepting the acoustic segments from noisy speech and mixing it with enrollment speech. To well exploit the hard samples, we propose an adaptive focal training (AFT) strategy by assigning adaptive loss weights to hard and non-hard samples during training. Both the DAC and AFT are proposed to improve and generalize our previous work, a densely-connected pyramid complex convolutional network with speaker encoder (sDPCCN) for personalized speech enhancement. In addition, a time-frequency multi-loss training is further introduced to enhance the improved sDPCCN. To examine the effectiveness of the proposed methods, we generate the noisy-reverb training and test data by utilizing non-overlapping segments of the 4th Deep Noise Suppression (DNS4) Challenge Dataset. Results show that, DAC effectively alleviates the acoustic environment mismatch and brings large improvements in terms of multiple evaluation metrics, and AFT reduces the hard sample rate significantly. When all proposed methods are applied, the perceptual evaluation of speech quality (PESQ) score is improved from 3.21 to 3.36, and the scale invariant signal-to-noise ratio (SISNR) is improved from 15.11 to 15.89 on the test set, which fully verify their effectiveness and practicality. © 2023 Elsevier Ltd"
35,34,88,34_Deep learning for point cloud analysis,Deep learning for point cloud analysis,"As 3D acquisition technology develops and 3D sensors become increasingly affordable, large quantities of 3D point cloud data are emerging. How to effectively learn and extract the geometric features from these point clouds has become an urgent problem to be solved. The point cloud geometric information is hidden in disordered, unstructured points, making point cloud analysis a very challenging problem. To address this problem, we propose a novel network framework, called Tree Graph Network (TGNet), which can sample, group, and aggregate local geometric features. Specifically, we construct a Tree Graph by explicit rules, which consists of curves extending in all directions in point cloud feature space, and then aggregate the features of the graph through a cross-attention mechanism. In this way, we incorporate more point cloud geometric structure information into the representation of local geometric features, which makes our network perform better. Our model performs well on several basic point clouds processing tasks such as classification, segmentation, and normal estimation, demonstrating the effectiveness and superiority of our network. Furthermore, we provide ablation experiments and visualizations to better understand our network. © 2023 Tech Science Press. All rights reserved.,In view of the difficulty of using raw 3D point clouds for component detection in the railway field, this paper designs a point cloud segmentation model based on deep learning together with a point cloud preprocessing mechanism. First, a special preprocessing algorithm is designed to resolve the problems of noise points, acquisition errors, and large data volume in the actual point cloud model of the bolt. The algorithm uses the point cloud adaptive weighted guided filtering for noise smoothing according to the noise characteristics. Then retaining the key points of the point cloud, this algorithm uses the octree to partition the point cloud and carries out iterative farthest point sampling in each partition for obtaining the standard point cloud model. The standard point cloud model is then subjected to hierarchical multi-scale feature extraction to obtain global features, which are combined with local features through a self-attention mechanism, while linear interpolation is used to further expand the perceptual field of local features of the model as a basis for segmentation, and finally the segmentation is completed. Experiments show that the proposed algorithm could deal with the scattered bolt point cloud well, realize the segmentation of train bolt and background, and could achieve high segmentation accuracy, which has important practical significance for train safety detection. © 2023 by the authors.,Point cloud data have been widely explored due to its superior accuracy and robustness under various adverse situations. Meanwhile, deep neural networks (DNNs) have achieved very impressive success in various applications such as surveillance and autonomous driving. The convergence of point cloud and DNNs has led to many deep point cloud models, largely trained under the supervision of large-scale and densely-labelled point cloud data. Unsupervised point cloud representation learning, which aims to learn general and useful point cloud representations from unlabelled point cloud data, has recently attracted increasing attention due to the constraint in large-scale point cloud labelling. This paper provides a comprehensive review of unsupervised point cloud representation learning using DNNs. It first describes the motivation, general pipelines as well as terminologies of the recent studies. Relevant background including widely adopted point cloud datasets and DNN architectures is then briefly presented. This is followed by an extensive discussion of existing unsupervised point cloud representation learning methods according to their technical approaches. We also quantitatively benchmark and discuss the reviewed methods over multiple widely adopted point cloud datasets. Finally, we share our humble opinion about several challenges and problems that could be pursued in the future research in unsupervised point cloud representation learning.  © 1979-2012 IEEE."
36,35,86,35_ICU Mortality Risk Prediction Using Machine Learning,ICU Mortality Risk Prediction Using Machine Learning,"Introduction: Complications after total hip arthroplasty (THA) may result in readmission or reoperation and impose a significant cost on the healthcare system. Understanding which patients are at-risk for complications can potentially allow for targeted interventions to decrease complication rates through pursuing preoperative health optimization. The purpose of the current was to develop and internally validate machine learning (ML) algorithms capable of performing patient-specific predictions of all-cause complications within two years of primary THA. Methods: This was a retrospective case–control study of clinical registry data from 616 primary THA patients from one large academic and two community hospitals. The primary outcome was all-cause complications at a minimum of 2-years after primary THA. Recursive feature elimination was applied to identify preoperative variables with the greatest predictive value. Five ML algorithms were developed on the training set using tenfold cross-validation and internally validated on the independent testing set of patients. Algorithms were assessed by discrimination, calibration, Brier score, and decision curve analysis to quantify performance. Results: The observed complication rate was 16.6%. The stochastic gradient boosting model achieved the best performance with an AUC = 0.88, calibration intercept = 0.1, calibration slope = 1.22, and Brier score = 0.09. The most important factors for predicting complications were age, drug allergies, prior hip surgery, smoking, and opioid use. Individual patient-level explanations were provided for the algorithm predictions and incorporated into an open access digital application: https://sorg-apps.shinyapps.io/tha_complication/ Conclusions: The stochastic boosting gradient algorithm demonstrated good discriminatory capacity for identifying patients at high-risk of experiencing a postoperative complication and proof-of-concept for creating office-based applications from ML that can perform real-time prediction. However, this clinical utility of the current algorithm is unknown and definitions of complications broad. Further investigation on larger data sets and rigorous external validation is necessary prior to the assessment of clinical utility with respect to risk-stratification of patients undergoing primary THA. Level of evidence: III, therapeutic study. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,An ICU is a critical care unit that provides advanced medical support and continuous monitoring for patients with severe illnesses or injuries. Predicting the mortality rate of ICU patients can not only improve patient outcomes, but also optimize resource allocation. Many studies have attempted to create scoring systems and models that predict the mortality of ICU patients using large amounts of structured clinical data. However, unstructured clinical data recorded during patient admission, such as notes made by physicians, is often overlooked. This study used the MIMIC-III database to predict mortality in ICU patients. In the first part of the study, only eight structured variables were used, including the six basic vital signs, the GCS, and the patient’s age at admission. In the second part, unstructured predictor variables were extracted from the initial diagnosis made by physicians when the patients were admitted to the hospital and analyzed using Latent Dirichlet Allocation techniques. The structured and unstructured data were combined using machine learning methods to create a mortality risk prediction model for ICU patients. The results showed that combining structured and unstructured data improved the accuracy of the prediction of clinical outcomes in ICU patients over time. The model achieved an AUROC of 0.88, indicating accurate prediction of patient vital status. Additionally, the model was able to predict patient clinical outcomes over time, successfully identifying important variables. This study demonstrated that a small number of easily collectible structured variables, combined with unstructured data and analyzed using LDA topic modeling, can significantly improve the predictive performance of a mortality risk prediction model for ICU patients. These results suggest that initial clinical observations and diagnoses of ICU patients contain valuable information that can aid ICU medical and nursing staff in making important clinical decisions. © 2023 by the authors.,BackgroundAseptic revision THA and TKA are associated with an increased risk of adverse outcomes compared with primary THA and TKA. Understanding the risk profiles for patients undergoing aseptic revision THA or TKA may provide an opportunity to decrease the risk of postsurgical complications. There are risk stratification tools for postoperative complications after aseptic revision TKA or THA; however, current tools only include nonmodifiable risk factors, such as medical comorbidities, and do not include modifiable risk factors.Questions/purposes(1) Can machine learning predict 30-day mortality and complications for patients undergoing aseptic revision THA or TKA using a cohort from the American College of Surgeons National Surgical Quality Improvement Program database? (2) Which patient variables are the most relevant in predicting complications?MethodsThis was a temporally validated, retrospective study analyzing the 2014 to 2019 National Surgical Quality Improvement Program database, as this database captures a large cohort of aseptic revision THA and TKA patients across a broad range of clinical settings and includes preoperative laboratory values. The training data set was 2014 to 2018, and 2019 was the validation data set. Given that predictive models learn expected prevalence of outcomes, this split allows assessment of model performance in contemporary patients. Between 2014 and 2019, a total of 24,682 patients underwent aseptic revision TKA and 17,871 patients underwent aseptic revision THA. Of those, patients with CPT codes corresponding to aseptic revision TKA or THA were considered as potentially eligible. Based on excluding procedures involving unclean wounds, 78% (19,345 of 24,682) of aseptic revision TKA procedures and 82% (14,711 of 17,871) of aseptic revision THA procedures were eligible. Ten percent of patients in each of the training and validation cohorts had missing predictor variables. Most of these missing data were preoperative sodium or hematocrit (8% in both the training and validation cohorts). No patients had missing outcome data. No patients were excluded due to missing data. The mean patient was age 66 ± 12 years, the mean BMI was 32 ± 7 kg/m2, and the mean American Society of Anesthesiologists (ASA) Physical Score was 3 (56%). XGBoost was then used to create a scoring tool for 30-day adverse outcomes. XGBoost was chosen because it can handle missing data, it is nonlinear, it can assess nuanced relationships between variables, it incorporates techniques to reduce model complexity, and it has a demonstrated record of producing highly accurate machine-learning models. Performance metrics included discrimination and calibration. Discrimination was assessed by c-statistics, which describe the area under the receiver operating characteristic curve. This quantifies how well a predictive model discriminates between patients who have the outcome of interest versus those who do not. Relevant ranges for c-statistics include good (0.70 to 0.79), excellent (0.80 to 0.89), and outstanding (> 0.90). We estimated 95% confidence intervals (CIs) for c-statistics by 500-sample bootstrapping. Calibration curves quantify reliability of model predictions. Reliable models produce prediction probabilities for outcomes that are similar to observed probabilities of those outcomes, so a well-calibrated model should demonstrate a calibration curve that does not deviate substantially from a line of slope 1 and intercept 0. Calibration curves were generated on the 2019 validation data. Shapley Additive Explanations (SHAP) visualizations were used to investigate feature importance to gain insight into how models made predictions. The models were built into an online calculator for ongoing testing and validation. The risk calculator, which is freely available (http://nb-group.org/rev2/), allows a user to input patient data to calculate postoperative risk of 30-day mortality, cardiac, and respiratory complications after aseptic revision TKA or THA. A post hoc analysis was performed to assess whether using data from 2020 would improve calibration on 2019 data.ResultsThe model accurately predicted mortality, cardiac complications, and respiratory complications after aseptic revision THA or TKA, with c-statistics of 0.88 (95% CI 0.83 to 0.93), 0.80 (95% CI 0.75 to 0.84), and 0.78 (95% CI 0.74 to 0.82), respectively, on internal validation and 0.87 (95% CI 0.77 to 0.96), 0.70 (95% CI 0.61 to 0.78), and 0.82 (95% CI 0.75 to 0.88), respectively, on temporal validation. Calibration curves demonstrated slight over-confidence in predictions (most predicted probabilities were higher than observed probabilities). Post hoc analysis of 2020 data did not yield improved calibration on the 2019 validation set. Important risk factors for all models included increased age and higher ASA, BMI, hematocrit level, and sodium level. Hematocrit and ASA were in the top three most important features for all models. The factor with the strongest association for mortality and cardiac complication models was age, and for the respiratory model, chronic obstructive pulmonary disease. Risk related to sodium followed a U-shaped curve. Preoperative hyponatremia and hypernatremia predicted an increased risk of mortality and respiratory complications, with a nadir of 138 mmol/L; hyponatremia was more strongly associated with mortality than hypernatremia. A hematocrit level less than 36% predicted an increased risk of all three adverse outcomes. A BMI less than 24 kg/m2 - and especially less than 20 kg/m2 - predicted an increased risk of all three adverse outcomes, with little to no effect for higher BMI.ConclusionThis temporally validated model predicted 30-day mortality, cardiac complications, and respiratory complications after aseptic revision THA or TKA with c-statistics ranging from 0.78 to 0.88. This freely available risk calculator can be used preoperatively by surgeons to educate patients on their individual postoperative risk of these specific adverse outcomes. Unanswered questions that remain include whether altering the studied preoperative patient variables, such as sodium or hematocrit, would affect postoperative risk of adverse outcomes; however, a prospective cohort study is needed to answer this question.  Copyright © 2022 by the Association of Bone and Joint Surgeons."
37,36,85,36_Electricity Load Forecasting,Electricity Load Forecasting,"The paper presents the results of studies of the predictive models development based on retrospective data on planned electricity consumption in the region with a significant share of enterprises in the mineral resource complex. Since the energy intensity of the industry remains quite high, the task of rationalizing the consumption of electricity is relevant. One of the ways to improve control accuracy when planning energy costs is to forecast electrical loads. Despite the large number of scientific papers on the topic of electricity consumption forecasting, this problem remains relevant due to the changing requirements of the wholesale electricity and power market to the accuracy of forecasts. Therefore, the purpose of this study is to support management decisions in the process of planning the volume of electricity consumption. To realize this, it is necessary to create a predictive model and determine the prospective power consumption of the power system. For this purpose, the collection and analysis of initial data, their preprocessing, selection of features, creation of models, and their optimization were carried out. The created models are based on historical data on planned power consumption, power system performance (frequency), as well as meteorological data. The research methods were: ensemble methods of machine learning (random forest, gradient boosting algorithms, such as XGBoost and CatBoost) and a long short-term memory recurrent neural network model (LSTM). The models obtained as a result of the conducted studies allow creating short-term forecasts of power consumption with a fairly high precision (for a period from one day to a week). The use of models based on gradient boosting algorithms and neural network models made it possible to obtain a forecast with an error of less than 1 %, which makes it possible to recommend the models described in the paper for use in forecasting the planned electricity power consumption of united power systems. © 2023, Saint-Petersburg Mining University. All rights reserved.,With the increasing numbers of smart meter installations, scalable and efficient load forecasting techniques are critically needed to ensure sustainable situation awareness within the distribution networks. Distribution networks include a large amount of different loads at various aggregation levels, such as individual consumers, low-voltage feeders, and transformer stations. It is impractical to develop individual (or so-called local) forecasting models for each load separately. Additionally, such local models also (i) (largely) ignore the strong dependencies between different loads that might be present due to their spatial proximity and the characteristics of the distribution network, (ii) require historical data for each load to be able to make forecasts, and (iii) are incapable of adjusting to changes in the load behavior without retraining. To address these issues, we propose a global modeling framework for load forecasting in distribution networks that, unlike its local competitors, relies on a single global model to generate forecasts for a large number of loads. The global nature of the framework, significantly reduces the computational burden typically required when training multiple local forecasting models, efficiently exploits the cross-series information shared among different loads, and facilitates forecasts even when historical data for a load is missing or the behavior of a load evolves over time. To further improve on the performance of the proposed framework, an unsupervised localization mechanism and optimal ensemble construction strategy are also proposed to localize/personalize the global forecasting model to different load characteristics. Our experimental results show that the proposed framework outperforms naive benchmarks by more than 25% (in terms of Mean Absolute Error) on real-world dataset while exhibiting highly desirable characteristics when compared to the local models that are predominantly used in the literature. All source code and data are made publicly available to enable reproducibility: https://github.com/mihagrabner/GlobalModelingFramework.  © 2010-2012 IEEE.,The accurate prediction of short-term load is crucial for the grid dispatching department in developing power generation plans, regulating unit output, and minimizing economic losses. However, due to the variability in customers’ electricity consumption behaviour and the randomness of load fluctuations, it is challenging to achieve high prediction accuracy. To address this issue, we propose an ensemble deep learning model that utilizes reduced dimensional clustering and decomposition strategies to mitigate large prediction errors caused by non-linearity and unsteadiness of load sequences. The proposed model consists of three steps: Firstly, the selected load features are dimensionally reduced using singular value decomposition (SVD), and the principal features are used for clustering different loads. Secondly, variable mode decomposition (VMD) is applied to decompose the total load of each class into intrinsic mode functions of different frequencies. Finally, an ensemble deep learning model is developed by combining the strengths of LSTM and CNN-GRU deep learning algorithms to achieve accurate load forecasting. To validate the effectiveness of our proposed model, we employ actual residential electricity load data from a province in northwest China. The results demonstrate that the proposed algorithm performs better than existing methods in terms of predictive accuracy. © 2023 by the authors."
38,37,84,37_Deep Learning-Based Channel Estimation and Beamforming in Massive MIMO Systems,Deep Learning-Based Channel Estimation and Beamforming in Massive MIMO Systems,"Hybrid analog/digital multiple input multiple output (MIMO) system is proposed to mitigate the challenges of millimeter wave (mmWave) communication. This architecture enables utilizing the large array gain with reasonable power consumption. However, new methods are required for the channel estimation problem of hybrid architecture-based systems due to the fewer number of radio frequency (RF) chains than antenna elements. Leveraging the sparse nature of the mmWave channels, compressed sensing (CS)-based channel estimation methods are proposed. Recently, machine learning (ML)-aided methods have been investigated to improve the channel estimation performance. Additionally, the Doppler effect should be considered for the high mobility scenarios, and we deal with the time-varying channel model. Therefore, in this article, we consider the scenario of time-varying channels for a multi-user mmWave hybrid MIMO system. By proposing a Deep Neural Network (DNN) and defining the inputs and outputs, we introduce a novel algorithm called Deep Learning Assisted Angle Estimation (DLA-AE) for improving the estimation of the Angles of Departure/Arrival (AoDs/AoAs) of the channel paths. In addition, we suggest Linear Phase Interpolation (LPI) to acquire the path gains for the data transmission instants. Simulation results show that utilizing the proposed DLA-AE and LPI methods enhance the time-varying channel estimation accuracy with low computational complexity. © 2022 Elsevier B.V.,In mm-wave massive multiple-input-multiple-output (MIMO) systems, accurate channel state information (CSI) at the base station (BS) is the key knowledge to obtain the performance gain. Consequently, the user is not only required to complete the channel estimation, but also to feedback CSI to the BS. However, large-scale antenna arrays result in a substantial feedback overhead, which poses a challenging issue. Furthermore, the accuracy requirements of channel estimation and CSI feedback depend on the computing capacity of the user. In this paper, we propose a joint channel estimation and multiple-compression-rate feedback (JCEMF) scheme, and adopt centralized learning (CL) and federated learning (FL) strategies for the scheme. According to the limited computational resources available to users, the JCEMF scheme enables various lengths of feedback bits to change the feedback overhead. Additionally, the users in FL train the local models using their own datasets and upload the local model updates to the BS, thereby reducing communication overhead and protecting data privacy. Specifically, an estimation network is designed for the user to estimate the channel from the received signal. In the CSI feedback process, we introduce an MCRF network, which can achieve CSI compression and reconstruction with different numbers of feedback bits. Simulation results verify that the proposed approach shows good performance of joint channel estimation and multiple-compression-rate CSI feedback in different channel conditions. © 2024 Elsevier B.V.,The latest research for applying deep learning in wireless communications gives several opportunities to reduce complex signal processing. The channel estimation is important to study the nature of the varying channel and to calculate channel state information (CSI) value which is utilized at the receiver to nullify the interference which occurs during multipath transmission. In the current article, considering the massive Multiple Input Multiple Output (MIMO) channel model, a DL approach is developed with a fully connected neural network (NN) architecture which is used to estimate the channel with minimum error. The proposed DL architecture uses an openly available channel dataset. Further, using generated pilot symbols of lengths 2 and 4, the performance of DL-based Fully connected NN (DL-FCNN) is analyzed to estimate the channel in uplink massive MIMO communication. The obtained results demonstrate that the channel estimation performance was calculated in terms of normalized mean square error((NMSE) for different values of SNR added at receiver base station (BS) to the signals over the range of BS antennas. Also, the channel estimation error over a large number of BS antennas for massive MIMO scenarios is observed, and it is observed that the NMSE reduces with a greater number of antennas. Hence, it can be inferred that the DL models will be the future for most physical layer signal processing techniques such as channel estimation, modulation detection, etc. within massive MIMO networks. © 2022 Taylor & Francis Group, LLC."
39,38,82,38_Education data mining and predictive analytics for student performance.,Education data mining and predictive analytics for student performance.,"The emergence of computer interactive teaching system greatly facilitates the needs of students to learn anytime and anywhere at this stage. At the same time, the increase in online course video, voice, files and other teaching resources also puts forward higher requirements for the operation of the teaching system. This article addresses the practical needs of online teaching and proposes a scalable computer interactive teaching system designed with the aid of large-scale multimedia data analysis algorithms. The system utilizes a principal component analysis linear data feature extraction algorithm to extract data features and implements data prediction through linear transformation methods for data reconstruction. This approach enables the system to effectively extract and analyze data, while also facilitating efficient data prediction for improved teaching outcomes. With its scalability and advanced data analysis capabilities, this system represents a promising solution for enhancing the effectiveness of online teaching and improving the overall learning experience for students. Most online course resources are displayed in the form of multimedia. Therefore, this paper uses robust component analysis to process multimedia data, especially for some problems such as data missing and multimedia data interference. This paper uses RC ranking-related evaluation method and R-value evaluation method to evaluate the prediction data, which proves that parameter selection plays an important role in controlling sparse regression. The algorithm proposed in this paper was tested using actual video and picture data related to the teaching system resources. The results indicated that the algorithm achieved the best data recognition performance when the number of theme models was set to 100. This finding provides an important reference for the system design. Finally, this paper designs a computer interactive teaching system which includes three modules: administrator, student and teacher, and tests the system, which proves the practical operation possibility and application value of the system. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,The current world is in an era of the explosive growth in information and rapid renewal. This objective reality puts new demands on education reform. Educational systems and educational models must undergo major changes to meet the needs of creative talent. With the wide application of modern computer technology and network technology, the traditional education structure based on classrooms is constantly changing. In the traditional education process, it emphasizes directly instilling a large amount of knowledge to the students to obtain the quality of teaching. In the network environment, it focuses on cultivating students. Learning methods allow students to have independent knowledge acquisition and also update skills. Online teaching provides a broader and free platform and environment for students' learning. It not only provides rich and colorful teaching resources, but also broadens and breaks the time and space constraints of traditional teaching. The current main method of management of teaching resources is the component teaching resource library, which uses information technology to carry out unified maintenance and management of the teaching resources with increased benefits, and realizes rapid retrieval and convenient use of teaching resources. The numerical verifications are conducted in the final section of the research to verify the performance. © 2021 John Wiley & Sons, Ltd.,With the continuous development of computers, colleges and universities need to attach importance to the combination of student teaching and computer-aided design. The teaching of parametric design is used to scientifically divide different teaching contents to help students meet the needs of the digital era. Use online resources to transfer offline teaching content to online to increase offline design analysis and discussion time. Therefore, how to improve the informatization level of university education management has become one of the important challenges that Chinese education departments are facing and must solve at this stage. For computer-aided design online courses is conducive to improving the quality of online teaching of such courses. Based on the implementation process and effect analysis of computer-aided design online course teaching, the formative multiple evaluation index of course effect is proposed. Based on the potential information of multimedia, a large number of researchers have begun to conduct data mining research on it and accumulated a large amount of data. The ROF-LGB model, a comprehensive classification model based on rotating forest and LightGBM, attempts to deeply explore the potential information of the actual teaching management data set. In this process, some thoughts on educational information management are put forward for readers' reference. © 2023 CAD Solutions, LLC."
40,39,82,39_Crop chlorophyll content estimation using UAV remote sensing,Crop chlorophyll content estimation using UAV remote sensing,"Context or problem: Nitrogen is one of the important elements of crops, which plays a decisive role in crop growth and development and the formation of yields. Monitoring of rice organ-scale nitrogen concentration based on the unmanned aerial vehicle (UAV) images is of great significance for rice field management and yield prediction. Objective or research question: Previous studies have focused on the use of traditional statistical methods and chlorophyll-related vegetation indices to construct plant nitrogen concentration, with models lacking generalizability. Methods: In this study, rice field trials of two varieties (NJ9108, YD6) and nitrogen fertilizer treatments (N0-N3: 0, 105, 210 and 315 kg/ha) were conducted for 3 years with manual sampling and UAV digital and hyperspectral images during key fertility periods. Based on the data of the whole growth periods and combined with vegetation indices (VIs), color indices (CIs), hyperspectral parameters (HPs), texture indices (TIs) and machine-learning algorithms, monitoring models of nitrogen concentration at the organ scale of rice were constructed and used to estimate the N content of multiple organs (leaf and stem) of rice at different periods. Field experiments were used to collect the multi-organ nitrogen concentration of rice and the remote sensing (RS) data of UAV during the critical growth period of the two years (2021, 2022), and machine-learning algorithms were used to construct the estimation models. Results: The results showed that VIs had good correlations with leaf nitrogen concentration (LNC), stem nitrogen concentration (SNC) and plant nitrogen concentration (PNC), with correlation coefficients (r) of 0.86, 0.74 and 0.81, respectively. Machine learning estimation models combining multiple types of RS indices were more accurate than single parameter models constructed by traditional statistical methods, with the LNC optimal model (R2 = 0.8, RMSE = 3.83 mg/g), the SNC optimal model (R2 = 0.7, RMSE = 2.43 mg/g) and the PNC optimal model (R2 = 0.7, RMSE = 3.19 mg/g). Validated using data from 2020, the machine-learning models were far more accurate than traditional methods. Conclusions: These results show that the use of multi-source remote sensing data based on machine-learning algorithms can effectively estimate the nitrogen concentration of organs in rice. Implications: This study provides an accurate, stable and universal method for estimating rice nitrogen concentration in rice organs, which can be used as a reference for estimating rice nitrogen concentration in large fields using UAV RS technology. © 2023 Elsevier B.V.,Leaf Area Index (LAI) is one of the indicators used to measure the growth status of rice fields. Rapid, accurate, and large-scale monitoring of LAI plays an important role in ensuring stable grain yield increase. In recent years, the spectral saturation problem and the parameter adjustment problem of machine learning algorithms have become the main limitations to improve the accuracy of LAI estimation. High-resolution Unmanned Aerial Vehicles (UAVs) images contain not only rich spectral information, but also texture information reflecting the crop canopy structure. Therefore, in order to fully understand the role of spectral information and texture information fusion in rice LAI estimation, this study used the hyperspectral sensor carried by the UAVs to obtain the spectral images of rice canopy of different varieties and different growth stages. Rice canopy reflectance and 8 basic texture features based on Gray-level Co-occurrence Matrix (GLCM) were extracted from hyperspectral images to calculate vegetation indexs (VIs) and combined texture features. Normalized difference texture index (NDTI), Non-linear texture index (NLTI), Enhanced vegetation texture index (EVTI), and Modified triangular texture index (MTTI) were calculated using two and three GLMC-based texture features to explore the effect of combinations of different basic texture features on LAI sensitivity. Two rice LAI estimation models were developed for single spectral indicators and combined with texture indicators, respectively. The results show that: (1) After preprocessing and feature band screening, the optimal spectral band, vegetation index, and trilateral parameters were obtained. When the combined spectral parameters (SP) of the three were used as the only input to the model, R2 showed an increasing trend throughout the growth period. The best results were achieved using the support vector regression (SVR) combined with the pelican optimization algorithm (POA) in the pre jointing stage: R2= 0.839, RMSE = 0.107, and MAPE = 7.02%. (2) When texture information based on hyperspectral images was incorporated into the model input, the results showed that the models based on spectral indicators combined with texture measurements were all superior to those using spectral indicators alone, with the best model having a coefficient of determination: R2 = 0.917, RMSE = 0.078, and MAPE = 4.19%, which has promising applications in crop growth index detection. This technology can quickly and effectively monitor the growth status of crops in the field, providing a theoretical basis for estimating crop yield in the later stage. © 2023,Leaf chlorophyll is an important dynamic biochemical parameter to assess crop phenology, health stress, and yield. Optical remote sensing data is a widely used technique for the estimation of vegetation leaf chlorophyll. Drone technology is a promising solution for high-resolution monitoring of the leaf chlorophyll content and employed in this study. Image fusion was also accomplished to provide high-resolution multi-spectral images by combining Sentinel-2A and drone imagery. Support vector machine regression was adopted to determine the best vegetation indices for the retrieval of leaf chlorophyll content. Random forest regression and support vector machine regression algorithms were adopted to develop the best models for chlorophyll retrieval. The 34 models derived from drone data and the 46 models derived from fusion data were evaluated for chlorophyll retrieval. It was found that the best support vector machine model, based on anthocyanin content index and enhanced vegetation index (M31), had the best correlation coefficient (r = 0.732) and root mean square error (1.93). The random forest regression models M32 (based on chlorophyll vegetation index and visible atmospherically resistant index) and M16 (based on canopy chlorophyll content index and visible atmospherically resistant index) for drone data were found to be the best, which performed equally well in terms of correlation coefficients (r = 0.77) and root mean square error (1.51). The multiple vegetation indices of drone-based leaf chlorophyll content random forest regression models (M16 and M32) provided higher performance than single vegetation indices-based linear (simple linear fit) and nonlinear leaf chlorophyll content models. Out of 46 models of fusion products, the random forest regression M14 (green normalized difference vegetation index with narrow near infrared band and visible atmospherically resistant index) offered the best performance for leaf chlorophyll content retrieval, as can be observed through a comparison of correlation (r = 0.77) and RMSE (=1.71) values. It was also investigated that inversion performances (r ? 0.8 and RMSD ? 1.4 with standard deviation ? 2.6) of fusion data-based models in cross-applicability and transferability were found to be suitable for large-scale inversion of Sentinel-2A data products. © 2023"
41,40,81,40_COVID-19 Mortality and Outbreak Risk Factors,COVID-19 Mortality and Outbreak Risk Factors,"Objective COVID-19 would kill fewer people if health programmes can predict who is at higher risk of mortality because resources can be targeted to protect those people from infection. We predict mortality in a very large population in Mexico with machine learning using demographic variables and pre-existing conditions. Design Cohort study. Setting March 2020 to November 2021 in Mexico, nationally represented. Participants 1.4 million laboratory-confirmed patients with COVID-19 in Mexico at or over 20 years of age. Primary and secondary outcome measures Analysis is performed on data from March 2020 to November 2021 and over three phases: (1) from March to October in 2020, (2) from November 2020 to March 2021 and (3) from April to November 2021. We predict mortality using an ensemble machine learning method, super learner, and independently estimate the adjusted mortality relative risk of each pre-existing condition using targeted maximum likelihood estimation. Results Super learner fit has a high predictive performance (C-statistic: 0.907), where age is the most predictive factor for mortality. After adjusting for demographic factors, renal disease, hypertension, diabetes and obesity are the most impactful pre-existing conditions. Phase analysis shows that the adjusted mortality risk decreased over time while relative risk increased for each pre-existing condition. Conclusions While age is the most important predictor of mortality, younger individuals with hypertension, diabetes and obesity are at comparable mortality risk as individuals who are 20 years older without any of the three conditions. Our model can be continuously updated to identify individuals who should most be protected against infection as the pandemic evolves.  © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ.,Background: At the end of 2019, the coronavirus disease 2019 (COVID-19) pandemic increased the hospital burden of COVID-19 caused by the SARS-Cov-2 and became the most significant health challenge for nations worldwide. The severity and high mortality of COVID-19 have been correlated with various demographic characteristics and clinical manifestations. Prediction of mortality rate, identification of risk factors, and classification of patients played a crucial role in managing COVID-19 patients. Our purpose was to develop machine learning (ML)-based models for the prediction of mortality and severity among patients with COVID-19. Identifying the most important predictors and unraveling their relationships by classification of patients to the low-, moderate- and high-risk groups might guide prioritizing treatment decisions and a better understanding of interactions between factors. A detailed evaluation of patient data is believed to be important since COVID-19 resurgence is underway in many countries. Results: The findings of this study revealed that the ML-based statistically inspired modification of the partial least square (SIMPLS) method could predict the in-hospital mortality among COVID-19 patients. The prediction model was developed using 19 predictors including clinical variables, comorbidities, and blood markers with moderate predictability (Q2 = 0.24) to separate survivors and non-survivors. Oxygen saturation level, loss of consciousness, and chronic kidney disease (CKD) were the top mortality predictors. Correlation analysis showed different correlation patterns among predictors for each non-survivor and survivor cohort separately. The main prediction model was verified using other ML-based analyses with a high area under the curve (AUC) (0.81?0.93) and specificity (0.94?0.99). The obtained data revealed that the mortality prediction model can be different for males and females with diverse predictors. Patients were classified into four clusters of mortality risk and identified the patients at the highest risk of mortality, which accentuated the most significant predictors correlating with mortality. Conclusion: An ML model for predicting mortality among hospitalized COVID-19 patients was developed considering the interactions between factors that may reduce the complexity of clinical decision-making processes. The most predictive factors related to patient mortality were identified by assessing and classifying patients into different groups based on their sex and mortality risk (low-, moderate-, and high-risk groups). Copyright © 2023 Banoei, Rafiepoor, Zendehdel, Seyyedsalehi, Nahvijou, Allameh and Amanpour.,Introduction. The risk of infectious disease transmission, including COVID-19, is disproportionately high in correctional facilities due to close living conditions, relatively low levels of vaccination, and reduced access to testing and treatment. While much progress has been made on describing and mitigating COVID-19 and other infectious disease risk in jails and prisons, there are open questions about which data can best predict future outbreaks. Methods. We used facility data and demographic and health data collected from 24 prison facilities in the Pennsylvania Department of Corrections from March 2020 to May 2021 to determine which sources of data best predict a coming COVID-19 outbreak in a prison facility. We used machine learning methods to cluster the prisons into groups based on similar facility-level characteristics, including size, rurality, and demographics of incarcerated people. We developed logistic regression classification models to predict for each cluster, before and after vaccine availability, whether there would be no cases, an outbreak defined as 2 or more cases, or a large outbreak, defined as 10 or more cases in the next 1, 2, and 3 d. We compared these predictions to data on outbreaks that occurred. Results. Facilities were divided into 8 clusters of sizes varying from 1 to 7 facilities per cluster. We trained 60 logistic regressions; 20 had test sets with between 35% and 65% of days with outbreaks detected. Of these, 8 logistic regressions correctly predicted the occurrence of an outbreak more than 55% of the time. The most common predictive feature was incident cases among the incarcerated population from 2 to 32 d prior. Other predictive features included the number of tests administered from 1 to 33 d prior, total population, test positivity rate, and county deaths, hospitalizations, and incident cases. Cumulative cases, vaccination rates, and race, ethnicity, or age statistics for incarcerated populations were generally not predictive. Conclusions. County-level measures of COVID-19, facility population, and test positivity rate appear as potential promising predictors of COVID-19 outbreaks in correctional facilities, suggesting that correctional facilities should monitor community transmission in addition to facility transmission to inform future outbreak response decisions. These efforts should not be limited to COVID-19 but should include any large-scale infectious disease outbreak that may involve institution-community transmission. The risk of infectious disease transmission, including COVID-19, is disproportionately high in correctional facilities. We used machine learning methods with data collected from 24 prison facilities in the Pennsylvania Department of Corrections to determine which sources of data best predict a coming COVID-19 outbreak in a prison facility. Key predictors included county-level measures of COVID-19, facility population, and the test positivity rate in a facility. Fortifying correctional facilities with the ability to monitor local community rates of infection (e.g., though improved interagency collaboration and data sharing) along with continued testing of incarcerated people and staff can help correctional facilities better predict—and respond to—future infectious disease outbreaks. © The Author(s) 2024."
42,41,81,41_Forest Biomass Monitoring and Analysis,Forest Biomass Monitoring and Analysis,"Forest canopy height is defined as the distance between the highest point of the tree canopy and the ground, which is considered to be a key factor in calculating above-ground biomass, leaf area index, and carbon stock. Large-scale forest canopy height monitoring can provide scientific information on deforestation and forest degradation to policymakers. The Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) was launched in 2018, with the Advanced Topographic Laser Altimeter System (ATLAS) instrument taking on the task of mapping and transmitting data as a photon-counting LiDAR, which offers an opportunity to obtain global forest canopy height. To generate a high-resolution forest canopy height map of Jiangxi Province, we integrated ICESat-2 and multi-source remote sensing imagery, including Sentinel-1, Sentinel-2, the Shuttle Radar Topography Mission, and forest age data of Jiangxi Province. Meanwhile, we develop four canopy height extrapolation models by random forest (RF), Support Vector Machine (SVM), K-nearest neighbor (KNN), Gradient Boosting Decision Tree (GBDT) to link canopy height in ICESat-2, and spatial feature information in multi-source remote sensing imagery. The results show that: (1) Forest canopy height is moderately correlated with forest age, making it a potential predictor for forest canopy height mapping. (2) Compared with GBDT, SVM, and KNN, RF showed the best predictive performance with a coefficient of determination (R2) of 0.61 and a root mean square error (RMSE) of 5.29 m. (3) Elevation, slope, and the red-edge band (band 5) derived from Sentinel-2 were significantly dependent variables in the canopy height extrapolation model. Apart from that, Forest age was one of the variables that the RF moderately relied on. In contrast, backscatter coefficients and texture features derived from Sentinel-1 were not sensitive to canopy height. (4) There is a significant correlation between forest canopy height predicted by RF and forest canopy height measured by field measurements (R2 = 0.69, RMSE = 4.02 m). In a nutshell, the results indicate that the method utilized in this work can reliably map the spatial distribution of forest canopy height at high resolution. © 2023 by the authors.,Continuous mapping of the height and canopy cover of forests is vital for measuring forest biomass, monitoring forest degradation and restoration. In this regard, the contribution of Light Detection and Ranging (LiDAR) sensors, which were developed to obtain detailed data on forest composition across large geographical areas, is immense. Accordingly, this study aims to predict forest canopy cover and height in tropical forest areas utilizing Global Ecosystem Dynamics Investigation (GEDI) LIDAR, multisensor images, and random forest regression. To achieve this, we gathered predictor variables from the Shuttle Radar Topography Mission (SRTM) digital elevation model (DEM), Sentinel-2 multispectral datasets, and Sentinel-1 synthetic aperture radar (SAR) backscatters. The model's accuracy was evaluated based on a validation dataset of GEDI Level 2A and Level 2B. The random forest method was used the combination of data layers from Sentinel-1, Sentinel-2, and topographic measurements to model forest canopy cover and height. The produced canopy height and cover maps had a resolution of 30 m with R2 = 0.86 and an RMSE of 3.65 m for forest canopy height and R2 = 0.87 and an RMSE of 0.15 for canopy cover for the year 2022. These results suggest that combining multiple variables and data sources improves canopy cover and height prediction accuracy compared to relying on a single data source. The output of this study could be helpful in creating forest management plans that support sustainable utilization of the forest resources. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.,With the implementation of large-scale ecological restoration projects, Southwest China has become one of the fastest forest growth areas in the world in terms of vegetation cover and above-ground biomass (AGB). It is expected to be a potential area for achieving the carbon neutrality target in China. Accurate estimation of forest AGB is becoming an increasingly urgent necessity for carbon neutrality and forest management. However, due to the complex geological background, there is significant uncertainty in estimating forest AGB in the southwestern region, which generally results in underestimating carbon sinks from forest restoration. To address the issue, we propose a method by incorporating forest age information and stack learning technique to estimate forest AGB. Based on remote sensing, forest inventory and in situ forest biomass data, three fundamental methods (Multiple regression, Random forest and Support vector machine) are employed and compared to build the AGB estimation model with vegetation indices, texture feature factors and forest age. Optimal basic models are further enhanced by integration learning to improve the estimation performance and then applied to the study area Guangxi to obtain regional AGB information of different forest types. The results show that: (1) forest age plays a vital role in reducing the uncertainty of AGB estimation. By incorporating forest age information, R2 of AGB estimation is improved by 0.07–0.27 and RMSE is decreased by 16.35%–47.47% for different forest types; (2) with R2 value >0.78, random forest model outperforms support vector machine and multiple linear regression models. Compared with the single optimal model, integration model by stack learning further enhances R2 of estimation by 0.02–0.03 and decreases RMSE by 5.20%–14.89%. (3) The total forest AGB in Guangxi is 988.17 Tg and the average forest AGB level is 73.30 t/ha. Natural broadleaf forest has the highest AGB level (86.75 t/ha), followed by natural coniferous forest (81.19 t/ha), planted coniferous forest (63.23 t/ha) and planted eucalyptus forest (49.71 t/ha). AGB level in karst areas is lower than that in non-karst areas due to soil and water constraints. The majority of plantation forests in Guangxi is in the early and middle stages of forest succession, with the rapid growth of forest AGB, and has hence significant potential as carbon sinks. This study indicates that stack learning and incorporation of forest age could significantly decrease the uncertainty of forest AGB estimation. Our study helps to provide more accurate AGB information for karst ecological project management and regional carbon neutrality assessment. © 2023 John Wiley & Sons Ltd."
43,42,80,42_Hyperspectral image classification using deep learning,Hyperspectral image classification using deep learning,"Hyperspectral imagery gives details of spectral information through hundreds of spectral bands also known as dimensionality. The bands with continuous spectral information model are capable of classifying various materials of interest. The enhanced dimensionality of such data allows for major changes in data relevant information, but it also presents a challenge to traditional methods for accurate hyperspectral image analysis so-called ""curse of dimensionality.""The hyperspectral images are used to identify objects on the earth's surface. Due to a large number of spectral bands, classifying objects using hyperspectral imagery has become more challenging. Feature reduction and extraction techniques are used to address these high-dimensionality issues. However, there are various challenges dealing with data classification with performance and computational time. As a result, a technique for hyperspectral image classification based on a convolutional neural network (CNN) along with geometric transformation and polygons segmentation process has been proposed. A CNN is used to convert compressed features into high-level features that were utilized to classify objects into buildings and road networks. The main objective of this paper is to design an automated building footprint extraction and road detection from hyperspectral imagery using CNN. The polygons segmentation is used for the extraction and detection of spectral features in hyperspectral data. CNN is used to classify these extracted spectral features, such as building footprints and road detection, using different kernels. When comparing the proposed techniques with other support vector machine and fully convolutional network methods, the experimental results show that CNN provides 97% classification accuracy.  © 2022 SPIE and IS&T.,Currently, long-range spectral and spatial dependencies have been widely demonstrated to be essential for hyperspectral image (HSI) classification. Due to the transformer superior ability to exploit long-range representations, the transformer-based methods have exhibited enormous potential. However, existing transformer-based approaches still face two crucial issues that hinder the further performance promotion of HSI classification: 1) treating HSI as 1-D sequences neglects spatial properties of HSI and 2) the dependence between spectral and spatial information is not fully considered. To tackle the above problems, a large kernel spectral-spatial attention network (LKSSAN) is proposed to capture the long-range 3-D properties of HSI, which is inspired by the visual attention network (VAN). Specifically, a spectral-spatial attention module (SSAM) is first proposed to effectively exploit discriminative 3-D spectral-spatial features while keeping the 3-D structure of HSI. This module introduces the large kernel attention (LKA) and convolutional feed-forward (CFF) to flexibly emphasize, model, and exploit the long-range 3-D feature dependencies with lower computational pressure. Finally, the features from the SSAM are fed into the classification module for the optimization of 3-D spectral-spatial representation. To verify the effectiveness of the proposed classification method, experiments are executed on four widely used HSI datasets. The experiments demonstrate that LKSSAN is indeed an effective way for long-range 3-D feature extraction of HSI.  © 1980-2012 IEEE.,Aimed at the hyperspectral image (HSI) classification under the condition of limited samples, this paper designs a joint spectral–spatial classification network based on metric meta-learning. First, in order to fully extract HSI fine features, the squeeze and excitation (SE) attention mechanism is introduced into the spectrum dimensional channel to selectively extract useful HSI features to improve the sensitivity of the network to information features. Second, in the part of spatial feature extraction, the VGG16 model parameters trained in advance on the HSRS-SC dataset are used to realize the transfer and learning of spatial feature knowledge, and then, the higher-level abstract features are extracted to mine the intrinsic attributes of ground objects. Finally, the gated feature fusion strategy is introduced to connect the extracted spectral and spatial feature information on HSI for mining more abundant feature information. In this paper, a large number of experiments are carried out on the public hyperspectral dataset, including Pavia University and Salinas. The results show that the meta-learning method can achieve fast learning of new categories with only a small number of labeled samples and has good generalization ability for different HSI datasets. Copyright © 2023 Wu, Li and Wang."
44,43,79,43_Quantum-Classical Variational Circuit Integration for Quantum Machine Learning,Quantum-Classical Variational Circuit Integration for Quantum Machine Learning,"Variational quantum algorithms have the potential for significant impact on high-dimensional optimization, with applications in classical combinatorics, quantum chemistry, and condensed matter. Nevertheless, the optimization landscape of these algorithms is generally nonconvex, leading the algorithms to converge to local, rather than global, minima and the production of suboptimal solutions. In this work, we introduce a variational quantum algorithm that couples classical Markov chain Monte Carlo techniques with variational quantum algorithms, allowing the former to provably converge to global minima and thus assure solution quality. Due to the generality of our approach, it is suitable for a myriad of quantum minimization problems, including optimization and quantum state preparation. Specifically, we devise a Metropolis-Hastings method that is suitable for variational quantum devices and use it, in conjunction with quantum optimization, to construct quantum ensembles that converge to Gibbs states. These performance guarantees are derived from the ergodicity of our algorithm’s state space and enable us to place analytic bounds on its time-complexity. We demonstrate both the effectiveness of our technique and the validity of our analysis through quantum circuit simulations for MaxCut instances, solving these problems deterministically and with perfect accuracy, as well as large-scale quantum Ising and transverse field spin models of up to 50 qubits. Our technique stands to broadly enrich the field of variational quantum algorithms, improving and guaranteeing the performance of these promising, yet often heuristic, methods. © 2022 The Author(s). Published by IOP Publishing Ltd.,The neural network and quantum computing are both significant and appealing fields, with their interactive disciplines promising for large-scale computing tasks that are untackled by conventional computers. However, both developments are restricted by the scope of the hardware development. Nevertheless, many neural network algorithms had been proposed before GPUs became powerful enough for running very deep models. Similarly, quantum algorithms can also be proposed as knowledge reserve before real quantum computers are easily accessible. Specifically, taking advantage of both the neural networks and quantum computation and designing quantum deep neural networks (QDNNs) for acceleration on the Noisy Intermediate-Scale Quantum (NISQ) processors is also an important research problem. As one of the most widely used neural network architectures, convolutional neural network (CNN) remains to be accelerated by quantum mechanisms, with only a few attempts having been demonstrated. In this article, we propose a new hybrid quantum-classical circuit, namely, Quantum Fourier Convolutional Network (QFCN). Our model achieves exponential speedup compared with classical CNN theoretically and improves over the existing best result of quantum CNN. We demonstrate the potential of this architecture by applying it on different deep learning tasks, including traffic prediction and image classification.  © 2023 Association for Computing Machinery.,The advent of noisy intermediate-scale quantum computers has put the search for possible applications to the forefront of quantum information science. One area where hopes for an advantage through near-term quantum computers are high is quantum machine learning, where variational quantum learning models based on parametrized quantum circuits are discussed. In this work, we introduce the concept of a classical surrogate, a classical model which can be efficiently obtained from a trained quantum learning model and reproduces its input-output relations. As inference can be performed classically, the existence of a classical surrogate greatly enhances the applicability of a quantum learning strategy. However, the classical surrogate also challenges possible advantages of quantum schemes. As it is possible to directly optimize the Ansatz of the classical surrogate, they create a natural benchmark the quantum model has to outperform. We show that large classes of well-analyzed reuploading models have a classical surrogate. We conducted numerical experiments and found that these quantum models show no advantage in performance or trainability in the problems we analyze. This leaves only generalization capability as a possible point of quantum advantage and emphasizes the dire need for a better understanding of inductive biases of quantum learning models. © 2023 American Physical Society."
45,44,78,44_SAR Ship Detection and Classification,SAR Ship Detection and Classification,"With the development of synthetic aperture radar (SAR) technology, more SAR datasets with high resolution and large scale have been obtained. Research using SAR images to detect and monitor marine targets has become one of the most important marine applications. In recent years, deep learning has been widely applied to target detection. However, it was difficult to use deep learning to train an SAR ship detection model in complex scenes. To resolve this problem, an SAR ship detection method combining YOLOv4 and the receptive field block (CY-RFB) was proposed in this paper. Extensive experimental results on the SAR-Ship-Dataset and SSDD datasets demonstrated that the proposed method had achieved supreme detection performance compared to the state-of-The-Art ship detection methods in complex scenes, whether they were in offshore or inshore scenes of SAR images.  © 2023 Society of Photo-Optical Instrumentation Engineers (SPIE).,Deep learning-based synthetic aperture radar (SAR) ship detection methods are significant in signal processing and radar imaging. However, these approaches always require large-scale SAR ship images with labels to train the model. Due to the inaccessibility of SAR sensors, it is difficult to acquire enough SAR images. Annotating ship targets also demands resources and manpower. To tackle this issue, we propose a novel SAR image generation method named SARGAN for SAR ship detection task. Given the position and category, SARGAN can generate realistic SAR images with SAR ship targets, land, and background in the desired location. In the SARGAN, there are five components: target encoder, scene constructor, SAR image generator, and target and image discriminators. The target encoder is introduced to predict the latent vector for each target, while the scene constructor integrates all targets in the entire scene using convolutional LSTM. We improve the structure of the SAR image generator by adding operations to generate high-quality images. The image and target discriminators are responsible for distinguishing between real and fake samples, with the latter also predicting the category. To promote the generation of diverse and realistic SAR ship images, multiple loss functions are employed for training. Additionally, we have annotated the lands and background in the high-resolution SAR images dataset (HRSID) and combined them with labeled ships to create a new dataset for training and testing of SARGAN. Extensive experiments demonstrate that SARGAN outperforms other SAR image generation methods, and the generated SAR ship images are highly conducive for SAR ship detection task.  © 2023 IEEE.,With the continuous development of earth observation technology, space-based synthetic aperture radar (SAR) has become an important source of information for maritime surveillance, and ship classification in SAR images has also become a hot research direction in the field of maritime ship monitoring. In recent years, the remote sensing community has proposed several solutions to the problem of ship object classification in SAR images. However, it is difficult to obtain an adequate amount of labeled SAR samples for training classifiers, which limits the application of machine learning, particularly deep learning methods, in SAR image ship object classification. In contrast, as a real-time automatic tracking system for monitoring ships at sea, a ship automatic identification system (AIS) can provide a large amount of relatively easy-to-obtain labeled ship samples. Therefore, to solve the problem of SAR image ship classification and improve the classification performance of learning models with limited samples, we proposed a SAR image ship classification method based on multiple classifiers ensemble learning (MCEL) and AIS data transfer learning. The core idea of our method is to transfer the MCEL model trained on AIS data to SAR image ship classification, which mainly includes three steps: first, we use the acquired global space-based AIS data to build a dataset for ship object classification models training; then, the ensemble learning model is constructed by combining multiple base classifiers; and finally, the trained classification model is transferred to SAR images for ship type prediction. Experiments show that the proposed method achieves a classification accuracy of 85.00% for the SAR ship classification, which is better than the performance of each base classifier. This proves that AIS data transfer learning can effectively solve the problem of SAR ship classification with limited samples, and has important application value in maritime surveillance. © 2022 by the authors."
46,45,77,45_Graph-based Multiview Clustering with Anchor Graphs and Bipartite Matrix Representation,Graph-based Multiview Clustering with Anchor Graphs and Bipartite Matrix Representation,"Cluster analysis, as one of the core methods of data mining, is critical in discovering the natural structure of data to obtain useful information from massive amounts of data. However, many existing clustering algorithms have problems such as poor clustering accuracy and high sensitivity to noise points. These problems are particularly prominent when solving high-dimensional and large-data clustering problems. To overcome these problems, a new feature analysis-based elastic net algorithm with a clustering objective function (FAENC) is proposed in this paper. The new algorithm redefines a cost function based on the goal of clustering, and a new energy function of the clustering elastic net is presented based on the cost function and maximum entropy principle. The proposed model is an unsupervised optimization method. By minimizing the energy function, clustering problems can be solved through self-learning, without manual training or intervention. Additionally, a method for calculating the dispersion degree of the feature attributes is proposed, and the noise attributes can be identified. Each feature attribute is weighted automatically according to the weighting strategy, which can eliminate the influence of noise variables and improve the clustering quality and efficiency. The proposed FAENC algorithm can significantly reduce the impact of the internal structure of the dataset, identify clusters of different sizes, shapes, and densities, and obtain higher clustering quality. Compared with several classical and state-of-the-art clustering methods, FAENC substantially improves the accuracy of clustering results on a large number of synthetic and real-world datasets. © 2022 Elsevier B.V.,Cluster assignment of large and complex datasets is a crucial but challenging task in pattern recognition and computer vision. In this study, we explore the possibility of employing fuzzy clustering in a deep neural network framework. Thus, we present a novel evolutionary unsupervised learning representation model with iterative optimization. It implements the deep adaptive fuzzy clustering (DAFC) strategy that learns a convolutional neural network classifier from given only unlabeled data samples. DAFC consists of a deep feature quality-verifying model and a fuzzy clustering model, where deep feature representation learning loss function and embedded fuzzy clustering with the weighted adaptive entropy is implemented. We joint fuzzy clustering to the deep reconstruction model, in which fuzzy membership is utilized to represent a clear structure of deep cluster assignments and jointly optimize for the deep representation learning and clustering. Also, the joint model evaluates current clustering performance by inspecting whether the resampled data from estimated bottleneck space have consistent clustering properties to improve the deep clustering model progressively. Experiments on various datasets show that the proposed method obtains a substantially better performance for both reconstruction and clustering quality compared to the other state-of-the-art deep clustering methods, as demonstrated with the in-depth analysis in the extensive experiments. IEEE,Fuzzy clustering algorithms have been widely used to reveal the possible hidden structure of data. However, with the increasing of data amount, large scale data has brought genuine challenges for fuzzy clustering. Most fuzzy clustering algorithms suffer from the long time-consumption problem since a large amount of distance calculations are involved to update the solution per iteration. To address this problem, we introduce the popular anchor graph technique into fuzzy clustering and propose a scalable fuzzy clustering algorithm referred to as Scalable Fuzzy Clustering with Anchor Graph (SFCAG). The main characteristic of SFCAG is that it addresses the scalability issue plaguing fuzzy clustering from two perspectives: anchor graph construction and membership matrix learning. Specifically, we select a small number of anchors and construct a sparse anchor graph, which is beneficial to reduce the computational complexity. We then formulate a trace ratio model, which is parameter-free, to learn the membership matrix of anchors to speed up the clustering procedure. In addition, the proposed method enjoys linear time complexity with the data size. Extensive experiments performed on both synthetic and real world datasets demonstrate the superiority (both effectiveness and scalability) of the proposed method over some representative large scale clustering methods.  © 1989-2012 IEEE."
47,46,77,46_Breast Cancer Diagnostics Using Deep Learning,Breast Cancer Diagnostics Using Deep Learning,"Breast cancer is responsible for the deaths of hundreds of women every year. The manual identification of breast cancer has more difficulties, and have the possibility of error. Many imaging approaches are being researched for their potential to identify breast cancer (BC). Incorrect identification might sometimes result in unneeded therapy and diagnosis. Because of this, accurate identification of breast cancer may save a great number of patients from needing unneeded surgery and biopsies. Deep learning's (DL) performance in the processing of medical images has substantially increased as a result of recent breakthroughs in the sector. Because of their improved capacity to anticipate outcomes, deep learning algorithms are able to reliably detect BC from ultrasound pictures. Transfer learning is a kind of machine learning that reuses knowledge representations from public models that were built with the use of large-scale datasets. Transfer learning has been shown to often result in overfitting. The primary purpose of this research is to develop and provide suggestions for a deep learning model that is effective and reliable in the detection and classification of breast cancer. A tissue biopsy is obtained from the suspicious region in order to ascertain the nature of a breast tumor and whether or not it is cancerous. Tumors may take any of these forms. When the images have been reconstructed with the help of a variational autoencoder (VAE) and a denoising variational autoencoder (DVAE), a convolutional neural network (CNN) model is used. This will be the case because it opens up a new area of the field to be investigated. The histological subtypes of breast cancer are used in conjunction with the degree of differentiation to execute the task of breast cancer categorization.  © 2023 - IOS Press. All rights reserved.,Breast cancer develops in breast cells. It is the most common type of cancer in women and the second most lethal disease after lung cancer. The presence of breast masses is an important symptom for detecting breast cancer in its early stages. This study proposes a hybrid features extraction method to improve the automatic detection of breast cancer by combining three feature extraction methods: Kinetic Features, convolutional neural network deep learning features, and the newly proposed Quantum Chebyshev polynomials model. The long short-term memory model is used as a classifier in this study to detect breast cancer automatically, which could reduce human errors in the diagnosis process. The experimental results using a large publicly available dataset achieved a detection accuracy of 99.50% for hybrid features in post-contrast 2, potentially reducing human errors in the diagnosis process. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Breast cancer is a common cause of female mortality in developing countries. Early detection and treatment are crucial for successful outcomes. Breast cancer develops from breast cells and is considered a leading cause of death in women. This disease is classified into two subtypes: invasive ductal carcinoma (IDC) and ductal carcinoma in situ (DCIS). The advancements in artificial intelligence (AI) and machine learning (ML) techniques have made it possible to develop more accurate and reliable models for diagnosing and treating this disease. From the literature, it is evident that the incorporation of MRI and convolutional neural networks (CNNs) is helpful in breast cancer detection and prevention. In addition, the detection strategies have shown promise in identifying cancerous cells. The CNN Improvements for Breast Cancer Classification (CNNI-BCC) model helps doctors spot breast cancer using a trained deep learning neural network system to categorize breast cancer subtypes. However, they require significant computing power for imaging methods and preprocessing. Therefore, in this research, we proposed an efficient deep learning model that is capable of recognizing breast cancer in computerized mammograms of varying densities. Our research relied on three distinct modules for feature selection: the removal of low-variance features, univariate feature selection, and recursive feature elimination. The craniocaudally and medial-lateral views of mammograms are incorporated. We tested it with a large dataset of 3002 merged pictures gathered from 1501 individuals who had digital mammography performed between February 2007 and May 2015. In this paper, we applied six different categorization models for the diagnosis of breast cancer, including the random forest (RF), decision tree (DT), k-nearest neighbors (KNN), logistic regression (LR), support vector classifier (SVC), and linear support vector classifier (linear SVC). The simulation results prove that our proposed model is highly efficient, as it requires less computational power and is highly accurate. © 2023 by the authors."
48,47,74,47_Concrete Strength Prediction,Concrete Strength Prediction,"This paper focuses on the compressive strength of Glass fiber reinforced polymer (GFRP)-confined reinforced concrete columns. Data from 114 sets of GFRP-confined reinforced concrete columns were collected to evaluate the researchers’ and proposed model. A data-driven machine learning model was used to model the compressive strength of the GFRP-confined reinforced concrete columns and investigate the importance and sensitivity of the parameters affecting the compressive strength. The results show that the researchers’ model facilitates the study of the compressive strength of confined columns but suffers from a large coefficient of variation and too high or conservative estimation of compressive strength. The back propagation (BP) neural network has the best accuracy and robustness in predicting the compressive strength of the confined columns, with the coefficient of variation of only 14.22%, and the goodness of fit for both the training and testing sets above 0.9. The parameters that have an enormous influence on compressive strength are the concrete strength and FRP thickness, and all the parameters, except the fracture strain of FRP, are positively or inversely related to the compressive strength. © 2023 by the authors.,Assessment of concrete strength in existing structures is a common engineering problem. Several attempts in the literature showed the potential of ML methods for predicting concrete strength using concrete properties and NDT values as inputs. However, almost all such ML efforts based on NDT data trained models to predict concrete strength for a specific concrete mix design. We trained a global ML-based model that can predict concrete strength for a wide range of concrete types. This study uses data with high variability for training a metaheuristic-guided ANN model that can cover most concrete mixes used in practice. We put together a dataset that has large variations of mix design components. Training an ANN model using this dataset introduced significant test errors as expected. We optimized hyperparameters, architecture of the ANN model and performed feature selection using genetic algorithm. The proposed model reduces test errors from 9.3 MPa to 4.8 MPa. © 2023 The Authors,Concrete compressive strength (CCS) is the most crucial structural engineering designing conventional concrete and high-performance concrete (HPC) structures. Accurately predicting high-performance concrete (HPC) compressive strength is crucial when considering this parameter for cost–benefit analysis and time-saving. Recent studies have found that the deep learning (DL) model is more popular, since it has a higher prediction accuracy than conventional machine learning (ML) techniques. This study proposes four deep learning approaches: BiLSTM, CNN, GRU, and LSTM models, which is rarely seen in the literature. The model is developed using a large database, including details about cement, fly ash, coarse aggregate, sand, water, age, and blast furnace slag as input variables and compressive strength as an output variable. In this research, 80% of the dataset is used for training, while the remaining 20% is used as a testing dataset. Deep learning models result showed an R-square value of the above around 0.960 at the training phase and almost overhead 0.940 at the testing phase. But GRU model performs better than other models, where the R-square value was the significant level of the overhead of 0.990 at the training phase and also the above almost 0.961 at the testing phase, which is a high-accuracy result for both phases. Thus, this research provides a novel and effective method for predicting HPC's compressive strength, which can help develop sustainable infrastructures without requiring time-consuming and costly experiments. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG."
49,48,74,48_Adversarial attacks and defenses on deep learning models.,Adversarial attacks and defenses on deep learning models.,"Recent improvements in deep learning models and their practical applications have raised concerns about the robustness of these models against adversarial examples. Adversarial training (AT) has been shown effective in reaching a robust model against the attack that is used during training. However, it usually fails against other attacks, i.e., the model overfits to the training attack scheme. In this paper, we propose a new method for generating adversarial perturbations during training that mitigates the mentioned issue. More specifically, we minimize the perturbation ?p norm while maximizing the classification loss in the Lagrangian form to craft adversarial examples. We argue that crafting adversarial examples based on this scheme results in enhanced attack generalization in the learned model. We compare our final model robust accuracy with the closely related state-of-the-art AT methods against attacks that were not used during training. This comparison demonstrates that our average robust accuracy against unseen attacks is 5.9% higher in the CIFAR-10 dataset and 3.2% higher in the ImageNet-100 dataset than corresponding state-of-the-art methods. We also demonstrate that our attack is faster than other attack schemes that are designed for unseen attack generalization and conclude that the proposed method is feasible for large datasets. Our code is available at https://github.com/rohban-lab/Lagrangian_Unseen . © 2023, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.,The outstanding performance of deep neural networks has promoted deep learning applications in a broad set of domains. However, the potential risks caused by adversarial samples have hindered the large-scale deployment of deep learning. In these scenarios, adversarial perturbations, imperceptible to human eyes, significantly decrease the model's final performance. Many papers have been published on adversarial attacks and their countermeasures in the realm of deep learning. Most focus on evasion attacks, where the adversarial examples are found at test time, as opposed to poisoning attacks where poisoned data is inserted into the training data. Further, it is difficult to evaluate the real threat of adversarial attacks or the robustness of a deep learning model, as there are no standard evaluation methods. Hence, with this article, we review the literature to date. Additionally, we attempt to offer the first analysis framework for a systematic understanding of adversarial attacks. The framework is built from the perspective of cybersecurity to provide a lifecycle for adversarial attacks and defenses.  © 2022 Association for Computing Machinery.,Deep neural networks (DNNs) are widely used to handle many difficult tasks, such as image classification and malware detection, and achieve outstanding performance. However, recent studies on adversarial examples, which have maliciously undetectable perturbations added to their original samples that are indistinguishable by human eyes but mislead the machine learning approaches, show that machine learning models are vulnerable to security attacks. Though various adversarial retraining techniques have been developed in the past few years, none of them is scalable. In this paper, we propose a new iterative adversarial retraining approach to robustify the model and to reduce the effectiveness of adversarial inputs on DNN models. The proposed method retrains the model with both Gaussian noise augmentation and adversarial generation techniques for better generalization. Furthermore, the ensemble model is utilized during the testing phase in order to increase the robust test accuracy. The results from our extensive experiments demonstrate that the proposed approach increases the robustness of the DNN model against various adversarial attacks, specifically, fast gradient sign attack, Carlini and Wagner (C&W) attack, Projected Gradient Descent (PGD) attack, and DeepFool attack. To be precise, the robust classifier obtained by our proposed approach can maintain a performance accuracy of 99% on average on the standard test set. Moreover, we empirically evaluate the runtime of two of the most effective adversarial attacks, i.e., C&W attack and BIM attack, to find that the C&W attack can utilize GPU for faster adversarial example generation than the BIM attack can. For this reason, we further develop a parallel implementation of the proposed approach. This parallel implementation makes the proposed approach scalable for large datasets and complex models. © 2022, The Author(s)."
50,49,73,49_Material Constitutive Modeling using Neural Networks for Microstructure Design,Material Constitutive Modeling using Neural Networks for Microstructure Design,"This paper introduces a novel integrated microstructure design methodology that replaces the common existing design approaches for multifunctional composites: 1) reconstruction of microstructures, 2) analyzing and quantifying material properties, and 3) inverse design of materials. The problem of microstructure reconstruction is addressed using the diffusion-based generative model (DGM), which is a state-of-the-art generative model formulated with a Markovian diffusion process. Then, the conditional formulation of DGM is introduced for guidance to the embedded desired material properties with a transformer-based attention mechanism, which enables the inverse design of multifunctional composites. Furthermore, a convolutional neural network (CNN)-based surrogate model is utilized to facilitate the prediction of linear/nonlinear material properties for building microstructure-property linkages. Combined, the proposed artificial intelligence-based design framework enables large data processing and database construction that is often not affordable with resource-intensive finite element method (FEM)-based direct numerical simulation (DNS) or iterative reconstruction methods. It is worth noting that the proposed DGM-based methodology is not susceptible to unstable training or mode collapse, which are common issues in generative models that are often difficult to address even with extensive hyperparameter tuning. An example case is presented to demonstrate the effectiveness of the proposed approach, which is designing mechanoluminescence (ML) particulate composites. The results show that the designed ML microstructure samples with the proposed generative and surrogate models meet the multiple design requirements (e.g., volume fraction, elastic constant, and light sensitivity). This assessment demonstrates that the proposed integrated methodology provides an end-to-end solution for practical material design applications. © 2023 Elsevier Ltd,Two-scale simulations are often employed to analyze the effect of the microstructure on a component's macroscopic properties. Understanding these structure–property relations is essential in the optimal design of materials for specific applications. However, these two-scale simulations are typically computationally expensive and infeasible in multi-query contexts such as optimization and material design. To make such analyses amenable, the microscopic simulations can be replaced by inexpensive-to-evaluate surrogate models. Such surrogate models must be able to handle microstructure parameters in order to be used for material design. A previous work focused on the construction of an accurate surrogate model for microstructures under varying loading and material parameters by combining proper orthogonal decomposition and Gaussian process regression. However, that method works only for a fixed geometry, greatly limiting the design space. This work hence focuses on extending the methodology to treat geometrical parameters. To this end, a method that transforms different geometries onto a parent domain is presented, that then permits existing methodologies to be applied. We propose to solve an auxiliary problem based on linear elasticity to obtain the geometrical transformations. The method has a good reducibility and can therefore be quickly solved for many different geometries. Using these transformations, combined with the nonlinear microscopic problem, we derive a fast-to-evaluate surrogate model with the following key features: (1) the predictions of the effective quantities are independent of the auxiliary problem, (2) the predicted stress fields automatically fulfill the microscopic balance laws and are periodic, (3) the method is non-intrusive, (4) the stress field for all geometries can be recovered, and (5) the sensitivities are available and can be readily used for optimization and material design. The proposed methodology is tested on several composite microstructures, where rotations and large variations in the shape of inclusions are considered. Finally, a two-scale example is shown, where the surrogate model achieves a high accuracy and significant speed up, thus demonstrating its potential in two-scale shape optimization and material design problems. © 2022 The Author(s),Hierarchical multiscale modeling of heterogeneous materials has traditionally relied upon a deterministic estimation of constitutive properties when making microstructure-sensitive predictions of effective response at each subsequent length-scale. Such an approach is wholly unsuitable for a variety of material classes, such as ceramic matrix composites, which exhibit large variability at multiple length-scales. This work demonstrates a framework for approaching two open problems towards improved microstructure-sensitive predictions, namely, (i) probabilistically calibrating complex constitutive models at the mesoscale to sparsely observed macroscale experimental data, and (ii) propagating this stochastic constituent behavior at the mesoscale towards low-cost homogenized predictions for unseen microstructures. The proposed stochastic scale-bridging framework displays a continuity of information flow where no portion of the experimental data is neglected out of convenience, facilitating the greatest information gain from oftentimes costly experiments. In this paper, suitable protocols were developed to address the challenges described above. The protocols were subsequently demonstrated on ceramic matrix composite's uniaxial tensile stress–strain response, where constituent behavior at the mesoscale was described using continuum damage mechanics, and predictions encapsulating constitutive model parameter uncertainty were made for novel microstructures. The methodology presented in this work is broadly applicable to various material classes and constitutive models with high-dimensional parameter sets. © 2023 Elsevier Ltd"
51,50,73,50_Structural crack detection and segmentation,Structural crack detection and segmentation,"In dam engineering, the presence of cracks and crack width are important indicators for diagnosing the health of dams. The accurate measurement of cracks facilitates the safe use of dams. The manual detection of such defects is unsatisfactory in terms of cost, safety, accuracy, and the reliability of evaluation. The introduction of deep learning for crack detection can overcome these issues. However, the current deep learning algorithms possess a large volume of model parameters, high hardware requirements, and difficulty toward embedding in mobile devices such as drones. Therefore, we propose a lightweight MobileNetV2_DeepLabV3 image segmentation network. Furthermore, to prevent interference by noise, light, shadow, and other factors for long-length targets when segmenting, the atrous spatial pyramid pooling (ASPP) module parameters in the DeepLabV3+ network structure were modified, and a multifeature fusion structure was used instead of the parallel structure in ASPP, allowing the network to obtain richer crack features. We collected the images of dam cracks from different environments, established segmentation datasets, and obtained segmentation models through network training. Experiments show that the improved MobileNetV2_DeepLabV3 algorithm exhibited a higher crack segmentation accuracy than the original MobileNetV2_DeepLabV3 algorithm; the average intersection rate attained 83.23%; and the crack detail segmentation was highly accurate. Compared with other semantic segmentation networks, its training time was at least doubled, and the total parameters were reduced by more than 2 to 7 times. After extracting cracks through the semantic segmentation, we proposed to use the method of inscribed circle of crack outline to calculate the maximum width of the detected crack image and to convert it into the actual width of the crack. The maximum relative error rate was 11.22%. The results demonstrated the potential of innovative deep learning methods for dam crack detection. Copyright © 2023 Zihao Wu et al.,A large number of newly built infrastructures as well as those constructed in the early stage are faced with the problems of detection and maintenance. However, it is difficult to detect building cracks because of its small size and complex background noise. In this study, a crack segmentation network based on Encoder-Crossor-Decoder structure is innovatively proposed to solve the problems of small cracks and easy to be disturbed by background. Then, a loss function is proposed to address the problem of large differences in the ratio of cracks to background pixels in architectural crack segmentation. The experiments show that the loss function can effectively improve the training effect of the model and make the model obtain better semantic segmentation ability. Finally, according to the requirements of building crack detection, a large dataset of concrete pavement cracks is produced, which fills the gap of large dataset of semantic segmentation of cracks. The excellent effect of the model and loss function is verified with three datasets containing most of the major material and structural scenes. In addition, we compare the model with other deep learning segmentation models to validate its effectiveness. The results show that the mIoU of the model of this study reaches 84.04%, 77.56% and 87.38% in the bridge non-steel crack dataset, steel surface crack dataset and our concrete crack dataset, respectively. The accuracy reaches 99.14%, 98.62% and 99.37%. F1 reaches 0.911, 0.873 and 0.963 respectively. It outperforms other deep learning based segmentation methods. © 2023 IOP Publishing Ltd,Crack detection is a crucial task in assessing the condition of concrete structures. Herein, a novel deep learning method based on convolutional neural networks, referred to as R-FPANet, is proposed for crack detection. The R-FPANet performs automatic segmentation and quantification of crack morphology at the pixel level. In this methodology, the modularization concept based on the following three modules is adopted: ResNet-50 is chosen as the backbone to extract features from images, the Feature Pyramid Network with Dense Block is integrated to promote the fusion of both shallow and deep features as well as enhance feature reuse, and self-attention mechanisms such as Channel Attention Module and Position Attention Module are introduced to strengthen the dependency between features. Based on the crack segmentation results, a suitably established framework is developed for quantitative analysis of the major geometric parameters, including crack area, crack length, crack mean width and crack max-width at the pixel level. To verify the effectiveness of the proposed method, a large-scale concrete crack image dataset was produced and carefully labeled at the pixel level and then utilized to train the model. Finally, our experiments reveal that the proposed approach achieves an Intersection over Union of 83.07%, further indicating that the segmentation performance of the proposed method is better than the state-of-the-art models and also confirming that the crack quantification results are close to reality. Overall, the proposed method performs well, contributing to crack detection and quantification with great potential for practical use. © 2023 Institution of Structural Engineers"
52,51,72,51_Building energy-saving retrofitting and thermal comfort prediction,Building energy-saving retrofitting and thermal comfort prediction,"The rapid development of smart home systems requires a transition of thermal comfort studies. A feasible, user-friendly, consumer-oriented thermal comfort assessment method for large-scale residential applications is extremely urgent. However, most studies of thermal comfort models have focused on large commercial buildings. The application and deployment of such methods often rely on multiple prerequisites like relatively steady thermal conditions, regular energy consumption schedule, and additional sensor information. While situations are different in residential scenarios. In this paper, over 100 thousand air conditioner users were analyzed and a fundamental but overlooked question in thermal comfort research was established. In the household environment, the thermal condition is more complicated. Difference in age, gender, and usage preference leads to diversity in individual thermal comfort. Furthermore, introducing more sensors for better comfort prediction performance is not acceptable both from the cost perspective and the user perspective. To bridge the above missing gap, a novel thermal comfort assessment framework combining user interaction and metric learning was constructed. The proposed framework can be used to construct thermal comfort assessment systems by exploiting user interaction actions, which is a low-cost alternative and complementary to the traditional methods based on thermal equilibrium. The ease of construction makes the framework easy to integrate into smart home systems, addressing the difficulty of applying thermal comfort studies in residential scenarios. © 2023 The Authors,The HVAC (Heating, Ventilation, and Air Conditioning) system is an important component of a building’s energy consumption, and its primary function is to provide a comfortable thermal environment for occupants. Accurate prediction of occupant thermal comfort is essential for improving building energy utilization as well as health and work efficiency. Therefore, the development of accurate thermal comfort prediction models is of great value. Deep learning based on data-driven techniques has excellent potential for predicting thermal comfort due to the development of artificial intelligence. However, the inability to obtain large quantities of detailed thermal comfort labeling data from residents presents a substantial challenge to the modeling endeavor. This paper proposes a building-to-building transfer learning framework to make deep learning models applicable in data-limited interior building environments, thereby resolving the issue and enhancing model predictive performance. The transfer learning method (TL) is applied to a novel technology dubbed the Transformer model, which has demonstrated outstanding performance in data trend prediction. The model exploits the spatiotemporal relationship of data regarding thermal comfort. Experiments are conducted using the source dataset (Scales project dataset and ASHRAE RP-884 dataset) and the target dataset (Medium US office dataset), and the results show that the proposed TL-Transformer achieves 62.6% accuracy, 57% precision, and a 59% F1 score, and the prediction performance is better than other existing methods. The model is useful for predicting indoor thermal comfort in buildings with limited data, and its validity is verified by experimental results. © 2023 by the authors.,The use of the minimum energy to maintain the indoor thermal comfort of the large-space public building is always a challenging task due to the complex outdoor environment and indoor requirements. The lack of monitoring data and effective approaches limits the understanding of building thermal and energetic performance. This paper thus proposes a hybrid machine learning model based on factor generators and an optimization approach to address this research topic, aiming to provide the essential guide for future retrofit and design of large-space public buildings. The four machine learning (ML)-based factor generators are compared using the one-year monitoring data of building facility and indoor thermal management, where the high-performance multilayer perceptron neural networks (MLPNN) model is chosen as the data-driven method to generate the input data as the parent or intermediate populations in the GA optimization algorithm. Such a hybrid machine learning model can solve the multi-objective functions of thermal comfort and carbon emissions. The optimization results demonstrate that this model can achieve a maximum 29% improvement for thermal comfort and a reduction of 386.9 kg CO2 (11.06%) for carbon emissions in comparisons with the human-based management. Moreover, such hybrid machine learning model exhibits tolerance for moderate deficit in one objective. Therefore, the optimal thermal comfort and carbon emissions of large-space public buildings are achieved and thus contributing to the carbon neutrality in the building sector. © 2023 Elsevier Ltd"
53,52,72,52_Face Recognition Methods for Masked and Unmasked Faces,Face Recognition Methods for Masked and Unmasked Faces,"Age invariant face recognition (AIFR) is a challenging problem in the area of the face recognition. To handle large age gap for face recognition, we proposed a robust approach based on deep learning for face recognition under a large age gap. The presented approach consists of four important steps. The pre-processing is done for face detection. Age face generation is processed with the help of modified age conditional generative adversarial network (acGAN). Generated age face images are mixed with train dataset and augmentation is applied to increase the size of training data for handling biasness of the deep learning models towards dataset size. A modified residual convolutional neural network is applied for training and testing of face images. The performance has been evaluated using two-fold cross-validation on standard and challenging LAG dataset. The proposed approach achieved the 92.5% recognition accuracy, which is better than the existing face recognition approaches for a large age gap. Copyright © 2023 Inderscience Enterprises Ltd.,Overcoming image acquisition perspectives and face pose variations is a key problem in unconstrained face recognition tasks. One of the practical approaches is by reconstructing the face with extreme pose into a version that is more easily recognized by the discriminator, such as a frontal face. Often, existing methods attempt to balance the accuracy of downstream tasks with human visual perception, but ignore the differences in propensity between the two. Besides, large-scale datasets of profile-frontal paired face images are absent, which further hinders the training of models. In this work, we investigate a variety of face reconstruction approaches and propose a very simple, but very effective method to match face images across different scenes, named facial representation learning (FRL). The core idea of FRL is to introduce a representation generator in front of a pre-trained face recognition model, which can extract face representations from arbitrary faces that are more suitable for recognition model discrimination. In particular, the representation generator reconstructs the facial representation by minimising identity differences from the frontal face and adds pixel-level and adversarial constraints to cater for discriminator preferences. Extensive benchmark experiments show that the proposed method not only achieves better performance than state-of-the-art methods, but also can further squeeze the inference potential of existing face recognition models. © 2005-2012 IEEE.,Deep learning approaches achieve highly accurate face recognition by training the models with huge face image datasets. Unlike 2D face image datasets, there is a lack of large 3D face datasets available to the public. Existing public 3D face datasets were usually collected with few subjects, leading to the over-fitting problem. This paper proposes two CNN models to improve the RGB-D face recognition task. The first is a segmentation-aware depth estimation network, called DepthNet, which estimates depth maps from RGB face images by exploiting semantic segmentation for more accurate face region localization. The other is a novel segmentation-guided RGB-D face recognition model that contains an RGB recognition branch, a depth map recognition branch, and an auxiliary segmentation mask branch. In our multi-modality face recognition model, a feature disentanglement scheme is employed to factorize the feature representation into identity-related and style-related components. DepthNet is applied to augment a large 2D face image dataset to a large RGB-D face dataset, which is used for training our RGB-D face recognition model. Our experimental results show that DepthNet can produce more reliable depth maps from face images with the segmentation mask. Our multi-modality face recognition model fully exploits the depth map and outperforms state-of-the-art methods on several public 3D face datasets with challenging variations. © 2019 IEEE."
54,53,71,53_Optimal Reinforcement Learning Control for Dynamic Systems,Optimal Reinforcement Learning Control for Dynamic Systems,"Most cyber–physical systems (CPS) encounter a large volume of data which is added to the system gradually in real time and not altogether in advance. In this paper, we provide a theoretical framework that yields optimal control strategies for such CPS at the intersection of control theory and learning. In the proposed framework, we use the actual CPS, i.e., the “true” system that we seek to optimally control online, in parallel with a model of the CPS that is available. We then institute an information state for the system which does not depend on the control strategy. An important consequence of this independence is that for any given choice of a control strategy and a realization of the system's variables until time t, the information states at future times do not depend on the choice of the control strategy at time t but only on the realization of the decision at time t, and thus they are related to the concept of separation between estimation of the state and control. Namely, the future information states are separated from the choice of the current control strategy. Such control strategies are called separated control strategies. Hence, we can derive offline the optimal control strategy of the system with respect to the information state, which might not be precisely known due to model uncertainties or complexity of the system, and then use standard learning approaches to learn the information state online while data are added gradually to the system in real time. We show that after the information state becomes known, the separated control strategy of the CPS model derived offline is optimal for the actual system. We illustrate the proposed framework in a dynamic system consisting of two subsystems with a delayed sharing information structure. © 2023 Elsevier Ltd,This paper mainly focuses on the development of a learning-based controller for a class of uncertain mechanical systems modeled by the Euler-Lagrange formulation. The considered system can depict the behavior of a large class of engineering systems, such as vehicular systems, robot manipulators and satellites. All these systems are often characterized by highly nonlinear characteristics, heavy modeling uncertainties and unknown perturbations, therefore, accurate-model-based nonlinear control approaches become unavailable. Motivated by the challenge, a reinforcement learning (RL) adaptive control methodology based on the actor-critic framework is investigated to compensate the uncertain mechanical dynamics. The approximation inaccuracies caused by RL and the exogenous unknown disturbances are circumvented via a continuous robust integral of the sign of the error (RISE) control approach. Different from a classical RISE control law, a tanh(·) function is utilized instead of a sign(·) function to acquire a more smooth control signal. The developed controller requires very little prior knowledge of the dynamic model, is robust to unknown dynamics and exogenous disturbances, and can achieve asymptotic output tracking. Eventually, co-simulations through ADAMS and MATLAB/Simulink on a three degrees-of-freedom (3-DOF) manipulator and experiments on a real-time electromechanical servo system are performed to verify the performance of the proposed approach. © 2023 China Ordnance Society,This paper investigates the robust optimal control problem of a class of continuous-time, partially linear, interconnected systems. In addition to the dynamic uncertainties resulted from the interconnected dynamic system, unknown bounded disturbances are taken into account throughout the learning process, wherein the system&#x2019;s dynamics and the disturbances are assumed unknown. These challenges lead the collected online data to be imperfect. In this scenario, traditional data-driven control techniques, such as adaptive dynamic programming (ADP) and robust ADP, encounter a challenge in approximating the optimal control policy precisely due to imperfect data and computational errors. In this paper, a novel data-driven robust policy iteration method is proposed to simultaneously solve the robust optimal control problems. Without relying on the knowledge of the system&#x2019;s dynamics, the external disturbances or the complete state, the implementation of the proposed method only needs to access the input and partial state information. Based on the small-gain theorem, the notions of strong unboundedness observability and input-to-output stability, it is guaranteed that the learned robust optimal control gain is stabilizing and that the solution of the closed-loop system is uniformly ultimately bounded despite the existence of dynamic uncertainties and unknown external disturbances. The simulation results reveal the efficiency and practicality of the proposed data-driven control method. <italic>Note to Practitioners</italic>&#x2014;This work is motivated by the use of reinforcement learning to improve the quality of designing adaptive optimal controllers for engineering applications. Adaptive dynamic programming methods, in particular policy iteration (PI), are widely used in solving optimal control problems. However, due to the iterative nature of PI, the approximated optimal control policy may be inaccurate and imprecise. Especially, when using imperfect system&#x2019;s measurements instead of the modelling information. This can result in causing the learned control policy to deviate from the actual optimal policy. This becomes more challenging in the existence of dynamic uncertainties and unknown external disturbances which corrupt the measurements and result in imperfect data. This work investigates the conditions on the uncertainties such that the proposed novel data-driven PI algorithm is robust to system&#x2019;s uncertainties, unknown external disturbances and imperfect measurements. The approximated robust optimal control policy performs robustly in the existences of imperfect data and uncertainties, and at the same time is close enough to the optimal control policy. IEEE"
55,54,68,54_Deep Learning for Solving Nonlinear Partial Differential Equations,Deep Learning for Solving Nonlinear Partial Differential Equations,"In this work, an unsupervised data-driven method based on sequential singular value filtering (Seq-SVF) is proposed to simultaneously identify multiple partial differential equations from observed data considering potential noises. This method is aimed to extend the Sparse Identification of Nonlinear Dynamics (SINDy) to the identification of general nonlinear partial differential equations by transforming the paradigm based on regression to an unsupervised paradigm. To discover the complex coupled equations of vector or tensor forms without prior knowledge, the techniques of singular value decomposition (SVD) and strong rank-revealing QR factorization (sRRQR) are applied to the data matrix, which ensures that the method can automatically identify the number and the corresponding linearly independent terms as the left-hand terms of governing equations. To balance the complexity and the precision of modeling, a strategy for filtering singular values is designed to determine the sparse structure of governing equations from a large number of nonlinear basis functions. We show the success of the method to extract explicit and succinct models from many complex linear, nonlinear, and multiphysics mechanical systems, and the examples show more accuracy compared with using traditional sparse learning methods. © 2023 Elsevier B.V.,Full-field measurements of the continuous spatiotemporal response of the physical processes such as structural vibration or fluid flow generate large datasets. In many scientific fields, such continuous spatiotemporal dynamic models are represented by partial differential equations (PDEs). In the past, attempts have been made to identify the PDE models from the measured response by inferring its parameters by the use of either regression or deep learning-based techniques. But the previously presented regression-based methods fail to estimate the parameters of the higher-order PDE models in the presence of moderate noise. Likewise, the deep learning-based methods lack the much-needed property of repeatability and robustness in the identification of PDE models from the measured response. The authors introduced the method of SimultaNeous Basis Function Approximation and Parameter Estimation (SNAPE) in a recent paper which addresses such drawbacks by fitting basis functions to the measured response and simultaneously infer the parameters of the PDE model. In this paper the theory and formulation of SNAPE is presented to perform physics-guided identification of the Euler–Bernoulli beam PDE model which is widely applied in the modeling of large scale infrastructures to nanoscale structures.The domain knowledge of the physics is used as a constraint in the formulation of the optimization framework. The alternating direction method of multipliers (ADMM) algorithm is used to simultaneously optimize the loss function over the parameter space of the PDE model and coefficient space of the basis functions. The proposed method not only infers the parameters but also estimates a continuous function that approximates the solution to the PDE model. The efficacy of the method is both numerically and experimentally validated on noise corrupted full-field vibration response. The method neither requires the knowledge of the initial or boundary conditions of the beam nor comprises of model discretization error as in the case of finite element model updating. SNAPE demonstrates its applicability on various homogeneous and time-varying nonhomogeneous boundary conditions. © 2023 Elsevier Ltd,Full-field discrete measurements of the continuous spatiotemporal response of physical processes often generate large datasets. Such continuous spatiotemporal dynamic models are represented by partial differential equations (PDEs). In the past, attempts have been made to identify the PDE models from the measured response by inferring its parameters via regression or deep learning-based techniques. But the previously presented regression-based methods fail to estimate the parameters of the higher-order PDE models in the presence of moderate noise. Likewise, the deep learning-based methods lack the much-needed property of repeatability and robustness in the identification of PDE models from the measured response. The proposed method of SimultaNeous Basis Function Approximation and Parameter Estimation (SNAPE) addresses such drawbacks by simultaneously fitting basis functions to the measured response and estimating the parameters of both ordinary and partial differential equations. The domain knowledge of the general multidimensional process is used as a constraint in the formulation of the optimization framework. The alternating direction method of multipliers (ADMM) algorithm is used to simultaneously optimize the loss function over the parameter space of the PDE model and coefficient space of the basis functions. The proposed method not only infers the parameters but also estimates a continuous function that approximates the solution to the PDE model. SNAPE not only demonstrates its applicability on various complex dynamic systems that encompass wide scientific domains including Schrödinger equation, chaotic duffing oscillator, and Navier–Stokes equation but also estimates an analytical approximation to the process response. The method systematically combines the knowledge of well-established scientific theories and the concepts of data science to infer the properties of the process from the observed data. © 2022 Elsevier Ltd"
56,55,67,55_Cancer Survival Prediction,Cancer Survival Prediction,"Background: Individualized therapeutic strategies can be carried out under the guidance of expected lifespan, hence survival prediction is important. Nonetheless, reliable survival estimation in individuals with bone metastases from cancer of unknown primary (CUP) is still scarce. The objective of the study is to construct a model as well as a web-based calculator to predict three-month mortality among bone metastasis patients with CUP using machine learning-based techniques. Methods: This study enrolled 1010 patients from a large oncological database, the Surveillance, Epidemiology, and End Results (SEER) database, in the United States between 2010 and 2018. The entire patient population was classified into two cohorts at random: a training cohort (n=600, 60%) and a validation cohort (410, 40%). Patients from the validation cohort were used to validate models after they had been developed using the four machine learning approaches of random forest, gradient boosting machine, decision tree, and eXGBoosting machine on patients from the training cohort. In addition, 101 patients from two large teaching hospital were served as an external validation cohort. To evaluate each model’s ability to predict the outcome, prediction measures such as area under the receiver operating characteristic (AUROC) curves, accuracy, and Youden index were generated. The study’s risk stratification was done using the best cut-off value. The Streamlit software was used to establish a web-based calculator. Results: The three-month mortality was 72.38% (731/1010) in the entire cohort. The multivariate analysis revealed that older age (P=0.031), lung metastasis (P=0.012), and liver metastasis (P=0.008) were risk contributors for three-month mortality, while radiation (P=0.002) and chemotherapy (P<0.001) were protective factors. The random forest model showed the highest area under curve (AUC) value (0.796, 95% CI: 0.746-0.847), the second-highest precision (0.876) and accuracy (0.778), and the highest Youden index (1.486), in comparison to the other three machine learning approaches. The AUC value was 0.748 (95% CI: 0.653-0.843) and the accuracy was 0.745, according to the external validation cohort. Based on the random forest model, a web calculator was established: https://starxueshu-codeok-main-8jv2ws.streamlitapp.com/. When compared to patients in the low-risk groups, patients in the high-risk groups had a 1.99 times higher chance of dying within three months in the internal validation cohort and a 2.37 times higher chance in the external validation cohort (Both P<0.001). Conclusions: The random forest model has promising performance with favorable discrimination and calibration. This study suggests a web-based calculator based on the random forest model to estimate the three-month mortality among bone metastases from CUP, and it may be a helpful tool to direct clinical decision-making, inform patients about their prognosis, and facilitate therapeutic communication between patients and physicians. Copyright © 2022 Cui, Wang, Shi, Ye, Lei and Wang.,Background: Identifying female individuals at highest risk of developing life-threatening breast cancers could inform novel stratified early detection and prevention strategies to reduce breast cancer mortality, rather than only considering cancer incidence. We aimed to develop a prognostic model that accurately predicts the 10-year risk of breast cancer mortality in female individuals without breast cancer at baseline. Methods: In this model development and validation study, we used an open cohort study from the QResearch primary care database, which was linked to secondary care and national cancer and mortality registers in England, UK. The data extracted were from female individuals aged 20–90 years without previous breast cancer or ductal carcinoma in situ who entered the cohort between Jan 1, 2000, and Dec 31, 2020. The primary outcome was breast cancer-related death, which was assessed in the full dataset. Cox proportional hazards, competing risks regression, XGBoost, and neural network modelling approaches were used to predict the risk of breast cancer death within 10 years using routinely collected health-care data. Death due to causes other than breast cancer was the competing risk. Internal–external validation was used to evaluate prognostic model performance (using Harrell's C, calibration slope, and calibration in the large), performance heterogeneity, and transportability. Internal–external validation involved dataset partitioning by time period and geographical region. Decision curve analysis was used to assess clinical utility. Findings: We identified data for 11 626 969 female individuals, with 70 095 574 person-years of follow-up. There were 142 712 (1·2%) diagnoses of breast cancer, 24 043 (0·2%) breast cancer-related deaths, and 696 106 (6·0%) deaths from other causes. Meta-analysis pooled estimates of Harrell's C were highest for the competing risks model (0·932, 95% CI 0·917–0·946). The competing risks model was well calibrated overall (slope 1·011, 95% CI 0·978–1·044), and across different ethnic groups. Decision curve analysis suggested favourable clinical utility across all age groups. The XGBoost and neural network models had variable performance across age and ethnic groups. Interpretation: A model that predicts the combined risk of developing and then dying from breast cancer at the population level could inform stratified screening or chemoprevention strategies. Further evaluation of the competing risks model should comprise effect and health economic assessment of model-informed strategies. Funding: Cancer Research UK. © 2023 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license,Purpose: This study aims to develop a prediction model to categorize the risk of early death among breast cancer patients with bone metastases using machine learning models. Methods: This study examined 16,189 bone metastatic breast cancer patients between 2010 and 2019 from a large oncological database in the United States. The patients were divided into two groups at random in a 90:10 ratio. The majority of patients (n = 14,582, 90%) were served as the training group to train and optimize prediction models, whereas patients in the validation group (n = 1,607, 10%) were utilized to validate the prediction models. Four models were introduced in the study: the logistic regression model, gradient boosting tree model, decision tree model, and random forest model. Results: Early death accounted for 17.4% of all included patients. Multivariate analysis demonstrated that older age; a separated, divorced, or widowed marital status; nonmetropolitan counties; brain metastasis; liver metastasis; lung metastasis; and histologic type of unspecified neoplasms were significantly associated with more early death, whereas a lower grade, a positive estrogen receptor (ER) status, cancer-directed surgery, radiation, and chemotherapy were significantly the protective factors. For the purpose of developing prediction models, the 12 variables were used. Among all the four models, the gradient boosting tree had the greatest AUC [0.829, 95% confident interval (CI): 0.802–0.856], and the random forest (0.828, 95% CI: 0.801–0.855) and logistic regression (0.819, 95% CI: 0.791–0.847) models came in second and third, respectively. The discrimination slopes for the three models were 0.258, 0.223, and 0.240, respectively, and the corresponding accuracy rates were 0.801, 0.770, and 0.762, respectively. The Brier score of gradient boosting tree was the lowest (0.109), followed by the random forest (0.111) and logistic regression (0.112) models. Risk stratification showed that patients in the high-risk group (46.31%) had a greater six-fold chance of early death than those in the low-risk group (7.50%). Conclusion: The gradient boosting tree model demonstrates promising performance with favorable discrimination and calibration in the study, and this model can stratify the risk probability of early death among bone metastatic breast cancer patients. Copyright © 2022 Xiong, Cao, Shi, Long, Liu and Lei."
57,56,65,56_Machine Learning for Materials Design and Property Prediction,Machine Learning for Materials Design and Property Prediction,"Progress in the application of machine learning (ML) methods to materials design is hindered by the lack of understanding of the reliability of ML predictions, in particular, for the application of ML to small data sets often found in materials science. Using ML prediction for transparent conductor oxide formation energy and band gap, dilute solute diffusion, and perovskite formation energy, band gap, and lattice parameter as examples, we demonstrate that (1) construction of a convex hull in feature space that encloses accurately predicted systems can be used to identify regions in feature space for which ML predictions are highly reliable; (2) analysis of the systems enclosed by the convex hull can be used to extract physical understanding; and (3) materials that satisfy all well-known chemical and physical principles that make a material physically reasonable are likely to be similar and show strong relationships between the properties of interest and the standard features used in ML. We also show that similar to the composition-structure-property relationships, inclusion in the ML training data set of materials from classes with different chemical properties will not be beneficial for the accuracy of ML prediction and that reliable results likely will be obtained by ML model for narrow classes of similar materials even in the case where the ML model will show large errors on the data set consisting of several classes of materials. © 2023 American Chemical Society.,Double half-Heusler alloys are promising materials for applications as magnetocaloric materials, topological insulators, but especially thermoelectric materials. Four different elements in their composition provide a wide range of possible compositions, which, on the other hand, is difficult to study directly by applying traditional first-principles approaches to large number of compositions. In this work, based on the gradient boosting method, regression models are constructed that allow rapid prediction of the lattice thermal conductivity, as well as a number of other thermal and elastic properties, based on the composition and crystal structure of a compound. This made it possible for the first time to calculate the lattice thermal conductivity, as well as Grüneisen parameter, Debye temperature, and elastic moduli for a number of double half-Heusler compounds. We observe that the predicted thermal conductivity is in better agreement with the experimental data than the results of density functional theory calculations available in the literature. Half-Heusler compounds with thermal conductivity values lower than those previously known have been found. In addition, we have analyzed the importance of various features for predicting each of the studied properties, and the effect of the crystallographic symmetry of the compound on the prediction accuracy. © 2023 Elsevier B.V.,High-throughput screening and material informatics have shown a great power in the discovery of novel materials, including batteries, high entropy alloys, and photocatalysts. However, the lattice thermal conductivity (?) oriented high-throughput screening of advanced thermal materials is still limited to the intensive use of first principles calculations, which is inapplicable to fast, robust, and large-scale material screening due to the unbearable computational cost demanding. In this study, 15 machine learning algorithms are utilized for fast and accurate ? prediction from basic physical and chemical properties of materials. The well-trained models successfully capture the inherent correlation between these fundamental material properties and ? for different types of materials. Moreover, deep learning combined with a semi-supervised technique shows the capability of accurately predicting diverse ? values spanning 4 orders of magnitude, especially the power of extrapolative prediction on 3716 new materials. The developed models provide a powerful tool for large-scale advanced thermal functional materials screening with targeted thermal transport properties. © 2023 The Royal Society of Chemistry."
58,57,65,57_Automatic Generation of Artistic Fonts,Automatic Generation of Artistic Fonts,"This work presents Unified Contrastive Arbitrary Style Transfer (UCAST), a novel style representation learning and transfer framework, that can fit in most existing arbitrary image style transfer models, such as CNN-based, ViT-based, and flow-based methods. As the key component in image style transfer tasks, a suitable style representation is essential to achieve satisfactory results. Existing approaches based on deep neural networks typically use second-order statistics to generate the output. However, these hand-crafted features computed from a single image cannot leverage style information sufficiently, which leads to artifacts such as local distortions and style inconsistency. To address these issues, we learn style representation directly from a large number of images based on contrastive learning by considering the relationships between specific styles and the holistic style distribution. Specifically, we present an adaptive contrastive learning scheme for style transfer by introducing an input-dependent temperature. Our framework consists of three key components: a parallel contrastive learning scheme for style representation and transfer, a domain enhancement (DE) module for effective learning of style distribution, and a generative network for style transfer. Qualitative and quantitative evaluations show the results of our approach are superior to those obtained via state-of-the-art methods. The code is available at https://github.com/zyxElsa/CAST_pytorch. © 2023 Copyright held by the owner/author(s).,Creating a novel font set requires domain expertise and is a laborious and time-consuming process, particularly for languages with a large number of characters and complicated structures. Existing deep learning based methods consider font generation (FG) as an image-to-image translation problem, mostly in a supervised setting, either in the form of pair images (paired data) or font labels (character or style labels), which requires extensive effort and is expensive to collect. Additionally, these supervised counter parts lack generalization for extending to other text image-related tasks, such as word image generation and font attribute control at inference time. We found that these drawbacks are mainly due to the supervised setting adopted by these existing methods for font generation. In this paper, we tackle the FG problem in a truly unsupervised fashion, where a complete font set can be generated by training the generator such that adjacent styles are not correlated and projecting the input glyph image into its corresponding font style latent space. To accomplish this, we propose the Font Mixing Generative Adversarial Network (FM-GAN), which employs mixing regularization to supervise the generator to localize the font styles, and a projection encoder to project an arbitrary glyph image into its corresponding semantic space that is compatible with the generator. In the experiments, we demonstrated that our unsupervised model synthesizes font images that are comparable to supervised state-of-the-art FG baselines. Furthermore, FM-GAN can be directly applied to other text image related tasks, such as multi-lingual font style transfer, word image generation, and font attribute control. © 2023 Elsevier B.V.,Designing and generating novels fonts manually is a laborious and time-consuming process owing to the large number and complexity of characters in the majority of language systems. Recent advancements in generative adversarial networks (GANs) have significantly improved font generation. These GAN-based approaches either handle the font generation as a vanilla GAN problem (that is, by synthesizing characters from a uniform latent vector) or an image-to-image translation problem. While the former approach has no limitation in generating diverse font styles, the generated fonts contain artifacts and can operate only on low-resolution images, thus impairing their usability. The latter approach generates high-quality font images for previously observed fonts, but the quality degrades during the inference phase while designing novel fonts. Furthermore, additional fine-tuning steps are required to achieve photorealistic results, which is computationally expensive and time-consuming. To address the shortcomings of these approaches, we propose a font generation method that employs the vanilla GAN approach to generate an infinite number of font styles but focuses on the real-time generation of photo-realistic font images. Additionally, we strive to create high-resolution images that can be used in practical applications. To accomplish this, we propose a conditional font GAN (CFGAN) with a sophisticated network architecture that is designed to generate novel style-consistent diverse font character sets. We control the generated characters in the proposed network using a non-trainable fixed character vector, while the style variation sampled from a Gaussian distribution is fused at all blocks of the generator through adaptive instance normalization (AdaIN) operation. Thus, the generator architecture can simultaneously generate an infinite number of font styles with style consistency and diversity during inference. We conducted various quantitative and qualitative experiments to demonstrate the effectiveness of the proposed model in terms of both image quality and computational cost. © 2022 Elsevier Ltd"
59,58,65,58_Facial Expression Recognition in Depression and Emotion Analysis,Facial Expression Recognition in Depression and Emotion Analysis,"Facial expression is commonly utilized by humans to deliver their mood and emotional state to other people. Facial expression recognition (FER) becomes a hot research area in recent days, and it is a tedious process owing to the presence of high intra-class variation. The conventional methods for FEC are mainly based on handcrafted features with a classification model trained on image or video datasets. Since the facial datasets involve large variations in the images and comprise partial faces, it is needed to design automated FER models. The latest advancements in artificial intelligence (AI) and deep learning (DL) models find useful for better understanding of facial emotions related to face images. In this aspect, this paper presents an intelligent FER using optimal deep transfer learning (IFER-DTFL) model. The proposed IFER-DTFL technique aims to detect the face and identify the facial expressions automatically. The IFER-DTFL technique encompasses a three state process: face detection, feature extraction, and expression classification. In addition, a mask RCNN model is used for the detection of faces. Moreover, the Adam optimizer with Densely Connected Networks (DenseNet121) model is employed for feature extraction process. Furthermore, the weighted kernel extreme learning machine (WKELM) model is utilized to classify the facial expressions. A comprehensive set of simulations were carried out on benchmark dataset and the results are inspected under varying aspects. The experimental results pointed out the supremacy of the IFER-DTFL technique over the other recent techniques interms of several performance measures. © 2022 Elsevier B.V.,To better apply deep convolutional neural networks for expression recognition in UAV-enabled B5G/6G networks, we propose a deep network expression recognition method based on transfer learning and fine-tuning on facial expression datasets. Initially, we train our model on a large-scale facial attribute dataset and subsequently fine-tune it on a facial expression dataset. This strategy effectively lowers the high costs and dependence associated with annotating facial expression datasets, while enhancing the accuracy and training speed of our model. This method is particularly suited for UAVs equipped with on-board cameras and image processing capabilities, enabling real-time expression recognition for various applications such as crowd monitoring, search and rescue, and human-UAV interaction. Compared to traditional methods that train exclusively on facial expression datasets, our method significantly reduces the number of training iterations on facial expression datasets and significantly improves the generalization ability of the model, especially for UAV applications. We use a large-scale facial attribute dataset, which is more closely related to the facial expression recognition task, as our source dataset, forming a contrast with methods that typically use a facial recognition dataset as the transfer learning source dataset. The experimental results on the CK + dataset, integrated with UAV-enabled B5G/6G networks, show that our method achieves a facial expression recognition accuracy of 97.6%, a significant improvement over the 97.3% accuracy rate of methods that only train on facial expression datasets, with less training time as well. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The generation of a large human-labelled facial expression dataset is challenging due to ambiguity in labelling the facial expression class, and annotation cost. However, facial expression recognition (FER) systems demand discriminative feature representation, and require many training samples to establish stronger decision boundaries. Recently, FER approaches have used data augmentation techniques to increase the number of training samples for model generation. However, these augmented samples are derived from existing training data, and therefore have limitations for developing an accurate FER system. To achieve meaningful facial expression representations, we introduce an augmentation technique based on deep learning and genetic algorithms for FER. The proposed approach exploits the hypothesis that augmenting the feature-set is better than augmenting the visual data for FER. By evaluating this relationship, we discovered that the genetic evolution of discriminative features for facial expression is significant in developing a robust FER approach. In this approach, facial expression samples are generated from RGB visual data from videos considering human face frames as regions of interest. The face detected frames are further processed to extract key-frames within particular intervals. Later, these key-frames are convolved through a deep convolutional network for feature generation. A genetic algorithm’s fitness function is gauged to select optimal genetically evolved deep facial expression receptive fields to represent virtual facial expressions. The extended facial expression information is evaluated through an extreme learning machine classifier. The proposed technique has been evaluated on five diverse datasets i.e. JAFFE, CK+, FER2013, AffectNet and our application-specific Instructor Facial Expression (IFEV) dataset. Experimentation results and analysis show the promising accuracy and significance of the proposed technique on all these datasets. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
60,59,64,59_Improved YOLOv5 for Small Target Detection in UAV Images,Improved YOLOv5 for Small Target Detection in UAV Images,"Target detection for aerial images has been the focus of research. However, there are several difficulties in aerial image detection, such as complex backgrounds, high resolution, and a large number of small targets in UAV (Unmanned Aerial Vehicles) aerial images. In order to achieve high-precision detection of ground targets, a lightweight aerial image target detection algorithm LATD-YOLO (Lightweight Aerial Image Target Detector) based on YOLOv7 is proposed in this paper. Firstly, a new network structure dedicated to small target detection is proposed, and the feature extraction network and feature fusion network architectures are both lightweight. The fusion relationship between shallow and deep features is reconstructed. Then, the ELAN-OD module is proposed to reduce the model computation and strengthen the feature extraction ability of the network. In addition, the hybrid attention mechanism is added to the structure of the feature extraction network, where the effective information is extracted to enhance the learning ability. Finally, a new anchor frame position metric is introduced to improve the model’s ability to handle small targets. The experimental results show that LATD-YOLO can effectively improve the detection effect. The detection accuracy is improved by 359.6% on the ITCVD and 3.6% on the VisDrone2019 datasets respectively; the volume of the model is decreased by 67.74%; the amount of parameters is reduced by 67.91%; the amount of computation is decreased by 30.92%. Therefore, LATD-YOLO can achieve excellent detection performance in high-precision and lightweight computation scenarios. 2023 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.,Small object detection is one of the difficulties in the development of computer vision, especially in the case of complex image backgrounds, and the accuracy of small object detection still needs to be improved. In this article, we present a small object detection network based on YOLOv4, which solves some obstacles that hinder the performance of traditional methods in small object detection tasks in complex road environments, such as few effective features, the influence of image noise, and occlusion by large objects, and improves the detection of small objects in complex background situations such as drone aerial survey images. The improved network architecture reduces the computation and GPU memory consumption of the network by including the cross-stage partial network (CSPNet) structure into the spatial pyramid pool (SPP) structure in the YOLOv4 network and convolutional layers after concatenation operation. Secondly, the accuracy of the model on the small object detection task is improved by adding a more suitable small object detection head and removing one used for large object detection. Then, a new branch is added to extract feature information at a shallow location in the backbone part, and the feature information extracted from this branch is fused in the neck part to enrich the small object location information extracted by the model; when fusing feature information from different levels in the backbone, the fusion weight of useful information is increased by adding a weighting mechanism to improve detection performance at each scale. Finally, a coordinated attention (CA) module is embedded at a suitable location in the neck part, which enables the model to focus on spatial location relationships and inter-channel relationships and enhances feature representation capability. The proposed model has been tested to detect 10 different target objects in aerial images from drones and five different road traffic signal signs in images taken from vehicles in a complex road environment. The detection speed of the model meets the criteria of real-time detection, the model has better performance in terms of accuracy compared to the existing state-of-the-art detection models, and the model has only 44M parameters. On the drone aerial photography dataset, the average accuracy of YOLOv4 and YOLOv5L is 42.79% and 42.10%, respectively, while our model achieves an average accuracy (mAP) of 52.76%; on the urban road traffic light dataset, the proposed model achieves an average accuracy of 96.98%, which is also better than YOLOv4 (95.32%), YOLOv5L (94.79%) and other advanced models. The current work provides an efficient method for small object detection in complex road environments, which can be extended to scenarios involving small object detection, such as drone cruising and autonomous driving © Copyright 2023 Wei et al,The targets of UAV target detection are usually small targets, and the backgrounds are complex. In this work, aiming at the problem that small targets are easy to be missed or misdetected during the UAV detection, an improved YOLOv5s_MSES target detection algorithm based on YOLOv5s is proposed. First of all, to solve the problem of UAV's difficulty in detecting small targets, the detection layer is ameliorated into the small target detection layer STD, which makes the model more easily detect the small targets. Then, the multi-scale feature fusion module is added to improve the detection accuracy of the small targets. Furthermore, by combining multi-scale module and attention module, a new connection method is proposed to retain the large scale of feature information. Finally, in contrast with some existent methods, the experimental results of VisDrone2019 UAV target detection dataset show that our proposed YOLOv5s_MSES can achieve the better detection effect, and more effectively complete the small target detection task for UAV aerial photography images. © 2023 Elsevier Inc."
61,60,64,60_Robot Control and Navigation for Socially Compliant and Safe Interactions,Robot Control and Navigation for Socially Compliant and Safe Interactions,"Mobile robots are playing an increasingly significant role in social life and industrial production, such as searching and rescuing robots, autonomous exploration of sweeping robots, and so on. Improving the accuracy of autonomous navigation of mobile robots is a hot issue to be solved. However, traditional navigation methods are unable to realize crash-free navigation in an environment with dynamic obstacles, more and more scholars are gradually using autonomous navigation based on deep reinforcement learning (DRL) to replace overly conservative traditional methods. But on the other hand, DRL's training time is too long, and the lack of long-term memory easily leads the robot to a dead end, which makes its application in the actual scene more difficult. To shorten training time and prevent mobile robots from getting stuck and spinning around, we design a new robot autonomous navigation framework which combines the traditional global planning and the local planning based on DRL. Therefore, the entire navigation process can be transformed into first using traditional navigation algorithms to find the global path, then searching for several high-value landmarks on the global path, and then using the DRL algorithm to move the mobile robot toward the designated landmarks to complete the final navigation, which makes the robot training difficulty greatly reduced. Furthermore, in order to improve the lack of long-term memory in deep reinforcement learning, we design a feature extraction network containing memory modules to preserve the long-term dependence of input features. Through comparing our methods with traditional navigation methods and reinforcement learning based on end-to-end depth navigation methods, it shows that while the number of dynamic obstacles is large and obstacles are rapidly moving, our proposed method is, on average, 20% better than the second ranked method in navigation efficiency (navigation time and navigation paths' length), 34% better than the second ranked method in safety (collision times), 26.6% higher than the second ranked method in success rate, and shows strong robustness. Copyright © 2023 Wang, Sun, Xie, Bin and Xiao.,In recent years, data-driven learning methods have been widely studied for autonomous robot skill learning. However, these methods rely on large amounts of robot-environment interaction data for training, which largely prevents them from being applied to real-world robots. To address this problem, this article proposes a novel simulation-reality closed-loop learning framework for autonomous robot skill learning that can improve data efficiency, enhance policy stability, and achieve effective policy simulation-to-reality (sim2real) transfer. First, a hybrid control model combining the asymmetric deep deterministic policy gradients (Asym-DDPGs) model and the forward prediction control (FPC) model is proposed to learn vision-based manipulation policies in simulations, which can decompose complex tasks to improve learning efficiency. Second, a novel pixel-level domain adaptation method named Position-CycleGAN is designed to translate real images to simulated images while also preserving the task-related information. The policy trained in simulations can be directly migrated into real robots in a reverse reality-to-simulation manner using the Position-CycleGAN model. The experimental results validate the effectiveness of the proposed framework. This work provides an efficient and feasible path for achieving autonomous skill learning.  © 2016 IEEE.,This work presents a reinforcement learning-based switching control mechanism to autonomously move a ferromagnetic object (representing a milliscale robot) around obstacles within a constrained environment in the presence of disturbances. This mechanism can be used to navigate objects (e.g., capsule endoscopy, swarms of drug particles) through complex environments when active control is a necessity but where direct manipulation can be hazardous. The proposed control scheme consists of a switching control architecture implemented by two sub-controllers. The first sub-controller is designed to employ the robot&#x2019;s inverse kinematic solutions to do an environment search for the to-be-carried ferromagnetic particle while being robust to disturbances. The second sub-controller uses a customized rainbow algorithm to control a robotic arm, i.e., the UR5 robot, to carry a ferromagnetic particle to a desired position through a constrained environment. For the customized Rainbow algorithm, Quantile Huber loss from the Implicit Quantile Networks (IQN) algorithm and ResNet are employed. The proposed controller is first trained and tested in a real-time physics simulation engine (PyBullet). Afterward, the trained controller is transferred to a UR5 robot to remotely transport a ferromagnetic particle in a real-world scenario to demonstrate the applicability of the proposed approach. The experimental results on the UR5 robot show an average success rate of 98.86% over 30 episodes for randomly generated trajectories, demonstrating the viability of the proposed approach for real-life applications. In addition, two classical path finding approaches, Attractor Dynamics and the execution extended Rapidly-Exploring Random Trees (ERRT), are also investigated and compared to the RL-based method. The proposed RL-based algorithm is shown to achieve performance comparable to that of the tested classical path planners whilst being more robust to deploy in dynamical environments. <italic>Note to Practitioners</italic>&#x2014;Deep reinforcement learning methods have been widely applied in computer games and simulations. However, employing these algorithms for practical, real-world applications such as robotics becomes challenging due to the difficulty of obtaining training samples. This paper predominantly focuses on bridging the gap between simulations and the real-world implementation of a reinforcement learning algorithm for a robotic application in the context of miniaturized drug delivery robots and robotic capsule endoscopes. This paper presents the derivation and experimental validation of a reinforcement learning-based algorithm for controlling a magnetically-actuated small-scale robot within a simplified model of the large intestine in the presence of disturbances. We demonstrate the possibility of training a high-fidelity reinforcement learning algorithm fully within a simulated environment before deploying it as-is in a real-world scenario by carrying out different experiments and simulations. Implementing the presented control framework complements a large body of this work, and the results offer a feasibility study of using reinforcement learning algorithms in practice. IEEE"
62,61,62,61_Turbulence modeling in fluid flows using deep learning and machine learning algorithms for large-eddy simulations and Reynolds-averaged Navier-Stokes predictions.,Turbulence modeling in fluid flows using deep learning and machine learning algorithms for large-eddy simulations and Reynolds-averaged Navier-Stokes predictions.,"A super-resolution reconstruction model for the subgrid scale (SGS) turbulent flow field in large-eddy simulation (LES) is proposed, and it is called the meta-learning deep convolutional neural network (MLDCNN). Direct numerical simulation (DNS) data of isotropic turbulence are used as the dataset of the model. The MLDCNN is an unsupervised learning model, which only includes high-resolution DNS data without manually inputting preprocessed low-resolution data. In this model, the training process adopts the meta-learning method. First, in the a priori test, the SGS turbulent flow motions in the filtered DNS (FDNS) flow field are reconstructed, and the energy spectrum and probability density function of the velocity gradient of the DNS flow field are reconstructed with high accuracy. Then, in the a posteriori test, the super-resolution reconstruction of the LES flow field is carried out. The difficulty of LES flow field reconstruction is that it contains filtering loss and subgrid model errors relative to the DNS flow field. The super-resolution reconstruction of the LES flow field achieves good results through this unsupervised learning model. The proposed model makes a good prediction of small-scale motions in the LES flow field. This work improves the prediction accuracy of LES, which is crucial for the phenomena dominated by small-scale motions, such as relative motions of particles suspended in turbulent flows.  © 2022 Author(s).,Near-wall flow simulation remains a central challenge in aerodynamics modelling: Reynolds-averaged Navier-Stokes predictions of separated flows are often inaccurate, and large-eddy simulation (LES) can require prohibitively small near-wall mesh sizes. A deep learning (DL) closure model for LES is developed by introducing untrained neural networks into the governing equations and training in situ for incompressible flows around rectangular prisms at moderate Reynolds numbers. The DL-LES models are trained using adjoint partial differential equation (PDE) optimization methods to match, as closely as possible, direct numerical simulation (DNS) data. They are then evaluated out-of-sample - for aspect ratios, Reynolds numbers and bluff-body geometries not included in the training data - and compared with standard LES models. The DL-LES models outperform these models and are able to achieve accurate LES predictions on a relatively coarse mesh (downsampled from the DNS mesh by factors of four or eight in each Cartesian direction). We study the accuracy of the DL-LES model for predicting the drag coefficient, near-wall and far-field mean flow, and resolved Reynolds stress. A crucial challenge is that the LES quantities of interest are the steady-state flow statistics; for example, a time-averaged velocity component. Calculating the steady-state flow statistics therefore requires simulating the DL-LES equations over a large number of flow times through the domain. It is a non-trivial question whether an unsteady PDE model with a functional form defined by a deep neural network can remain stable and accurate on, especially when trained over comparatively short time intervals. Our results demonstrate that the DL-LES models are accurate and stable over long time horizons, which enables the estimation of the steady-state mean velocity, fluctuations and drag coefficient of turbulent flows around bluff bodies relevant to aerodynamics applications. © The Author(s), 2023. Published by Cambridge University Press.,A wall model for large-eddy simulation (LES) is proposed by devising the flow as a combination of building blocks. The core assumption of the model is that a finite set of simple canonical flows contains the essential physics to predict the wall shear stress in more complex scenarios. The model is constructed to predict zero/favourable/adverse mean pressure gradient wall turbulence, separation, statistically unsteady turbulence with mean flow three-dimensionality, and laminar flow. The approach is implemented using two types of artificial neural networks: A classifier, which identifies the contribution of each building block in the flow, and a predictor, which estimates the wall shear stress via a combination of the building-block flows. The training data are obtained directly from wall-modelled LES (WMLES) optimised to reproduce the correct mean quantities. This approach guarantees the consistency of the training data with the numerical discretisation and the gridding strategy of the flow solver. The output of the model is accompanied by a confidence score in the prediction that aids the detection of regions where the model underperforms. The model is validated in canonical flows (e.g. laminar/turbulent boundary layers, turbulent channels, turbulent Poiseuille-Couette flow, turbulent pipe) and two realistic aircraft configurations: The NASA Common Research Model High-lift and NASA Juncture Flow experiment. It is shown that the building-block-flow wall model outperforms (or matches) the predictions by an equilibrium wall model. It is also concluded that further improvements in WMLES should incorporate advances in subgrid-scale modelling to minimise error propagation to the wall model. © The Author(s), 2023. Published by Cambridge University Press."
63,62,59,62_Sparse Bayesian Gaussian Process Regression using Compact Support Radial Kernels,Sparse Bayesian Gaussian Process Regression using Compact Support Radial Kernels,"We consider Bayesian inference for large-scale inverse problems, where computational challenges arise from the need for repeated evaluations of an expensive forward model. This renders most Markov chain Monte Carlo approaches infeasible, since they typically require O ( 1 0 4 ) model runs, or more. Moreover, the forward model is often given as a black box or is impractical to differentiate. Therefore derivative-free algorithms are highly desirable. We propose a framework, which is built on Kalman methodology, to efficiently perform Bayesian inference in such inverse problems. The basic method is based on an approximation of the filtering distribution of a novel mean-field dynamical system, into which the inverse problem is embedded as an observation operator. Theoretical properties are established for linear inverse problems, demonstrating that the desired Bayesian posterior is given by the steady state of the law of the filtering distribution of the mean-field dynamical system, and proving exponential convergence to it. This suggests that, for nonlinear problems which are close to Gaussian, sequentially computing this law provides the basis for efficient iterative methods to approximate the Bayesian posterior. Ensemble methods are applied to obtain interacting particle system approximations of the filtering distribution of the mean-field model; and practical strategies to further reduce the computational and memory cost of the methodology are presented, including low-rank approximation and a bi-fidelity approach. The effectiveness of the framework is demonstrated in several numerical experiments, including proof-of-concept linear/nonlinear examples and two large-scale applications: learning of permeability parameters in subsurface flow; and learning subgrid-scale parameters in a global climate model. Moreover, the stochastic ensemble Kalman filter and various ensemble square-root Kalman filters are all employed and are compared numerically. The results demonstrate that the proposed method, based on exponential convergence to the filtering distribution of a mean-field dynamical system, is competitive with pre-existing Kalman-based methods for inverse problems. © 2022 IOP Publishing Ltd.,The topology identification of sparse networks is crucial for network modeling in many fields. The variational Bayesian inference has been proved to be effective for solving this issue. However, since all the observed data are used to compute the posterior distributions of the global variables at each iteration of the classical variational inference, the computation complexity is too high to be suitable for large data sets, especially for large-scale networks, where more data is needed for the inference. In this paper, we derive an efficient algorithm to maximize a lower bound function in the Bayesian inference based on stochastic optimization, where only a part of data is used at each iteration. Compared with the traditional variational Bayesian inference approach, the proposed method can significantly decrease the computation so that it is more suitable for the network identification with large data sets. Several typical sparse networks are used to test the performance of the proposed method, and the results demonstrate its merits. © 2023 Elsevier Ltd,Bayesian neural networks harness the power of Bayesian inference which provides an approach to neural learning that not only focuses on accuracy but also uncertainty quantification. Markov Chain Monte Carlo (MCMC) methods implement Bayesian inference by sampling from the posterior distribution of the model parameters. In the case of Bayesian neural networks, the model parameters refer to weights and biases. MCMC methods suffer from scalability issues in large models, such as deep neural networks with thousands to millions of parameters. In this paper, we present a Bayesian ensemble learning framework that utilizes gradient boosting by combining multiple shallow neural networks (base learners) that are trained by MCMC sampling. We present two Bayesian gradient boosting strategies that employ simple neural networks as base learners with Langevin MCMC sampling. We evaluate the performance of these methods on various classification and time-series prediction problems. We demonstrate that the proposed framework improves the prediction accuracy of canonical gradient boosting while providing uncertainty quantification via Bayesian inference. Furthermore, we demonstrate that the respective methods scale well when the size of the dataset and model increases. © 2023 The Author(s)"
64,63,59,63_Pore-scale modeling and permeability prediction in porous media,Pore-scale modeling and permeability prediction in porous media,"Multiphase flow properties of fractures are important in engineering applications such as hydraulic fracturing, evaluating the sealing capacity of caprocks, and the productivity of hydrocarbon-bearing tight rocks. Due to the computational requirements of high fidelity simulations, investigations of flow and transport through fractures typically rely on simplified assumptions applied to large fracture networks. These simplifications ignore the effect of pore-scale capillary phenomena and 3D realistic fracture morphology (for instance, tortuosity, contact points, and crevasses) that lead to macro-scale effective transport properties. The effect of these properties can be studied through lattice Boltzmann simulations, but they require high performance computing clusters and are generally limited in their domain size. In this work, we develop a technique to represent 3D fracture geometries and fluid distributions in 2D without losing any information. Using this innovative approach, we present a specialized machine learning model which only requires a few simulations for training but still accurately predicts fluid flow through 3D fractures. We demonstrate our technique using simulations of a water filled fracture being displaced by supercritical CO (Formula presented.). By generating highly efficient simulations of micro-scale multiphase flow in fractures, we hope to investigate a wide range of fracture types and generalize our method to be incorporated into larger discrete fracture network simulations. © 2022 by the authors.,Data-driven deep learning models are emerging as a promising method for characterizing pore-scale flow through complex porous media while requiring minimal computational power. However, previous models often require extensive computation to simulate flow through synthetic porous media for use as training data. We propose a convolutional neural network trained solely on periodic unit cells to predict pore-scale velocity fields of complex heterogeneous porous media from binary images without the need for further image processing. Our model is trained using a range of simple and complex unit cells that can be obtained analytically or numerically at a low computational cost. Our results show that the model accurately predicts the permeability and pore-scale flow characteristics of synthetic porous media and real reticulated foams. We significantly improve the convergence of numerical simulations by using the predictions from our model as initial guesses. Our approach addresses the limitations of previous models and improves computational efficiency, enabling the rigorous characterization of large batches of complex heterogeneous porous media for a variety of engineering applications. © 2023 Author(s).,Predicting permeability over a wide range of thermodynamic and geometric conditions is necessary to understand the transport of fluid through porous materials. Permeability is typically calculated over a representative elementary volume (REV), which is the minimum volume that captures all of the inherent features of the porous material. However, there are many instances where the representative volume is so large that permeability must be computed on a volume smaller than an REV. Furthermore, when the flow in the pores occurs in the non-continuum regime, in addition to the dependence on geometric configuration, permeability is also influenced by the temperature and pressure of the gases flowing through the pores. In these cases, permeability must be computed over a multidimensional nonlinear space of length scales, temperature, and pressure. The current state-of-the-art approach is to compute permeability for a limited set of conditions and interpolate across the multidimensional space, which leads to errors because of the nonlinear dependency of permeability on geometric and flow features of the porous material. Alternatively, permeability could be numerically computed in real time for a given set of thermodynamic and geometric parameters, but it would be computationally prohibitive. To overcome these issues, a supervised learning model capable of capturing the nonlinear relationship of permeability with geometric and fluid flow parameters has been developed. The supervised learning model maps the complex features of the multidimensional geometric and flow parameters to an analytical function that can be used to predict the permeability of the porous material under investigation. Additionally, since the function is both continuous and differentiable, it can be easily incorporated into tools that are used for design and analysis of porous systems. The performance of the newly developed model is evaluated by computing the root-mean-square train and test errors indicating no over-fit or under-fit of the model to the dataset. The permeability model is further validated against experimental data to justify the use of the model as an accurate, cost-effective alternative for determining permeability of porous materials. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
65,64,59,64_Additive manufacturing process optimization and prediction,Additive manufacturing process optimization and prediction,"This paper explores the potential of using Chat Generative Pre-trained Transformer (ChatGPT), a Large Language Model (LLM) developed by OpenAI, to address the main challenges and improve the efficiency of the Gcode generation process in Additive Manufacturing (AM), also known as 3D printing. The Gcode generation process, which controls the movements of the printer's extruder and the layer-by-layer build process, is a crucial step in the AM process and optimizing the Gcode is essential for ensuring the quality of the final product and reducing print time and waste. ChatGPT can be trained on existing Gcode data to generate optimized Gcode for specific polymeric materials, printers, and objects, as well as analyze and optimize the Gcode based on various printing parameters such as printing temperature, printing speed, bed temperature, fan speed, wipe distance, extrusion multiplier, layer thickness, and material flow. Here the capability of ChatGPT in performing complex tasks related to AM process optimization was demonstrated. In particular performance tests were conducted to evaluate ChatGPT's expertise in technical matters, focusing on the evaluation of printing parameters and bed detachment, warping, and stringing issues for Fused Filament Fabrication (FFF) methods using thermoplastic polyurethane polymer as feedstock material. This work provides effective feedback on the performance of ChatGPT and assesses its potential for use in the AM field. The use of ChatGPT for AM process optimization has the potential to revolutionize the industry by offering a user-friendly interface and utilizing machine learning algorithms to improve the efficiency and accuracy of the Gcode generation process and optimal printing parameters. Furthermore, the real-time optimization capabilities of ChatGPT can lead to significant time and material savings, making AM a more accessible and cost-effective solution for manufacturers and industry. © 2023 Kingfa Scientific and Technological Co. Ltd.,Purpose: Material extrusion is one of the most commonly used approaches within the additive manufacturing processes available. Despite its popularity and related technical advancements, process reliability and quality assurance remain only partially solved. In particular, the surface roughness caused by this process is a key concern. To solve this constraint, experimental plans have been exploited to optimize surface roughness in recent years. However, the latter empirical trial and error process is extremely time- and resource consuming. Thus, this study aims to avoid using large experimental programs to optimize surface roughness in material extrusion. Design/methodology/approach: This research provides an in-depth analysis of the effect of several printing parameters: layer height, printing temperature, printing speed and wall thickness. The proposed data-driven predictive modeling approach takes advantage of Machine Learning (ML) models to automatically predict surface roughness based on the data gathered from the literature and the experimental data generated for testing. Findings: Using ten-fold cross-validation of data gathered from the literature, the proposed ML solution attains a 0.93 correlation with a mean absolute percentage error of 13%. When testing with our own data, the correlation diminishes to 0.79 and the mean absolute percentage error reduces to 8%. Thus, the solution for predicting surface roughness in extrusion-based printing offers competitive results regarding the variability of the analyzed factors. Research limitations/implications: There are limitations in obtaining large volumes of reliable data, and the variability of the material extrusion process is relatively high. Originality/value: Although ML is not a novel methodology in additive manufacturing, the use of published data from multiple sources has barely been exploited to train predictive models. As available manufacturing data continue to increase on a daily basis, the ability to learn from these large volumes of data is critical in future manufacturing and science. Specifically, the power of ML helps model surface roughness with limited experimental tests. © 2023, Emerald Publishing Limited.,Metal 3D printing has gained a lot of attention among industries since it offers a practical solution to problems rising during the manufacturing of parts and components with complex geometry. This is an additive technology that eliminated several fabrication steps and at the same time reduces material waste during the manufacturing process. However, in all additive manufacturing technologies, the final properties of the parts are determined by the operational process parameters. In this study, several machine learning algorithms were examined to characterize the effects of the printing process parameters on relative density, hardness, yield strength, and tensile strength in manufactured parts. It was possible by using “Big Data” collected from a large number of previously published articles on the application of laser powder bed fusion (LPBF) for the 3D printing of 316L stainless steel samples. Among different process parameters, laser power, laser energy density, and scanning speed were proven to have the largest effects directly on the physical and mechanical properties of LPBF-processed parts. Six different classification models and five support vector machine regression-based models were tested to find the most accurate prediction algorithm. To validate the obtained results from the applied machine learning models, a set of 316L specimens were produced using LPBF technology using a random set of process parameters. The physical and mechanical properties of 3D printed samples were tested and compared to the ones predicted from the optimum models from machine learning analysis. The results were in great agreement, which shows the high accuracy of the developed machine learning algorithms in this study. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
66,65,59,65_Unsupervised cross-domain person re-identification using minimal target domain data.,Unsupervised cross-domain person re-identification using minimal target domain data.,"Person re-identification (ReID) aims at identifying the same person’s images across different cameras. However large domain gaps between source and target domains, as well as lack of label information in the target domain poses a huge challenge for unsupervised domain adaptive the ReID model. This paper tackles the challenge through three aspects : (1) we design a robust visual-spatiotemporal fusion model, which improves the quality of pseudo labels based on visual probability evaluation, spatiotemporal probability evaluation and visual-spatiotemporal fusion. (2) we propose a novel sampling strategy for deep mutual information estimation and maximization algorithm (DIM), which employs data augmentation and dynamic storage stack to improve the reliability of the selected samples. (3) We combine the DIM loss and other supervised losses together to construct a new multi-objective loss function and present a corresponding dynamic adjustment strategy for the weights of loss functions, which contribute to stable the convergence of the training process. As shown in experimental results, our model has achieved excellent results on two ReID datasets, Market-1501 and DukeMTMC-ReID. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The challenge of unsupervised person re-identification (ReID) lies in learning discriminative features without true labels. Most of previous works predict single-class pseudo labels through clustering. To improve the quality of generated pseudo labels, this paper formulates unsupervised person ReID as a multi-label classification task to progressively seek true labels. Our method starts by assigning each person image with a single-class label, then evolves to multi-label classification by leveraging the updated ReID model for label prediction. We first investigate the effect of precision and recall rates of pseudo labels to the ReID accuracy. This study motivates the Clustering-guided Multi-class Label Prediction (CMLP), which adopts clustering and cycle consistency to ensure high recall rate and reasonably good precision rate in pseudo labels. To boost the unsupervised learning efficiency, we further propose the Memory-based Multi-label Classification Loss (MMCL). MMCL works with memory-based non-parametric classifier and integrates local loss and global loss to seek high optimization efficiency. CMLP and MMCL work iteratively and substantially boost the ReID performance. Experiments on several large-scale person ReID datasets demonstrate the superiority of our method in unsupervised person ReID. For instance, with fully unsupervised setting we achieve rank-1 accuracy of 90.1% on Market-1501, already outperforming many transfer learning and supervised learning methods. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons could have the same attribute, and persons' appearances look different, e.g., with viewpoint changes. Recent reID methods focus on learning person features discriminative only for a particular factor of variations (e.g., human pose), which also requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to factorize person images into identity-related and -unrelated features. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose). To this end, we propose a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN). It disentangles identity-related and -unrelated features from person images through an identity-shuffling technique that exploits identification labels alone without any auxiliary supervisory signals. We restrict the distribution of identity-unrelated features, or encourage the identity-related and -unrelated features to be uncorrelated, facilitating the disentanglement process. Experimental results validate the effectiveness of IS-GAN, showing state-of-the-art performance on standard reID benchmarks, including Market-1501, CUHK03 and DukeMTMC-reID. We further demonstrate the advantages of disentangling person representations on a long-term reID task, setting a new state of the art on a Celeb-reID dataset. Our code and models are available online: https://cvlab-yonsei.github.io/projects/ISGAN/.  © 1979-2012 IEEE."
67,66,59,66_Deep Learning Methods for Seismic Inversion and Noise Denoising,Deep Learning Methods for Seismic Inversion and Noise Denoising,"Deep learning (DL) has been widely used in seismic inversion. Since the label data obtained in production are actually a small amount of 1-D well-log data, most DL-based seismic inversion is 1-D trace-to-trace method based on poststack seismic data. However, the 1-D algorithm does not take seismic data spatial correlation into consideration and the results lack of stability with good lateral continuity. Meanwhile, it is still a challenge to obtain high-precision multiparameter inversion results simultaneously for prestack inversion. To mitigate the above issues, we propose a 2-D multitask full attention U-Net (FAUNet) for prestack three-parameter inversion. The proposed network can capture geological structural features in shared information and allow each task to automatically learn task-specific attention weights in a separate branch. The full attention mechanism fuses the shared features and the internal features of each task in stages and pays attention in both channels and feature maps. Moreover, a joint hybrid loss function based on mean squared error (mse) and structural similarity (SSIM) is used to further enhance the continuity and resolution of inversion results. To obtain sufficient training samples, we generate a large number of small sample patches by sliding window both laterally and vertically with equidistant around the wells. To obtain stable inversion results, we incorporate the initial models and feed them into the network together with seismic data as input. Experiments on synthetic and field data show that our proposed method can simultaneously obtain three-parameter inversion results with high vertical resolution and transverse continuity. © 2004-2012 IEEE.,Seismic inversion allows the prediction of subsurface properties from seismic reflection data and is a key step in reservoir modeling and characterization. With the generalization of machine learning in geophysics, deep learning methods have been proposed as efficient seismic inversion methods. However, most of these methods lack a probabilistic approach to deal with the uncertainties inherent in the seismic inversion problem and/or rely on complete and representative training data, which often is partially or scarcely available. We have explored the ability of deep convolutional neural networks to extract meaningful and complex representations from spatially structured data, combined with geostatistical simulation, to efficiently invert poststack seismic data directly for high-resolution models of acoustic impedance. Our model incorporates physics constraints and sparse direct measurements while leveraging the use of imprecise but widely distributed indirect measurements as represented by the seismic data. The models generated with geostatistical simulation provide additional information with higher spatial resolution than the original seismic data and allow assessing uncertainty in the model predictions by generating multiple realizations of the subsurface properties. Our method can (1) provide an uncertainty assessment of the predictions, (2) model the complex and nonlinear relationship between data and model, (3) extend the seismic bandwidth at low and high ends of the frequency parameters spectrum, and (4) lessen the need for large, annotated training data. Our method is applied to a 1D synthetic example and a real 3D application example from a Brazilian reservoir. The predicted impedance models are compared with those obtained from a full iterative geostatistical seismic inversion. Our method allows retrieving similar models but has the advantage of generating alternative solutions in greater numbers, providing a larger exploration of the model parameter space in less time than the iterative geostatistical seismic inversion. © 2023 Society of Exploration Geophysicists. All rights reserved.,We propose to use a Few-Shot Learning (FSL) method for the pre-stack seismic inversion problem in obtaining a high resolution reservoir model from recorded seismic data. Recently, artificial neural network (ANN) demonstrates great advantages for seismic inversion because of its powerful feature extraction and parameter learning ability. Hence, ANN method could provide a high resolution inversion result that are critical for reservoir characterization. However, the ANN approach requires plenty of labeled samples for training in order to obtain a satisfactory result. For the common problem of scarce samples in the ANN seismic inversion, we create a novel pre-stack seismic inversion method that takes advantage of the FSL. The results of conventional inversion are used as the auxiliary dataset for ANN based on FSL, while the well log is regarded the scarce training dataset. According to the characteristics of seismic inversion (large amount and high dimensional), we construct an arch network (A-Net) architecture to implement this method. An example shows that this method can improve the accuracy and resolution of inversion results. © 2022 The Authors"
68,67,59,67_Structural damage detection using deep learning and vibration-based methods for bridges.,Structural damage detection using deep learning and vibration-based methods for bridges.,"Gathering properly labeled, adequately rich, and case-specific data for successfully training a purely data-driven or hybrid model for structural health monitoring (SHM) applications is a challenging task. We posit that a Transfer Learning (TL) method that utilizes available data in any relevant source domain and directly applies to the target domain through domain adaptation can provide substantial remedies to address this issue. Accordingly, we present a novel TL method that differentiates between the source's no-damage and damage cases and utilizes a domain adaptation (DA) technique. The DA module transfers the accumulated knowledge in contrasting no-damage and damage cases in the source domain to the target domain, given only the target's no-damage case. High-dimensional features allow employing signal processing domain knowledge to devise a generalizable DA approach. The Generative Adversarial Network (GAN) architecture is adopted for learning since its optimization process accommodates high-dimensional inputs in a zero-shot setting. At the same time, its training objective conforms seamlessly with the case of no-damage and damage data in SHM since its discriminator network differentiates between real (no-damage) and fake (possibly unseen damage) data. An extensive set of experimental results demonstrates the method's success in transferring knowledge on differences between no-damage and damage cases across three strongly heterogeneous independent target structures. The area under the Receiver Operating Characteristics curves (Area Under the Curve — AUC) is used to evaluate the differentiation between no-damage and damage cases in the target domain, reaching values as high as 0.95. With no-damage and damage cases discerned from each other, zero-shot structural damage detection is carried out. The mean F1 scores for all damages in the three independent datasets are 0.971, 0.986, and 0.975. The success of the proposed TL approach is expected to pave the way for further improvements in the accuracy and generalizability of data-driven SHM applications, thereby offering new possibilities for large-scale SHM applications in urban settings. © 2023 Elsevier Ltd,Structural health monitoring (SHM) approaches have offered potential solutions for monitoring and analyzing the behavior of transport infrastructures. Recently, vibration-based damage detection using deep learning approaches has received considerable attention in the SHM community. Previous studies on structural damage detection have employed supervised deep learning models, which require large amounts of labeled data. However, acquiring labeled data in practical engineering is challenging, costly, and sometimes impractical. This study uses a deep autoencoder model to extract the damage-sensitive features from acceleration data in the frequency domain without any labels. For this, a numerical example of a concrete highway bridge model subjected to a single-vehicle load under varying temperatures, low-extent damages, vehicle speeds, road surface conditions, and measurement noises was used to evaluate the effectiveness of the proposed method. This study demonstrated that the trained model is sensitive to damage in terms of reconstruction loss. In addition, the damage index (DI) between different damage-sensitive features was calculated using the Gaussian process-based z-scores. The results show that the proposed method has good damage detection capability, with the model struggling only at a higher speed (V3 = 8.33 m/s) with poor road surface roughness, where damage becomes evident after 10% or 15% damage severity. These results emphasize the potential of using the proposed method in practical engineering. © 2023 Institution of Structural Engineers,In cases with a large number of sensors and complex spatial distribution, correctly learning the spatial characteristics of the sensors is vital for structural damage identification. Graph convolutional neural networks (GCNs), unlike other methods, have the ability to learn the spatial characteristics of the sensors, which is targeted at the above problems in structural damage identification. However, under the influence of environmental interference, sensor instability, and other factors, part of the vibration signal can easily change its fundamental characteristics, and there is a possibility of misjudging structural damage. Therefore, on the basis of building a high-performance graphical convolutional deep learning model, this paper considers the integration of data fusion technology in the model decision-making layer and proposes a single-model decision-making fusion neural network (S_DFNN) model. Through experiments involving the frame model and the self-designed cable-stayed bridge model, it is concluded that this method has a better performance of damage recognition for different structures, and the accuracy is improved based on a single model and has good damage recognition performance. The method has better damage identification performance in different structures, and the accuracy rate is improved based on the single model, which has a very good damage identification effect. It proves that the structural damage diagnosis method proposed in this paper with data fusion technology combined with deep learning has a strong generalization ability and has great potential in structural damage diagnosis. © 2023 by the authors."
69,68,58,68_Battery State of Health (SOH) Estimation for Lithium-ion Batteries,Battery State of Health (SOH) Estimation for Lithium-ion Batteries,"Due to the large share of production costs, the lifespan of an electric vehicle's (EV) lithium-ion traction battery should be as long as possible. The optimisation of the EV's operating strategy with regard to battery life requires a regular evaluation of the battery's state-of-health (SOH). Yet the SOH, the remaining battery capacity, cannot be measured directly through sensors but requires the elaborate conduction of special characterisation tests. Considering the limited number of test facilities as well as the rapidly growing number of EVs, time-efficient and scalable SOH estimation methods are urgently needed and are the object of investigation in this work. The developed virtual SOH experiment originates from the incremental capacity measurement and solely relies on the commonly logged battery management system (BMS) signals to train the digital battery twins. The first examined dataset with identical load profiles for new and aged battery state serves as proof of concept. The successful SOH estimation based on the second dataset that consists of varying load profiles with increased complexity constitutes a step towards the application on real driving cycles. Assuming that the load cycles contain pauses and start from the fully charged battery state, the SOH estimation succeeds either through a steady shift of the load sequences (variant one) with an average deviation of 0.36% or by random alignment of the dataset's subsequences (variant two) with 1.04%. In contrast to continuous capacity tests, the presented framework does not impose restrictions to small currents. It is entirely independent of the prevailing and unknown ageing condition due to the application of battery models based on the novel encoder–decoder architecture and thus provides the cornerstone for a scalable and robust estimation of battery capacity on a pure data basis. © 2022 Elsevier Ltd,With the continuous concern on the safety of battery systems, accurate and rapid assessment of battery degradation is essential for practical applications. In this paper, a transferable attention network model based on deep learning is developed to evaluate battery degradation, which can simultaneously predict state of health (SOH) and remaining useful life (RUL) for lithium-ion batteries. First, degradation patterns of the cells are identified by K-means clustering based on the difference of the cells at their early cycles. Secondly, the attention mechanisms are designed to suppress noises in extracted feature maps and trace the interaction between long- and short-term degradation modes. Thirdly, the common knowledge represented by the reference cells and the unique degradation features of the target cell are fused by transfer learning, then SOH and RUL prediction are realized through multi-task learning. Finally, a large-scale battery dataset containing different cycle conditions is used to verify the accuracy and generalization of the developed method. The results show that the developed method achieves accurate SOH and RUL prediction with the average root mean square error within 0.14% and six cycles, respectively. © 2023 Elsevier Ltd,State-of-health (SOH) of lithium-ion batteries plays a vital role in the safe and reliable operation of electric vehicles. However, most of the existing SOH estimation methods still require a large number of battery aging data while the established model usually lacks generalization. Here, we build a data-driven transfer learning model to obtain more generality on the SOH estimation. First, potential health features are extracted from battery charging data and then pruned via the importance function. Second, support vector regression (SVR) is employed to establish source models with different battery data, which takes selected features as the input and capacity as the output. Third, the transfer-stacking (TS) method is utilized to combine all source models. A TS-SVR method for SOH estimation is then established only using the first 30% of target battery data after solving the optimization problem of assigning weight to each source model. Finally, the proposed algorithm is verified by three different battery datasets and shows better estimation performance than the comparative algorithms. It is proved that the proposed method uses only a small amount of target battery data while together with the source battery data can achieve an accurate SOH estimation during its life cycle.  © 1982-2012 IEEE."
70,69,58,69_Multi-Agent Reinforcement Learning for Environment Exploration,Multi-Agent Reinforcement Learning for Environment Exploration,"Many research results have emerged in the past decade regarding multi-agent reinforcement learning. These include the successful application of asynchronous advantage actor-critic, double deep Q-network and other algorithms in multi-agent environments, and the more representative multi-agent training method based on the classical centralized training distributed execution algorithm QMIX. However, in a large-scale multi-agent environment, training becomes a major challenge due to the exponential growth of the state-action space. In this article, we design a training scheme from small-scale multi-agent training to large-scale multi-agent training. We use the transfer learning method to enable the training of large-scale agents to use the knowledge accumulated by training small-scale agents. We achieve policy transfer between tasks with different numbers of agents by designing a new dynamic state representation network, which uses a self-attention mechanism to capture and represent the local observations of agents. The dynamic state representation network makes it possible to expand the policy model from a few agents (4 agents, 10 agents) task to large-scale agents (16 agents, 50 agents) task. Furthermore, we conducted experiments in the famous real-time strategy game Starcraft II and the multi-agent research platform MAgent. And also set unmanned aerial vehicles trajectory planning simulations. Experimental results show that our approach not only reduces the time consumption of a large number of agent training tasks but also improves the final training performance. © The Author(s) 2023.,Cooperative multi-agent reinforcement learning (MARL) has achieved significant results, most notably by leveraging the representation-learning abilities of deep neural networks. However, large centralized approaches quickly become infeasible as the number of agents scale, and fully decentralized approaches can miss important opportunities for information sharing and coordination. Furthermore, not all agents are equal—in some cases, individual agents may not even have the ability to send communication to other agents or explicitly model other agents. This paper considers the case where there is a single, powerful, central agent that can observe the entire observation space, and there are multiple, low-powered local agents that can only receive local observations and are not able to communicate with each other. The central agent’s job is to learn what message needs to be sent to different local agents based on the global observations, not by centrally solving the entire problem and sending action commands, but by determining what additional information an individual agent should receive so that it can make a better decision. In this work, we present our MARL algorithm hammer, describe where it would be most applicable, and implement it in the cooperative navigation and multi-agent walker domains. Empirical results show that (1) learned communication does indeed improve system performance, (2) results generalize to heterogeneous local agents, and (3) results generalize to different reward structures. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Large-scale multiagent reinforcement learning requires huge computation and space costs, and the too-long execution process makes it hard to train policies for agents. This work proposes a concept of fuzzy agent, which is a new paradigm for training homogeneous agents. Aiming at a lightweight and affordable reinforcement learning mechanism for large-scale homogeneous multiagent systems, we break the one-to-one correspondence between agent and policy, designing abstract agents as the substitute for the multiagent to interact with the environment. The Markov decision process models for these abstract agents are conducted by fuzzy logic, which also acts on the behavior mapping from abstract agent to entity. Specifically, just the abstract agents execute their policy at a time step, and the concrete behaviors are generated by simple matrix operations. The proposal has lower space and computation complexities because the number of abstract agents is far less than that of entities, and the coupling among agents is retained implicitly. Compared with other approximation and simplification methods, the proposed fuzzy agent not only greatly reduces required computing resources but also ensures the effectiveness of the learned policies. Several experiments are conducted to validate our method. The results show that the proposal outperforms the baseline methods, while it has satisfactory zero-shot and few-shot transfer abilities.  © 1993-2012 IEEE."
71,70,57,70_Genetic Variation and Imputation in Genomics,Genetic Variation and Imputation in Genomics,"Whole-genome sequencing (WGS) is the gold standard for fully characterizing genetic variation but is still prohibitively expensive for large samples. To reduce costs, many studies sequence only a subset of individuals or genomic regions, and genotype imputation is used to infer genotypes for the remaining individuals or regions without sequencing data. However, not all variants can be well imputed, and the current state-of-the-art imputation quality metric, denoted as standard Rsq, is poorly calibrated for lower-frequency variants. Here, we propose MagicalRsq, a machine-learning-based method that integrates variant-level imputation and population genetics statistics, to provide a better calibrated imputation quality metric. Leveraging WGS data from the Cystic Fibrosis Genome Project (CFGP), and whole-exome sequence data from UK BioBank (UKB), we performed comprehensive experiments to evaluate the performance of MagicalRsq compared to standard Rsq for partially sequenced studies. We found that MagicalRsq aligns better with true R2 than standard Rsq in almost every situation evaluated, for both European and African ancestry samples. For example, when applying models trained from 1,992 CFGP sequenced samples to an independent 3,103 samples with no sequencing but TOPMed imputation from array genotypes, MagicalRsq, compared to standard Rsq, achieved net gains of 1.4 million rare, 117k low-frequency, and 18k common variants, where net gains were gained numbers of correctly distinguished variants by MagicalRsq over standard Rsq. MagicalRsq can serve as an improved post-imputation quality metric and will benefit downstream analysis by better distinguishing well-imputed variants from those poorly imputed. MagicalRsq is freely available on GitHub. © 2022 The Authors,Purpose: The analysis of exome and genome sequencing data for the diagnosis of rare diseases is challenging and time-consuming. In this study, we evaluated an artificial intelligence model, based on machine learning for automating variant prioritization for diagnosing rare genetic diseases in the Baylor Genetics clinical laboratory. Methods: The automated analysis model was developed using a supervised learning approach based on thousands of manually curated variants. The model was evaluated on 2 cohorts. The model accuracy was determined using a retrospective cohort comprising 180 randomly selected exome cases (57 singletons, 123 trios); all of which were previously diagnosed and solved through manual interpretation. Diagnostic yield with the modified workflow was estimated using a prospective “production” cohort of 334 consecutive clinical cases. Results: The model accurately pinpointed all manually reported variants as candidates. The reported variants were ranked in top 10 candidate variants in 98.4% (121/123) of trio cases, in 93.0% (53/57) of single proband cases, and 96.7% (174/180) of all cases. The accuracy of the model was reduced in some cases because of incomplete variant calling (eg, copy number variants) or incomplete phenotypic description. Conclusion: The automated model for case analysis assists clinical genetic laboratories in prioritizing candidate variants effectively. The use of such technology may facilitate the interpretation of genomic data for a large number of patients in the era of precision medicine. © 2023 American College of Medical Genetics and Genomics,Background: Genomic variants of the disease are often discovered nowadays through population-based genome-wide association studies (GWAS). Identifying genomic variations potentially underlying a phenotype, such as hypertension, in an individual is important for designing personalized treatment; however, population-level models, such as GWAS, may not capture all the important, individualized factors well. In addition, GWAS typically requires a large sample size to detect the association of low-frequency genomic variants with sufficient power. Here, we report an individualized Bayesian inference (IBI) algorithm for estimating the genomic variants that influence complex traits, such as hypertension, at the level of an individual (e.g., a patient). By modeling at the level of the individual, IBI seeks to find genomic variants observed in the individual’s genome that provide a strong explanation of the phenotype observed in this individual. Results: We applied the IBI algorithm to the data from the Framingham Heart Study to explore the genomic influences of hypertension. Among the top-ranking variants identified by IBI and GWAS, there is a significant number of shared variants (intersection); the unique variants identified only by IBI tend to have relatively lower minor allele frequency than those identified by GWAS. In addition, IBI discovered more individualized and diverse variants that explain hypertension patients better than GWAS. Furthermore, IBI found several well-known low-frequency variants as well as genes related to blood pressure that GWAS missed in the same cohort. Finally, IBI identified top-ranked variants that predicted hypertension better than GWAS, according to the area under the ROC curve. Conclusions: The results support IBI as a promising approach for complementing GWAS, especially in detecting low-frequency genomic variants as well as learning personalized genomic variants of clinical traits and disease, such as the complex trait of hypertension, to help advance precision medicine. © 2023, The Author(s)."
72,71,56,71_Feature Selection Algorithms for High-Dimensional Datasets,Feature Selection Algorithms for High-Dimensional Datasets,"Feature selection is an essential task in the field of machine learning, data mining, and pattern recognition, primarily, when we deal with a large number of features. Feature selection assists in enhancing prediction accuracy, reducing computation time, and creating more comprehensible models. In feature selection, each feature has two possibilities, either it would be taken for computation or not, which implies for n number of features, there are 2 n possible feature subsets. So, identifying a relevant feature subset in a reasonable amount of time is an NP-hard problem, but by using an approximation algorithm, a near-optimal solution can be achieved. However, many of the feature selection algorithms use a sequential search strategy to select relevant features, which adds or removes features from the dataset sequentially and leads to trapped into a local optimum solution. In this paper, we propose a novel clustering-based hybrid feature selection approach using ant colony optimization that selects features randomly and measures the qualities of features by K-means clustering in terms of silhouette index and Laplacian score. The proposed feature selection approach allows random selection of features, which allows a better exploration of feature space and thus avoids the problem of being trapped in a local optimal solution, and generates a global optimal solution. The same is verified when compared with another state-of-the-art method. © 2023, King Fahd University of Petroleum & Minerals.,Companies have an increasing access to very large datasets within their domain. Analysing these datasets often requires the application of feature selection techniques in order to reduce the dimensionality of the data and prioritize features for downstream knowledge generation tasks. Effective feature selection is a key part of clustering, regression and classification. It presents a myriad of opportunities to improve the machine learning pipeline: eliminating redundant and irrelevant features, reducing model over-fitting, faster model training times and more explainable models. By contrast, and despite the widespread availability and use of categorical data in practice, feature selection for categorical and/or mixed data has received relatively little attention in comparison to numerical data. Furthermore, existing feature selection methods for mixed data are sensitive to number of objects by having nonlinear time complexities with respect to number of objects. In this work, we propose a generic multiple association measure for mixed datasets and a novel feature selection algorithm that uses multiple association across features. Our algorithm is based upon the belief that the most representative chosen set of features should be as diverse and minimally dependent on each other as possible. The proposed algorithm formulates the problem of feature selection as an optimization problem, searching for the set of features that have minimum association amongst them. We present a generic multiple association measure and two associated feature selection algorithms: Naive and Greedy Feature Selection Algorithms called NFSA and GFSA, respectively. Our proposed GFSA algorithm is evaluated on 15 benchmark datasets, and compared to four existing state of the art feature selection techniques. We demonstrate that our approach provides comparable downstream classification performance outperforming other leading techniques on several datasets. Both time complexity analysis and experimental results show that our proposed algorithm significantly reduces the processing time required for unsupervised feature selection algorithms especially for long datasets which have a huge number of objects, whilst also yielding comparable clustering and classification performance. On the other hand, we do not recommend our approach for wide datasets where the number of features is huge with respect to the number of objects e.g., image, text and genome datasets. © 2022 Elsevier Ltd,Feature Selection (FS) is an important preprocessing step in data analytics. It is used to select a subset of the original feature set such that the selected subset does not affect the classification performance significantly. Its objective is to remove irrelevant and redundant features from the original dataset. FS can be done either in offline mode or in online mode. The basic assumption in the former mode is that the entire dataset has been available for the FS algorithm; and the FS algorithm takes multiple epochs to select optimal feature subset that gives good accuracy. In contrast, the FS algorithms in online mode take input data one instance at a time and accumulate knowledge by learning each one of them. In online mode each instance of the original dataset is considered as training and testing sample as well. The offline FS algorithms require long time periods, if the data to be processed is large such as Big data. Whereas online FS algorithms will take only one epoch to learn the entire data and can produce the results swiftly which is highly desirable in the case of Big data. This paper deals with the online FS problem and provides a novel Feature Selection algorithm which uses the Sparse Gradient method to build a sparse classifier. In this proposed method, an online classifier is built and maintained throughout the learning process and feature weights, which are limited to a particular boundary limit, are reduced in a step by step decrement process. This method creates sparsity in the classifier. Effectively, the built classifier is used to select optimal feature subset from the incoming data. As this method reduces the weights in the classifier in step by step manner, only those important features which have value higher than the boundary survive from this repeated decrement process. The resultant optimal feature subset is formed using these non-zero weighted features. Most significantly, this particular method can be used with any learning algorithm. To show its applicability with different learning algorithms, various online feature selection models have been built using Learning Vector Quantization, Radial Basis Function Networks and Adaptive Resonance Theory MAP. In all these models, the proposed Sparse Gradient method is used. The encouraging results shows the effectiveness of the proposed method with different learning algorithms in medium and large sized benchmark datasets.  © 2022 World Scientific Publishing Company."
73,72,56,72_Relation Extraction and Named Entity Recognition,Relation Extraction and Named Entity Recognition,"Spatial relation extraction is the process of identifying geographic entities from text and determining their corresponding spatial relations. Traditional spatial relation extraction mainly uses rule-based pattern matching, supervised learning-based or unsupervised learning-based methods. However, these methods suffer from poor time-sensitive, high labor cost and high dependence on large-scale data. With the development of pre-trained language models greatly alleviating the shortcomings of traditional methods, supervised learning methods incorporating pre-trained language models have become the mainstream relation extraction methods. Pipeline extraction and joint extraction, as the two most dominant ideas of relation extraction, both have obtained good performance on different datasets, and whether to share the contextual information of entities and relations is the main differences between the two ideas. In this paper, we compare the performance of two ideas oriented to spatial relation extraction based on Chinese corpus data in the field of geography and verify which method based on pre-trained language models is more suitable for Chinese spatial relation extraction. We fine-tuned the hyperparameters of the two models to optimize the extraction accuracy before the comparison experiments. The results of the comparison experiments show that pipeline extraction performs better than joint extraction of spatial relation extraction for Chinese text data with sentence granularity, because different tasks have different focus on contextual information, and it is difficult to take account into the needs of both tasks by sharing contextual information. In addition, we further compare the performance of the two models with the rule-based template approach in extracting topological, directional and distance relations, summarize the shortcomings of this experiment and provide an outlook for future work. © 2022 Wuhan University. Published by Informa UK Limited, trading as Taylor & Francis Group.,Sentence-level relation classification is a technique for classifying the relation between the head entity and the tail entity in a sentence. Currently, it is popularly used to realize relation classification based on deep learning methods. However, these methods rely heavily on large-scale annotated data, and the role of head and tail entities’ information is not fully explored. In response to the above problems, we propose a prototypical networks model based on entity convolution for relation classification, which deforms the head entity and tail entity vectors encoded by BERT into multiple different convolution kernels and then performs convolution operations on the original sentence. Thus we can extract the features related to the entities in the sentence and classify the extracted features by using prototypical networks to realize relation classification. Experimental results strongly demonstrate that our model achieves state-of-the-art results compared with baseline models. © 2022 Elsevier Ltd,Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information. We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements (4.1 percent F1 on average). Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes and data can be found in https://github.com/CrisJk/PA-TRP. © 1989-2012 IEEE."
74,73,55,73_Deep Hashing for Large-Scale Image Retrieval,Deep Hashing for Large-Scale Image Retrieval,"Online hashing is a valid storage and online retrieval scheme, which is meeting the rapid increase in data in the optical-sensor network and the real-time processing needs of users in the era of big data. Existing online-hashing algorithms rely on data tags excessively to construct the hash function, and ignore the mining of the structural features of the data itself, resulting in a serious loss of the image-streaming features and the reduction in retrieval accuracy. In this paper, an online hashing model that fuses global and local dual semantics is proposed. First, to preserve the local features of the streaming data, an anchor hash model, which is based on the idea of manifold learning, is constructed. Second, a global similarity matrix, which is used to constrain hash codes is built by the balanced similarity between the newly arrived data and previous data, which makes hash codes retain global data features as much as possible. Then, under a unified framework, an online hash model that integrates global and local dual semantics is learned, and an effective discrete binary-optimization solution is proposed. A large number of experiments on three datasets, including CIFAR10, MNIST and Places205, show that our proposed algorithm improves the efficiency of image retrieval effectively, compared with several existing advanced online-hashing algorithms. © 2023 by the authors.,Deep hashing methods utilize an end-to-end framework to mutually learn feature representations and hash codes, thereby achieving a better retrieval performance. Traditional supervised hashing methods adopt handcrafted features for hashing function learning and then generate hash codes through classification and quantization. The lack of adaptability and independence of the quantization procedure leads to low retrieval accuracy of supervised hashing methods with handcrafted features in image retrieval. In this study, a non-relaxation deep hashing method for fast image retrieval is proposed. In this method, a differentiable host thresholding function is used to encourage hash-like codes to approach -1 or 1 non-linearly at the output of the convolutional neural, instead of the symbol function for quantization used in the traditional method. The output of the host thresholding function is directly used to compute the network training error, and a loss function is elaborately designed with the norm to constrain each bit of the hash-like code to be as binary as possible. Finally, a symbol function is added outside the trained network model to generate binary hash codes for image storage and retrieval in a low-dimensional binary space. Extensive experiments on two large-scale public datasets show that our method can effectively learn image features, generate accurate binary hash codes, and outperform state-of-the-art methods in terms of the mean average precision. © 2013 IEEE.,Hashing can facilitate efficient retrieval and storage for large-scale images due to the binary representation. In the real applications, the trade-off between retrieval accuracy and speed is essential for designing a hashing framework, which is reflected by variable hash code lengths. In light of this, the existing hashing methods need to train different models for different lengths of hash codes, leading to considerable training time cost and hashing flexibility reduction. Given that a sample can be represented by various hash codes with different lengths, there are some helpful relationships that can boost the performance of hashing methods. However, the existing hashing methods do not fully utilize these relationships. To address the aforementioned issues, we propose a new model, known as supervised discrete multiple-length hashing (SDMLH), to simultaneously learn hash codes with multiple lengths. In this proposed SDMLH method, three types of information are respectively derived, from the hash codes with different lengths. The original features of the samples, and the label, are applied for hash learning. Unlike the existing hashing methods, SDMLH can fully employ the assistance among hash codes with different lengths and learn them in one step. Furthermore, given a hash length meeting the demand of users, we propose a hash fusion strategy to obtain the hash code with this desirable length by fusing the multiple-length hash codes. This obtained hash code outperforms the one learned directly. In addition, SDMLH can generate the hash code of any length that is shorter than the sum length of given multiple hash codes with the fusion strategy. To the best of our knowledge, SDMLH is one of the first attempts for learning multiple-length hash codes simultaneously. We conduct extensive experiments based on three benchmark datasets, demonstrating the superiority of this proposed method.  © 2022 IEEE."
75,74,55,74_Automated Classification and Detection of Bone Fractures and Osteoarthritis using Radiographic Imaging,Automated Classification and Detection of Bone Fractures and Osteoarthritis using Radiographic Imaging,"BackgroundOccult scaphoid fractures on initial radiographs of an injury are a diagnostic challenge to physicians. Although artificial intelligence models based on the principles of deep convolutional neural networks (CNN) offer a potential method of detection, it is unknown how such models perform in the clinical setting.Questions/purposes(1) Does CNN-assisted image interpretation improve interobserver agreement for scaphoid fractures? (2) What is the sensitivity and specificity of image interpretation performed with and without CNN assistance (as stratified by type: normal scaphoid, occult fracture, and apparent fracture)? (3) Does CNN assistance improve time to diagnosis and physician confidence level?MethodsThis survey-based experiment presented 15 scaphoid radiographs (five normal, five apparent fractures, and five occult fractures) with and without CNN assistance to physicians in a variety of practice settings across the United States and Taiwan. Occult fractures were identified by follow-up CT scans or MRI. Participants met the following criteria: Postgraduate Year 3 or above resident physician in plastic surgery, orthopaedic surgery, or emergency medicine; hand fellows; and attending physicians. Among the 176 invited participants, 120 completed the survey and met the inclusion criteria. Of the participants, 31% (37 of 120) were fellowship-trained hand surgeons, 43% (52 of 120) were plastic surgeons, and 69% (83 of 120) were attending physicians. Most participants (73% [88 of 120]) worked in academic centers, whereas the remainder worked in large, urban private practice hospitals. Recruitment occurred between February 2022 and March 2022. Radiographs with CNN assistance were accompanied by predictions of fracture presence and gradient-weighted class activation mapping of the predicted fracture site. Sensitivity and specificity of the CNN-assisted physician diagnoses were calculated to assess diagnostic performance. We calculated interobserver agreement with the Gwet agreement coefficient (AC1). Physician diagnostic confidence was estimated using a self-assessment Likert scale, and the time to arrive at a diagnosis for each case was measured.ResultsInterobserver agreement among physicians for occult scaphoid radiographs was higher with CNN assistance than without (AC1 0.42 [95% CI 0.17 to 0.68] versus 0.06 [95% CI 0.00 to 0.17], respectively). No clinically relevant differences were observed in time to arrive at a diagnosis (18 ± 12 seconds versus 30 ± 27 seconds, mean difference 12 seconds [95% CI 6 to 17]; p < 0.001) or diagnostic confidence levels (7.2 ± 1.7 seconds versus 6.2 ± 1.6 seconds; mean difference 1 second [95% CI 0.5 to 1.3]; p < 0.001) for occult fractures.ConclusionCNN assistance improves physician diagnostic sensitivity and specificity as well as interobserver agreement for the diagnosis of occult scaphoid fractures. The differences observed in diagnostic speed and confidence is likely not clinically relevant. Despite these improvements in clinical diagnoses of scaphoid fractures with the CNN, it is unknown whether development and implementation of such models is cost effective.Level of EvidenceLevel II, diagnostic study.  © 2023 by the Association of Bone and Joint Surgeons.,Objective: In this proof-of-concept study, we aimed to develop deep-learning-based classifiers to identify rib fractures on frontal chest radiographs in children under 2 years of age. Methods: This retrospective study included 1311 frontal chest radiographs (radiographs with rib fractures, n = 653) from 1231 unique patients (median age: 4 m). Patients with more than one radiograph were included only in the training set. A binary classification was performed to identify the presence or absence of rib fractures using transfer learning and Resnet-50 and DenseNet-121 architectures. The area under the receiver operating characteristic curve (AUC-ROC) was reported. Gradient-weighted class activation mapping was used to highlight the region most relevant to the deep learning models’ predictions. Results: On the validation set, the ResNet-50 and DenseNet-121 models obtained an AUC-ROC of 0.89 and 0.88, respectively. On the test set, the ResNet-50 model demonstrated an AUC-ROC of 0.84 with a sensitivity of 81% and specificity of 70%. The DenseNet-50 model obtained an AUC of 0.82 with 72% sensitivity and 79% specificity. Conclusion: In this proof-of-concept study, a deep learning-based approach enabled the automatic detection of rib fractures in chest radiographs of young children with performances comparable to pediatric radiologists. Further evaluation of this approach on large multi-institutional data sets is needed to assess the generalizability of our results. Advances in knowledge: In this proof-of-concept study, a deep learning-based approach performed well in identifying chest radiographs with rib fractures. These findings provide further impetus to develop deep learning algorithms for identifying rib fractures in children, especially those with suspected physical abuse or non-accidental trauma. © 2023 The Authors. Published by the British Institute of Radiology.,Background: Osteoarthritis (OA) is a global healthcare problem. The increasing population of OA patients demands a greater bandwidth of imaging and diagnostics. It is important to provide automatic and objective diagnostic techniques to address this challenge. This study demonstrates the utility of unsupervised domain adaptation (UDA) for automated OA phenotype classification. Methods: We collected 318 and 960 three-dimensional double-echo steady-state magnetic resonance images from the Osteoarthritis Initiative (OAI) dataset as the source dataset for phenotype cartilage/ meniscus and subchondral bone, respectively. Fifty three-dimensional turbo spin echo (TSE)/fast spin echo (FSE) MR images from our institute were collected as the target datasets. For each patient, the degree of knee OA was initially graded according to the MRI Knee Osteoarthritis Knee Score before being converted to binary OA phenotype labels. The proposed four-step UDA pipeline included (I) pre-processing, which involved automatic segmentation and region-of-interest cropping; (II) source classifier training, which involved pre-training a convolutional neural network (CNN) encoder for phenotype classification using the source dataset; (III) target encoder adaptation, which involved unsupervised adjustment of the source encoder to the target encoder using both the source and target datasets; and (IV) target classifier validation, which involved statistical analysis of the classification performance evaluated by the area under the receiver operating characteristic curve (AUROC), sensitivity, specificity and accuracy. We compared our model on the target data with the source pre-trained model and the model trained with the target data from scratch. Results: For phenotype cartilage/meniscus, our model has the best performance out of the three models, giving 0.90 [95% confidence interval (CI): 0.79–1.02] of the AUROC score, while the other two model show 0.52 (95% CI: 0.13–0.90) and 0.76 (95% CI: 0.53–0.98). For phenotype subchondral bone, our model gave 0.75 (95% CI: 0.56–0.94) at AUROC, which has a close performance of the source pre-trained model (0.76, 95% CI: 0.55–0.98), and better than the model trained from scratch on the target dataset only (0.53, 95% CI: 0.33–0.73). Conclusions: By utilising a large, high-quality source dataset for training, the proposed UDA approach enhances the performance of automated OA phenotype classification for small target datasets. As a result, our technique enables improved downstream analysis of locally collected datasets with a small sample size. © Quantitative Imaging in Medicine and Surgery. All rights reserved."
76,75,55,75_Sentiment analysis of COVID-19 related tweets and social media posts.,Sentiment analysis of COVID-19 related tweets and social media posts.,"When public health emergencies occur, a large amount of low-credibility information is widely disseminated by social bots, and public sentiment is easily manipulated by social bots, which may pose a potential threat to the public opinion ecology of social media. Therefore, exploring how social bots affect the mechanism of information diffusion in social networks is a key strategy for network governance. This study combines machine learning methods and causal regression methods to explore how social bots influence information diffusion in social networks with theoretical support. Specifically, combining stakeholder perspective and emotional contagion theory, we proposed several questions and hypotheses to investigate the influence of social bots. Then, the study obtained 144,314 pieces of public opinion data related to COVID-19 in J city from March 1, 2022, to April 18, 2022, on Weibo, and selected 185,782 pieces of data related to the outbreak of COVID-19 in X city from December 9, 2021, to January 10, 2022, as supplement and verification. A comparative analysis of different data sets revealed the following findings. Firstly, through the STM topic model, it is found that some topics posted by social bots are significantly different from those posted by humans, and social bots play an important role in certain topics. Secondly, based on regression analysis, the study found that social bots tend to transmit information with negative sentiments more than positive sentiments. Thirdly, the study verifies the specific distribution of social bots in sentimental transmission through network analysis and finds that social bots are weaker than human users in the ability to spread negative sentiments. Finally, the Granger causality test is used to confirm that the sentiments of humans and bots can predict each other in time series. The results provide practical suggestions for emergency management under sudden public opinion and provide a useful reference for the identification and analysis of social bots, which is conducive to the maintenance of network security and the stability of social order. © 2022,Introduction: The development of COVID-19 vaccines has been a great relief in many countries that have been affected by the pandemic. As a result, many governments have made significant efforts to purchase and administer vaccines to their populations. However, accommodating such vaccines is typically confronted with people’s reluctance and fear. Like any other important event, COVID-19 vaccines have attracted people’s discussions on social media and impacted their opinions about vaccination. Objective: The goal of this study is twofold: First, it conducts a sentiment analysis around COVID-19 vaccines by automatically analyzing Arabic users’ tweets. This analysis has been spread over time to better capture the changes in vaccine perceptions. This will provide us with some insights into the most popular and accepted vaccine(s) in the Arab countries, as well as the reasons behind people’s reluctance to take the vaccine. Second, it develops models to detect any vaccine-related tweets, to help with gathering all information related to people’s perception of the virus, and potentially detecting vaccine-related tweets that are not necessarily tagged with the virus’s main hashtags. Methods: Arabic Tweets were collected by the authors, starting from January 1st, 2021, until April 20th, 2021. We deployed various Natural Language Processing (NLP) to distill our selected tweets. The curated dataset included in the analysis consisted of 1,098,376 unique tweets. To achieve the first goal, we designed state-of-the-art sentiment analysis techniques to extract knowledge related to the degree of acceptance of all existing vaccines and what are the main obstacles preventing the wide audience from accepting them. To achieve the second goal, we tackle the detection of vaccine-related tweets as a binary classification problem, where various Machine Learning (ML) models were designed to identify such tweets regardless of whether they use the vaccine hashtags or not. Results: Generally, we found that the highest positive sentiments were registered for Pfizer-BioNTech, followed by Sinopharm-BIBP and Oxford-AstraZeneca. In addition, we found that 38% of the overall tweets showed negative sentiment, and only 12% had a positive sentiment. It is important to note that the majority of the sentiments vary between neutral and negative, showing the lack of conviction of the importance of vaccination among the large majority of tweeters. This paper extracts the top concerns raised by the tweets and advocates for taking them into account when advertising for the vaccination. Regarding the identification of vaccine-related tweets, the Logistic Regression model scored the highest accuracy of 0.82. Our findings are concluded with implications for public health authorities and the scholarly community to take into account to improve the vaccine’s acceptance. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.,Since the spread of the coronavirus flu in 2019 (hereafter referred to as COVID-19), millions of people worldwide have been affected by the pandemic, which has significantly impacted our habits in various ways. In order to eradicate the disease, a great help came from unprecedentedly fast vaccines development along with strict preventive measures adoption like lockdown. Thus, world wide provisioning of vaccines was crucial in order to achieve the maximum immunization of population. However, the fast development of vaccines, driven by the urge of limiting the pandemic caused skeptical reactions by a vast amount of population. More specifically, the people’s hesitancy in getting vaccinated was an additional obstacle in fighting COVID-19. To ameliorate this scenario, it is important to understand people’s sentiments about vaccines in order to take proper actions to better inform the population. As a matter of fact, people continuously update their feelings and sentiments on social media, thus a proper analysis of those opinions is an important challenge for providing proper information to avoid misinformation. More in detail, sentiment analysis (Wankhade et al. in Artif Intell Rev 55(7):5731–5780, 2022. https://doi.org/10.1007/s10462-022-10144-1) is a powerful technique in natural language processing that enables the identification and classification of people feelings (mainly) in text data. It involves the use of machine learning algorithms and other computational techniques to analyze large volumes of text and determine whether they express positive, negative or neutral sentiment. Sentiment analysis is widely used in industries such as marketing, customer service, and healthcare, among others, to gain actionable insights from customer feedback, social media posts, and other forms of unstructured textual data. In this paper, Sentiment Analysis will be used to elaborate on people reaction to COVID-19 vaccines in order to provide useful insights to improve the correct understanding of their correct usage and possible advantages. In this paper, a framework that leverages artificial intelligence (AI) methods is proposed for classifying tweets based on their polarity values. We analyzed Twitter data related to COVID-19 vaccines after the most appropriate pre-processing on them. More specifically, we identified the word-cloud of negative, positive, and neutral words using an artificial intelligence tool to determine the sentiment of tweets. After this pre-processing step, we performed classification using the BERT + NBSVM model to classify people’s sentiments about vaccines. The reason for choosing to combine bidirectional encoder representations from transformers (BERT) and Naive Bayes and support vector machine (NBSVM) can be understood by considering the limitation of BERT-based approaches, which only leverage encoder layers, resulting in lower performance on short texts like the ones used in our analysis. Such a limitation can be ameliorated by using Naive Bayes and Support Vector Machine approaches that are able to achieve higher performance in short text sentiment analysis. Thus, we took advantage of both BERT features and NBSVM features to define a flexible framework for our sentiment analysis goal related to vaccine sentiment identification. Moreover, we enrich our results with spatial analysis of the data by using geo-coding, visualization, and spatial correlation analysis to suggest the most suitable vaccination centers to users based on the sentiment analysis outcomes. In principle, we do not need to implement a distributed architecture to run our experiments as the available public data are not massive. However, we discuss a high-performance architecture that will be used if the collected data scales up dramatically. We compared our approach with the state-of-art methods by comparing most widely used metrics like Accuracy, Precision, Recall and F-measure. The proposed BERT + NBSVM outperformed alternative models by achieving 73% accuracy, 71% precision, 88% recall and 73% F-measure for classification of positive sentiments while 73% accuracy, 71% precision, 74% recall and 73% F-measure for classification of negative sentiments respectively. These promising results will be properly discussed in next sections. The use of artificial intelligence methods and social media analysis can lead to a better understanding of people’s reactions and opinions about any trending topic. However, in the case of health-related topics like COVID-19 vaccines, proper sentiment identification could be crucial for implementing public health policies. More in detail, the availability of useful findings on user opinions about vaccines can help policymakers design proper strategies and implement ad-hoc vaccination protocols according to people’s feelings, in order to provide better public service. To this end, we leveraged geospatial information to support effective recommendations for vaccination centers. © 2023, The Author(s)."
77,76,54,76_Federated Learning for Privacy-Preserving Medical Imaging,Federated Learning for Privacy-Preserving Medical Imaging,"Federated learning (FL), a relatively new area of research in medical image analysis, enables collaborative learning of a federated deep learning model without sharing the data of participating clients. In this paper, we propose FedDropoutAvg, a new federated learning approach for detection of tumor in images of colon tissue slides. The proposed method leverages the power of dropout, a commonly employed scheme to avoid overfitting in neural networks, in both client selection and federated averaging processes. We examine FedDropoutAvg against other FL benchmark algorithms for two different image classification tasks using a publicly available multi-site histopathology image dataset. We train and test the proposed model on a large dataset consisting of 1.2 million image tiles from 21 different sites. For testing the generalization of all models, we select held-out test sets from sites that were not used during training. We show that the proposed approach outperforms other FL methods and reduces the performance gap (to less than 3% in terms of AUC on independent test sites) between FL and a central deep learning model that requires all data to be shared for centralized training, demonstrating the potential of the proposed FedDropoutAvg model to be more generalizable than other state-of-the-art federated models. To the best of our knowledge, ours is the first study to effectively utilize the dropout strategy in a federated setting for tumor detection in histology images. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Brain tumor segmentation in medical imaging is a critical task for diagnosis and treatment while preserving patient data privacy and security. Traditional centralized approaches often encounter obstacles in data sharing due to privacy regulations and security concerns, hindering the development of advanced AI-based medical imaging applications. To overcome these challenges, this study proposes the utilization of federated learning. The proposed framework enables collaborative learning by training the segmentation model on distributed data from multiple medical institutions without sharing raw data. Leveraging the U-Net-based model architecture, renowned for its exceptional performance in semantic segmentation tasks, this study emphasizes the scalability of the proposed approach for large-scale deployment in medical imaging applications. The experimental results showcase the remarkable effectiveness of federated learning, significantly improving specificity to 0.96 and the dice coefficient to 0.89 with the increase in clients from 50 to 100. Furthermore, the proposed approach outperforms existing convolutional neural network (CNN)- and recurrent neural network (RNN)-based methods, achieving higher accuracy, enhanced performance, and increased efficiency. The findings of this research contribute to advancing the field of medical image segmentation while upholding data privacy and security. © 2023 by the authors.,With the advent of the Internet of Things (IoT), Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) algorithms, the landscape of data-driven medical applications has emerged as a promising avenue for designing robust and scalable diagnostic and prognostic models from medical data. This has gained a lot of attention from both academia and industry, leading to significant improvements in healthcare quality. However, the adoption of AI-driven medical applications still faces tough challenges, including meeting security, privacy, and quality of service (QoS) standards. Recent developments in Federated Learning (FL) have made it possible to train complex machine-learned models in a distributed manner and has become an active research domain, particularly processing the medical data at the edge of the network in a decentralized way to preserve privacy and address security concerns. To this end, in this paper, we explore the present and future of FL technology in medical applications where data sharing is a significant challenge. We delve into the current research trends and their outcomes, unravelling the complexities of designing reliable and scalable FL models. Our paper outlines the fundamental statistical issues in FL, tackles device-related problems, addresses security challenges, and navigates the complexity of privacy concerns, all while highlighting its transformative potential in the medical field. Our study primarily focuses on medical applications of FL, particularly in the context of global cancer diagnosis. We highlight the potential of FL to enable computer-aided diagnosis tools that address this challenge with greater effectiveness than traditional data-driven methods. Recent literature has shown that FL models are robust and generalize well to new data, which is essential for medical applications. We hope that this comprehensive review will serve as a checkpoint for the field, summarizing the current state-of-the-art and identifying open problems and future research directions. Authors"
78,77,53,77_Unsupervised Domain Adaptation for Cross-Domain Transfer,Unsupervised Domain Adaptation for Cross-Domain Transfer,"The automatic segmentation of organs or tissues is crucial for early diagnosis and treatment. Existing deep learning methods either need massive annotation data or use Unsupervised Domain Adaptation (UDA) approaches with labeled source domain data to train a model for unlabeled target domain data. The methods mentioned above require accessing the data of the source domain. However, in medical imaging, source domain data from a hospital is usually restricted due to patient data privacy regulations. Therefore, it is crucial to perform privacy-preserving domain adaptation, which simultaneously improves the model’s performance on target domain data and preserves the privacy of source domain data. Aiming to achieve Source Free Unsupervised Domain Adaptation (SFUDA), we propose a two-stage framework Roughing and Finishing to extract the relevant features and knowledge from the pre-trained model and protect the patient’s privacy. Specifically, in the Roughing stage, batch statistics are updated using unlabelled target domain data to alleviate feature distribution shifts. Then, a non-parametric weighted entropy-minimization is used to reduce the domain gap further. In the Finishing stage, we aim to enhance the model’s generalization ability by increasing the noise robustness of the feature representation, leading to a more generalizable target feature representation. Besides, we propose pseudo-label training based on a confidence-weighted feature distance to utilize the knowledge from confident samples. The proposed method is evaluated on two medical domain transfer challenges, (1) MRI to CT liver segmentation and (2) cross-domain fundus image segmentation. Extensive experiments and ablation studies demonstrate the superiority of the proposed methods over state-of-the-art SFUDA methods by a large margin. The proposed method outperforms previous UDA approaches that access the source domain data. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Although domain adaptation approaches have been proposed to tackle cross-regional, multitemporal, and multisensor remote sensing applications since they do not require any human interpretation in the target domain, most current works assume identical label space across the source and the target domains. However, in real-world applications, we often transfer knowledge from a large-scale dataset with rich annotations to a small-scale target dataset with scarcity of labels. In most cases, the label space of the source domain is usually large enough to subsume that of the target domain, which is termed partial domain adaptation. In this article, we propose a new partial domain adaptation algorithm for remote sensing scene classification and our proposed method contains three major parts. First, we employ a progressive auxiliary domain module to alleviate the negative transfer effect caused by outlier classes. Second, we adopt an improved domain adversarial neural network (DANN) with multiweights to better encourage domain confusion. Last but not least, we design an attentive complement entropy regularization to improve the prediction confidence for samples and avoid untransferable samples (such as the samples belonging to outlier classes in the source domain) being mistakenly classified. We collect three common remote sensing datasets to evaluate our proposed method. Our method achieves an average accuracy of 79.36%, which considerably outperforms other state-of-the-art partial domain adaptation methods with an average accuracy improvement of 1.90%-12.45% and attaining a 13.67% gain compared to the straightforward deep learning model (ResNet-50). The experiment results indicate that our approach shows promising prospects for solving more general and practical domain adaptation problems where the label space of the source domain subsumes that of the target domain.  © 1980-2012 IEEE.,By leveraging the labeled data samples of the source domain to learn the unlabeled data samples of the target domain, unsupervised domain adaptation (DA) has achieved promising performance. However, it is still a vital problem for unsupervised domain adaptation to deal with cross-domain distribution mismatch. Therefore, we present a new model framework for cross-domain image classification in the paper, which is termed manifold transfer subspace learning based on double relaxed discriminative regression (MTSL-DRDR). First, the global geometry information of the samples from the source and target domain can be preserved by utilizing the low-rank constraint. Second, the two transformation projections are employed to project both domains to a unified subspace, in which each data sample of the target domain can be represented by some samples from the source domain with the sparse and low-rank coefficient matrix. Third, the local structure information of the data points with the same semantics from the different domains is preserved by means of the adaptive weight graph based on the low-rank coefficient matrix. Last, for fully use the discriminative information of data from the source domain, the discriminant information of the source domain based on intra-class and inter-class graphs is encoded to the target domain. Our MTSL-DRDR algorithm is evaluated on challenging benchmark datasets, and a large number of experiment results show the superiority of the proposed method. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
79,78,52,78_Building Extraction Methods for Urban Remote Sensing Images,Building Extraction Methods for Urban Remote Sensing Images,"Building extraction from very high-resolution remote sensing images is a fundamental task and is widely used in applications, such as change detection, disaster assessment, and real-time update of geographic information databases. However, due to the complexity of the geographical environment and the diversity of target features, accurate automatic building extraction remains very challenging. With the fast development of deep learning techniques, convolutional neural networks (CNN) have been widely used in remote sensing research and have achieved considerable results. But for large urban area-based building detection tasks, the CNN-based method usually gets into local optima and generates many false positive detections around building boundaries. To avoid the local optima and be aware of nonlocal information, this article proposes a hybrid feature extraction model based on the combination of the CNN and Transformer to realize the automatic building detection from very high-resolution remote sensing images. Meanwhile, a multiconstraint weighting mechanism is proposed to enhance the ability of the model to recognize the regular geometric boundaries of buildings. Comprehensive experiments are conducted on the three different datasets. The proposed MC-TRANSU achieves the best F1-score and intersection over union, compared with the state-of-the-art methods, such as SegNet, TransUnet, and Swin-Unet, and the detection accuracy improved around 5%. Quantitative and qualitative results verify the superiority and effectiveness of our model.  © 2008-2012 IEEE.,Accurate building heights are essential to understand the mechanisms of urban systems and provide new insights into research on infrastructure equality, urban environment, population modeling, etc. However, relatively few studies have been conducted to estimate high-resolution building heights using Chinese GaoFen-7 imagery across multiple cities. In this study, we developed an effective Building Height Estimation method by synergizing Photogrammetry and Deep learning methods (BHEPD) to leverage Chinese multi-view GaoFen-7 (GF-7) images for high-resolution building height estimation. In particular, BHEPD incorporates the StereoNet model and is capable of generating an edge-preserving digital surface model (DSM) from GF-7 images. This improvement enhances the accuracy of heights estimation, particularly for high-rise, high-density, and multi-scale buildings. BHEPD extracted over 700,000 building heights in local areas of Tianjin, Lanzhou, Chongqing, Ningbo, Guangzhou, Foshan, Shenzhen, and Hong Kong. The estimated building heights in the different cities are highly consistent with the reference building height data obtained from field-measured building height data, building floor data, and airborne light detection and ranging (LiDAR) data. The R2 varies from 0.75 to 0.90, while the root mean square error (RMSE) ranges from 4.06 m to 8.06 m. The validation of the estimated building heights over 106,366 buildings using airborne LiDAR height data yielded R2 and RMSE of 0.90 and 6.72 m for Shenzhen and 0.89 and 4.06 m for Hong Kong, respectively. Furthermore, a comparison with the building height estimation based on the traditional DSM generation method (i.e., semi-global matching (SGM) algorithm) confirmed the superiority of BHEPD in alleviating the underestimation of high-rise buildings (>60 m). Overall, BHEPD shows great potential for high-resolution building height estimation and can be expected to reduce the manual workload involved in collecting building height information on large scale. © 2023 Elsevier Inc.,Building heights are one of the crucial data for comprehending the functions of urban systems. Employing optical remote sensing imagery, the shadow-based method is one of the most promising methods which have been proposed for estimating building height. However, the existing shadow-based studies for building height estimation are restricted to a small area due to the lack of building height annotations and ignorance of building azimuth variations. The Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) allows large-scale building height retrieval in the along-track direction and thus can be taken as ground truth building height annotations to support the shadow-based algorithms for large-scale building height extraction. Here, we proposed an approach for extracting building height by combining Google Earth Satellite (GES) images and ICESat-2 photons. Building and shadow instances were first extracted using a U-Net deep learning framework. Based on the building height annotations retrieved from ICESat-2 photons, an improved shadow-based building height estimation model by minimizing the global error across all sample buildings was developed. A typical urban area located in the city center of Shanghai, China with an area of around 90 km2 was selected to validate the proposed method. In total 15,966 buildings were successfully extracted and the results indicated that the estimated building heights have high accuracy with an absolute mean error of 4.08 m. Moreover, the proposed method shows a better performance compared to the existing shadow-based method and existing building height datasets. The method holds great potential for large-scale building-level height retrieval which contributes to further studies of urban morphologies. © 2023 The Author(s)"
80,79,52,79_Search for new heavy resonances decaying to boson pairs in the all-jets final state at the LHC,Search for new heavy resonances decaying to boson pairs in the all-jets final state at the LHC,"The QCD-like dark sector with GeV-scale dark hadrons has the potential to generate new signatures at the Large Hadron Collider (LHC). In this paper, we consider a singlet scalar mediator in the tens of GeV-scale that connects the dark sector and the Standard Model (SM) sector via the Higgs portal. We focus on the Higgs-strahlung process, qq¯ ? ? W * ? WH, to produce a highly boosted Higgs boson. Our scenario predicts two different processes that can generate dark mesons: (1) the cascade decay from the Higgs boson to two light scalar mediators and then to four dark mesons; (2) the Higgs boson decaying to two dark quarks, which then undergo a QCD-like shower and hadronization to produce dark mesons. We apply machine learning techniques, such as Convolutional Neural Network (CNN) and Energy Flow Network (EFN), to the fat-jet structure to distinguish these signal processes from large SM backgrounds. We find that the branching ratio of the Higgs boson to two light scalar mediators can be constrained to be less than about 10% at 14 TeV LHC with L = 3000 fb ?1. © 2023, The Author(s).,In this article we probe resonant associated production of a Standard Model Higgs boson with new heavy scalar resonance in proton-proton collisions at a center-of-mass energy s=13 TeV. The Higgs boson and new scalar resonant are required to decay into a pair of bottom quarks and a pair of top quarks, respectively. Semileptonic decay of top quarks is considered. The searches are projected into operation conditions of the Large Hadron Collider during Run II data taking period at a center-of-mass energy of 13 TeV using Monte Carlo generated events, realistic detector response simulation and available Open Data samples. Analysis strategies are presented and machine learning approach using Deep Neural Network is proposed to resolve ambiguous in jets assignment and improve kinematic reconstruction of signal events. Sensitivity of the CMS detector is estimated as 95% expected upper limits on the product of the production cross section and the branching fractions of the searched particles. © 2023 The Author(s),A data sample containing top quark pairs ( t t ¯ ) produced in association with a Lorentz-boosted Z or Higgs boson is used to search for signs of new physics using effective field theory. The data correspond to an integrated luminosity of 138 fb - 1 of proton-proton collisions produced at a center-of-mass energy of 13 TeV at the LHC and collected by the CMS experiment. Selected events contain a single lepton and hadronic jets, including two identified with the decay of bottom quarks, plus an additional large-radius jet with high transverse momentum identified as a Z or Higgs boson decaying to a bottom quark pair. Machine learning techniques are employed to discriminate between t t ¯ Z or t t ¯ H events and events from background processes, which are dominated by t t ¯ + jets production. No indications of new physics are observed. The signal strengths of boosted t t ¯ Z and t t ¯ H production are measured, and upper limits are placed on the t t ¯ Z and t t ¯ H differential cross sections as functions of the Z or Higgs boson transverse momentum. The effects of new physics are probed using a framework in which the standard model is considered to be the low-energy effective field theory of a higher energy scale theory. Eight possible dimension-six operators are added to the standard model Lagrangian, and their corresponding coefficients are constrained via fits to the data. © 2023 CERN, for the CMS Collaboration."
81,80,52,80_Fake News Detection Techniques and Methods,Fake News Detection Techniques and Methods,"The proliferation of social networks has presented a significant challenge in combating the pervasive issue of fake news within modern societies. Due to the large amount of information and news produced daily in text, audio, and video, the validation and verification of this information have become crucial tasks. Leveraging advancements in artificial intelligence, distinguishing between fake news and factual information through automatic fake news detection systems has become more feasible. Automatic fake news detection has been explored from diverse perspectives, employing various feature extraction and classification models. Nonetheless, empirical evaluations, categorization, and comparisons of existing techniques for handling this problem remain limited. In this paper, we revisit the definitions and perspectives of fake news and propose an updated taxonomy for the field based on multiple criteria: (1) Type of features used in fake news detection; (2) Fake news detection perspectives; (3) Feature representation methods; and (4) Classification approaches. Moreover, we conduct an extensive empirical study to evaluate several feature representation techniques and classification approaches based on accuracy and computational cost. Our experimental results demonstrate that the optimal feature extraction techniques vary depending on the characteristics of the dataset. Notably, context-dependent models based on transformer models consistently exhibit superior performance. Additionally, employing transformer models as feature extraction methods, rather than solely fine-tuning the network for the downstream task, improves overall performance. Through extensive error analysis, we identify that a combination of feature representation methods and classification algorithms, including classical ones, offer complementary aspects and should be considered for achieving better generalization performance while maintaining a relatively low computational cost. For further details, including source codes, figures, and datasets, please refer to our project's GitHub repository: [https://github.com/FFarhangian/Fake-news-detection-Comparative-Study]. © 2023 Elsevier B.V.,The use of digital media, such as social networks, has promoted the spreading of fake news on a large scale. Therefore, several Machine Learning techniques, such as artificial neural networks, have been used for fake news detection and classification. These techniques are widely used due to their learning capabilities. Besides, models based on artificial neural networks can be easily integrated into social media and websites to spot fake news early and avoid their propagation. Nevertheless, most fake news classification models are available only for English news, limiting the possibility of detecting fake news in other languages, such as Spanish. For this reason, this study proposes implementing a web service that integrates a deep learning model for the classification of fake news in Spanish. To determine the best model, the performance of several neural network architectures, including MLP, CNN, and LSTM, was evaluated using the F1 score., and LSTM using the F1 score. The LSTM architecture was the best, with an F1 score of 0.746. Finally, the efficiency of web service was evaluated, applying temporal behavior as a metric, resulting in an average response time of 1.08 seconds © 2023, International Journal of Advanced Computer Science and Applications.All Rights Reserved.,As the number of people using social networks increases, more people are using social media platforms to meet their news needs. Users think that it is easier to follow the agenda by accessing news, especially on Twitter, rather than newspaper news pages. However, fake news is increasingly appearing on social media, and it is not always possible for people to obtain correct news from partial news pages or short Twitter posts. Understanding whether the news shared on Twitter is true or not is an important problem. Detecting fake tweets is of great importance in Turkish as well as in any language. In this study, fake news obtained from verification platforms on Twitter and real news obtained from the Twitter accounts of mainstream newspapers were labeled and, preprocessed using the Zemberek natural language processing tool developed for the Turkish language, and a dataset named TR_FaRe_News was created. Then, the TR_FaRe_News dataset was explored using ensemble methods and BoW, TF-IDF, and Word2Vec vectorization methods for fake news detection. Then a pre-trained BERT deep learning model was fine-tuned, and variations of the model extended with Bi-LSTM and Convolutional Neural Network (CNN) layers with the frozen and unfrozen parameters methods were explored. The performance evaluation was conducted using seven comparable datasets, namely BuzzFeedNews, GossipCop, ISOT, LIAR, Twitter15, and Twitter16, including an LLM-generated fake news dataset. Analyzing Turkish tweets and using fake news datasets generated by LLM is considered an important contribution. Accuracy values between 90 and 94% were obtained with the BERT and BERTurk + CNN models with 94% accuracy.  © 2013 IEEE."
82,81,52,81_Human Activity Recognition using Wearable Sensors and Deep Learning,Human Activity Recognition using Wearable Sensors and Deep Learning,"Recently, Human Activity Recognition (HAR) is becoming one of the prevalent study fields. HAR is a powerful tool for monitoring a person's dynamism, and it can be accomplished through machine learning (ML) techniques. HAR is a technique of automatically analysing and recognizing human activities depending on information from several wearable devices and smartphone sensors, like location, accelerometer, gyroscope, duration, and other environmental sensors. This study introduces a new Robust Human Activity Recognition using Equilibrium Optimizer with Deep Extreme Learning Machine (RHAR-EODELM) model. The presented RHAR-EODELM technique mainly identifies different classes of human activities. It follows a three-stage process. Initially, the RHAR-EODELM technique employs a min-max normalization process for scaling the activity data. Next, the RHAR-EODELM technique exploits a deep extreme learning machine with a radial basis function (DELM-RBF) model for the prediction process. Finally, the EO approach is enforced to adjust the parameters associated with the DELM-RBF method. A large-scale simulating process highlights the improved HAR results of the RHAR-EODELM method. The experimental values signify that the RHAR-EODELM method reaches improved predictive outcomes over other models. © 2023 Seventh Sense Research Group.,Human Activity Recognition (HAR) has been proven to be effective in various healthcare and telemonitoring applications. Current HAR methods, especially deep learning, are extensively employed owing to their exceptional recognition capabilities. However, in pursuit of enhancing feature expression abilities, deep learning often introduces a trade-off by increasing Time complexity. Moreover, the intricate nature of human activity data poses a challenge as it can lead to a notable decrease in recognition accuracy when affected by additional noise. These aspects will significantly impair recognition performance. To advance this field further, we present a HAR method based on an edge-computing-assisted and GRU deep-learning network. We initially proposed a model for edge computing to optimize the energy consumption and processing time of wearable devices. This model transmits HAR data to edge-computable nodes, deploys analytical models on edge servers for remote training, and returns results to wearable devices for processing. Then, we introduced an initial convolution method to preprocess large amounts of training data more effectively. To this end, an attention mechanism was integrated into the network structure to enhance the analysis of confusing data and improve the accuracy of action classification. Our results demonstrated that the proposed approach achieved an average accuracy of 85.4% on the 200 difficult-to-identify HAR data, which outperforms the Recurrent Neural Network (RNN) method’s accuracy of 77.1%. The experimental results showcase the efficacy of the proposed method and offer valuable insights for the future application of HAR. © 2023 by the authors.,Healthcare is an area of concern where the application of human-centred design practices and principles can enormously affect well-being and patient care. The provision of high-quality healthcare services requires a deep understanding of patients&#x2019; needs, experiences, and preferences. Human activity recognition (HAR) is paramount in healthcare monitoring by using machine learning (ML), sensor data, and artificial intelligence (AI) to track and discern individuals&#x2019; behaviours and physical movements. This technology allows healthcare professionals to remotely monitor patients, thereby ensuring they adhere to prescribed rehabilitation or exercise routines, and identify falls or anomalies, improving overall care and safety of the patient. HAR for healthcare monitoring, driven by deep learning (DL) algorithms, leverages neural networks and large quantities of sensor information to autonomously and accurately detect and track patients&#x2019; behaviors and physical activities. DL-based HAR provides a cutting-edge solution for healthcare professionals to provide precise and more proactive interventions, reducing the burden on healthcare systems and improving patient well-being while increasing the overall quality of care. Therefore, the study presents an improved coyote optimization algorithm with a deep learning-assisted HAR (ICOADL-HAR) approach for healthcare monitoring. The purpose of the ICOADL-HAR technique is to analyze the sensor information of the patients to determine the different kinds of activities. In the primary stage, the ICOADL-HAR model allows a data normalization process using the Z-score approach. For activity recognition, the ICOADL-HAR technique employs an attention-based long short-term memory (ALSTM) model. Finally, the hyperparameter tuning of the ALSTM model can be performed by using ICOA. The stimulation validation of the ICOADL-HAR model takes place using benchmark HAR datasets. The wide-ranging comparison analysis highlighted the improved recognition rate of the ICOADL-HAR method compared to other existing HAR approaches in terms of various measures. Authors"
83,82,51,82_Low-power neuromorphic circuits for spiking neural networks (SNNs) and their hardware implementation,Low-power neuromorphic circuits for spiking neural networks (SNNs) and their hardware implementation,"Neuromorphic Computing, a concept pioneered in the late 1980s, is receiving a lot of attention lately due to its promise of reducing the computational energy, latency, as well as learning complexity in artificial neural networks. Taking inspiration from neuroscience, this interdisciplinary field performs a multi-stack optimization across devices, circuits, and algorithms by providing an end-to-end approach to achieving brain-like efficiency in machine intelligence. On one side, neuromorphic computing introduces a new algorithmic paradigm, known as Spiking Neural Networks (SNNs), which is a significant shift from standard deep learning and transmits information as spikes (""1""or ""0"") rather than analog values. This has opened up novel algorithmic research directions to formulate methods to represent data in spike-trains, develop neuron models that can process information over time, design learning algorithms for event-driven dynamical systems, and engineer network architectures amenable to sparse, asynchronous, event-driven computing to achieve lower power consumption. On the other side, a parallel research thrust focuses on development of efficient computing platforms for new algorithms. Standard accelerators that are amenable to deep learning workloads are not particularly suitable to handle processing across multiple timesteps efficiently. To that effect, researchers have designed neuromorphic hardware that rely on event-driven sparse computations as well as efficient matrix operations. While most large-scale neuromorphic systems have been explored based on CMOS technology, recently, Non-Volatile Memory (NVM) technologies show promise toward implementing bio-mimetic functionalities on single devices. In this article, we outline several strides that neuromorphic computing based on spiking neural networks (SNNs) has taken over the recent past, and we present our outlook on the challenges that this field needs to overcome to make the bio-plausibility route a successful one.  © 2023 Copyright held by the owner/author(s).,Investigations in the field of spiking neural networks (SNNs) encompass diverse, yet overlapping, scientific disciplines. Examples range from purely neuroscientific investigations, researches on computational aspects of neuroscience, or applicative-oriented studies aiming to improve SNNs performance or to develop artificial hardware counterparts. However, the simulation of SNNs is a complex task that can not be adequately addressed with a single platform applicable to all scenarios. The optimization of a simulation environment to meet specific metrics often entails compromises in other aspects. This computational challenge has led to an apparent dichotomy of approaches, with model-driven algorithms dedicated to the detailed simulation of biological networks, and data-driven algorithms designed for efficient processing of large input datasets. Nevertheless, material scientists, device physicists, and neuromorphic engineers who develop new technologies for spiking neuromorphic hardware solutions would find benefit in a simulation environment that borrows aspects from both approaches, thus facilitating modeling, analysis, and training of prospective SNN systems. This manuscript explores the numerical challenges deriving from the simulation of spiking neural networks, and introduces SHIP, Spiking (neural network) Hardware In PyTorch, a numerical tool that supports the investigation and/or validation of materials, devices, small circuit blocks within SNN architectures. SHIP facilitates the algorithmic definition of the models for the components of a network, the monitoring of states and output of the modeled systems, and the training of the synaptic weights of the network, by way of user-defined unsupervised learning rules or supervised training techniques derived from conventional machine learning. SHIP offers a valuable tool for researchers and developers in the field of hardware-based spiking neural networks, enabling efficient simulation and validation of novel technologies. Copyright © 2024 Gemo, Spiga and Brivio.,With the help of Artificial Neural Networks?ANNs?? deep reinforcement learning algorithms have achieved great success in complex tasks? such as playing games and robotic control?etc. However?compared with the mechanism of reward-modulated learning in the brain?deep reinforcement learning still has a huge gap in cognitive ability and computational efficiency. Inspired by the spike-driven communication in the brain?Spiking Neural Networks?SNNs?adopt the spiking neuronal models for calculation and exchange information through discrete action potentials?i. e.?spikes?which greatly fit the mechanism of biological neurons. It is demonstrated by much research that SNNs have distinctive properties?such as complex time-series information processing capability? extremely low energy consumption? and strong robustness. In addition?SNNs also show the potential for continual learning. In the field of neuromorphic engineering and brain-inspired computing? SNNs are widely concerned and known as a new generation of neural networks. By combining SNNs with reinforcement learning? spiking reinforcement learning algorithms are considered as a feasible way to develop the artificial brain? and can effectively explain the discovery in the biological brain. As a cross-discipline research area of neuroscience and artificial intelligence? spiking reinforcement learning algorithms cover a large number of outstanding research works. According to the emphasis on different fields? these research works can be divided into two categories?one is to better understand the mechanism of reward-modulated learning in the brain? which is used to explain the findings in animal experiments and simulate the learning process of the brain? such as R-STDP learning rules?The other is to improve the performance? energy efficiency and other specific indicators of various control tasks requiring reinforcement learning algorithms to solve. With the unique advantages of SNNs? this kind of algorithm acts as a robust and energy-efficient solution for artificial intelligence?and shows great application potential in the fields of robotics and autonomous control. In this paper?the first part presents the cornerstone of spiking reinforcement learning algorithms?that is?spiking neural networks and reinforcement learning? and then analyzes the research characteristics and progress of these two kinds of spiking reinforcement learning algorithms. For the first kind of algorithms?this paper focuses on analyzing the reinforcement learning algorithms using the three-factor learning rules? and introduces their physiological background and specific implementation methods. Based on whether to use ANNs during training? this paper further divides the second kind of algorithms into spiking reinforcement learning algorithms using ANNs and spike-based direct reinforcement learning algorithms. As far as we know? this paper takes the lead in systematically sorting out and analyzing the latest progress of spiking reinforcement learning algorithms? and comprehensively shows the different ways of applying SNNs in deep reinforcement learning algorithms. Finally?this paper makes an in-depth discussion of the current challenges and follow-up research directions in this field. We systematically summarize the advantages and disadvantages of the current research?and look forward to its future impact on the field of neuroscience and artificial intelligence? so as to attract more researchers to participate in communication and cooperation in this new direction. © 2023 Science Press. All rights reserved."
84,83,51,83_Crop Yield Prediction and Climate Change Impacts,Crop Yield Prediction and Climate Change Impacts,"Context: Collection and analysis of large volumes of on-farm production data are widely seen as key to understanding yield variability among farmers and improving resource-use efficiency. Objective: The aim of this study was to assess the performance of statistical and machine learning methods to explain and predict crop yield across thousands of farmers’ fields in contrasting farming systems worldwide. Methods: A large database of 10,940 field-year combinations from three countries in different stages of agricultural intensification was analyzed. Random effects models were used to partition crop yield variability and random forest models were used to explain and predict crop yield within a cross-validation scheme with data re-sampling over space and time. Results: Yield variability in relative terms was smallest for wheat and barley in the Netherlands and for wheat in Ethiopia, intermediate for rice in the Philippines, and greatest for maize in Ethiopia. Random forest models comprising a total of 87 variables explained a maximum of 65 % of cereal yield variability in the Netherlands and less than 45 % of cereal yield variability in Ethiopia and in the Philippines. Crop management related variables were important to explain and predict cereal yields in Ethiopia, while predictive (i.e., known before the growing season) climatic variables and explanatory (i.e., known during or after the growing season) climatic variables were most important to explain and predict cereal yield variability in the Philippines and in the Netherlands, respectively. Finally, model cross-validation for regions or years not seen during model training reduced the R2 considerably for most crop x country combinations, while for wheat in the Netherlands this was model dependent. Conclusion: Big data from farmers’ fields is useful to explain on-farm yield variability to some extent, but not to predict it across time and space. Significance: The results call for moderate expectations towards big data and machine learning in agronomic studies, particularly for smallholder farms in the tropics where model performance was poorest independently of the variables considered and the cross-validation scheme used. © 2023 The Authors,Food security has become a real challenge for some organizations in charge of the food program and for the majority of countries, especially African countries. The United Nations Organizations’ has recently defined the end of hunger and the improvement of food security in 2030 as its primary goal. Improving food security could also pass through the handling of agricultural yield. Agricultural yield is affected by climate changes since this latest decade. Climate change is considered one of the major threats to agricultural development in Africa. Decision-making level and farmers need efficient analytical tools to help them in decision making. Machine learning has become an impressive predictive analytical tool for large volume of data. It has been used in many domains such as medicine, finance, sport, and recently in agriculture. In this work, we propose three crop prediction models : Crop Random Forest, Crop Gradient Boosting Machine and Crop Support Vector Machine. We combine climate data, crop production data, and pesticides data to develop a decision system based on advanced machine learning models. Despite the poor availability of data related to agriculture in Africa, we were able to propose a decision system able to predict the crop yield at the country level in fourteen East African countries. Our experimental results show that the three proposed machine learning models fit well the crop data with a high accuracy R2. The Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE) associated to our models are very minimal because the agricultural prediction values are very close to reality. Our proposed models are reliable and generalize well the agricultural predictions in East Africa. © 2022,Simulations of crop yield due to climate change vary widely between models, locations, species, management strategies, and Representative Concentration Pathways (RCPs). To understand how climate and adaptation affects yield change, we developed a meta-model based on 8703 site-level process-model simulations of yield with different future adaptation strategies and climate scenarios for maize, rice, wheat and soybean. We tested 10 statistical models, including some machine learning models, to predict the percentage change in projected future yield relative to the baseline period (2000–2010) as a function of explanatory variables related to adaptation strategy and climate change. We used the best model to produce global maps of yield change for the RCP4.5 scenario and identify the most influential variables affecting yield change using Shapley additive explanations. For most locations, adaptation was the most influential factor determining the projected yield change for maize, rice and wheat. Without adaptation under RCP4.5, all crops are expected to experience average global yield losses of 6%–21%. Adaptation alleviates this average projected loss by 1–13 percentage points. Maize was most responsive to adaptive practices with a projected mean yield loss of ?21% [range across locations: ?63%, +3.7%] without adaptation and ?7.5% [range: ?46%, +13%] with adaptation. For maize and rice, irrigation method and cultivar choice were the adaptation types predicted to most prevent large yield losses, respectively. When adaptation practices are applied, some areas are predicted to experience yield gains, especially at northern high latitudes. These results reveal the critical importance of implementing adequate adaptation strategies to mitigate the impact of climate change on crop yields. © 2023 The Authors. Earth's Future published by Wiley Periodicals LLC on behalf of American Geophysical Union."
85,84,50,84_Brain Tumor Classification and Segmentation using MRI and Deep Learning,Brain Tumor Classification and Segmentation using MRI and Deep Learning,"A brain tumor can have an impact on the symmetry of a person’s face or head, depending on its location and size. If a brain tumor is located in an area that affects the muscles responsible for facial symmetry, it can cause asymmetry. However, not all brain tumors cause asymmetry. Some tumors may be located in areas that do not affect facial symmetry or head shape. Additionally, the asymmetry caused by a brain tumor may be subtle and not easily noticeable, especially in the early stages of the condition. Brain tumor classification using deep learning involves using artificial neural networks to analyze medical images of the brain and classify them as either benign (not cancerous) or malignant (cancerous). In the field of medical imaging, Convolutional Neural Networks (CNN) have been used for tasks such as the classification of brain tumors. These models can then be used to assist in the diagnosis of brain tumors in new cases. Brain tissues can be analyzed using magnetic resonance imaging (MRI). By misdiagnosing forms of brain tumors, patients’ chances of survival will be significantly lowered. Checking the patient’s MRI scans is a common way to detect existing brain tumors. This approach takes a long time and is prone to human mistakes when dealing with large amounts of data and various kinds of brain tumors. In our proposed research, Convolutional Neural Network (CNN) models were trained to detect the three most prevalent forms of brain tumors, i.e., Glioma, Meningioma, and Pituitary; they were optimized using Aquila Optimizer (AQO), which was used for the initial population generation and modification for the selected dataset, dividing it into 80% for the training set and 20% for the testing set. We used the VGG-16, VGG-19, and Inception-V3 architectures with AQO optimizer for the training and validation of the brain tumor dataset and to obtain the best accuracy of 98.95% for the VGG-19 model. © 2023 by the authors.,An abnormal growth of cells in the brain, often known as a brain tumor, has the potential to develop into cancer. Carcinogenesis of glial cells in the brain and spinal cord is the root cause of gliomas, which are the most prevalent type of primary brain tumor. After receiving a diagnosis of glioblastoma, it is anticipated that the average patient will have a survival time of less than 14 months. Magnetic resonance imaging (MRI) is a well-known non-invasive imaging technology that can detect brain tumors and gives a variety of tissue contrasts in each imaging modality. Until recently, only neuroradiologists were capable of performing the tedious and time-consuming task of manually segmenting and analyzing structural MRI scans of brain tumors. This was because neuroradiologists have specialized training in this area. The development of comprehensive and automatic segmentation methods for brain tumors will have a significant impact on both the diagnosis and treatment of brain tumors. It is now possible to recognize tumors in photographs because of developments in computer-aided design (CAD), machine learning (ML), and deep learning (DL) approaches. The purpose of this study is to develop, through the application of MRI data, an automated model for the detection and classification of brain tumors based on deep learning (DLBTDC-MRI). Using the DLBTDC-MRI method, brain tumors can be detected and characterized at various stages of their progression. Preprocessing, segmentation, feature extraction, and classification are all included in the DLBTDC-MRI methodology that is supplied. The use of adaptive fuzzy filtering, often known as AFF, as a preprocessing technique for photos, results in less noise and higher-quality MRI scans. A method referred to as “chicken swarm optimization” (CSO) was used to segment MRI images. This method utilizes Tsallis entropy-based image segmentation to locate parts of the brain that have been injured. In addition to this, a Residual Network (ResNet) that combines handcrafted features with deep features was used to produce a meaningful collection of feature vectors. A classifier developed by combining DLBTDC-MRI and CSO can finally be used to diagnose brain tumors. To assess the enhanced performance of brain tumor categorization, a large number of simulations were run on the BRATS 2015 dataset. It would appear, based on the findings of these trials, that the DLBTDC-MRI method is superior to other contemporary procedures in many respects. © 2022 by the authors.,Accurate diagnosis of the brain tumor type at an earlier stage is crucial for the treatment process and helps to save the lives of a large number of people worldwide. Because they are non-invasive and spare patients from having an unpleasant biopsy, magnetic resonance imaging (MRI) scans are frequently employed to identify tumors. The manual identification of tumors is difficult and requires considerable time due to the large number of three-dimensional images that an MRI scan of one patient’s brain produces from various angles. Moreover, the variations in location, size, and shape of the brain tumor also make it challenging to detect and classify different types of tumors. Thus, computer-aided diagnostics (CAD) systems have been proposed for the detection of brain tumors. In this paper, we proposed a novel unified end-to-end deep learning model named TumorDetNet for brain tumor detection and classification. Our TumorDetNet framework employs 48 convolution layers with leaky ReLU (LReLU) and ReLU activation functions to compute the most distinctive deep feature maps. Moreover, average pooling and a dropout layer are also used to learn distinctive patterns and reduce overfitting. Finally, one fully connected and a softmax layer are employed to detect and classify the brain tumor into multiple types. We assessed the performance of our method on six standard Kaggle brain tumor MRI datasets for brain tumor detection and classification into (malignant and benign), and (glioma, pituitary, and meningioma). Our model successfully identified brain tumors with remarkable accuracy of 99.83%, classified benign and malignant brain tumors with an ideal accuracy of 100%, and meningiomas, pituitary, and gliomas tumors with an accuracy of 99.27%. These outcomes demonstrate the potency of the suggested methodology for the reliable identification and categorization of brain tumors. Copyright: © 2023 Ullah et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
86,85,50,85_Structure-based virtual screening for drug discovery using machine learning-based models and molecular docking to identify high affinity binders and inhibitors.,Structure-based virtual screening for drug discovery using machine learning-based models and molecular docking to identify high affinity binders and inhibitors.,"Structure-based virtual screening has been a crucial tool in drug discovery for decades. However, as the chemical space expands, the existing structure-based virtual screening techniques based on molecular docking and scoring struggle to handle billion-entry ultralarge libraries due to the high computational cost. To address this challenge, people have resorted to machine learning techniques to enhance structure-based virtual screening for efficiently exploring the vast chemical space. In those cases, compounds are usually treated as sequential strings or two-dimensional topology graphs, limiting their ability to incorporate three-dimensional structural information for downstream tasks. We herein propose a novel deep learning protocol, GEM-Screen, which utilizes the geometry-enhanced molecular representation of the compounds docking to a specific target and is trained on docking scores of a small fraction of a library through an active learning strategy to approximate the docking outcome for yet nontraining entries. This protocol is applied to virtual screening campaigns against the AmpC and D4 targets, demonstrating that GEM-Screen enriches more than 90% of the hit scaffolds for AmpC in the top 4% of model predictions and more than 80% of the hit scaffolds for D4 in the same top-ranking size of library. GEM-Screen can be used in conjunction with traditional docking programs for docking of only the top-ranked compounds to avoid the exhaustive docking of the whole library, thus allowing for discovering top-scoring compounds from billion-entry libraries in a rapid yet accurate fashion. © 2023 American Chemical Society.,Ligand docking is one of the core technologies in structure-based virtual screening for drug discovery. However, conventional docking tools and existing deep learning tools may suffer from limited performance in terms of speed, pose quality and binding affinity accuracy. Here we propose KarmaDock, a deep learning approach for ligand docking that integrates the functions of docking acceleration, binding pose generation and correction, and binding strength estimation. The three-stage model consists of the following components: (1) encoders for the protein and ligand to learn the representations of intramolecular interactions; (2) E(n) equivariant graph neural networks with self-attention to update the ligand pose based on both protein–ligand and intramolecular interactions, followed by post-processing to ensure chemically plausible structures; (3) a mixture density network for scoring the binding strength. KarmaDock was validated on four benchmark datasets and tested in a real-world virtual screening project that successfully identified experiment-validated active inhibitors of leukocyte tyrosine kinase (LTK). © 2023, The Author(s), under exclusive licence to Springer Nature America, Inc.,Background: Molecular docking-based virtual screening (VS) aims to choose ligands with potential pharmacological activities from millions or even billions of molecules. This process could significantly cut down the number of compounds that need to be experimentally tested. However, during the docking calculation, many molecules have low affinity for a particular protein target, which waste a lot of computational resources. Methods: We implemented a fast and practical molecular screening approach called DL-DockVS (deep learning dock virtual screening) by using deep learning models (regression and classification models) to learn the outcomes of pipelined docking programs step-by-step. Results: In this study, we showed that this approach could successfully weed out compounds with poor docking scores while keeping compounds with potentially high docking scores against 10 DUD-E protein targets. A self-built dataset of about 1.9 million molecules was used to further verify DL-DockVS, yielding good results in terms of recall rate, active compounds enrichment factor and runtime speed. Conclusions: We comprehensively evaluate the practicality and effectiveness of DL-DockVS against 10 protein targets. Due to the improvements of runtime and maintained success rate, it would be a useful and promising approach to screen ultra-large compound libraries in the age of big data. It is also very convenient for researchers to make a well-trained model of one specific target for predicting other chemical libraries and high docking-score molecules without docking computation again. © The Author(s) 2023. Published by Higher Education Press."
87,86,50,86_Power Grid Control with Reinforcement Learning,Power Grid Control with Reinforcement Learning,"With the development of clean energy systems, large-scale renewable energy is being connected to the traditional distribution network, which also brings new challenges to the reliable and economic scheduling of the power grid. To address these challenges, this paper proposes an intelligent scheduling strategy for a wind energy dominated distribution network, which aims to reduce the fluctuation caused by the wind energy. First, the energy scheduling model and objective function of the distribution network system are established and the constraints of various types of components are considered. Then, deep reinforcement learning is introduced to realize real-time decision in distribution network to solve the problem of fluctuation caused by the uncertain wind power output. The energy scheduling method is developed into a Markov decision process based on deep deterministic policy gradient (DDPG) algorithm. Finally, the simulation is verified on the IEEE14 node system. The results verify that the proposed approach can effectively reduce power fluctuations in the distribution network. The superiority of the adopted DDPG algorithm is demonstrated by comparing with the deep Q network algorithm. © 2022 Elsevier Ltd,Renewable energy sources (RES) are increasingly being developed and used to address the energy crisis and protect the environment. However, the large-scale integration of wind and solar energy into the power grid is still challenging and limits the adoption of these new energy sources. Microgrids (MGs) are small-scale power generation and distribution systems that can effectively integrate renewable energy, electric loads, and energy storage systems (ESS). By using MGs, it is possible to consume renewable energy locally and reduce energy losses from long-distance transmission. This paper proposes a deep reinforcement learning (DRL)-based energy management system (EMS) called DRL-MG to process and schedule energy purchase requests from customers in real-time. Specifically, the aim of this paper is to enhance the quality of service (QoS) for customers and reduce their electricity costs by proposing an approach that utilizes a Deep Q-learning Network (DQN) model. The experimental results indicate that the proposed method outperforms commonly used real-time scheduling methods significantly. © 2023 The Authors. IET Generation, Transmission & Distribution published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.,Taking into account the challenges of obtaining accurate physical parameters and uncertainties arising from the integration of a large number of sources and loads, this paper proposes a real-time voltage control method for AC/DC distribution networks. The method utilizes model-free generation and coordinated control of multiple converters, and employs a combination of agent modeling and multi-agent soft actor critic (MASAC) techniques for modeling and solving the problem. Firstly, a complex nonlinear mapping relationship between bus power and voltage is established by training an power-voltage model, to address the issue of obtaining physical parameters in AC/DC distribution networks. Next, a Markov decision process is established for the voltage control problem, with multiple intelligent agents distributed to control the active and reactive power at each converter, in response to the uncertainties of photovoltaic (PV) and load variations. Using the MASAC method, a centralized training strategy and decentralized execution policy are implemented to achieve distributed control of the converters, with each converter making optimal decisions based on its local observation state. Finally, the proposed method is verified by numerical simulations, demonstrating its sound effectiveness and generalization ability. Copyright © 2023 Zhao, Han, Wang, Dong and Qian."
88,87,49,87_Knowledge distillation for model compression,Knowledge distillation for model compression,"Deep neural models have achieved remarkable performance on various supervised and unsupervised learning tasks, but it is a challenge to deploy these large-size networks on resource-limited devices. As a representative type of model compression and acceleration methods, knowledge distillation (KD) solves this problem by transferring knowledge from heavy teachers to lightweight students. However, most distillation methods focus on imitating the responses of teacher networks but ignore the information redundancy of student networks. In this article, we propose a novel distillation framework difference-based channel contrastive distillation (DCCD), which introduces channel contrastive knowledge and dynamic difference knowledge into student networks for redundancy reduction. At the feature level, we construct an efficient contrastive objective that broadens student networks&#x2019; feature expression space and preserves richer information in the feature extraction stage. At the final output level, more detailed knowledge is extracted from teacher networks by making a difference between multiview augmented responses of the same instance. We enhance student networks to be more sensitive to minor dynamic changes. With the improvement of two aspects of DCCD, the student network gains contrastive and difference knowledge and reduces its overfitting and redundancy. Finally, we achieve surprising results that the student approaches and even outperforms the teacher in test accuracy on CIFAR-100. We reduce the top-1 error to 28.16% on ImageNet classification and 24.15% for cross-model transfer with ResNet-18. Empirical experiments and ablation studies on popular datasets show that our proposed method can achieve state-of-the-art accuracy compared with other distillation methods. IEEE,Knowledge distillation is a simple yet effective technique for deep model compression, which aims to transfer the knowledge learned by a large teacher model to a small student model. To mimic how the teacher teaches the student, existing knowledge distillation methods mainly adapt an unidirectional knowledge transfer, where the knowledge extracted from different intermedicate layers of the teacher model is used to guide the student model. However, it turns out that the students can learn more effectively through multi-stage learning with a self-reflection in the real-world education scenario, which is nevertheless ignored by current knowledge distillation methods. Inspired by this, we devise a new knowledge distillation framework entitled multi-target knowledge distillation via student self-reflection or MTKD-SSR, which can not only enhance the teacher’s ability in unfolding the knowledge to be distilled, but also improve the student’s capacity of digesting the knowledge. Specifically, the proposed framework consists of three target knowledge distillation mechanisms: a stage-wise channel distillation (SCD), a stage-wise response distillation (SRD), and a cross-stage review distillation (CRD), where SCD and SRD transfer feature-based knowledge (i.e., channel features) and response-based knowledge (i.e., logits) at different stages, respectively; and CRD encourages the student model to conduct self-reflective learning after each stage by a self-distillation of the response-based knowledge. Experimental results on five popular visual recognition datasets, CIFAR-100, Market-1501, CUB200-2011, ImageNet, and Pascal VOC, demonstrate that the proposed framework significantly outperforms recent state-of-the-art knowledge distillation methods. © 2023, The Author(s).,Knowledge distillation is the technique of compressing a larger neural network, known as the teacher, into a smaller neural network, known as the student, while still trying to maintain the performance of the larger neural network as much as possible. Existing methods of knowledge distillation are mostly applicable for classification tasks. Many of them also require access to the data used to train the teacher model. To address the problem of knowledge distillation for regression tasks in the absence of original training data, the existing method uses a generator model trained adversarially against the student model to generate synthetic data to train the student model. In this study, we propose a new synthetic data generation strategy that directly optimizes for a large but bounded difference between the student and teacher model. Our results on benchmark experiments demonstrate that the proposed strategy allows the student model to learn better and emulate the performance of the teacher model more closely. © 2023 Elsevier Ltd"
89,88,49,88_Remote Sensing Scene Classification with Self-Supervised Learning,Remote Sensing Scene Classification with Self-Supervised Learning,"Remarkable progress based on deep neural networks has been achieved in the semantic segmentation of remote sensing images (RSIs). However, pixel-level labeling is expensive for RSIs. Semisupervised semantic segmentation becomes an alternative approach to reduce the cost of annotation, and it is crucial to utilize efficiently a large number of unlabeled data. Nevertheless, inevitably, there is an unbalanced class distribution between labeled and unlabeled data in a remote sensing scene. Existing semisupervised methods train unlabeled images in isolation from labeled images and only learn reliable pixel pseudo-labels, leading to underutilization of unlabeled images. This article proposes a novel semisupervised semantic segmentation approach based on label propagation and contrastive regularization for RSIs. Specifically, the unlabeled images are augmented by randomly copy-pasting the class regions from labeled images. A prototype feature constraint module is used to enforce the constraint on the pixel features of unlabeled images relying on the prototype features from labeled images, achieving feature alignment on the entire dataset. Furthermore, we present the region contrastive learning (RCL) module that guides the model to learn feature consistency under different perturbations and compact feature representations over class regions on unlabeled images. Extensive experimental results on multiple remote sensing datasets demonstrate that our proposed approach achieves superior performance compared with state-of-the-art semisupervised semantic segmentation methods.  © 1980-2012 IEEE.,Remote sensing data has been widely used for various Earth Observation (EO) missions such as land use and cover classification, weather forecasting, agricultural management, and environmental monitoring. Most existing remote-sensing-data-based models are based on supervised learning that requires large and representative human-labeled data for model training, which is costly and time-consuming. The recent introduction of self-supervised learning (SSL) enables models to learn a representation from orders of magnitude more unlabeled data. The success of SSL is heavily dependent on a pre-designed pretext task, which introduces an inductive bias into the model from a large amount of unlabeled data. Since remote sensing imagery has rich spectral information beyond the standard RGB color space, it may not be straightforward to extend to the multi/hyperspectral domain the pretext tasks established in computer vision based on RGB images. To address this challenge, this work proposed a generic self-supervised learning framework based on remote sensing data at both the object and pixel levels. The method contains two novel pretext tasks, one for object-based and one for pixel-based remote sensing data analysis methods. One pretext task is used to reconstruct the spectral profile from the masked data, which can be used to extract a representation of pixel information and improve the performance of downstream tasks associated with pixel-based analysis. The second pretext task is used to identify objects from multiple views of the same object in multispectral data, which can be used to extract a representation and improve the performance of downstream tasks associated with object-based analysis. The results of two typical downstream task evaluation exercises (a multilabel land cover classification task on Sentinel-2 multispectral datasets and a ground soil parameter retrieval task on hyperspectral datasets) demonstrate that the proposed SSL method learns a target representation that covers both spatial and spectral information from massive unlabeled data. A comparison with currently available SSL methods shows that the proposed method, which emphasizes both spectral and spatial features, outperforms existing SSL methods on multi- and hyperspectral remote sensing datasets. We believe that this approach has the potential to be effective in a wider range of remote sensing applications and we will explore its utility in more remote sensing applications in the future. © 2023 by the authors.,Scene classification is a crucial research problem in remote sensing (RS) that has attracted many researchers recently. It has many challenges due to multiple issues, such as: the complexity of remote sensing scenes, the classes overlapping (as a scene may contain objects that belong to foreign classes), and the difficulty of gaining sufficient labeled scenes. Deep learning (DL) solutions and in particular convolutional neural networks (CNN) are now state-of-the-art solution in RS scene classification; however, CNN models need huge amounts of annotated data, which can be costly and time-consuming. On the other hand, it is relatively easy to acquire large amounts of unlabeled images. Recently, Self-Supervised Learning (SSL) is proposed as a method that can learn from unlabeled images, potentially reducing the need for labeling. In this work, we propose a deep SSL method, called RS-FewShotSSL, for RS scene classification under the few shot scenario when we only have a few (less than 20) labeled scenes per class. Under this scenario, typical DL solutions that fine-tune CNN models, pre-trained on the ImageNet dataset, fail dramatically. In the SSL paradigm, a DL model is pre-trained from scratch during the pretext task using the large amounts of unlabeled scenes. Then, during the main or the so-called downstream task, the model is fine-tuned on the labeled scenes. Our proposed RS-FewShotSSL solution is composed of an online network and a target network both using the EfficientNet-B3 CNN model as a feature encoder backbone. During the pretext task, RS-FewShotSSL learns discriminative features from the unlabeled images using cross-view contrastive learning. Different views are generated from each image using geometric transformations and passed to the online and target networks. Then, the whole model is optimized by minimizing the cross-view distance between the online and target networks. To address the problem of limited computation resources available to us, our proposed method uses a novel DL architecture that can be trained using both high-resolution and low-resolution images. During the pretext task, RS-FewShotSSL is trained using low-resolution images, thereby, allowing for larger batch sizes which significantly boosts the performance of the proposed pipeline on the task of RS classification. In the downstream task, the target network is discarded, and the online network is fine-tuned using the few labeled shots or scenes. Here, we use smaller batches of both high-resolution and low-resolution images. This architecture allows RS-FewshotSSL to benefit from both large batch sizes and full image sizes, thereby learning from the large amounts of unlabeled data in an effective way. We tested RS-FewShotSSL on three RS public datasets, and it demonstrated a significant improvement compared to other state-of-the-art methods such as: SimCLR, MoCo, BYOL and IDSSL. © 2023, The Author(s)."
90,89,48,89_Enhancer-Gene Interactions and RNA Modification Predictions,Enhancer-Gene Interactions and RNA Modification Predictions,"Utilizing large-scale epigenomics data, deep learning tools can predict the regulatory activity of genomic sequences, annotate non-coding genetic variants, and uncover mechanisms behind complex traits. However, these tools primarily rely on human or mouse data for training, limiting their performance when applied to other species. Furthermore, the limited exploration of many species, particularly in the case of livestock, has led to a scarcity of comprehensive and high-quality epigenetic data, posing challenges in developing reliable deep learning models for decoding their non-coding genomes. The cross-species prediction of the regulatory genome can be achieved by leveraging publicly available data from extensively studied organisms and making use of the conserved DNA binding preferences of transcription factors within the same tissue. In this study, we introduced DeepSATA, a novel deep learning-based sequence analyzer that incorporates the transcription factor binding affinity for the cross-species prediction of chromatin accessibility. By applying DeepSATA to analyze the genomes of pigs, chickens, cattle, humans, and mice, we demonstrated its ability to improve the prediction accuracy of chromatin accessibility and achieve reliable cross-species predictions in animals. Additionally, we showcased its effectiveness in analyzing pig genetic variants associated with economic traits and in increasing the accuracy of genomic predictions. Overall, our study presents a valuable tool to explore the epigenomic landscape of various species and pinpoint regulatory deoxyribonucleic acid (DNA) variants associated with complex traits. © 2023 by the authors.,Background: The largest sequence-based models of transcription control to date are obtained by predicting genome-wide gene regulatory assays across the human genome. This setting is fundamentally correlative, as those models are exposed during training solely to the sequence variation between human genes that arose through evolution, questioning the extent to which those models capture genuine causal signals. Results: Here we confront predictions of state-of-the-art models of transcription regulation against data from two large-scale observational studies and five deep perturbation assays. The most advanced of these sequence-based models, Enformer, by and large, captures causal determinants of human promoters. However, models fail to capture the causal effects of enhancers on expression, notably in medium to long distances and particularly for highly expressed promoters. More generally, the predicted impact of distal elements on gene expression predictions is small and the ability to correctly integrate long-range information is significantly more limited than the receptive fields of the models suggest. This is likely caused by the escalating class imbalance between actual and candidate regulatory elements as distance increases. Conclusions: Our results suggest that sequence-based models have advanced to the point that in silico study of promoter regions and promoter variants can provide meaningful insights and we provide practical guidance on how to use them. Moreover, we foresee that it will require significantly more and particularly new kinds of data to train models accurately accounting for distal elements. © 2023, The Author(s).,Background: Evolutionary conservation is an invaluable tool for inferring functional significance in the genome, including regions that are crucial across many species and those that have undergone convergent evolution. Computational methods to test for sequence conservation are dominated by algorithms that examine the ability of one or more nucleotides to align across large evolutionary distances. While these nucleotide alignment-based approaches have proven powerful for protein-coding genes and some non-coding elements, they fail to capture conservation of many enhancers, distal regulatory elements that control spatial and temporal patterns of gene expression. The function of enhancers is governed by a complex, often tissue- and cell type-specific code that links combinations of transcription factor binding sites and other regulation-related sequence patterns to regulatory activity. Thus, function of orthologous enhancer regions can be conserved across large evolutionary distances, even when nucleotide turnover is high. Results: We present a new machine learning-based approach for evaluating enhancer conservation that leverages the combinatorial sequence code of enhancer activity rather than relying on the alignment of individual nucleotides. We first train a convolutional neural network model that can predict tissue-specific open chromatin, a proxy for enhancer activity, across mammals. Next, we apply that model to distinguish instances where the genome sequence would predict conserved function versus a loss of regulatory activity in that tissue. We present criteria for systematically evaluating model performance for this task and use them to demonstrate that our models accurately predict tissue-specific conservation and divergence in open chromatin between primate and rodent species, vastly out-performing leading nucleotide alignment-based approaches. We then apply our models to predict open chromatin at orthologs of brain and liver open chromatin regions across hundreds of mammals and find that brain enhancers associated with neuron activity have a stronger tendency than the general population to have predicted lineage-specific open chromatin. Conclusion: The framework presented here provides a mechanism to annotate tissue-specific regulatory function across hundreds of genomes and to study enhancer evolution using predicted regulatory differences rather than nucleotide-level conservation measurements. © 2022, The Author(s)."
91,90,47,"90_Deep learning methods for earthquake detection, magnitude estimation, and early warning systems.","Deep learning methods for earthquake detection, magnitude estimation, and early warning systems.","Earthquake early warning (EEW) systems aim to forecast the shaking intensity rapidly after an earthquake occurs and send warnings to affected areas before the onset of strong shaking. The system relies on rapid and accurate estimation of earthquake source parameters. However, it is known that source estimation for large ruptures in real-time is challenging, and it often leads to magnitude underestimation. In a previous study, we showed that machine learning, HR-GNSS, and realistic rupture synthetics can be used to reliably predict earthquake magnitude. This model, called Machine-Learning Assessed Rapid Geodetic Earthquake model (M-LARGE), can rapidly forecast large earthquake magnitudes with an accuracy of 99%. Here, we expand M-LARGE to predict centroid location and fault size, enabling the construction of the fault rupture extent for forecasting shaking intensity using existing ground motion models. We test our model in the Chilean Subduction Zone with thousands of simulated and five real large earthquakes. The result achieves an average warning time of 40.5 s for shaking intensity MMI4+, surpassing the 34 s obtained by a similar GNSS EEW model. Our approach addresses a critical gap in existing EEW systems for large earthquakes by demonstrating real-time fault tracking feasibility without saturation issues. This capability leads to timely and accurate ground motion forecasts and can support other methods, enhancing the overall effectiveness of EEW systems. Additionally, the ability to predict source parameters for real Chilean earthquakes implies that synthetic data, governed by our understanding of earthquake scaling, is consistent with the actual rupture processes. © 2023. American Geophysical Union. All Rights Reserved.,In this study, magnitude estimation in earthquake early warning (EEW) systems is seen as a classification problem: the single-channel waveform, starting from the P-wave onset and lasting 4 s, is given in the input, and earthquake severity (medium and large earthquakes: local magnitude (ML) ? 5; small earthquakes: ML < 5) is the classification result. The convolutional neural network (CNN) is proposed to estimate the severity of the earthquake, which is composed of several blocks that can extract the latent representation of the input from different receptive fields automatically. We train and test the proposed CNN model using two data sets. One is recorded by the China Earthquake Networks Center (CENC), and the other is the Stanford Earthquake Dataset (STEAD). Accordingly, the proposed CNN model achieves a test accuracy of 97.90 per cent. The proposed CNN model is applied to estimate two real-world earthquake swarms in China (the Changning earthquake and the Tangshan earthquake swarms) and the INSTANCE data set, and demonstrated the promising performance of generalization. In addition, the proposed CNN model has been connected to the CENC for further testing using real-world real-time seismic data.  © 2023 The Author(s).,Earthquake prediction is a long-standing problem in seismology that has garnered attention from the scientific community and the public. Despite ongoing efforts to understand the physical mechanisms of earthquake occurrence, there is no convincing physical or statistical model for predicting large earthquakes. Machine learning methods, such as random forest and long short-term memory (LSTM) neural networks, excel at identifying patterns in large-scale databases and offer a potential means to improve earthquake prediction performance. Differing from physical and statistical approaches to earthquake prediction, we explore whether small earthquakes can be used to predict large earthquakes within the framework of machine learning. Specifically, we attempt to answer two questions for a given region: (1) Is there a likelihood of a large earthquake (e.g., M ? 6.0) occurring within the next year? (2) What is the maximum magnitude of an earthquake expected to occur within the next year? Our results show that the random forest method performs best in classifying large earthquake occurrences, while the LSTM method provides a rough estimation of earthquake magnitude. We conclude that small earthquakes contain information relevant to predicting future large earthquakes and that machine learning provides a promising avenue for improving the prediction of earthquake occurrences. © 2023 by the authors."
92,91,47,91_Credit Risk Analysis and Predictive Modeling,Credit Risk Analysis and Predictive Modeling,"Commercial banks are required to explain the credit evaluation results to their customers. Therefore, banks attempt to improve the performance of their credit scoring models while ensuring the interpretability of the results. However, there is a tradeoff between the logistic regression model and machine learning-based techniques regarding interpretability and model performance because machine learning-based models are a black box. To deal with the tradeoff, in this study, we present a two-stage logistic regression method based on the Bayesian approach. In the first stage, we generate the derivative variables by linearly combining the original features with their explanatory powers based on the Bayesian inference. The second stage involves developing a credit scoring model through logistic regression using these derivative variables. Through this process, the explanatory power of a large number of original features can be utilized for default prediction, and the use of logistic regression maintains the model's interpretability. In the empirical analysis, the independent sample t-test reveals that our proposed approach significantly improves the model’s performance compared to that based on the conventional single-stage approach, i.e., the baseline model. The Kolmogorov–Smirnov statistics show a 3.42 percentage points (%p) increase, and the area under the receiver operating characteristic shows a 2.61%p increase. Given that our two-stage modeling approach has the advantages of interpretability and enhanced performance of the credit scoring model, our proposed method is essential for those in charge of banking who must explain credit evaluation results and find ways to improve the performance of credit scoring models. © 2022, The Author(s).,Corporate credit ratings are comprehensive indicators of a company's management performance, earnings quality, and future prospects; they represent its market evaluation and status in the industry and are relevant to the financing and investment decision-making process. Financial institutions determine corporate credit ratings using corporate financial and governance indicators. With advancements in the Internet and the popularity of social media, the appeal of enterprises on social media has become a relevant research topic. Social media represents an alternative method for financial institutions to determine corporate credit ratings. Therefore, the large amounts of data from social media have been used to effectively analyze and predict corporate credit ratings in risk management departments of financial institutions in the field of financial technology (FinTech). This study develops an approach to forecasting corporate credit ratings by analyzing public opinion toward corporations on social media to assist financial institutions in effectively evaluating and controlling corporate risk. This objective is achieved through the following steps: (i) designing a corporate credit rating forecasting process based on big data from social media, (ii) developing techniques for corporate credit rating forecasting, and (iii) implementing and evaluating the corporate credit rating forecasting mechanism. The experimental results of this research show that the accuracy of corporate credit rating prediction based on social media big data is higher than that of traditional financial report, corporate governance and macroeconomic indicators. Moreover, the adopted forecasting model, K-Nearest Neighbor (KNN), is superior to the other machine learning models in terms of accuracy. © 2022 Elsevier Ltd,Financial institutions and regulators increasingly rely on large-scale data analysis, particularly machine learning, for credit decisions. This paper assesses ten machine learning algorithms using a dataset of over 2.5 million observations from a financial institution. We also summarize key statistical and machine learning models in credit scoring and review current research findings. Our results indicate that ensemble models, particularly XGBoost, outperform traditional algorithms such as logistic regression in credit classification. Researchers and experts in the subject of credit risk can use this work as a practical reference as it covers crucial phases of data processing, exploratory data analysis, modeling, and evaluation metrics. © 2023 by the authors."
93,92,47,92_Interpretable AI for Ruleout of Abnormal Large Bowel Biopsies,Interpretable AI for Ruleout of Abnormal Large Bowel Biopsies,"Whole slide image (WSI) analysis is increasingly being adopted as an important tool in modern pathology. Recent deep learning-based methods have achieved state-of-the-art performance on WSI analysis tasks such as WSI classification, segmentation, and retrieval. However, WSI analysis requires a significant amount of computation resources and computation time due to the large dimensions of WSIs. Most of the existing analysis approaches require the complete decompression of the whole image exhaustively, which limits the practical usage of these methods, especially for deep learning-based workflows. In this paper, we present compression domain processing-based computation efficient analysis workflows for WSIs classification that can be applied to state-of-the-art WSI classification models. The approaches leverage the pyramidal magnification structure of WSI files and compression domain features that are available from the raw code stream. The methods assign different decompression depths to the patches of WSIs based on the features directly retained from compressed patches or partially decompressed patches. Patches from the low-magnification level are screened by attention-based clustering, resulting in different decompression depths assigned to the high-magnification level patches at different locations. A finer-grained selection based on compression domain features from the file code stream is applied to select further a subset of the high-magnification patches that undergo a full decompression. The resulting patches are fed to the downstream attention network for final classification. Computation efficiency is achieved by reducing unnecessary access to the high zoom level and expensive full decompression. With the number of decompressed patches reduced, the time and memory costs of downstream training and inference procedures are also significantly reduced. Our approach achieves a 7.2× overall speedup, and the memory cost is reduced by 1.1 orders of magnitudes, while the resulting model accuracy is comparable to the original workflow. © 2023 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.,Colorectal cancer is one of the most common cancers worldwide, accounting for an annual estimated 1.8 million incident cases. With the increasing number of colonoscopies being performed, colorectal biopsies make up a large proportion of any histopathology laboratory workload. We trained and validated a unique artificial intelligence (AI) deep learning model as an assistive tool to screen for colonic malignancies in colorectal specimens, in order to improve cancer detection and classification; enabling busy pathologists to focus on higher order decision-making tasks. The study cohort consists of Whole Slide Images (WSI) obtained from 294 colorectal specimens. Qritive’s unique composite algorithm comprises both a deep learning model based on a Faster Region Based Convolutional Neural Network (Faster-RCNN) architecture for instance segmentation with a ResNet-101 feature extraction backbone that provides glandular segmentation, and a classical machine learning classifier. The initial training used pathologists’ annotations on a cohort of 66,191 image tiles extracted from 39 WSIs. A subsequent application of a classical machine learning-based slide classifier sorted the WSIs into ‘low risk’ (benign, inflammation) and ‘high risk’ (dysplasia, malignancy) categories. We further trained the composite AI-model’s performance on a larger cohort of 105 resections WSIs and then validated our findings on a cohort of 150 biopsies WSIs against the classifications of two independently blinded pathologists. We evaluated the area under the receiver-operator characteristic curve (AUC) and other performance metrics. The AI model achieved an AUC of 0.917 in the validation cohort, with excellent sensitivity (97.4%) in detection of high risk features of dysplasia and malignancy. We demonstrate an unique composite AI-model incorporating both a glandular segmentation deep learning model and a classical machine learning classifier, with excellent sensitivity in picking up high risk colorectal features. As such, AI plays a role as a potential screening tool in assisting busy pathologists by outlining the dysplastic and malignant glands. © 2022, The Author(s).,Considering their gigapixel sizes, the representation of whole slide images (WSIs) for classification and retrieval systems is a non-trivial task. Patch processing and multi-Instance Learning (MIL) are common approaches to analyze WSIs. However, in end-to-end training, these methods require high GPU memory consumption due to the simultaneous processing of multiple sets of patches. Furthermore, compact WSI representations through binary and/or sparse representations are urgently needed for real-time image retrieval within large medical archives. To address these challenges, we propose a novel framework for learning compact WSI representations utilizing deep conditional generative modeling and the Fisher Vector Theory. The training of our method is instance-based, achieving better memory and computational efficiency during the training. To achieve efficient large-scale WSI search, we introduce new loss functions, namely gradient sparsity and gradient quantization losses, for learning sparse and binary permutation-invariant WSI representations called Conditioned Sparse Fisher Vector (C-Deep-SFV), and Conditioned Binary Fisher Vector (C-Deep-BFV). The learned WSI representations are validated on the largest public WSI archive, The Cancer Genomic Atlas (TCGA) and also Liver-Kidney-Stomach (LKS) dataset. For WSI search, the proposed method outperforms Yottixel and Gaussian Mixture Model (GMM)-based Fisher Vector both in terms of retrieval accuracy and speed. For WSI classification, we achieve competitive performance against state-of-art on lung cancer data from TCGA and the public benchmark LKS dataset. © 2023 The Author(s)"
94,93,47,93_Aquatic Remote Sensing for Water Quality Monitoring,Aquatic Remote Sensing for Water Quality Monitoring,"Urban rivers are complex ecosystems that directly determine the living environment of human beings. Monitoring the urban river water quality indexes is a challenge in water quality evaluation. The purpose of this study was to propose a multi-source remote sensing water quality inversion method based on a small number of samples to solve the problem of scale inconsistency among multi-source remote sensing data, so as to achieve large-scale and efficient inversion of urban river water quality. Since there is a very important problem that the complex nonlinear relationships must be solved between simple ground point data and remote sensing data in water quality inversion, a novel self-optimizing machine learning monitoring method is proposed, which can automatically find the optimal parameters of the model from a small number of samples, and reduce the training time. Meanwhile, in order to strengthen the correlation between water quality parameters and remote sensing data, the feature enhancement method was used for generating the input data. Moreover, to solve the problem of the multi-source data quantity and quality, the spatial mapping method was used to achieve consistency in the water quality information since these data have different nonlinear characteristics. The experimental results show that for unmanned aerial vehicle (UAV) images, the R2 of chlorophyll a (Chla), turbidity (TUB), and ammonia nitrogen (NH3-N) can reached 0.917, 0.877 and 0.846, respectively. Using a satellite image, the R2 of Chla, TUB, and NH3-N can reach 0.827, 0.679 and 0.779, respectively. This method provides a new way to realize the integration of air-space-ground monitoring of urban inland rivers in the future. © 2022 The Author(s),Water quality monitoring of medium-sized inland water is important for water environment protection given the large number of small-to-medium size water bodies in China. A case study was conducted on Yuandang Lake in the Yangtze Delta region, with a surface area of 13 km2. This study proposed utilising a multispectral uncrewed aerial vehicle (UAV) to collect large-scale data and retrieve multiple water quality parameters using machine learning algorithms. An alternate processing method is proposed to process large and repetitive lake surface images for mapping the water quality data to the image. Machine learning regression methods (Random Forest, Gradient Boosting, Backpropagation Neural Network, and Convolutional Neural Network) were used to construct separate water quality inversion models for ten water parameters. The results showed that several water quality parameters (CODMn, temperature, pH, DO, and NC) can be retrieved with reasonable accuracy (R2 = 0.77, 0.75, 0.73, 0.67, and 0.64, respectively), although others (NH3-N, BGA, TP, Turbidity, and Chl-a) have a determination coefficient (R2) less than 0.6. This work demonstrated the tremendous potential of employing multispectral data in conjunction with machine learning algorithms to retrieve multiple water quality parameters for monitoring medium-sized bodies of water. © 2023 by the authors.,Dissolved oxygen (DO) concentration is a widely used and effective indicator for assessing water quality and pollution in aquatic environments. Continuous and large-scale inversion of water environments using remote sensing imagery has become a hot topic in water environmental research. Remote sensing technology has been extensively applied in water quality monitoring, but its limited sampling frequency necessitates the development of a high-frequency dynamic water quality monitoring model. In this study, we utilized Lake Chaohu as a case study. Firstly, we constructed a dynamic water quality inversion model for monitoring DO concentrations using machine learning methods, with Himawari-8 (H8) satellite imagery as input data and DO concentrations in Lake Chaohu as output data. Secondly, the developed DO concentration inversion model was employed to estimate the overall grid-based DO concentration in the Lake Chaohu region for the years 2019 to 2021. Lastly, Pearson correlation analysis and significance tests were performed to examine the correlation and significance between the estimated grid-based DO concentration and the ERA5 reanalysis dataset. The results demonstrate that the Random Forest (RF) model performs best in DO concentration inversion, with a high R2 score of 0.84, and low RMSE and MAE values of 0.69 and 0.54, respectively. Compared to other models, the RF model improves average performance with a 38% increase in R2, 13% decrease in RMSE, and 33% decrease in MAE. The model accurately predicts DO concentrations. Furthermore, the inversion results reveal seasonal differences in DO concentrations in Lake Chaohu from 2019 to 2021, with higher concentrations in spring and winter, and lower concentrations in summer and autumn. The average DO concentrations in the northwest, central-south, and northeast regions of Lake Chaohu are 10.12 mg/L, 9.98 mg/L, and 9.96 mg/L, respectively, with higher concentrations in the northwest region. Pearson correlation analysis indicates a significant correlation (p < 0.01) between DO concentrations and temperature, surface pressure, latent heat flux from the atmosphere to the surface, and latent heat flux from the surface to the atmosphere, with correlation coefficients of ?0.615, 0.583, ?0.480, and 0.444, respectively. The results verify the feasibility of using synchronous satellites for real-time inversion of DO concentrations, providing a more efficient, economical, and accurate means for real-time monitoring of DO concentrations. This study has practical value in improving the efficiency and accuracy of water environmental monitoring. © 2023 by the authors."
95,94,46,94_Predictive Modeling for Type 2 Diabetes Risk and Management,Predictive Modeling for Type 2 Diabetes Risk and Management,"Background: Diabetes mellitus (DM) is a chronic metabolic disease that could produce severe complications threatening life. Its early detection is thus quite important for the timely prevention and treatment. Normally, fasting blood glucose (FBG) by physical examination is used for large-scale screening of DM; however, some people with normal fasting glucose (NFG) actually have suffered from diabetes but are missed by the examination. This study aimed to investigate whether common physical examination indexes for diabetes can be used to identify the diabetes individuals from the populations with NFG. Methods: The physical examination data from over 60,000 individuals with NFG in three Chinese cohorts were used. The diabetes patients were defined by HbA1c ? 48 mmol/mol (6.5%). We constructed the models using multiple machine learning methods, including logistic regression, random forest, deep neural network, and support vector machine, and selected the optimal one on the validation set. A framework using permutation feature importance algorithm was devised to discover the personalized risk factors. Results: The prediction model constructed by logistic regression achieved the best performance with an AUC, sensitivity, and specificity of 0.899, 85.0%, and 81.1% on the validation set and 0.872, 77.9%, and 81.0% on the test set, respectively. Following feature selection, the final classifier only requiring 13 features, named as DRING (diabetes risk of individuals with normal fasting glucose), exhibited reliable performance on two newly recruited independent datasets, with the AUC of 0.964 and 0.899, the balanced accuracy of 84.2% and 81.1%, the sensitivity of 100% and 76.2%, and the specificity of 68.3% and 86.0%, respectively. The feature importance ranking analysis revealed that BMI, age, sex, absolute lymphocyte count, and mean corpuscular volume are important factors for the risk stratification of diabetes. With a case, the framework for identifying personalized risk factors revealed FBG, age, and BMI as significant hazard factors that contribute to an increased incidence of diabetes. DRING webserver is available for ease of application (http://www.cuilab.cn/dring). Conclusions: DRING was demonstrated to perform well on identifying the diabetes individuals among populations with NFG, which could aid in early diagnosis and interventions for those individuals who are most likely missed. © 2023, BioMed Central Ltd., part of Springer Nature.,Diabetes is a complex disease that can lead to serious health complications if left unmanaged. Early detection and treatment of diabetes is crucial, and data analysis and predictive techniques can play a significant role. Data mining techniques, such as classification and prediction models, can be used to analyse various aspects of data related to diabetes, and extract useful information for early detection and prediction of the disease. XGBoost classifier is a machine learning algorithm that effectively predicts diabetes with high accuracy. This algorithm uses a gradient-boosting framework and can handle large and complex datasets with high-dimensional features. However, it is important to note that the choice of the best algorithm for predicting diabetes may depend on the specific characteristics of the data and the research question being addressed. In addition to predicting diabetes, data analysis and predictive techniques can also be used to identify risk factors for diabetes and its complications, monitor disease progression, and evaluate the effectiveness of treatments. These techniques can provide valuable insights into the underlying mechanisms of the disease and help healthcare providers make informed decisions about patient care. Data analysis and predictive techniques have the potential to significantly improve the early detection and management of diabetes, a fast-growing chronic disease that notable health hazards. The XGBoost classifier showed the most effectiveness, with an accuracy rate of 89%. © 2023 by the author.,Objective: Diabetes mellitus is a global epidemic disease. Long-time exposure of patients to hyperglycemia can lead to various type of chronic tissue damage. Early diagnosis of and screening for diabetes are crucial to population health. Methods: We collected the national physical examination data in Xinjiang, China, in 2020 (a total of more than 4 million people). Three types of physical examination indices were analyzed: questionnaire, routine physical examination and laboratory values. Integrated learning, deep learning and logistic regression methods were used to establish a risk model for type-2 diabetes mellitus. In addition, to improve the convenience and flexibility of the model, a diabetes risk score card was established based on logistic regression to assess the risk of the population. Results: An XGBoost-based risk prediction model outperformed the other five risk assessment algorithms. The AUC of the model was 0.9122. Based on the feature importance ranking map, we found that hypertension, fasting blood glucose, age, coronary heart disease, ethnicity, parental diabetes mellitus, triglycerides, waist circumference, total cholesterol, and body mass index were the most important features of the risk prediction model for type-2 diabetes. Conclusions: This study established a diabetes risk assessment model based on multiple ethnicities, a large sample and many indices, and classified the diabetes risk of the population, thus providing a new forecast tool for the screening of patients and providing information on diabetes prevention for healthy populations. © 2023, The Author(s)."
96,95,45,95_Solar Power Generation Forecasting,Solar Power Generation Forecasting,"Solar energy is one of the most common and promising sources of renewable energy. In photovoltaic (PV) systems, operators can benefit from future solar irradiance predictions for efficient load balancing and grid stability. Therefore, short-term solar irradiance forecasting plays a crucial role in the transition to renewable energy. Modern PV grids collect large volumes of data that provide valuable information for forecasting models. Although the nature of these data presents an ideal setting for online learning methodologies, research to date has mainly focused on offline approaches. Hence, this work proposes a novel data streaming method for real-time solar irradiance forecasting on days with variable weather conditions and cloud coverage. Our method operates under an asynchronous dual-pipeline framework using deep learning models. For the experimental study, two datasets from a Canadian PV solar plant have been simulated as streams at different data frequencies. The experiments involve an exhaustive parameter grid search to evaluate four state-of-the-art deep learning architectures: multilayer perceptron (MLP), long-short term memory network (LSTM), convolutional network (CNN), and Transformer network. The obtained results demonstrate the suitability of deep learning models for this problem. In particular, MLP and CNN achieved the best accuracy, with a high capacity to adapt to the evolving data stream. © 2023,The scarcity of energy resources and global warming over the past few decades have prompted the widespread adoption of renewable energy sources. Among the potential renewable energy sources, solar energy has emerged as one of the most promising renewable energy sources. However, the uncertainty and fluctuations of solar power generation create negative impacts on the stability and reliability of the electric grid, planning of operation, economic feasibility, and investment. Therefore, accurate prediction of solar power generation is crucial to ensure the stability of the power grid and promote a large-scale investment in a solar energy system. A large number of research studies have been conducted on predicting solar power generation under different perspectives. However, no existing study analyses and predicts power generation of multi-solar energy sites by only one prediction model. The integration of multiple sites into one predictive model will reduce the number of required models for each site, thereby saving the computing resources and required calculation time. This paper proposes a novel methodology to group multiple solar sites and develop an integrated model by using a machine learning algorithm to predict power generation of each group. Firstly, the K-means clustering algorithm is utilized to cluster multiple solar sites which have similar power generation properties into one group. Then, a machine learning algorithm has been developed to predict power generation in a computationally fast and reliable manner. The proposed approach is verified by the real data of 223 solar sites in Taiwan. The experimental results show that the training time can be reduced by 93.2% without reducing the accuracy of the prediction model. Therefore, the cluster-based prediction approach gives better performance as compared with existing models. © The Author(s) 2023.,The challenges in applications of solar energy lies in its intermittency and dependency on meteorological parameters such as; solar radiation, ambient temperature, rainfall, wind-speed etc., and many other physical parameters like dust accumulation etc. Hence, it is important to estimate the amount of solar photovoltaic (PV) power generation for a specific geographical location. Machine learning (ML) models have gained importance and are widely used for prediction of solar power plant performance. In this paper, the impact of weather parameters on solar PV power generation is estimated by several Ensemble ML (EML) models like Bagging, Boosting, Stacking, and Voting for the first time. The performance of chosen ML algorithms is validated by field dataset of a 10kWp solar PV power plant in Eastern India region. Furthermore, a complete test-bed framework has been designed for data mining as well as to select appropriate learning models. It also supports feature selection and reduction for dataset to reduce space and time complexity of the learning models. The results demonstrate greater prediction accuracy of around 96% for Stacking and Voting EML models. The proposed work is a generalized one and can be very useful for predicting the performance of large-scale solar PV power plants also. © 2023 Elsevier Ltd"
97,96,45,96_Sign Language Recognition and Gesture Recognition,Sign Language Recognition and Gesture Recognition,"Gesture recognition has found versatile applications in natural human–computer interaction (HCI). Compared with traditional camera-based or wearable sensors-based solutions, gesture recognition using the millimeter wave (mmWave) radar has attracted growing attention for its characteristics of contact-free, privacy-preserving and less environment-dependence. Recently, most of studies adopted one of the Range Doppler Image (RDI), Range Angle Image (RAI), Doppler Angle Image (DAI) or Micro-Doppler Spectrogram extracted from the raw radar signal as the input of a deep neural network to realize gesture recognition. However, the effectiveness of these four inputs in gesture recognition has attracted little attention so far. Moreover, the lack of large amounts of labeled data restricts the performance of traditional supervised learning network. In this paper, we first conducted extensive experiments to compare the effectiveness of these four inputs in the gesture recognition, respectively. Then we proposed a semi-supervised leaning framework by utilizing few labeled data in the source domain and large amounts of unlabeled data in the target domain. Specially, we combine the ?-model and some specific data augmentation tricks on the mmWave signal to realize the domain-independent gesture recognition. Extensive experiments on a public mmWave gesture dataset demonstrate the superior effectiveness of the proposed system. © 2022 Elsevier Ltd,Developments in radio detection and ranging (radar) technology have made hand gesture recognition feasible. In heat map-based gesture recognition, feature images have a large size and require complex neural networks to extract information. Machine learning methods typically require large amounts of data and collecting hand gestures with radar is time- and energy-consuming. Therefore, a low computational complexity algorithm for hand gesture recognition based on a frequency-modulated continuous-wave (FMCW) radar and a synthetic hand gesture feature generator are proposed. In the low computational complexity algorithm, two-dimensional Fast Fourier Transform is implemented on the radar raw data to generate a range-Doppler matrix. After that, background modelling is applied to separate the dynamic object and the static background. Then a bin with the highest magnitude in the range-Doppler matrix is selected to locate the target and obtain its range and velocity. The bins at this location along the dimension of the antenna can be utilised to calculate the angle of the target using Fourier beam steering. In the synthetic generator, the Blender software is used to generate different hand gestures and trajectories and then the range, velocity and angle of targets are extracted directly from the trajectory. The experimental results demonstrate that the average recognition accuracy of the model on the test set can reach 89.13% when the synthetic data are used as the training set and the real data are used as the test set. This indicates that the generation of synthetic data can make a meaningful contribution in the pre-training phase. © 2022 by the authors.,In recent years, hand gesture recognition in human-computer interfaces is usually based on surface electromyography because the signals are non-intrusive and are not affected by the variations of light, position, and orientation of the hand. Deep learning algorithms have become increasingly more prominent in gesture recognition for the ability to automatically learn features from large amounts of data. However, delicate and complicated network structures brought by deep learning, which are elaborately designed for cross session tasks, need more computing time to be trained and tested, which can hardly be applied to the online system. In this study, an online electromyographic hand gesture recognition method using deep learning and transfer learning is proposed. The deep learning model includes a feature extractor, a label classifier, and a gesture predictor. The feature extractor is based on the temporal convolutional network, which is designed to learn high-level discriminant features from the input signals. The label classifier includes three fully connected layers, designed to classify hand gesture labels using the feature vector which is produced by the feature extractor. The gesture predictor uses a threshold voting algorithm to predict the gesture, used at the stage of testing to perform the online recognition. Transfer learning technique is used to transfer model parameters from one pre-trained model, which costs less time and can be applied for online applications. The proposed model is verified on both the Myo dataset and the public NinaPro database. The proposed transfer learning scheme is shown to systematically and significantly enhance the performance of the proposed model on the two datasets, only using no more than three sessions to retrain the label predictor can achieve the accuracy of more than 90% of that obtained though the normal training of the whole parts of the model using full training sessions. © 2023 Elsevier Ltd"
98,97,44,97_Arabic Handwritten Character Recognition and Document Layout Analysis,Arabic Handwritten Character Recognition and Document Layout Analysis,"Optical Character Recognition (OCR), helps to convert different types of scanned documents, such as images into searchable and editable content. OCR is language dependant and very limited research has been carried out in this field for Urdu and Urdu like scriptures (E.g. Farsi, Arabic, and Urdu) unlike other languages like English, Hindi, etc. The lack of research work is attributed to a lack of publically available benchmark databases and inherent complexities involved in these languages like cursive nature and change in the shape of a character depending upon its position in a ligature. Each character has 2–4 different shapes depending upon its position in the word; initial, medial, or final. In this article, the we have proposed a methodology to automate the data collection process and collected a large handwritten dataset of 110,785 Urdu characters and laid out the comaparative analysis of two deep learning models SimpleRNN and LSTM to showcase the potential of RNN models for chararacter recognition. Data was collected from 250 authors on the A4 size sheet. Each sheet contains 132 shapes for Urdu characters and 10 numerals. As far as the authors know, this is the first time that such a large dataset has been proposed which contains all the possible shapes of Urdu character numerals as well. Experimentation has been done for the numeral, full characters, and for whole data set separately to lay a comparative analysis of classification capabilities of RNN and LSTM models. Despite of such inherit complexities in Urdu script, the RNN and LSTM models proved to be more effective in achieving a high accuracy rates. Respective accuracy for RNN achieved for each category are: 96.96% for numerals, 85.22% for full characters and 73.62% for whole data and LSTM outperforms the prior one with max accuracy for each category of data as 97.80% for numerals, 97.43% for full characters and 91.30% for whole data. Besides, the proposed dataset opens a new window for future research, showcasing the huge potential of this dataset for data analysis not only for Urdu language but for other languages like Arabic, Persian,etc. which uses similar kind of character sets. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,With the development of society, the intangible cultural heritage of Chinese Nüshu is in danger of extinction. To promote the research and popularization of traditional Chinese culture, we use deep learning to automatically detect and recognize handwritten Nüshu characters. To address difficulties such as the creation of a Nüshu character dataset, uneven samples, and difficulties in character recognition, we first build a large-scale handwritten Nüshu character dataset, HWNS2023, by using various data augmentation methods. This dataset contains 5500 Nüshu images and 1364 labeled character samples. Second, in this paper, we propose a two-stage scheme model combining GoogLeNet-tiny and YOLOv5-CBAM (SGooTY) for Nüshu recognition. In the first stage, five basic deep learning models including AlexNet, VGGNet16, GoogLeNet, MobileNetV3, and ResNet are trained and tested on the dataset, and the model structure is improved to enhance the accuracy of recognising handwritten Nüshu characters. In the second stage, we combine an object detection model to re-recognize misidentified handwritten Nüshu characters to ensure the accuracy of the overall system. Experimental results show that in the first stage, the improved model achieves the highest accuracy of 99.3% in recognising Nüshu characters, which significantly improves the recognition rate of handwritten Nüshu characters. After integrating the object recognition model, the overall recognition accuracy of the model reached 99.9%. © 2023 by the authors.,Any optical character recognition (OCR) system recognizes each and every character present in any document image. But, the task of performing OCR in ancient handwritten document images is challenging due to the existence of faded text and dark spots in the ancient document images. The presence of intrinsic patterns of characters and large number of character classes in most of the Indian scripts make this task even more challenging. This research article proposes a novel hybrid deep learning based OCR method to recognize each character present in ancient handwritten document image written in two different Indian scripts, Devanagari and Maithili. Various discriminating features have been extracted from each character present in the document using several convolutional layers and each extracted feature vector has been classified into a proper character class using hybrid deep learning model. The hybrid deep learning model consists of one dense layer and several recurrently connected hidden layers. Both long-short-term-memory (LSTM) and Bidirectional-long-short-term-memory (Bi-LSTM) variants of recurrent neural network (RNN) have been employed in the portion of recurrently connected hidden layers of hybrid deep learning model. The performance of the proposed OCR method has been evaluated on two self-generated datasets of ancient handwritten document images in Devanagari and Maithili scripts. The proposed method has achieved the character recognition accuracy of 96.97 percent and 95.83 percent in Devanagari and Maithili scripts respectively. The experimental results demonstrate that the proposed OCR method outperforms the state-of-the-art methods in this regard. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
99,98,44,98_Alzheimer's Disease Diagnosis and Classification with Neuroimaging,Alzheimer's Disease Diagnosis and Classification with Neuroimaging,"Alzheimer’s disease (AD) is a neurodegenerative disorder characterized by cognitive impairment and aberrant protein deposition in the brain. Therefore, the early detection of AD is crucial for the development of effective treatments and interventions, as the disease is more responsive to treatment in its early stages. It is worth mentioning that deep learning techniques have been successfully applied in recent years to a wide range of medical imaging tasks, including the detection of AD. These techniques have the ability to automatically learn and extract features from large datasets, making them well suited for the analysis of complex medical images. In this paper, we propose an improved lightweight deep learning model for the accurate detection of AD from magnetic resonance imaging (MRI) images. Our proposed model achieves high detection performance without the need for deeper layers and eliminates the use of traditional methods such as feature extraction and classification by combining them all into one stage. Furthermore, our proposed method consists of only seven layers, making the system less complex than other previous deep models and less time-consuming to process. We evaluate our proposed model using a publicly available Kaggle dataset, which contains a large number of records in a small dataset size of only 36 Megabytes. Our model achieved an overall accuracy of 99.22% for binary classification and 95.93% for multi-classification tasks, which outperformed other previous models. Our study is the first to combine all methods used in the publicly available Kaggle dataset for AD detection, enabling researchers to work on a dataset with new challenges. Our findings show the effectiveness of our lightweight deep learning framework to achieve high accuracy in the classification of AD. © 2023 by the authors.,Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that affects millions of people worldwide. Early and accurate prediction of AD progression is crucial for early intervention and personalized treatment planning. Although AD does not yet have a reliable therapy, several medications help slow down the disease’s progression. However, more study is still needed to develop reliable methods for detecting AD and its phases. In the recent past, biomarkers associated with AD have been identified using neuroimaging methods. To uncover biomarkers, deep learning techniques have quickly emerged as a crucial methodology. A functional molecular imaging technique known as fluorodeoxyglucose positron emission tomography (18F-FDG-PET) has been shown to be effective in assisting researchers in understanding the morphological and neurological alterations to the brain associated with AD. Convolutional neural networks (CNNs) have also long dominated the field of AD progression and have been the subject of substantial research, while more recent approaches like vision transformers (ViT) have not yet been fully investigated. In this paper, we present a self-supervised learning (SSL) method to automatically acquire meaningful AD characteristics using the ViT architecture by pretraining the feature extractor using the self-distillation with no labels (DINO) and extreme learning machine (ELM) as classifier models. In this work, we examined a technique for predicting mild cognitive impairment (MCI) to AD utilizing an SSL model which learns powerful representations from unlabeled 18F-FDG PET images, thus reducing the need for large-labeled datasets. In comparison to several earlier approaches, our strategy showed state-of-the-art classification performance in terms of accuracy (92.31%), specificity (90.21%), and sensitivity (95.50%). Then, to make the suggested model easier to understand, we highlighted the brain regions that significantly influence the prediction of MCI development. Our methods offer a precise and efficient strategy for predicting the transition from MCI to AD. In conclusion, this research presents a novel Explainable SSL-ViT model that can accurately predict AD progress based on 18F-FDG PET scans. SSL, attention, and ELM mechanisms are integrated into the model to make it more predictive and interpretable. Future research will enable the development of viable treatments for neurodegenerative disorders by combining brain areas contributing to projection with observed anatomical traits. © 2023 by the authors.,Alzheimer’s disease (AD) is a chronic and common form of dementia that mainly affects elderly individuals. The disease is dangerous because it causes damage to brain cells and tissues before the symptoms appear, and there is no medicinal or surgical treatment available yet for AD. AD causes loss of memory and functionality control in multiple degrees according to AD’s progression level. However, early diagnosis of AD can hinder its progression. Brain imaging tools such as magnetic resonance imaging (MRI), computed tomography (CT) scans, positron emission tomography (PET), etc. can help in medical diagnosis of AD. Recently, computer-aided diagnosis (CAD) such as deep learning applied to brain images obtained with these tools, has been an established strategic methodology that is widely used for clinical assistance in prognosis of AD. In this study, we proposed an intelligent methodology for building a convolutional neural network (CNN) from scratch to detect AD stages from the brain MRI images dataset and to improve patient care. It is worth mentioning that training a deep-learning model requires a large amount of data to produce accurate results and prevent the model from overfitting problems. Therefore, for better understanding of classifiers and to overcome the model overfitting problem, we applied data augmentation to the minority classes in order to increase the number of MRI images in the dataset. All experiments were conducted using Alzheimer’s MRI dataset consisting of brain MRI scanned images. The performance of the proposed model determines detection of the four stages of AD. Experimental results show high performance of the proposed model in that the model achieved a 99.38% accuracy rate, which is the highest so far. Moreover, the proposed model performance in terms of accuracy, precision, sensitivity, specificity, and f-measures is promising when compared to the very recent state-of-the-art domain-specific models existing in the literature. © 2023 Tech Science Press. All rights reserved."
100,99,44,99_Human Gut Microbiome and Disease Outcomes,Human Gut Microbiome and Disease Outcomes,"Background: The gut-lung axis is generally recognized, but there are few large studies of the gut microbiome and incident respiratory disease in adults. Objective: We sought to investigate the association and predictive capacity of the gut microbiome for incident asthma and chronic obstructive pulmonary disease (COPD). Methods: Shallow metagenomic sequencing was performed for stool samples from a prospective, population-based cohort (FINRISK02; N = 7115 adults) with linked national administrative health register–derived classifications for incident asthma and COPD up to 15 years after baseline. Generalized linear models and Cox regressions were used to assess associations of microbial taxa and diversity with disease occurrence. Predictive models were constructed using machine learning with extreme gradient boosting. Models considered taxa abundances individually and in combination with other risk factors, including sex, age, body mass index, and smoking status. Results: A total of 695 and 392 statistically significant associations were found between baseline taxonomic groups and incident asthma and COPD, respectively. Gradient boosting decision trees of baseline gut microbiome abundance predicted incident asthma and COPD in the validation data sets with mean area under the curves of 0.608 and 0.780, respectively. Cox analysis showed that the baseline gut microbiome achieved higher predictive performance than individual conventional risk factors, with C-indices of 0.623 for asthma and 0.817 for COPD. The integration of the gut microbiome and conventional risk factors further improved prediction capacities. Conclusions: The gut microbiome is a significant risk factor for incident asthma and incident COPD and is largely independent of conventional risk factors. © 2023 The Authors,Background: A growing body of evidence suggests that the gut microbiota is strongly linked to general human health. Microbiome-directed interventions, such as diet and exercise, are acknowledged as a viable and achievable strategy for preventing disorders and improving human health. However, due to the significant inter-individual diversity of the gut microbiota between subjects, lifestyle recommendations are expected to have distinct and highly variable impacts to the microbiome structure. Results: Here, through a large-scale meta-analysis including 1448 shotgun metagenomics samples obtained longitudinally from 396 individuals during lifestyle studies, we revealed Bacteroides stercoris, Prevotella copri, and Bacteroides vulgatus as biomarkers of microbiota’s resistance to structural changes, and aromatic and non-aromatic amino acid biosynthesis as important regulator of microbiome dynamics. We established criteria for distinguishing between significant compositional changes from normal microbiota fluctuation and classified individuals based on their level of response. We further developed a machine learning model for predicting “responders” and “non-responders” independently of the type of intervention with an area under the curve of up to 0.86 in external validation cohorts of different ethnicities. Conclusions: We propose here that microbiome-based stratification is possible for identifying individuals with highly plastic or highly resistant microbial structures. Identifying subjects that will not respond to generalized lifestyle therapeutic interventions targeting the restructuring of gut microbiota is important to ensure that primary end-points of clinical studies are reached. [MediaObject not available: see fulltext.]. © 2023, BioMed Central Ltd., part of Springer Nature.,Research on microecology has been carried out with broad perspectives in recent decades, which has enabled a better understanding of the gut microbiota and its roles in human health and disease. It is of great significance to routinely acquire the status of the human gut microbiota; however, there is no method to evaluate the gut microbiome through small amounts of fecal microbes. In this study, we found ten predominant groups of gut bacteria that characterized the whole microbiome in the human gut from a large-sample Chinese cohort, constructed a real-time quantitative polymerase chain reaction (qPCR) method and developed a set of analytical approaches to detect these ten groups of predominant gut bacterial species with great maneuverability, efficiency, and quantitative features. Reference ranges for the ten predominant gut bacterial groups were established, and we found that the concentration and pairwise ratios of the ten predominant gut bacterial groups varied with age, indicating gut microbial dysbiosis. By comparing the detection results of liver cirrhosis (LC) patients with those of healthy control subjects, differences were then analyzed, and a classification model for the two groups was built by machine learning. Among the six established classification models, the model established by using the random forest algorithm achieved the highest area under the curve (AUC) value and sensitivity for predicting LC. This research enables easy, rapid, stable, and reliable testing and evaluation of the balance of the gut microbiota in the human body, which may contribute to clinical work. © 2023 Chinese Academy of Engineering"
101,100,44,100_Sheep and Pig Recognition in Smart Farming,Sheep and Pig Recognition in Smart Farming,"Motion and aggressive behaviors in pigs provide important information for the study of social hierarchies in pigs and can be used as a selection indicator for pig health and aggression parameters. However, relying only on visual observation or surveillance video to record the number of aggressive acts is time-consuming, labor-intensive, and lasts for only a short period of time. Manual observation is too short compared to the growth cycle of pigs, and complete recording is impractical in large farms. In addition, due to the complex process of assessing the intensity of pig aggression, manual recording is highly influenced by human subjective vision. In order to efficiently record pig motion and aggressive behaviors as parameters for breeding selection and behavioral studies, the videos and pictures were collected from typical commercial farms, with each unit including 8~20 pigs in 7~25 m2 space; they were bred in stable social groups and a video was set up to record the whole day’s activities. We proposed a deep learning-based recognition method for detecting and recognizing the movement and aggressive behaviors of pigs by recording and annotating head-to-head tapping, head-to-body tapping, neck biting, body biting, and ear biting during fighting. The method uses an improved EMA-YOLOv8 model and a target tracking algorithm to assign a unique digital identity code to each pig, while efficiently recognizing and recording pig motion and aggressive behaviors and tracking them, thus providing statistics on the speed and duration of pig motion. On the test dataset, the average precision of the model was 96.4%, indicating that the model has high accuracy in detecting a pig’s identity and its fighting behaviors. The model detection results were highly correlated with the manual recording results (R2 of 0.9804 and 0.9856, respectively), indicating that the method has high accuracy and effectiveness. In summary, the method realized the detection and identification of motion duration and aggressive behavior of pigs under natural conditions, and provided reliable data and technical support for the study of the social hierarchy of pigs and the selection of pig health and aggression phenotypes. © 2023 by the authors.,In modern dairy farms, accurate and reliable identification of each individual cow is of great significance for precision livestock farming. Individual cow identification is the basis for applications such as disease detection, automatic behaviour analysis, intelligent milking, and individual counting and is crucial for improving the welfare and breeding efficiency of dairy cows. Computer vision-based method is a low-cost, non-contact, automatic, and efficient way. To improve the accuracy and efficiency of cow recognition in different large-scale dairy farms, we proposed a BottleNet Transformer (BoTNet) model based on Graph Sampling and Counterfactual Attention Learning for cow surveillance videos. First, we replace the 3 × 3 spatial convolution with Multi-Head Attention in the final three bottleneck blocks of the ResNet. The BoT block module combines attention mechanisms and residual connection to enhance the global representation of cow images, which in turn better captures the features of the cow's back pattern region and ignores the influence of irrelevant information, such as the background of the dairy barn. Subsequently, counterfactual learning measures the quality of attention by comparing the difference between the generated output and the true label. The difference can be used to enhance the causal relationship between prediction results and cow feature attention, allowing the model to obtain more comprehensive cow appearance features. Finally, we added a Graph Sampling module before the feature extraction phase to produce small batches of samples for training. The GS sampler improves the learning efficiency while reducing the memory and computation consumption compared with the usual adopted PK sampling. We conducted comparison experiments on the public dataset Dataset1, and the experimental results reveal that the Rank-1, Rank-5, and mAP values of this study's method are 4%, 3.2%, and 5.3% higher than the optimal results, respectively, when compared with the existing state-of-the-art methods for animal individual recognition. In particular, we construct a challenging dataset by intercepting individual cow images from videos in the public dataset of farms. Experimental results indicate that the proposed method has better generalization performance. © 2024 Elsevier B.V.,Behavior is one of the important factors reflecting the health status of dairy cows, and when dairy cows encounter health problems, they exhibit different behavioral characteristics. Therefore, identifying dairy cow behavior not only helps in assessing their physiological health and disease treatment but also improves cow welfare, which is very important for the development of animal husbandry. The method of relying on human eyes to observe the behavior of dairy cows has problems such as high labor costs, high labor intensity, and high fatigue rates. Therefore, it is necessary to explore more effective technical means to identify cow behaviors more quickly and accurately and improve the intelligence level of dairy cow farming. Automatic recognition of dairy cow behavior has become a key technology for diagnosing dairy cow diseases, improving farm economic benefits and reducing animal elimination rates. Recently, deep learning for automated dairy cow behavior identification has become a research focus. However, in complex farming environments, dairy cow behaviors are characterized by multiscale features due to large scenes and long data collection distances. Traditional behavior recognition models cannot accurately recognize similar behavior features of dairy cows, such as those with similar visual characteristics, i.e., standing and walking. The behavior recognition method based on 3D convolution solves the problem of small visual feature differences in behavior recognition. However, due to the large number of model parameters, long inference time, and simple data background, it cannot meet the demand for real-time recognition of dairy cow behaviors in complex breeding environments. To address this, we developed an effective yet lightweight model for fast and accurate dairy cow behavior feature learning from video data. We focused on four common behaviors: standing, walking, lying, and mounting. We recorded videos of dairy cow behaviors at a dairy farm containing over one hundred cows using surveillance cameras. A robust model was built using a complex background dataset. We proposed a two-pathway X3DFast model based on spatiotemporal behavior features. The X3D and fast pathways were laterally connected to integrate spatial and temporal features. The X3D pathway extracted spatial features. The fast pathway with R(2 + 1)D convolution decomposed spatiotemporal features and transferred effective spatial features to the X3D pathway. An action model further enhanced X3D spatial modeling. Experiments showed that X3DFast achieved 98.49% top-1 accuracy, outperforming similar methods in identifying the four behaviors. The method we proposed can effectively identify similar dairy cow behaviors while improving inference speed, providing technical support for subsequent dairy cow behavior recognition and daily behavior statistics. © 2023, The Author(s)."
102,101,44,101_Fish and Coral Monitoring with Underwater Cameras and Deep Learning,Fish and Coral Monitoring with Underwater Cameras and Deep Learning,"Accurate fish individual recognition is one of the critical technologies for large-scale fishery farming when trying to achieve accurate, green farming and sustainable development. It is an essential link for aquaculture to move toward automation and intelligence. However, existing fish individual data collection methods cannot cope with the interference of light, blur, and pose in the natural underwater environment, which makes the captured fish individual images of poor quality. These low-quality images can cause significant interference with the training of recognition networks. In order to solve the above problems, this paper proposes an underwater fish individual recognition method (FishFace) that combines data quality assessment and loss weighting. First, we introduce the Gem pooing and quality evaluation module, which is based on EfficientNet. This module is an improved fish recognition network that can evaluate the quality of fish images well, and it does not need additional labels; second, we propose a new loss function, FishFace Loss, which will weigh the loss according to the quality of the image so that the model focuses more on recognizable fish images, and less on images that are difficult to recognize. Finally, we collect a dataset for fish individual recognition (WideFish), which contains and annotates 5000 images of 300 fish. The experimental results show that, compared with the state-of-the-art individual recognition methods, Rank1 accuracy is improved by 2.60% and 3.12% on the public dataset DlouFish and the proposed WideFish dataset, respectively. © 2023 by the authors.,The classification of underwater fish species holds significant importance for fisheries management. Nevertheless, existing deep fish classification models require high computational resources, which hamper their deployment on underwater devices. Additionally, the complex underwater environment, the camouflaged appearance of fish, and the similarity among fish species pose challenges to the accuracy of lightweight fish classification models. To address the above issues, this paper proposes a novel two-tier knowledge distillation (T-KD) method to improve the accuracy and reduce the parameters of the underwater fish species classification models. Specifically, the T-KD involves the following key steps. Firstly, a new fish species dataset, Fish37, is constructed to augment the diversity of fish species present in existing datasets. Subsequently, we introduce a novel interlayer mapping similarity-preserving (IMSP), to facilitate the learning of richer discriminative features by capturing the mapping relationships between teacher and student network layers. Moreover, a new layer tail response (LTR) is proposed to mimic the predictions of the teacher network, efficiently improving classification performance and generalization capability. The proposed T-KD approach demonstrates remarkable performance in fish species classification, surpassing that of well-known lightweight models. The effectiveness of T-KD is extensively validated across various network depths, including ResNet and EfficientNet, and compared to other knowledge distillation methods like KD, PKT, RKD, and SP. Notably, T-KD outperforms MobileNetv3-large and obtains an impressive Top-1 accuracy of 97.20% on the Fish37 dataset only using about 1/15 model size of Vision Transformer. Furthermore, detailed generalization experiments are conducted to assess T-KD’s performance on popular benchmark datasets, such as A_Large_Scale_Fish_Dataset, Fish4knowledge, WildFish and WildFish++. In conclusion, the results indicate the potential of the T-KD to facilitate underwater fish species classification with limited computational resources on underwater devices. This research also opens up promising avenues for the practical implementation of lightweight fish classification models. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.,Fish assessment and monitoring are important for the development of a modern aquatic ecosystem. Fish are a vital part of the marine and freshwater environments. Morphological and computational details of fish, such as size, shape, and position, are important in fish observation and fisheries. Typically, manual, or low-efficient techniques are used to acquire fish details. However, existing typical methods are usually time-consuming, less accurate, and resource-intensive. Computer-aided methods are crucial for intelligent and automatic fish assessment. Two novel networks, namely parallel feature fusion-based segmentation network (PFFS-Net) and progressive information fusion-based segmentation network (PIFS-Net), were developed for pixel-wise fish segmentation. PFFS-Net is a base network that uses parallel feature fusion to achieve a better segmentation performance. PIFS-Net is the final model of this work and uses a progressive spatial feature fusion (SFF) mechanism to enhance segmentation accuracy. PIFS-Net also employs rapid feature reduction and pre-prediction low-level information fusion blocks to further boost performance. The proposed models were evaluated using the following three publicly available databases: semantic segmentation of underwater imagery (SUIM), DeepFish, and Large-scale fish. The proposed networks outperformed the state-of-the-art methods in challenging underwater conditions with superior computational efficiency. PIFS-Net needs only 2.02 million trainable parameters for its complete training. Automatic and accurate fish segmentation can be a major step towards an intelligent aquatic ecosystem. The codes of our algorithms and trained models are available on Github. © 2023 The Author(s)"
103,102,43,102_Detection of Fungal Spores using Raman Spectroscopy and Deep Learning,Detection of Fungal Spores using Raman Spectroscopy and Deep Learning,"Deep learning techniques provide powerful solutions to several pattern-recognition problems, including Raman spectral classification. However, these networks require large amounts of labeled data to perform well. Labeled data, which are typically obtained in a laboratory, can potentially be alleviated by data augmentation. This study investigated various data augmentation techniques and applied multiple deep learning methods to Raman spectral classification. Raman spectra yield fingerprint-like information about chemical compositions, but are prone to noise when the particles of the material are small. Five augmentation models were investigated to build robust deep learning classifiers: weighted sums of spectral signals, imitated chemical backgrounds, extended multiplicative signal augmentation, and generated Gaussian and Poisson-distributed noise. We compared the performance of nine state-of-the-art convolutional neural networks with all the augmentation techniques. The LeNet5 models with background noise augmentation yielded the highest accuracy when tested on real-world Raman spectral classification at 88.33% accuracy. A class activation map of the model was generated to provide a qualitative observation of the results. © The Korea Institute of Information and Communication Engineering,The overuse of plastics releases large amounts of microplastics. These tiny and complex pollutants may cause immeasurable damage to human social life. Raman spectroscopy detection technology is widely used in the detection, identification and analysis of microplastics due to its advantages of fast speed, high sensitivity and non-destructive. In this work, we first recorded the Raman spectra of eight common plastics in daily life. By adjusting parameters such as laser wavelength, laser power, and acquisition time, the Raman data under different acquisition conditions were diversified, and the corresponding Raman spectra were obtained, and a database of eight household plastics was established. Combined with deep learning algorithms, an accurate, fast and simple classification and identification method for 8 types of plastics is established. Firstly, the acquired spectral data were preprocessed for baseline correction and noise reduction, Then, four machine learning algorithms, linear discriminant analysis (LDA), decision tree, support vector machine (SVM) and one-dimensional convolutional neural network (1D-CNN), are used to classify and identify the preprocessed data. The results showed that the classification accuracy of the three machine learning models for the Raman spectra of standard plastic samples were 84%, 93% and 93% respectively. The 1D-CNN model has an accuracy rate of up to 97% for Raman spectroscopy. Our study shows that the combination of Raman spectroscopy detection techniques and deep learning algorithms is a very valuable approach for microplastic classification and identification. © 2024 Elsevier B.V.,Rapid and early identification of pathogens is critical to guide antibiotic therapy. Raman spectroscopy as a noninvasive diagnostic technique provides rapid and accurate detection of pathogens. Raman spectrum of single cells serves as the “fingerprint” of the cell, revealing its metabolic characteristics. Rapid identification of pathogens can be achieved by combining Raman spectroscopy and deep learning. Traditional classification techniques frequently require lots of data for training, which is time costing to collect Raman spectra. For trace samples and strains that are difficult to culture, it is difficult to provide an accurate classification model. In order to reduce the number of samples collected and improve the accuracy of the classification model, a new pathogen detection method integrating Raman spectroscopy, variational auto-encoder (VAE), and long short-term memory network (LSTM) is proposed in this paper. We collect the Raman signals of pathogens and input them to VAE for training. VAE will generate a large number of Raman spectral data that cannot be distinguished from the real spectrum, and the signal-to-noise ratio is higher than that of the real spectrum. These spectra are input into the LSTM together with the real spectrum for training, and a good classification model is obtained. The results of the experiments reveal that this method not only improves the average accuracy of pathogen classification to 96.9% but also reduces the number of Raman spectra collected from 1000 to 200. With this technology, the number of Raman spectra collected can be greatly reduced, so that strains that are difficult to culture or trace can be rapidly identified. © 2022 Wiley-VCH GmbH."
104,103,43,103_Noisy Labels in Semi-supervised Learning,Noisy Labels in Semi-supervised Learning,"The success of deep learning is mainly dependent on large-scale and accurately labeled datasets. However, real-world datasets are marked with much noise. Directly training on datasets with label noise may lead to the overfitting. Recent research is under the spotlight on how to design algorithms that can learn robust models from noisy datasets, via designing the loss function and integrating the idea of Semi-supervised learning (SSL). This paper proposes a robust algorithm for learning with label noise that does not require additional clean data and an auxiliary model. Specifically, on the one hand, Jensen–Shannon (JS) divergence is introduced as a component of the loss function, which measures the distance between the predicted distribution and the noisy label distribution. It can alleviate the overfitting problem caused by the traditional cross entropy loss theoretically and experimentally. On the other hand, a dynamic sample selection mechanism is also proposed. The dataset is divided into the pseudo-clean labeled subset and the pseudo-noisy labeled subset. Two subsets are treated differently to exploit prior information about the data, and then learned by SSL. The dynamic sample selection is performed with the iteration between two subsets and the model parameters, which are different from the conventional training. Considering the label of the pseudo-clean labeled subset is not correct entirely, they are also refined by linear interpolation. Furthermore, we experimentally show that the integration of SSL helps the model divide two subsets more precise and build decision boundaries more explicit. Extensive experimental results on corrupted data from benchmark datasets and the real-world dataset, including CIFAR-10, CIFAR-100, and Clothing1M, demonstrate that our method is superior to many state-of-the-art approaches for learning with label noise. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Deep neural networks have achieved significant success in the artificial intelligence community and various downstream tasks. They encode images or texts into dense feature representations and are supervised by a large amount of labeled data. Due to the expensiveness of high-quality labeled data, a huge number of easy-to-access instances are collected to conduct supervised learning. However, they have not been annotated by experts and thus can contain numerous noisy instances, which will degrade the performance. To learn robust feature representations despite misleading noisy labels, we employ supervised contrastive learning to directly perform supervision in the hidden space, rather than in the prediction space like the prevalent cross-entropy loss function. However, cutting-edge noisy label learning methods with supervised contrastive learning always discard the data considered to be noisy, and thus cannot tolerate high-ratio noisy datasets. Therefore, we propose a novel training strategy named Supervised Contrastive Learning with Corrected Labels (Scl 2) to defend against the attack of noisy labels. Scl 2 corrects the noisy labels with an empirical small-loss assumption and conducts supervised contrastive learning using these corrected data. Specifically, we employ the generated soft labels as supervisory information to facilitate our implementation of supervised contrastive learning. This expansion of contrastive learning ensures the integrity of the supervisory information while effectively enhancing the learning process. In addition, samples sharing the same soft labels are treated as positive sample pairs, while those with different soft labels are considered to be negative sample pairs. With this strategy, the representations from neural networks keep the local discrimination in one mini-batch. Besides, we also employ a prototype contrastive learning technique to ensure global discrimination. Our Scl 2 has demonstrated excellent performance on numerous benchmark datasets, showcasing its effectiveness in various standardized evaluation scenarios. Additionally, our model has proven to be highly valuable when applied to real-world noisy datasets. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Deep neural networks (DNNs) require large amounts of labeled data for model training. However, label noise is a common problem in datasets due to the difficulty of classification and high cost of labeling processes. Introducing the concepts of curriculum learning and progressive learning, this paper presents a novel solution that is able to handle massive noisy labels and improve model generalization ability. It proposes a new network model training strategy that considers mislabeled samples directly in the network training process. The new learning curriculum is designed to measures the complexity of the data with their distribution density in a feature space. The sample data in each category are then divided into easy-to-classify (clean samples), relatively easy-to-classify, and hard-to-classify (noisy samples) subsets according to the smallest intra-class local density with each cluster. On this basis, DNNs are trained progressively in three stages, from easy to hard, i.e., from clean to noisy samples. The experimental results demonstrate that the accuracy of image classification can be improved through data augmentation, and the classification accuracy of the proposed method is clearly higher than that of standard Inception_v2 for the NEU dataset after data augmentation, when the proportion of noisy labels in the training set does not exceed 60%. With 50% noisy labels in the training set, the classification accuracy of the proposed method outperformed recent state-of-the-art label noise learning methods, CleanNet and MentorNet. The proposed method also performed well in practical applications, where the number of noisy labels was uncertain and unevenly distributed. In this case, the proposed method not only can alleviate the adverse effects of noisy labels, but it can also improve the generalization ability of standard deep networks and their overall capability. © 2022 by the authors."
105,104,43,104_Mammal and Bird Monitoring using Camera Traps and Deep Learning,Mammal and Bird Monitoring using Camera Traps and Deep Learning,"Computer vision has found many applications in automatic wildlife data analytics and biodiversity monitoring. Automating tasks like animal recognition or animal detection usually require machine learning models (e.g., deep neural networks) trained on annotated datasets. However, image datasets built for general purposes fail to capture realistic conditions of ecological studies, and existing datasets collected with camera-traps mainly focus on medium to large-sized animals. There is a lack of annotated small-sized animal datasets in the field. Small-sized animals (e.g., small mammals, frogs, lizards, arthropods) play an important role in ecosystems but are difficult to capture on camera-traps. They also present additional challenges: small animals can be more difficult to identify and blend more easily with their surroundings. To fill this gap, we introduce in this paper a new dataset dedicated to ecological studies of small-sized animals, and provide benchmark results of computer vision-based wildlife monitoring. The novelty of our work lies on SAWIT (small-sized animal wild image dataset), the first real-world dataset of small-sized animals, collected from camera traps and in realistic conditions. Our dataset consists of 34,434 images and is annotated by experts in the field with object-level annotations (bounding boxes) providing 34,820 annotated animals for seven animal categories. The dataset encompasses a wide range of challenging scenarios, such as occlusions, blurriness, and instances where animals blend into the dense vegetation. Based on the dataset, we benchmark two prevailing object detection algorithms: Faster RCNN and YOLO, and their variants. Experimental results show that all the variants of YOLO (version 5) perform similarly, ranging from 59.3% to 62.6% for the overall mean Average Precision (mAP) across all the animal categories. Faster RCNN with ResNet50 and HRNet backbone achieve 61.7% mAP and 58.5% mAP respectively. Through experiments, we indicate challenges and suggest research directions for computer vision-based wildlife monitoring. We provide both the dataset and the animal detection code at https://github.com/dtnguyen0304/sawit . © 2023, The Author(s).,As the capacity to collect and store large amounts of data expands, identifying and evaluating strategies to efficiently convert raw data into meaningful information is increasingly necessary. Across disciplines, this data processing task has become a significant challenge, delaying progress and actionable insights. In ecology, the growing use of camera traps (i.e., remotely triggered cameras) to collect information on wildlife has led to an enormous volume of raw data (i.e., images) in need of review and annotation. To expedite camera trap image processing, many have turned to the field of artificial intelligence (AI) and use machine learning models to automate tasks such as detecting and classifying wildlife in images. To contribute understanding of the utility of AI tools for processing wildlife camera trap images, we evaluated the performance of a state-of-the-art computer vision model developed by Microsoft AI for Earth named MegaDetector using data from an ongoing camera trap study in Arctic Alaska, USA. Compared to image labels determined by manual human review, we found MegaDetector reliably determined the presence or absence of wildlife in images generated by motion detection camera settings (?94.6% accuracy), however, performance was substantially poorer for images collected with time-lapse camera settings (?61.6% accuracy). By examining time-lapse images where MegaDetector failed to detect wildlife, we gained practical insights into animal size and distance detection limits and discuss how those may impact the performance of MegaDetector in other systems. We anticipate our findings will stimulate critical thinking about the tradeoffs of using automated AI tools or manual human review to process camera trap images and help to inform effective implementation of study designs. © 2022,As human activities in natural areas increase, understanding human–wildlife interactions is crucial. Big data approaches, like large-scale camera trap studies, are becoming more relevant for studying these interactions. In addition, open-source object detection models are rapidly improving and have great potential to enhance the image processing of camera trap data from human and wildlife activities. In this study, we evaluate the performance of the open-source object detection model MegaDetector in cross-regional monitoring using camera traps. The performance at detecting and counting humans, animals and vehicles is evaluated by comparing the detection results with manual classifications of more than 300 000 camera trap images from three study regions. Moreover, we investigate structural patterns of misclassification and evaluate the results of the detection model for typical temporal analyses conducted in ecological research. Overall, the accuracy of the detection model was very high with 96.0% accuracy for animals, 93.8% for persons and 99.3% for vehicles. Results reveal systematic patterns in misclassifications that can be automatically identified and removed. In addition, we show that the detection model can be readily used to count people and animals on images with underestimating persons by ?0.05, vehicles by ?0.01 and animals by ?0.01 counts per image. Most importantly, the temporal pattern in a long-term time series of manually classified human and wildlife activities was highly correlated with classification results of the detection model (Pearson's r = 0.996, p < 0.001) and diurnal kernel densities of activities were almost equivalent for manual and automated classification. The results thus prove the overall applicability of the detection model in the image classification process of cross-regional camera trap studies without further manual intervention. Besides the great acceleration in processing speed, the model is also suitable for long-term monitoring and allows reproducibility in scientific studies while complying with privacy regulations. © 2023 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London."
106,105,42,105_Malware detection and classification for Android-based apps,Malware detection and classification for Android-based apps,"The popularity of the Android platform has led to an explosion in malware. The current research on Android malware mainly focuses on malware detection or malware family classification. These studies need to extract a large number of features, which consumes a lot of manpower and material resources. Moreover, some malware use obfuscation to evade decompiler tools extracting features. To address these problems, we propose ImageDroid, a method based on the image format of Android applications that can not only detect and classify malware without prior knowledge but also detect the obfuscated malware. Furthermore, we utilize the Grad-CAM interpretable mechanism of the deep learning model to automatically label the image that play a key role in determining maliciousness in a visual way. We evaluate ImageDroid over 10,000 Android applications. Experimental results show that the accuracy of malicious detection and multifamily classification achieve 97.2% and 95.1%, respectively, and the detection accuracy of obfuscated malware achieves 94.6%.  © 2023 Pengfei Liu et al.,To accurately find malware in a large number of mobile APPs, and determine which family it belongs to is one of the most important challenges in Android malware detection. Existed research focuses on using the extracted features to distinguish Android malicious APPs, and less attention is paid to the category and family classification of Android malware. Meanwhile, feature selection has always been a choose-difficult issue in malware detection with machine learning methods. In this paper, SelAttConvLstm was designed to classify android malware by category and family without manually selecting features. To identify Android malware, we first convert all the network traffic flows into grayscale images according to chronological order through data preprocessing. Second, we design SelAttConvLstm, a deep learning model to detect malicious Android APPs with network flows images. This model can consider both the spatial and temporal features of network flow at the same time. In addition, to improve the performance of the model, self-attention weights are added to focus on different features of the input. Finally, comprehensive experiments are conducted to verify the effectiveness of the detection model. Experimental results showed that our method can not only effectively detect malware, but also classify malware in detail and accurately by category and family. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The number of complex and novel malware attacks is increasing exponentially in the cyberworld. Malware detection systems are facing new challenges due to the volume, velocity, and complexity of malware. The current malware detection system relies on a time-consuming, resource-intensive, and knowledge-intensive classification approach. Most of the existing malware detection system is ineffective in detecting novel malware attacks. A deep learning approach can be used to build a malware detection system that can effectively detect novel malware attacks without much human intervention. The current circumstance necessitates not just a malware system with excellent accuracy, but also one that can serve a large volume of demand in near real-time. A scalable malware detection system capable of detecting complex attacks is the need of time. This article discusses a scalable and distributed deep learning approach for malware detection using convolutional neural network and bidirectional long short-term memory (CNN-BiLSTM). The deep learning approach has been used to make the system learn and make predictive decisions without human intervention. The performance of the deep learning approach depends on various parameters and training data sets. Hence, different combinations of deep learning algorithms have been used to design and test the models to achieve the desired result. The experimental results show that the double layer of CNN and BiLSTM has better performance than single-layer CNN. © 2022 Taylor & Francis Group, LLC."
107,106,42,106_Gait analysis and recognition,Gait analysis and recognition,"Walking ability of broilers can be improved by selective breeding, but large-scale phenotypic records are required. Currently, gait of individual broilers is scored by trained experts, however, precision phenotyping tools could offer a more objective and high-throughput alternative. We studied whether specific walking characteristics determined through pose estimation are linked to gait in broilers. We filmed male broilers from behind, walking through a 3 m × 0.4 m (length × width) corridor one by one, at 3 time points during their lifetime (at 14, 21, and 33 d of age). We used a deep learning model, developed in DeepLabCut, to detect and track 8 keypoints (head, neck, left and right knees, hocks, and feet) of broilers in the recorded videos. Using the keypoints of the legs, 6 pose features were quantified during the double support phase of walking, and 1 pose feature was quantified during steps, at maximum leg lift. Gait was scored on a scale from 0 to 5 by 4 experts, using the videos recorded on d 33, and the broilers were further classified as having either good gait (mean gait score ?2) or suboptimal gait (mean gait score >2). The relationship of pose features on d 33 with gait was analyzed using the data of 84 broilers (good gait: 57.1%, suboptimal gait: 42.9%). Birds with suboptimal gait had sharper hock joint lateral angles and lower hock-feet distance ratios during double support on d 33, on average. During steps, relative step height was lower in birds with suboptimal gait. Step height and hock-feet distance ratio showed the largest mean deviations in broilers with suboptimal gait compared to those with good gait. We demonstrate that pose estimation can be used to assess walking characteristics during a large part of the productive life of broilers, and to phenotype and monitor broiler gait. These insights can be used to understand differences in the walking patterns of lame broilers, and to build more sophisticated gait prediction models. © 2023 The Authors,Walking speed provides a good proxy for gait abnormalities as individuals with medical morbidities tend to walk slower than healthy subjects. The walking speed assessment can be utilized as a powerful predictor of health events, which are related to musculoskeletal disorder and mental disease. The expanding need to distinguish gait pattern of individual according to health status has driven various analytical methods such as observational and instrumented gait analysis methods in capturing the human movement. Significant advances in 3D-gait analysis system have enabled a myriad of studies that advance our understanding of gait biomechanics. However, the data samples obtained from this system are large, with high degrees of variability. Hence, developing a reliable approach to distinguish gait patterns specific to the underlying pathologies is of paramount importance. Through this study, we have proposed the use of a deep learning framework with recurrent neural network (RNN) to interpret human walking speed based on kinematic data, whereby RNN is capable for time series data processing. Nevertheless, this model can hardly learn long-range dependencies across time steps in a sequence due to vanishing gradient. In this study, an improved RNN integrated with NVIDIA CUDA® Deep Neural Network Library Long Short-Term Memory (cuDNN LSTM) is introduced. This model is capable to classify the gait patterns of different walking speeds from seventeen healthy subjects, with a total of 453 gait cycles. Gait kinematic parameters were employed as the input layer of the deep learning architecture based on RNN is integrated with cuDNN LSTM. Our proposed framework has achieved an accuracy of 97% to classify different speeds (slow, normal and fast). This study therefore presents a method towards establishing a powerful tool to translate machine learning for gait analysis into clinical practice, whereby automated classifications of gait pattern could now improve acuity of clinical diagnoses. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,A quantitative gait assessment system is crucial for clinical analysis and decision-making. Such rigorous evaluation involves costly clinical setups and domain experts for observation and analysis. To circumvent such constraints, the proposed work is conducted in a markerless environment and divided into three stages: First, we prepared a markerless gait database using videos from MNIT RAMAN LABORATORY in Jaipur. Second, we adapt the skeletal landmark data to generate kinematic gait characteristics comparable to gold-standard marker-based techniques. We provide a novel set of parameters based on video sequences and spatiotemporal and sagittal kinematic parameters to optimize accuracy and reliability. Third, we develop multi-feature based gait analysis, an ensemble model based on Convolutional Neural Networks + LSTM (Long-Short Term Memory), for gait classification. In addition, we deployed transfer learning models to correlate with our ensemble model. The findings indicate that gait analysis can be used successfully in a low-cost clinical gait monitoring system in a constraint-free environment. While considering the multiple gait variables, our proposed model attained an accuracy of 95.3%. Our model for quantifying gait analysis will improve access to quantitative gait analysis in clinics and rehabilitation centers and enable researchers to conduct large-scale studies for gait-related disorders. © 2023 John Wiley & Sons Ltd."
108,107,41,107_Optimization of Search Algorithms for Expensive Problem Instances Using Evolutionary Surrogate-Assisted Methods,Optimization of Search Algorithms for Expensive Problem Instances Using Evolutionary Surrogate-Assisted Methods,"For machine learning algorithms, fine-tuning hyperparameters is a computational challenge due to the large size of the problem space. An efficient strategy for adjusting hyperparameters can be established with the use of the greedy search and Swarm intelligence algorithms. The Random Search and Grid Search optimization techniques show promise and efficiency for this task. The small population of solutions used at the outset, and the costly goal functions used by these searches, can lead to slow convergence or execution time in some cases. In this research, we propose using the machine learning model known as Support Vector Machine and optimizing it using four distinct algorithms—the Ant Bee Colony Algorithm, the Genetic Algorithm, the Whale Optimization, and the Particle Swarm Optimization—to evaluate the computational cost of SVM after hyper-tuning. Computational complexity comparisons of these optimization algorithms were performed to determine the most effective strategies for hyperparameter tuning. It was found that the Genetic Algorithm had a lower temporal complexity than other algorithms. © 2023 by the authors.,Surrogate-assisted evolutionary algorithms (SAEAs) are increasingly used in solving computationally expensive optimization problems. However, when tackling high-dimensional expensive problems, a large number of exact function evaluations (FEs) need to be consumed for existing SAEAs to achieve an acceptable solution. In this paper, a hierarchical surrogate assisted optimization algorithm (HSAOA) using teaching-learning-based optimization and differential evolution is proposed for solving high-dimensional expensive problems with a relatively small number of exact FEs. To keep a balance between global exploration and local exploitation, a hierarchical surrogate framework with hybrid evolutionary algorithms is devised. In the global search phase, a radial basis function surrogate is utilized to assist the teaching-learning-based optimization in locating the promising sub-regions. In the local search phase, a novel dynamic ensemble of surrogates is proposed to assist the differential evolution in speeding up the convergence process. Eight test functions with 10 to 100 dimensions and a spatial truss design problem are employed to compare the proposed method with several state-of-the-art SAEAs. The results show that the proposed HSAOA is superior to the comparison algorithms for solving expensive optimization problems, and needs a much smaller number of exact FEs than other competing SAEAs to produce competitive or even better results for high-dimensional expensive problems. © 2024 Elsevier B.V.,Many real-world engineering and industrial optimization problems involve expensive function evaluations (e.g., computer simulations and physical experiments) and possess a large number of decision variables. In many such scenarios, the optimization task has to be performed based on the previously available simulation data only. Also, to cut down the experimental expenses, it has been an open-ended research area to approximate these expensive function evaluations using a less expensive data-driven model trained on historical offline data collected from various expensive simulations. Offline data-driven evolutionary algorithms (DDEAs) efficiently use available data and surrogates to guide the optimization process. However, while building the surrogate models from limited offline data, the existing methods lack properties like reliability, scalability, and robustness. To address these challenges, in this work, we have proposed novel active learning with a reliability sampling-based evolutionary framework, where an ensemble of heterogeneous RBFNs (radial basis function neural networks), acting as an active learner, can gain insightful knowledge from unlabeled data. Consequently, a novel reliability sampling for selecting query individuals is also introduced for model management. It selects the utmost reliable candidate solutions for which the ensemble members have the least conflict to enrich the training data. These reliable query individuals are labeled as a weighted sum of RBFNs output, where the RBFN model with less error has more weightage. Furthermore, this article discusses the theoretical idea from which reliability sampling-based query strategy is inspired and provides the complexity analysis. Moreover, the results on five benchmark problems and CES 2017 test suit with 10, 30, 50, and 100-dimensional decision variables show that the proposed algorithm has achieved highly competitive results compared with four state-of-the-art offline DDEAs and three online DDEAs. Finally, our algorithm is applied to an offline data-driven expensive engineering optimization problem-aerodynamic airfoil design optimization from offline data to verify its efficacy on real-world problems. The experimental results demonstrate that the proposed active learning-based framework provides a highly reliable, efficient, and robust surrogate to assist evolutionary algorithms in solving offline data-driven expensive engineering problems. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
109,108,41,108_Materials for Energy Storage and Electrochemical Capacitors,Materials for Energy Storage and Electrochemical Capacitors,"The rise in demand for lithium-ion batteries has led to a large-scale search for electrode materials and intercalating ion species to meet the demands of next-generation energy technologies. Recent efforts largely focus on searching for cathodes that can accommodate large amounts of intercalating ions, but similar work on anodes is relatively limited. This study utilizes machine learning methods to find alternative two-dimensional (2D) materials and intercalating ions beyond Li for metal-ion batteries with high-power efficiencies. The approach first uses density functional theory (DFT) calculations to estimate the theoretical capacities and voltages of various metal ions on 2D materials. The DFT-generated data also provide insights into the local structural accommodation upon ion intercalation on various 2D materials. Significant changes to the lattice can result in irreversible changes to the bonding environments in the anode material, resulting in poor cycling stability. Next, this study develops a binding energy and structural accommodation-based classification model to screen anode materials for next-generation batteries. The classification model selects intercalating ions and 2D material pairs suitable for batteries based on the calculated voltage and volumetric changes in the 2D material upon intercalation. Finally, this study builds a regression model to accurately predict the binding energies of the various intercalating ions on 2D materials. The approach highlights the importance of different elemental and structural features for classification and regression tasks. The insights gained from this study on the role of involved features, such as electronegativities of the constituent ions and the presence of unfilled electronic levels, will help to streamline further studies towards the search for future layered battery materials. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Na-ion batteries are considered a promising alternative to the analogous Li-ion batteries because of their low manufacturing cost, large abundance, and similar chemical/electrochemical properties. In particular, research on Na-ion solid electrolytes, which resolve the flammability issues associated with liquid electrolytes and increase the energy density obtained using a particular metal anode, is rapidly growing. However, the ionic conductivities of these materials are lower than those of liquids. We present a novel classification approach based on machine learning for identifying Na superionic conductor (NASICON) materials with outstanding ionic conductivities. We obtained new features based on chemical descriptors such as Na content, elemental radii, and electronegativity. We then classified 3573 NASICON structures by implementing the ensemble model of gradient boosting algorithms, with an average prediction accuracy of 84.2%. We further validated the thermodynamic stability and ionic conductivity values of the materials classified as superionic materials by employing density functional theory calculations and ab initio molecular dynamics simulations. Na3YTaSi2PO12, Na3HfZrSi2PO12, Na3LaTaSi2PO12, and Na3ScTaSi2PO12 were confirmed as promising NASICON structures that fulfill the requirements of solid-state electrolytes. © 2023 American Chemical Society,Among energy storage devices, the last decades have witnessed the rapid spread of usage of carbon-based electrodes for electric double-layer capacitors (EDLCs) due to their large surface area, low cost, and high porosity. It is crucial to develop an accurate and efficient forecasting model for electrochemical performance to reduce the time needed for making suitable designs and choosing testing electrode materials. As a result, the use of machine learning (ML) approaches in creating a predicting model for the capacitance of carbon-based supercapacitors looks critical and provides the electrode characteristics' relative relevance. Data extracted from nearly a hundred published experimental research papers to select supercapacitors with certain electrode morphologies such as mesoporous, nanoporous, microporous, and hierarchical porous carbon electrode. The data was examined using machine learning techniques to predict the supercapacitor's specific capacitance (F/g). Electrode material structural qualities and various physicochemical test features such as electrolyte material, pore volume, and specific surface area. Electrochemical test features acquired via electrochemical impedance spectroscopy (EIS) and galvanostatic charge-discharge (GCD) test investigations for the same purpose include: cell configuration, current density, applied potential window, charge-transfer resistance (RCT), and equivalent series resistance (ESR) were used as input features to predict the corresponding capacitance performance. In the present study, Lasso, Support Vector Machine Regression (SVMR), and Artificial Neural Networks (ANN) with different structures were examined to predict the capacitance of the supercapacitor. The exhibition of the ML models measured concerning the root mean square error (RMSE), the correlation between expected yield and yield provided by the system. The developed ANN model with RMSE, MAE, and R values of 30.82, 46.5624 and 0.89537, respectively, provides outcomes for the prediction that are highly accurate compared to other models created for this purpose. According to the analysis of the input features done using the SHAP (SHapley Additive exPlanations) framework, the specific surface area had the biggest impact on the ANN model. © 2023"
110,109,41,109_Automatic Segmentation of Organs at Risk for Radiation Treatment using Deep Learning,Automatic Segmentation of Organs at Risk for Radiation Treatment using Deep Learning,"Purpose: The large variability in tumor appearance and shape makes manual delineation of the clinical target volume (CTV) time-consuming, and the results depend on the oncologists’ experience. Whereas deep learning techniques have allowed oncologists to automate the CTV delineation, multi-site tumor analysis is often lacking in the literature. This study aimed to evaluate the deep learning models that automatically contour CTVs of tumors at various sites on computed tomography (CT) images from objective and subjective perspectives. Methods and Materials: 577 patients were selected for the present study, including nasopharyngeal (n = 34), esophageal (n = 40), breast-conserving surgery (BCS) (left-sided, n = 71; right-sided, n = 71), breast-radical mastectomy (BRM) (left-sided, n = 43; right-sided, n = 37), cervical (radical radiotherapy, n = 45; postoperative, n = 85), prostate (n = 42), and rectal (n = 109) carcinomas. Manually delineated CTV contours by radiation oncologists are served as ground truth. Four models were evaluated: Flexnet, Unet, Vnet, and Segresnet, which are commercially available in the medical product “AccuLearning AI model training platform”. The data were divided into the training, validation, and testing set at a ratio of 5:1:4. The geometric metrics, including Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), were calculated for objective evaluation. For subjective assessment, oncologists rated the segmentation contours of the testing set visually. Results: High correlations were observed between automatic and manual contours. Based on the results of the independent test group, most of the patients achieved satisfactory quantitative results (DSC > 0.8), except for patients with esophageal carcinoma (DSC: 0.62–0.64). The subjective review indicated that 82.65% of predicted CTVs scored either as clinically accepting (8.68%) or requiring minor revision (73.97%), and no patients were scored as rejected. Conclusion: This experimental work demonstrated that auto-generated contours could serve as an initial template to help oncologists save time in CTV delineation. The deep learning-based auto-segmentations achieve acceptable accuracy and show the potential to improve clinical efficiency for radiotherapy of a variety of cancer. © 2023, Italian Society of Medical Radiology.,Purpose: Segmenting organs in cone-beam CT (CBCT) images would allow to adapt the radiotherapy based on the organ deformations that may occur between treatment fractions. However, this is a difficult task because of the relative lack of contrast in CBCT images, leading to high inter-observer variability. Deformable image registration (DIR) and deep-learning based automatic segmentation approaches have shown interesting results for this task in the past years. However, they are either sensitive to large organ deformations, or require to train a convolutional neural network (CNN) from a database of delineated CBCT images, which is difficult to do without improvement of image quality. In this work, we propose an alternative approach: to train a CNN (using a deep learning-based segmentation tool called nnU-Net) from a database of artificial CBCT images simulated from planning CT, for which it is easier to obtain the organ contours. Methods: Pseudo-CBCT (pCBCT) images were simulated from readily available segmented planning CT images, using the GATE Monte Carlo simulation. CT reference delineations were copied onto the pCBCT, resulting in a database of segmented images used to train the neural network. The studied segmentation contours were: bladder, rectum, and prostate contours. We trained multiple nnU-Net models using different training: (1) segmented real CBCT, (2) pCBCT, (3) segmented real CT and tested on pseudo-CT (pCT) generated from CBCT with cycleGAN, and (4) a combination of (2) and (3). The evaluation was performed on different datasets of segmented CBCT or pCT by comparing predicted segmentations with reference ones thanks to Dice similarity score and Hausdorff distance. A qualitative evaluation was also performed to compare DIR-based and nnU-Net-based segmentations. Results: Training with pCBCT was found to lead to comparable results to using real CBCT images. When evaluated on CBCT obtained from the same hospital as the CT images used in the simulation of the pCBCT, the model trained with pCBCT scored mean DSCs of 0.92 ± 0.05, 0.87 ± 0.02, and 0.85 ± 0.04 and mean Hausdorff distance 4.67 ± 3.01, 3.91 ± 0.98, and 5.00 ± 1.32 for the bladder, rectum, and prostate contours respectively, while the model trained with real CBCT scored mean DSCs of 0.91 ± 0.06, 0.83 ± 0.07, and 0.81 ± 0.05 and mean Hausdorff distance 5.62 ± 3.24, 6.43 ± 5.11, and 6.19 ± 1.14 for the bladder, rectum, and prostate contours, respectively. It was also found to outperform models using pCT or a combination of both, except for the prostate contour when tested on a dataset from a different hospital. Moreover, the resulting segmentations demonstrated a clinical acceptability, where 78% of bladder segmentations, 98% of rectum segmentations, and 93% of prostate segmentations required minor or no corrections, and for 76% of the patients, all structures of the patient required minor or no corrections. Conclusion: We proposed to use simulated CBCT images to train a nnU-Net segmentation model, avoiding the need to gather complex and time-consuming reference delineations on CBCT images. © 2022 American Association of Physicists in Medicine.,Background: The high-dose rate (HDR) brachytherapy treatment planning workflow for cervical cancer is a labor-intensive, time-consuming, and expertise-driven process. These issues are amplified in low/middle-income countries with large deficits in experienced healthcare professionals. Automation has the ability to substantially reduce bottlenecks in the planning process but often require a high level of expertise to develop. Purpose: To implement the out of the box self-configuring nnU-Net package for the auto-segmentation of the organs at risk (OARs) and high-risk CTV (HR CTV) for Ring-Tandem (R-T) HDR cervical brachytherapy treatment planning. Methods: The computed tomography (CT) scans of 100 previously treated patients were used to train and test three different nnU-Net configurations (2D, 3DFR, and 3DCasc). The performance of the models was evaluated by calculating the Sørensen-dice similarity coefficient, Hausdorff distance (HD), 95th percentile Hausdorff distance, mean surface distance (MSD), and precision score for 20 test patients. The dosimetric accuracy between the manual and predicted contours was assessed by looking at the various dose volume histogram (DVH) parameters and volume differences. Three different radiation oncologists (ROs) scored the predicted bladder, rectum, and HR CTV contours generated by the best performing model. The manual contouring, prediction, and editing times were recorded. Results: The mean DSC, HD, HD95, MSD and precision scores for our best performing model (3DFR) were 0.92/7.5 mm/3.0 mm/ 0.8 mm/0.91 for the bladder, 0.84/13.8 mm/5.3 mm/1.4 mm/0.84 for the rectum, and 0.81/8.5 mm/6.0 mm/2.2 mm/0.80 for the HR CTV. Mean dose differences (D2cc/90%) and volume differences were 0.08 Gy/1.3 cm3 for the bladder, 0.02 Gy/0.7 cm3 for the rectum, and 0.33 Gy/1.5 cm3 for the HR CTV. On average, 65% of the generated contours were clinically acceptable, 33% requiring minor edits, 2% required major edits, and no contours were rejected. Average manual contouring time was 14.0 min, while the average prediction and editing times were 1.6 and 2.1 min, respectively. Conclusion: Our best performing model (3DFR) provided fast accurate auto generated OARs and HR CTV contours with a large clinical acceptance rate. © 2023 The Authors. Journal of Applied Clinical Medical Physics published by Wiley Periodicals, LLC on behalf of The American Association of Physicists in Medicine."
111,110,40,110_Knowledge Graph Completion using LP-BERT and GCKG,Knowledge Graph Completion using LP-BERT and GCKG,"Due to the heterogeneous structure of the knowledge graph (KG), relationships between entities remain missing. However, optimal use of KG requires inference of missing fact triplet (entity-relation-entity). The fact inference predicts a missing relationship using an embedding approach in a supervised learning setup, representing entities and relationships in a low-dimensional vector space. Recent work uses attention-aware embeddings, but when applied directly to entire KG, attention mechanisms can be computationally expensive, especially for large graphs. The attention-based KG embedding model uses negative sampling, which can cause a gradient vanishing problem during learning. This paper proposes a novel triplet subgraph attention embedding (TSAE) model that combines a simplified graph attention mechanism with a neural network to learn embedding without negative sampling requirements. The attention layer processes the triplet-level subgraph entities to learn the central entity features by aggregating the neighbor’s features. A neural network processes attention-aware triplet entity features through hidden layers to compute the likelihood of relationship types between triplet entities. TSAE generates more fine-grained entity embeddings using simplified attention mechanism, reduces computational complexity, and offers interpretable embeddings. Experimental results on the benchmark data sets exhibit TSAE superiority over the baselines. The case study shows the efficacy of the model for the KG completion task. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Most knowledge graphs(KGs) are large and incomplete graph-structure database, which can be completed by predicting miss links according to the existing knowledge. The mainstream method is knowledge graph embedding (KGE) which is designed to learn low dimensional embedding of entities and relations. However, knowledge graph embedding still faces two major issues: (1) How to generate more expressive embeddings? (2) How to solve semantic polysemy of entities in different relations? In this paper, we propose a novel KG embedding model, RIECN (Relation-based Interactive Embedding Convolutional Network), which achieves high-quality performance and shows some advancements in modeling complex relations. In RIECN, FIR (Feature Interaction Reshaping) method is introduced to increase the feature interactions between entity and relation embeddings to generate more expressive feature maps. In addition, a new method of generating relation-based dynamic convolution filters, RDCF, is proposed. RDCF generates specific relation and hybird-size convolution filters, which enriches the feature maps of each entity improving the accuracy of link prediction task especially in complex relations scenario. We tested the performance of our model on five benchmark datasets. The experimental results show that the RIECN model significantly outperforms recent state-of-the-art models by 0.1–3.2% and 1.1–3.7%, in terms of MMR metric and Hit@1 metric, respectively. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Reasoning over knowledge graphs (KGs) has received increasing attention recently due to its promising applications in many areas, such as semantic search and recommendation systems. Subsequently, most reasoning models are inherently transductive and ignore uncertainties of KGs, making it difficult to generalize to unseen entities. Moreover, existing approaches usually require each entity in the KG to have sufficient training samples, which leads to the overfitting of the entity having few instances. In fact, long-tail distributions are quite widespread in KGs, and newly emerging entities will tend to have only a few related triples. In this work, we aim at studying knowledge graph reasoning under a challenging setting where only limited training samples are available. Specifically, we propose a Bayesian inductive reasoning method and incorporate meta-learning techniques in few-shot learning to solve data deficiency and uncertainties. We design a Bayesian graph neural network as a meta-learner to achieve Bayesian inference, which can extrapolate meta-knowledge from observed KG to emerging entities. We conduct extensive experiments on two large-scale benchmark datasets, and the results demonstrate considerable performance improvement with the proposed approach over other baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM."
112,111,39,111_Traffic data analysis using satellite imagery and deep learning,Traffic data analysis using satellite imagery and deep learning,"Traffic sign recognition and detection is a key technology in automatic vehicle driving and driver assistance systems. However, existing traffic sign recognition algorithms suffer from problems such as large model size, complex computation, high computational cost, which make it difficult to achieve an effective balance between detection speed and detection accuracy. This paper proposed an improved lightweight recognition algorithm, which is based on YOLOv5. This algorithm replaces the convolutional structure in the original YOLOv5 neck network with Ghost Module and C3Ghost Module, thereby reducing redundant features in the feature fusion process, lowering computational cost and the number of parameters. The structure of the PAN network was improved and the hybrid attention mechanism module CBAM was introduced to capture key information in traffic signs. Cross-layer connections were added to shorten the path of information transfer in feature pyramid network, which fused more features and improved the network feature recognition accuracy. In addition, the EIoU-Loss function was adopted as the bounding box regression loss function to improve the localization accuracy of the algorithm. The performance of the improved algorithm was also verified on the Chinese traffic sign dataset. Experimental results showed that the improved algorithm's detection accuracy was enhanced by 1.2%, while mAP@0.5 and mAP@0.5:0.95 were enhanced by 1.5% and 3.4% respectively over the existing YOLOv5 algorithm, and the overall parameter numbers and computational cost of the model were reduced by 14.5% and 16%. The proposed algorithm performs better than the current mainstream detection algorithms, has higher recognition accuracy in multiple environments, and meets the demand for real-time traffic sign recognition. © 2013 IEEE.,As one of the most indispensable means of transportation in modern society, vehicles guarantee our daily commuting and logistics transportation. However, with the increasing number of vehicles, vehicles have also caused increasingly serious traffic safety problems while providing convenience to our lives. One of the most common of these is traffic accidents caused by vehicle yaw due to driver distraction. As a potential solution to this problem, lane departure warning systems (LDWS) focus on detecting and determining whether the vehicle is deviating from the driveway, considered an essential part of autonomous driving technology, and have received significant attention in recent years. A large number of different types of LDWS systems have been developed, especially in recent years, with the development of artificial intelligence technology, many methods based on deep learning and machine vision have been proposed. However, it is well known that due to the complexity of the network structure in deep learning-based object detection algorithms, the operation of such methods relies on a large amount of computing power support. However, due to the limitation of the overall energy supply of the vehicle, it is usually unable to support computing power similar to the laboratory level. Therefore, how to realize efficient lane departure warnings under the condition of limited computing power is a critical problem to be solved. Accordingly, in this paper, we propose a novel lightweight LDWS model. Different from deep learning methods of LDWS, our LDWS model LEHA can achieve high accuracy and efficiency by relying only on simple hardware. The proposed LEHA consists of three modules: the image processing module, the lane detection module, and the lane departure recognition module. The image pre-processing module is applied to pre-process the original road image, which can improve the accuracy and efficiency of the following lane detection module. After obtaining the processed image, lane detection begins to detect and label the lanes. Finally, lane departure recognition is used to calculate the deviation distance and direction to determine whether the warning should be initiated. To evaluate the performance of LEHA, we compare our method with other state-of-the-art LDWS models in terms of detection accuracy and processing time under ideal and non-ideal lane conditions based on the KITTI dataset. The experimental results demonstrate that our LEHA outperforms state-of-the-art techniques. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,A traffic sign recognition system is crucial for safely operating an autonomous driving car and efficiently managing road facilities. Recent studies on traffic sign recognition tasks show significant advances in terms of accuracy on several benchmarks. However, they lack performance evaluation in driving cars in diverse road environments. In this study, we develop a traffic sign recognition framework for a vehicle to evaluate and compare deep learning-based object detection and tracking models for practical validation. We collect a large-scale highway image set using a camera-installed vehicle for training models, and evaluate the model inference during a test drive in terms of accuracy and processing time. In addition, we propose a novel categorization method for urban road scenes with possible scenarios. The experimental results show that the YOLOv5 detector and strongSORT tracking model result in better performance than other models in terms of accuracy and processing time. Furthermore, we provide an extensive discussion on possible obstacles in traffic sign recognition tasks to facilitate future research through numerous experiments for each road condition. © 2023 by the authors."
113,112,39,112_Pyrolysis and Bioenergy Production,Pyrolysis and Bioenergy Production,"Co-pyrolysis of biomass and coal presents a promising opportunity for large-scale biomass utilization while reducing fossil fuel consumption. However, this process is highly complex and influenced by various factors, including the physicochemical properties of biomass and coal, their blending ratio, and operating parameters. To effectively comprehend and optimize the biomass-coal co-pyrolysis process, many experiments are required to achieve the desired product quantity and quality. In addressing this challenge, machine learning technology emerges as a valuable solution, as it can learn the relationships between input and output variables from available examples without explicit knowledge of the underlying mechanisms. This study conducts a comprehensive literature review to establish an extensive database encompassing biomass-coal compositions and pyrolysis reaction conditions. Using the collected data, robust statistical analyses are applied to understand better the underlying mechanisms governing the biomass-coal co-pyrolysis process. Four machine learning methods, namely support vector regression, artificial neural network, random forest regression, and gradient boosting regression, are employed to model the process. The best-performing model is subjected to an extensive feature importance analysis to identify the essential input features associated with each output response. The gradient boosting regression technique demonstrates superior performance with excellent results, characterized by higher coefficients of determination (R2 > 0.96) and lower errors (RMSE < 3.01 and MAE < 2.27). The temperature range of 450 to 550 °C, biomass blending ratios between 30 and 60%, and heating rates of 30 to 50 °C/min were identified as the conditions that maximize pyrolysis oil yield. Furthermore, the feature importance analysis reveals that the operating temperature and the biomass blending ratio are the most significant descriptors in the biomass-coal co-pyrolysis process. © 2023 Elsevier Ltd,Biochar is a high-carbon-content organic compound that has potential applications in the field of energy storage and conversion. It can be produced from a variety of biomass feedstocks such as plant-based, animal-based, and municipal waste at different pyrolysis conditions. However, it is difficult to produce biochar on a large scale if the relationship between the type of biomass, operating conditions, and biochar properties is not understood well. Hence, the use of machine learning-based data analysis is necessary to find the relationship between biochar production parameters and feedstock properties with biochar energy properties. In this work, a rough set-based machine learning (RSML) approach has been applied to generate decision rules and classify biochar properties. The conditional attributes were biomass properties (volatile matter, fixed carbon, ash content, carbon, hydrogen, nitrogen, and oxygen) and pyrolysis conditions (operating temperature, heating rate residence time), while the decision attributes considered were yield, carbon content, and higher heating values. The rules generated were tested against a set of validation data and evaluated for their scientific coherency. Based on the decision rules generated, biomass with ash content of 11–14 wt%, volatile matter of 60–62 wt% and carbon content of 42–45.3 wt% can generate biochar with promising yield, carbon content and higher heating value via a pyrolysis process at an operating temperature of 425°C–475°C. This work provided the optimal biomass feedstock properties and pyrolysis conditions for biochar production with high mass and energy yield. © 2023 Informa UK Limited, trading as Taylor & Francis Group.,This work aims to implement and use machine learning algorithms to predict the yield of bio-oil during the pyrolysis of lignocellulosic biomass based on the physicochemical properties and composition of the biomass feed and pyrolysis conditions. The biomass pyrolysis process is influenced by different process parameters, such as pyrolysis temperature, heating rate, composition of biomass, and purge gas flow rate. The inter-relation between the yield of different pyrolysis products and process parameters can be well predicted by using different machine learning algorithms. In this study, different machine learning algorithms, namely, multi-linear regression, gradient boosting, random forest, and decision tree, have been trained on the dataset and the models are compared to identify the optimum method for the determination of bio-oil yield prediction model. Analysis of the results showed the gradient boosting method to possess a regression score of 0.97 and 0.89 for the training and testing sets with root-mean-squared error (RMSE) values of 1.19 and 2.39, respectively, and overcome the problem of overfitting. Therefore, the present study provides an approach to train a generalized machine learning model, which can be employed on large datasets while avoiding the error of overfitting. © 2022 Canadian Society for Chemical Engineering."
114,113,39,113_Cancer-related gene expression and immune response in diverse tumor types.,Cancer-related gene expression and immune response in diverse tumor types.,"Early detection of lung cancer is crucial for patient survival and treatment. Recent advancements in next-generation sequencing (NGS) analysis enable cell-free DNA (cfDNA) liquid biopsy to detect changes, like chromosomal rearrangements, somatic mutations, and copy number variations (CNVs), in cancer. Machine learning (ML) analysis using cancer markers is a highly promising tool for identifying patterns and anomalies in cancers, making the development of ML-based analysis methods essential. We collected blood samples from 92 lung cancer patients and 80 healthy individuals to analyze the distinction between them. The detection of lung cancer markers Cyfra21 and carcinoembryonic antigen (CEA) in blood revealed significant differences between patients and controls. We performed machine learning analysis to obtain AUC values via Adaptive Boosting (AdaBoost), Multi-Layer Perceptron (MLP), and Logistic Regression (LR) using cancer markers, cfDNA concentrations, and CNV screening. Furthermore, combining the analysis of all multi-omics data for ML showed higher AUC values compared with analyzing each element separately, suggesting the potential for a highly accurate diagnosis of cancer. Overall, our results from ML analysis using multi-omics data obtained from blood demonstrate a remarkable ability of the model to distinguish between lung cancer and healthy individuals, highlighting the potential for a diagnostic model against lung cancer. © 2023 by the authors.,Abstract: The heterogeneous nature of breast cancer necessitates exploring its molecular subtypes for the early prognosis and treatment of cancer patients. Recent advances in genomics have enabled the investigation of gene expression data in breast cancer research as an alternative to traditional methods. In this regard, a project like The Cancer Genome Atlas (TCGA) provided easy access to the vast high-throughput sequencing gene expression data, including Breast cancer. However, finding evidence of the involvement of a set of genes in a particular breast cancer subtype from this large bulk of gene expression dataset is a demanding task. Here, we propose to develop a classification model based on machine learning to uncover the significant genes associated with different breast cancer subtypes like Basal, human epidermal growth factor receptor 2, luminal A, and luminal B. The RNA-Sequence gene expression data from The Cancer Genome Atlas is used for the tumor and normal sample classification and breast cancer subtype-specific optimal set of gene identification for this experiment. Experimental results show that the average classification accuracy value for different gene subsets varies from 75.36–77.74% depending upon the breast cancer subtype and feature selection method. Additionally, the feature scoring mechanism introduced in our model ranks the Feature Importance genes as three*, four*, five*, and six*. Besides this, Kaplan–Meier survival analysis, Composite network analysis, and Gene Ontology analysis are conducted to highlight the biological significance of the Feature Importancegenes. Given the classification results and the biological insight, we may conclude that the proposed model extracts a set of informative genes involved in breast cancer development, particularly the Basal, human epidermal growth factor receptor 2, luminal A, and luminal B subtypes. © 2023, Pleiades Publishing, Inc.,Introduction: Monitoring the response after treatment of liver cancer and timely adjusting the treatment strategy are crucial to improve the survival rate of liver cancer. At present, the clinical monitoring of liver cancer after treatment is mainly based on serum markers and imaging. Morphological evaluation has limitations, such as the inability to measure small tumors and the poor repeatability of measurement, which is not applicable to cancer evaluation after immunotherapy or targeted treatment. The determination of serum markers is greatly affected by the environment and cannot accurately evaluate the prognosis. With the development of single cell sequencing technology, a large number of immune cell-specific genes have been identified. Immune cells and microenvironment play an important role in the process of prognosis. We speculate that the expression changes of immune cell-specific genes can indicate the process of prognosis. Method: Therefore, this paper first screened out the immune cell-specific genes related to liver cancer, and then built a deep learning model based on the expression of these genes to predict metastasis and the survival time of liver cancer patients. We verified and compared the model on the data set of 372 patients with liver cancer. Result: The experiments found that our model is significantly superior to other methods, and can accurately identify whether liver cancer patients have metastasis and predict the survival time of liver cancer patients according to the expression of immune cell-specific genes. Discussion: We found these immune cell-specific genes participant multiple cancer-related pathways. We fully explored the function of these genes, which would support the development of immunotherapy for liver cancer. Copyright © 2023 Liu, Qu, Xu, Qiao, Shao, Liu, He and Zhang."
115,114,39,114_Causal Inference and Propensity Score Estimation in Observational Studies,Causal Inference and Propensity Score Estimation in Observational Studies,"Existing studies have suggested superior performance of nonparametric machine learning over logistic regression for propensity score estimation. However, it is unclear whether the advantages of nonparametric propensity score modeling are carried to settings where there is clustering of individuals, especially when there is unmeasured cluster-level confounding. In this work we examined the performance of logistic regression (all main effects), Bayesian additive regression trees and generalized boosted modeling for propensity score weighting in clustered settings, with the clustering being accounted for by including either cluster indicators or random intercepts. We simulated data for three hypothetical observational studies of varying sample and cluster sizes. Confounders were generated at both levels, including a cluster-level confounder that is unobserved in the analyses. A binary treatment and a continuous outcome were generated based on seven scenarios with varying relationships between the treatment and confounders (linear and additive, nonlinear/nonadditive, nonadditive with the unobserved cluster-level confounder). Results suggest that when the sample and cluster sizes are large, nonparametric propensity score estimation may provide better covariate balance, bias reduction, and 95% confidence interval coverage, regardless of the degree of nonlinearity or nonadditivity in the true propensity score model. When the sample or cluster sizes are small, however, nonparametric approaches may become more vulnerable to unmeasured cluster-level confounding and thus may not be a better alternative to multilevel logistic regression. We applied the methods to the National Longitudinal Study of Adolescent to Adult Health data, estimating the effect of team sports participation during adolescence on adulthood depressive symptoms. © 2022 John Wiley & Sons Ltd.,There is a long-standing debate in the statistical, epidemiological, and econometric fields as to whether nonparametric estimation that uses machine learning in model fitting confers any meaningful advantage over simpler, parametric approaches in finite sample estimation of causal effects. We address the question: when estimating the effect of a treatment on an outcome, how much does the choice of nonparametric vs parametric estimation matter? Instead of answering this question with simulations that reflect a few chosen data scenarios, we propose a novel approach to compare estimators across a large number of datagenerating mechanisms drawn from nonparametric models with semi-informative priors. We apply this proposed approach and compare the performance of two nonparametric estimators (Bayesian adaptive regression tree and a targeted minimum loss-based estimator) to two parametric estimators (a logistic regression- based plug-in estimator and a propensity score estimator) in terms of estimating the average treatment effect across thousands of data-generating mechanisms. We summarize performance in terms of bias, confidence interval coverage, and mean squared error. We find that the two nonparametric estimators can substantially reduce bias as compared to the two parametric estimators in large-sample settings characterized by interactions and nonlinearities while compromising very little in terms of performance even in simple, small-sample settings.  © 2023 the author(s).,Propensity score matching is commonly used in observational studies to control for confounding and estimate the causal effects of a treatment or exposure. Frequently, in observational studies data are clustered, which adds to the complexity of using propensity score techniques. In this article, we give an overview of propensity score matching methods for clustered data, and highlight how propensity score matching can be used to account for not just measured confounders, but also unmeasured cluster level confounders. We also consider using machine learning methods such as generalized boosted models to estimate the propensity score and show that accounting for clustering when using these methods can greatly reduce the performance, particularly when there are a large number of clusters and a small number of subjects per cluster. In order to get around this we highlight scenarios where it may be possible to control for measured covariates using propensity score matching, while using fixed effects regression in the outcome model to control for cluster level covariates. Using simulation studies we compare the performance of different propensity score matching methods for clustered data across a number of different settings. Finally, as an illustrative example we apply propensity score matching methods for clustered data to study the causal effect of aspirin on hearing deterioration using data from the conservation of hearing study. © The Author(s) 2022."
116,115,38,115_Skin Lesion Detection and Classification for Dermatology using Deep Learning and Transformers,Skin Lesion Detection and Classification for Dermatology using Deep Learning and Transformers,"Skin cancer is a risky ailment that can be effectively managed if detected promptly. However, timely recognition of skin cancer remains a challenge. In this study, a robotic computer-assisted tactic is proposed for the early detection of skin cancer. The motivation behind this research is to improve the accuracy and efficiency of skin cancer recognition. The proposed methodology involves a series of steps. Initially, the input images undergo preprocessing to enhance their quality and extract relevant features. These preprocessed images are then fed into a Gated Recurrent Unit (GRU) Network, a type of deep learning model known for its ability to capture sequential information. To optimize the performance of the GRU Network, we employ an enhanced variant of the Orca Predation Algorithm (OPA). This algorithm helps fine-tune the network parameters, improving its diagnostic capabilities. To validate and evaluate the effectiveness of our skin cancer diagnosis algorithm, we conducted experiments using the HAM10000 dataset, which contains a large collection of skin lesion images. We compared the results obtained from our proposed method, named GRU/IOPA, with eight existing techniques commonly used for skin cancer diagnosis. Five execution indices, namely sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy, were used to assess the performance of these methods. Our findings demonstrate that the GRU/IOPA system outperforms other existing methods in terms of sensitivity (0.95), specificity (0.97), PPV (0.95), NPV (0.96), and accuracy. These results indicate the effectiveness of our proposed method in diagnosing skin cancer compared to traditional approaches. The superior performance of GRU/IOPA highlights its potential impact on improving skin cancer diagnosis and reinforces its promise as an advanced tool in the field. In conclusion, our study presents a novel approach for timely skin cancer recognition through a robotic computer-assisted tactic. By utilizing the GRU/IOPA system, we achieve superior accuracy and efficiency in diagnosing skin cancer compared to existing techniques. This research offers significant contributions to the field of skin cancer diagnosis and opens up new avenues for future advancements in this area. © 2023 Elsevier Ltd,Skin is a most essential and extraordinary part of the human structure. Exposure to chemicals such as nitrates, sunlight, arsenic, and UV rays due to pollution and depletion of the ozone layer is causing various skin diseases to spread rapidly. Digital healthcare offers many opportunities to reduce time, and human error, and improve clinical outcomes. However, the automatic recognition of skin disease is a major challenge due to high visual similarity between different skin diseases, low contrast, and large inter variation. Early detection of skin cancer can prevent death. Thus, Artificial intelligence (AI) and Machine Learning (ML) helps the physicians to improve clinical judgment or change manual perception. For skin cancer diagnostics, the ML/AI algorithm can outperform or match professional dermatologists in multiple studies. Different pre-trained architectures such as ResNet152, AlexNet, VGGNet, etc. are used for fusing different skin disease features such as texture, color, etc. and they are also utilized for conducting segmentation tasks. The variations in reflection, lesion size, shape, illumination, etc. often make automatic skin disease classification a complex task. ISIC 2019 and HAM 10000 are the widely used public datasets for skin disease prediction. More technical paper on skin cancer diagnosis is compared in this study. This report examines the majority of technical papers published between 2018 and October 2022 in order to appreciate current trends in the disciplines of skin cancer prediction. A study that combined clinical patient data with deep learning models (DL) increased the accuracy of predicting skin cancer. This article presents a visually attractive and well-organized summary of the current study findings. © 2024, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Background: The field of dermatological image analysis using deep neural networks includes the semantic segmentation of skin lesions, pivotal for lesion analysis, pathology inference, and diagnoses. While biases in neural network-based dermatoscopic image classification against darker skin tones due to dataset imbalance and contrast disparities are acknowledged, a comprehensive exploration of skin color bias in lesion segmentation models is lacking. It is imperative to address and understand the biases in these models. Methods: Our study comprehensively evaluates skin tone bias within prevalent neural networks for skin lesion segmentation. Since no information about skin color exists in widely used datasets, to quantify the bias we use three distinct skin color estimation methods: Fitzpatrick skin type estimation, Individual Typology Angle estimation as well as manual grouping of images by skin color. We assess bias across common models by training a variety of U-Net-based models on three widely-used datasets with 1758 different dermoscopic and clinical images. We also evaluate commonly suggested methods to mitigate bias. Results: Our findings expose a significant and large correlation between segmentation performance and skin color, revealing consistent challenges in segmenting lesions for darker skin tones across diverse datasets. Using various methods of skin color quantification, we have found significant bias in skin lesion segmentation against darker-skinned individuals when evaluated both in and out-of-sample. We also find that commonly used methods for bias mitigation do not result in any significant reduction in bias. Conclusions: Our findings suggest a pervasive bias in most published lesion segmentation methods, given our use of commonly employed neural network architectures and publicly available datasets. In light of our findings, we propose recommendations for unbiased dataset collection, labeling, and model development. This presents the first comprehensive evaluation of fairness in skin lesion segmentation. © 2024 The Author(s)"
117,116,38,116_Depth Estimation and 3D Reconstruction,Depth Estimation and 3D Reconstruction,"With the fast development and wide application of stereo depth estimation, adequate high-quality stereo training data with groundtruth depth information plays an important role, but is not easily acquired in underwater environments. Therefore, satisfactory performance of depth estimation is difficult to achieve in underwater environments. In addition, the domain gap also leads to the failure of directly applying existing models of terrestrial scene to underwater scene. Therefore, this paper proposes a novel underwater depth estimation network which can infer depth maps from real underwater stereo images in an adaptation manner. The proposed learning pipeline mainly contains three different adaptation modules, i.e., style adaptation, semantic adaptation and disparity range adaptation, to progressively adapt a terrestrial depth estimation model to the underwater domain. Specifically, due to the lack of underwater training data, we first propose a depth-aware stereo image translation network to synthesize stylized underwater stereo images from terrestrial dataset, thus benefiting the effective training of depth estimation network. Then, considering the weak generalization to the real underwater data when only trained on the above synthetic data, we present a self-ensembling semantic adaptation for depth estimation network to minimize the semantic domain discrepancy between synthetic and real underwater data. Meanwhile, we design a disparity range adaptation module to address the problem of disparity range miss-match between both data, thus obtaining more accurate depth predictions for large-disparity-span underwater images. Experimental results show that by integrating the proposed adaptation modules into the off-the-shelf depth estimation backbones, our method successfully achieves superior performance of underwater depth estimation compared to other state-of-the-art methods. © 1991-2012 IEEE.,Despite significant progress made in the past few years, challenges remain for depth estimation using a single monocular image. First, it is nontrivial to train a metric-depth prediction model that can generalize well to diverse scenes mainly due to limited training data. Thus, researchers have built large-scale relative depth datasets that are much easier to collect. However, existing relative depth estimation models often fail to recover accurate 3D scene shapes due to the unknown depth shift caused by training with the relative depth data. We tackle this problem here and attempt to estimate accurate scene shapes by training on large-scale relative depth data, and estimating the depth shift. To do so, we propose a two-stage framework that first predicts depth up to an unknown scale and shift from a single monocular image, and then exploits 3D point cloud data to predict the depth shift and the camera's focal length that allow us to recover 3D scene shapes. As the two modules are trained separately, we do not need strictly paired training data. In addition, we propose an image-level normalized regression loss and a normal-based geometry loss to improve training with relative depth annotation. We test our depth model on nine unseen datasets and achieve state-of-the-art performance on zero-shot evaluation. Code is available at: https://github.com/aim-uofa/depth/.  © 1979-2012 IEEE.,Recent advances in monocular 3D detection leverage a depth estimation network explicitly as an intermediate stage of the 3D detection network. Depth map approaches yield more accurate depth to objects than other methods thanks to the depth estimation network trained on a large-scale dataset. However, depth map approaches can be limited by the accuracy of the depth map, and sequentially using two separated networks for depth estimation and 3D detection significantly increases computation cost and inference time. In this work, we propose a method to boost the RGB image-based 3D detector by jointly training the detection network with a depth prediction loss analogous to the depth estimation task. In this way, our 3D detection network can be supervised by more depth supervision from raw LiDAR points, which does not require any human annotation cost, to estimate accurate depth without explicitly predicting the depth map. Our novel object-centric depth prediction loss focuses on depth around foreground objects, which is important for 3D object detection, to leverage pixel-wise depth supervision in an object-centric manner. Our depth regression model is further trained to predict the uncertainty of depth to represent the 3D confidence of objects. To effectively train the 3D detector with raw LiDAR points and to enable end-to-end training, we revisit the regression target of 3D objects and design a network architecture. Extensive experiments on KITTI and nuScenes benchmarks show that our method can significantly boost the monocular image-based 3D detector to outperform depth map approaches while maintaining the real-time inference speed. © 2000-2011 IEEE."
118,117,38,117_Habitat suitability and conservation for coastal species,Habitat suitability and conservation for coastal species,"Wildlife conservation strategies focused on one season or population segment may fail to adequately protect populations, especially when a species’ habitat preferences vary among seasons, age-classes, geographic regions, or other factors. Conservation of golden eagles (Aquila chrysaetos) is an example of such a complex scenario, in which the distribution, habitat use, and migratory strategies of this species of conservation concern vary by age-class, reproductive status, region, and season. Nonetheless, research aimed at mapping priority use areas to inform management of golden eagles in western North America has typically focused on territory-holding adults during the breeding period, largely to the exclusion of other seasons and life-history groups. To support population-wide conservation planning across the full annual cycle for golden eagles, we developed a distribution model for individuals in a season not typically evaluated–winter–and in an area of the interior western U.S. that is a high priority for conservation of the species. We used a large GPS-telemetry dataset and library of environmental variables to develop a machine-learning model to predict spatial variation in the relative intensity of use by golden eagles during winter in Wyoming USA, and surrounding ecoregions. Based on a rigorous series of evaluations including cross-validation, withheld and independent data, our winter-season model accurately predicted spatial variation in intensity of use by multiple age- and life-history groups of eagles not associated with nesting territories (i.e., all age classes of long-distance migrants, and resident non-adults and adult “floaters”, and movements of adult territory holders and their offspring outside their breeding territories). Important predictors in the model were wind and uplift (40.2% contribution), vegetation and landcover (27.9%), topography (14%), climate and weather (9.4%), and ecoregion (8.7%). Predicted areas of high-use winter habitat had relatively low spatial overlap with nesting habitat, suggesting a conservation strategy targeting high-use areas for one season would capture as much as half and as little as one quarter of high-use areas for the other season. The majority of predicted high-use habitat (top 10% quantile) occurred on private lands (55%); lands managed by states and the Bureau of Land Management (BLM) had a lower amount (33%), but higher concentration of high-use habitat than expected for their area (1.5–1.6x). These results will enable those involved in conservation and management of golden eagles in our study region to incorporate spatial prioritization of wintering habitat into their existing regulatory processes, land-use planning tasks, and conservation actions. Copyright: This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose.,Seagrass systems are in decline, mainly due to anthropogenic pressures and ongoing climate change. Implementing seagrass protection and restoration measures requires accurate assessment of suitable habitats. Commonly, such assessments have been performed using single-algorithm habitat suitability models, nearly always based on low environmental resolution information and short-term species data series. Here we address eelgrass (Zoostera marina) meadows’ large-scale decline (>80%) in Shandong province (Yellow Sea, China) by developing an ensemble habitat model (EHM) to inform eelgrass conservation and restoration strategies in the Swan Lake (SL). For this, we applied a weighted EHM derived from ten single-algorithm models including profile, regression, classification, and machine learning methods to generate a high-resolution habitat suitability map. The EHM was constructed based on the predictive performances of each model, by combining a series of present-absent eelgrass datasets from recent years coupled with oceanographic and sediment data. The model was cross-validated with independent historical datasets, and a final habitat suitability map for conservation and restoration was generated. Our EHM scheme outperformed all single models in terms of habitat suitability, scoring ?0.95 for both true statistic skill (TSS) and area under the curve (AUC) performance criteria. Machine learning methods outperformed profile, regression and classification methods. Regarding model explanatory variables, overall, topographic characteristics such as depth (DEP) and seafloor slope (SSL) are the most significant factors determining the distribution of eelgrass. The EHM predicted that the overlapping area was almost 90% of the current eelgrass habitat. Using results from our EHM, a LOESS regression model for the relationship of the habitat suitability to both the biomass and density of Z. marina outperformed better than the classic Ordinary Least Squares regression model. The EHM is a promising tool for supporting eelgrass protection and restoration areas in temperate lagoons as data availability improves. © 2022,Aim: The increasing availability of animal tracking datasets collected across many sites provides new opportunities to move beyond local assessments to enable detailed and consistent habitat mapping at biogeographical scales. However, integrating wildlife datasets across large areas and study sites is challenging, as species' varying responses to different environmental contexts must be reconciled. Here, we compare approaches for large-area habitat mapping and assess available habitat for a recolonizing large carnivore, the Eurasian lynx (Lynx lynx). Location: Europe. Methods: We use a continental-scale animal tracking database (450 individuals from 14 study sites) to systematically assess modelling approaches, comparing (1) global strategies that pool all data for training versus building local, site-specific models and combining them, (2) different approaches for incorporating regional variation in habitat selection and (3) different modelling algorithms, testing nonlinear mixed effects models as well as machine-learning algorithms. Results: Testing models on training sites and simulating model transfers, global and local modelling strategies achieved overall similar predictive performance. Model performance was the highest using flexible machine-learning algorithms and when incorporating variation in habitat selection as a function of environmental variation. Our best-performing model used a weighted combination of local, site-specific habitat models. Our habitat maps identified large areas of suitable, but currently unoccupied lynx habitat, with many of the most suitable unoccupied areas located in regions that could foster connectivity between currently isolated populations. Main Conclusions: We demonstrate that global and local modelling strategies can achieve robust habitat models at the continental scale and that considering regional variation in habitat selection improves broad-scale habitat mapping. More generally, we highlight the promise of large wildlife tracking databases for large-area habitat mapping. Our maps provide the first high-resolution, yet continental assessment of lynx habitat across Europe, providing a consistent basis for conservation planning for restoring the species within its former range. © 2023 The Authors. Diversity and Distributions published by John Wiley & Sons Ltd."
119,118,37,118_Indoor Localization Using WiFi and Crowdsourced Data,Indoor Localization Using WiFi and Crowdsourced Data,"Seamless positioning ability has become an essential requirement in large-scale smart city scenes with the development of Artificial Intelligence of Things technology. The performance of seamless positioning is limited by the inaccurate crowdsourced navigation database, cumulative error of built-in sensors, and changeable measurement errors of different location sources. In order to solve these problems, this paper presents the CrowdLOC-S framework, which provides a concrete and accurate indoor/outdoor localization performance using the combination of crowdsourced Wi-Fi fingerprinting, Global Navigation Satellite System (GNSS), and low-cost sensors. A data and model dual-driven based trajectory estimator is developed for improving the long-term positioning performance of built-in sensors, and a hybrid one-dimensional convolutional neural network (1D-CNN), Bi-directional Long Short-Term Memory (Bi-LSTM), and Multilayer Perceptron (MLP) enhanced quality indicator is proposed for quality evaluation of crowdsourced trajectories and further Wi-Fi fingerprinting database construction. Besides, the transfer learning approach is applied in the quality indicator for autonomously predicting the location errors towards different indoor and outdoor location sources and realizing seamless scenes switching. Finally, a unified extended Kalman filter is developed to realize multi-source integration-based seamless localization using the positioning information provided by indoor and outdoor location sources and corresponding quality indicator results. Comprehensive experiments demonstrate that the presented CrowdLOC-S system is proven to realize precise and efficient indoor and outdoor positioning performance in complex and large-scale urban environments. © 2023 Elsevier Ltd,Indoor 3D positioning is useful in multistory buildings, such as shopping malls, libraries, and airports. This study focuses on indoor 3D positioning using wireless access points (AP) in an environment without adding additional hardware facilities in large-scale complex places. The integration of a deep learning algorithm into indoor 3D positioning is studied, and a 3D dynamic positioning model based on temporal fingerprints is proposed. In contrast to the traditional positioning models with a single input, the proposed method uses a sliding time window to build a temporal fingerprint chip as the input of the positioning model to provide abundant information for positioning. Temporal information can be used to distinguish locations with similar fingerprint vectors and to improve the accuracy and robustness of positioning. Moreover, deep learning has been applied for the automatic extraction of spatiotemporal features. A temporal convolutional network (TCN) feature extractor is proposed in this paper, which adopts a causal convolution mechanism, dilated convolution mechanism, and residual connection mechanism and is not limited by the size of the convolution kernel. It is capable of learning hidden information and spatiotemporal relationships from the input features and the extracted spatiotemporal features are connected with a deep neural network (DNN) regressor to fit the complex nonlinear mapping relationship between the features and position coordinates to estimate the 3D position coordinates of the target. Finally, an open-source public dataset was used to verify the performance of the localization algorithm. Experimental results demonstrated the effectiveness of the proposed positioning model and a comparison between the proposed model and existing models proved that the proposed model can provide more accurate three-dimensional position coordinates. © 2022 by the authors.,Accurate indoor positioning has become an indispensable technology for location-based service in indoor environments. Geomagnetic field fingerprinting for indoor positioning is an attractive alternative to Wi-Fi and Bluetooth because of the omnipresent signals and the infrastructure-free mode. However, heterogeneous devices and users with fluctuant geomagnetism and massive crowdsourced data covering large-scale indoor scenes may result in a low degree of discernibility and degrade the rerecognizable performance for pedestrian positioning. To overcome the problem, a deep-learning-based indoor geomagnetic positioning method with direction-aware multiscale recurrent neural networks (DM-RNNs) is presented. First, the direction information is taken into the fingerprint construction and localization process to increase spatial identification, which is necessary for indoor pedestrian navigation. Second, instead of using a single holistic feature from sequence fingerprints directly, we employ the different scale-based feature extraction units for variational anomalies of the signal by using multiscale RNNs, increasing the model adaptability and generality for random pedestrian motion and device heterogeneity. Third, the ensemble learning mechanism is adopted in the model to obtain robust localization results. Furthermore, we employ the convenient visual-inertial odometry (VIO) to collect the geomagnetic signal dataset and utilize the sequence augmentation approach to generate synthesized trajectories from multiple single-point magnetic values for training and testing the model. Extensive experiments in three different indoor scenes demonstrate that the proposed approach outperforms state-of-the-art competing schemes by a wide margin, reducing mean localization error by more than 30% and obtaining over 79% direction estimation precision in different indoor scenarios.  © 2001-2012 IEEE."
120,119,37,119_Single-cell RNA sequencing methods for cell type annotation,Single-cell RNA sequencing methods for cell type annotation,"Single-cell RNA-seq analysis has become a powerful tool to analyse the transcriptomes of individual cells. In turn, it has fostered the possibility of screening thousands of single cells in parallel. Thus, contrary to the traditional bulk measurements that only paint a macroscopic picture, gene measurements at the cell level aid researchers in studying different tissues and organs at various stages. However, accurate clustering methods for such high-dimensional data remain exiguous and a persistent challenge in this domain. Of late, several methods and techniques have been promulgated to address this issue. In this article, we propose a novel framework for clustering large-scale single-cell data and subsequently identifying the rare-cell sub-populations. To handle such sparse, high-dimensional data, we leverage PaCMAP (Pairwise Controlled Manifold Approximation), a feature extraction algorithm that preserves both the local and the global structures of the data and Gaussian Mixture Model to cluster single-cell data. Subsequently, we exploit Edited Nearest Neighbours sampling and Isolation Forest/One-class Support Vector Machine to identify rare-cell sub-populations. The performance of the proposed method is validated using the publicly available datasets with varying degrees of cell types and rare-cell sub-populations. On several benchmark datasets, the proposed method outperforms the existing state-of-the-art methods. The proposed method successfully identifies cell types that constitute populations ranging from 0.1 to 8% with F1-scores of 0.91 0.09. © The Author(s) 2023. Published by Oxford University Press. All rights reserved.,Single-cell RNA sequencing (RNA-seq) has been demonstrated to be a proven method for quantifying gene-expression heterogeneity and providing insight into the transcriptome at the single-cell level. When combining multiple single-cell transcriptome datasets for analysis, it is common to first correct the batch effect. Most of the state-of-the-art processing methods are unsupervised, i.e., they do not utilize single-cell cluster labeling information, which could improve the performance of batch correction methods, especially in the case of multiple cell types. To better utilize known labels for complex dataset scenarios, we propose a novel deep learning model named IMAAE (i.e., integrating multiple single-cell datasets via an adversarial autoencoder) to correct the batch effects. After conducting experiments with various dataset scenarios, the results show that IMAAE outperforms existing methods for both qualitative measures and quantitative evaluation. In addition, IMAAE is able to retain both corrected dimension reduction data and corrected gene expression data. These features make it a potential new option for large-scale single-cell gene expression data analysis. © 2023 by the authors.,Cell type identification from single-cell transcriptomic data is a common goal of single-cell RNA sequencing (scRNAseq) data analysis. Deep neural networks have been employed to identify cell types from scRNAseq data with high performance. However, it requires a large mount of individual cells with accurate and unbiased annotated types to train the identification models. Unfortunately, labeling the scRNAseq data is cumbersome and time-consuming as it involves manual inspection of marker genes. To overcome this challenge, we propose a semi-supervised learning model 'SemiRNet' to use unlabeled scRNAseq cells and a limited amount of labeled scRNAseq cells to implement cell identification. The proposed model is based on recurrent convolutional neural networks (RCNN), which includes a shared network, a supervised network and an unsupervised network. The proposed model is evaluated on two large scale single-cell transcriptomic datasets. It is observed that the proposed model is able to achieve encouraging performance by learning on the very limited amount of labeled scRNAseq cells together with a large number of unlabeled scRNAseq cells.  © 2004-2012 IEEE."
121,120,37,120_Anomaly Detection in Multivariate Time Series for Industrial Monitoring,Anomaly Detection in Multivariate Time Series for Industrial Monitoring,"Detecting anomalies in large complex systems is a critical and challenging task. The difficulties arise from several aspects. First, collecting ground truth labels or prior knowledge for anomalies is hard in real-world systems, which often lead to limited or no anomaly labels in the dataset. Second, anomalies in large systems usually occur in a collective manner due to the underlying dependency structure among devices or sensors. Lastly, real-time anomaly detection for high-dimensional data requires efficient algorithms that are capable of handling different types of data (i.e. continuous and discrete). We propose a correlation structure-based collective anomaly detection (CSCAD) model for high-dimensional anomaly detection problem in large systems, which is also generalizable to semi-supervised or supervised settings. Our framework utilize graph convolutional network combining a variational autoencoder to jointly exploit the feature space correlation and reconstruction deficiency of samples to perform anomaly detection. We propose an extended mutual information (EMI) metric to mine the internal correlation structure among different data features, which enhances the data reconstruction capability of CSCAD. The reconstruction loss and latent standard deviation vector of a sample obtained from reconstruction network can be perceived as two natural anomalous degree measures. An anomaly discriminating network can then be trained using low anomalous degree samples as positive samples, and high anomalous degree samples as negative samples. Experimental results on five public datasets demonstrate that our approach consistently outperforms all the competing baselines. © 1989-2012 IEEE.,In a large-scale cloud environment, many key performance indicators (KPIs) of entities are monitored in real time. These multivariate time series consist of high-dimensional, high-noise, random and time-dependent data. As a common method implemented in artificial intelligence for IT operations (AIOps), time series anomaly detection has been widely studied and applied. However, the existing detection methods cannot fully consider the influence of multiple factors and cannot quickly and accurately detect anomalies in multivariate KPIs of entities. Concurrently, fine-grained root cause locations cannot be determined for detected anomalies and often require abundant normal data that are difficult to obtain for model training. To solve these problems, we propose a long short-term memory (LSTM)-based semisupervised variational autoencoder (VAE) anomaly detection strategy called LR-SemiVAE. First, LR-SemiVAE uses VAE to perform feature dimension reduction and reconstruction of multivariate time series data and judges whether the entity is abnormal by calculating the reconstruction probability score. Second, by introducing an LSTM network into the VAE encoder and decoder, the model can fully learn the time dependence of multivariate time series. Then, LR-SemiVAE predicts the data labels by introducing a classifier to reduce the dependence on the original labeled data during model training. Finally, by proposing a new evidence lower bound (ELBO) loss function calculation method, LR-SemiVAE pays attention to the normal pattern and ignores the abnormal pattern during training to reduce the time cost of removing random anomaly and noise data. However, due to the limitations of LSTM in learning the long-term dependence of time series data, based on LR-SemiVAE, we propose a transformer-based semisupervised VAE anomaly detection and location strategy called RT-SemiVAE for cluster systems with complex service dependencies. This method learns the long-term dependence of multivariate time series by introducing a parallel multihead attention mechanism transformer, while LSTM is used to capture short-term dependence, and the introduction of parallel computing also markedly reduces model training time. After RT-SemiVAE detects entity anomalies, it traces the root entities according to the obtained service dependence graph and locates the root causes at the indicator level. We verify the strategies by using public data sets and constructing a system prototype. Experimental results show that compared with existing baseline methods, the LR-SemiVAE and RT-SemiVAE strategies can detect anomalies more quickly and accurately and perform fine-grained and accurate localization of the root causes of anomalies. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Large-scale sewage treatment plants are one of the typical Industrial Internet of Things systems, where the presence of a large number of sensors generates massive dynamic time series data, and such multivariate time series data are usually time-dependent and random. Therefore, there is a certain risk when fitting the potential anomalies of real-world data, which will bring great challenges to anomaly detection. In this article, we propose a time-series mutual adversarial network (TMAN), a novel reconstruction model for anomaly detection on multivariate time series. It is based on the idea of adversarial learning and consists of two identical subnetworks. During the training process, two subnetworks can independently complete the learning of the time distribution of normal samples of industrial time series data for mutual adversarial. In the process of detecting, we obtain the residual values of TMAN reconstructed for different time series samples to discriminate anomalies. We combine TMAN and anomaly determination mechanisms to build a new industrial time series anomaly detection framework named TMANomaly. In addition, we select the dataset features with a grey correlation algorithm to achieve very high performance with a small number of features. Experimental results show that our proposed TMANomaly outperforms five popular anomaly detection methods and effectively improves the accuracy of industrial multivariate time series anomaly detection.  © 2005-2012 IEEE."
122,121,36,121_Adsorption characteristics of MOFs for gas separation and pollutant removal.,Adsorption characteristics of MOFs for gas separation and pollutant removal.,"Nanoporous materials such as metal-organic frameworks (MOFs) have been extensively studied for their potential for adsorption and separation applications. In this respect, grand canonical Monte Carlo (GCMC) simulations have become a well-established tool for computational screenings of the adsorption properties of large sets of MOFs. However, their reliance on empirical force field potentials has limited the accuracy with which this tool can be applied to MOFs with challenging chemical environments such as open-metal sites. On the other hand, density-functional theory (DFT) is too computationally demanding to be routinely employed in GCMC simulations due to the excessive number of required function evaluations. Therefore, we propose in this paper a protocol for training machine learning potentials (MLPs) on a limited set of DFT intermolecular interaction energies (and forces) of CO2 in ZIF-8 and the open-metal site containing Mg-MOF-74, and use the MLPs to derive adsorption isotherms from first principles. We make use of the equivariant NequIP model which has demonstrated excellent data efficiency, and as such an error on the interaction energies below 0.2 kJ mol-1 per adsorbate in ZIF-8 was attained. Its use in GCMC simulations results in highly accurate adsorption isotherms and heats of adsorption. For Mg-MOF-74, a large dependence of the obtained results on the used dispersion correction was observed, where PBE-MBD performs the best. Lastly, to test the transferability of the MLP trained on ZIF-8, it was applied to ZIF-3, ZIF-4, and ZIF-6, which resulted in large deviations in the predicted adsorption isotherms and heats of adsorption. Only when explicitly training on data for all ZIFs, accurate adsorption properties were obtained. As the proposed methodology is widely applicable to guest adsorption in nanoporous materials, it opens up the possibility for training general-purpose MLPs to perform highly accurate investigations of guest adsorption. © 2023 American Chemical Society.,Conspectus Carbon capture, utilization, and storage have been identified as key technologies to decarbonize the energy and industrial sectors. Although postcombustion CO2 capture by absorption in aqueous amines is a mature technology, the required high regeneration energy, losses due to degradation and evaporation, and corrosion carry a high economic cost, precluding this technology to be used today at the scale required to mitigate climate change. Solid adsorbent-based systems with high CO2 capacities, high selectivity, and lower regeneration energy are becoming an attractive alternative for this purpose. Conscious of this opportunity, the search for optimal adsorbents for the capture of CO2 has become an urgent task. To accurately assess the performance of CO2 separation by adsorption at the needed scale, adsorbents should be synthesized and fully characterized under the required operating conditions, and the proper design and simulation of the process should be implemented along with techno-economic and environmental assessments. Several works have examined pure CO2 single-component adsorption or binary mixtures of CO2 with nitrogen for different families of adsorbents, primarily addressing their CO2 adsorption capacity and selectivity; however, very limited data is available under other conditions and/or with impurities, mainly due to the intensive experimental (modeling) efforts required for the large number of adsorbents to be studied, posing a challenge for their assessment under the needed conditions. In this regard, molecular simulations can be employed in synergy with experiments, reliably generating missing adsorption properties of mixtures while providing understanding at the molecular level of the mechanism of the adsorption process. This Account provides an outlook on strategies used for the rational design of materials for CO2 capture from different sources from the understanding of the adsorption mechanism at the molecular level. We illustrate with practical examples from our work and others’ work how molecular simulations can be reliably used to link the molecular knowledge of novel adsorbents for which limited data exist for CO2 capture adsorption processes. Molecular simulation results of different adsorbents, including MOFs, zeolites, and carbon-based and silica-based materials, are discussed, focusing on understanding the role of physical and chemical adsorption obtained from simulations and quantifying the impact of impurities in the performance of the materials. Furthermore, simulation results can be used for screening adsorbents from basic key performance indicators, such as cycling the working capacity, selectivity, and energy requirement, or for feeding detailed dynamic models to assess their performance in swing adsorption processes on the industrial scale, additionally including monetized performance indicators such as operating expenses, equipment sizes, and compression cost. Moreover, we highlight the role of molecular simulations in guiding strategies for improving the performance of these materials by functionalization with amines or creating hybrid solid materials. We show how integrating models at different scales provides a robust and reliable assessment of the performance of the adsorbent materials under the required industrial conditions, rationally guiding the search for best performers. Trends in additional computational resources that can be used, including machine learning, and perspectives on practical requirements for leveraging CO2 capture adsorption technologies on the needed scale are also discussed. © 2023 The Authors. Published by American Chemical Society.,Adsorption-based separations using metal-organic frameworks (MOFs) are promising candidates for replacing common energy-intensive separation processes. The so-called adsorption space formed by the combination of billions of possible molecules and thousands of reported MOFs is vast. It is very challenging to comprehensively evaluate the performance of MOFs for chemical separation through experiments. Molecular simulations and machine learning (ML) have been widely applied to make predictions for adsorption-based separations. Previous ML approaches to these issues were typically limited to smaller molecules and often had poor accuracy in the dilute limit. To enable exploration of a wider adsorption space, we carefully selected a diverse set of 45 molecules and 335 MOFs and generated single-component isotherms of 15,075 MOF-molecule pairs by grand canonical Monte Carlo. Using this database, we successfully developed accurate (r2 > 0.9) machine learning models predicting adsorption isotherms of diverse molecules in large libraries of MOFs. With this approach, we can efficiently make predictions of large collections of MOFs for arbitrary mixture separations. By combining molecular simulation data and ML predictions with Ideal Adsorbed Solution Theory, we tested the ability of these approaches to make predictions of adsorption selectivity and loading for challenging near-azeotropic mixtures. © 2023 The Authors. Published by American Chemical Society."
123,122,36,122_Remote Sensing Techniques for Soil Moisture Monitoring in Arid Regions,Remote Sensing Techniques for Soil Moisture Monitoring in Arid Regions,"Motivated by the lack of long-term global soil moisture products with both high spatial and temporal resolutions, a global 1km daily spatiotemporally continuous soil moisture product (GLASS SM) was generated from 2000 to 2020 using an ensemble learning model (eXtreme Gradient Boosting - XGBoost). The model was developed by integrating multiple datasets, including albedo, land surface temperature, and leaf area index products from the Global Land Surface Satellite (GLASS) product suite, as well as the European reanalysis (ERA5-Land) soil moisture product, in situ soil moisture dataset from the International Soil Moisture Network (ISMN), and auxiliary datasets (Multi-Error-Removed Improved-Terrain (MERIT) DEM and Global gridded soil information (SoilGrids)). Given the relatively large-scale differences between point-scale in situ measurements and other datasets, the triple collocation (TC) method was adopted to select the representative soil moisture stations and their measurements for creating the training samples. To fully evaluate the model performance, three validation strategies were explored: random, site independent, and year independent. Results showed that although the XGBoost model achieved the highest accuracy on the random test samples, it was clearly a result of model overfitting. Meanwhile, training the model with representative stations selected by the TC method could considerably improve its performance for site- or year-independent test samples. The overall validation accuracy of the model trained using representative stations on the site-independent test samples, which was least likely to be overfitted, was a correlation coefficient (R) of 0.715 and root mean square error (RMSE) of 0.079m3m-3. Moreover, compared to the model developed without station filtering, the validation accuracies of the model trained with representative stations improved significantly for most stations, with the median R and unbiased RMSE (ubRMSE) of the model for each station increasing from 0.64 to 0.74 and decreasing from 0.055 to 0.052m3m-3, respectively. Further validation of the GLASS SM product across four independent soil moisture networks revealed its ability to capture the temporal dynamics of measured soil moisture (R=0.69-0.89; ubRMSE = 0.033-0.048m3m-3). Lastly, the intercomparison between the GLASS SM product and two global microwave soil moisture datasets - the 1km Soil Moisture Active Passive/Sentinel-1 L2 Radiometer/Radar soil moisture product and the European Space Agency Climate Change Initiative combined soil moisture product at 0.25- indicated that the derived product maintained a more complete spatial coverage and exhibited high spatiotemporal consistency with those two soil moisture products. The annual average GLASS SM dataset from 2000 to 2020 can be freely downloaded from 10.5281/zenodo.7172664 (Zhang et al., 2022a), and the complete product at daily scale is available at http://glass.umd.edu/soil_moisture/ (last access: 12 May 2023).  © 2023 Yufang Zhang et al.,Large-scale surface soil moisture (SSM) distribution is very necessary for agricultural drought monitoring, water resource management, and climate change research. However, the current large-scale SSM products have relatively coarse spatial resolution, which limits their application. In this study, we estimate the 1 km daily SSM in China based on ensemble learning using a multi-source data set including in situ soil moisture measurements from 2980 meteorological stations, MODIS Surface Reflectance products, SMAP (Soil Moisture Active Passive) soil moisture products, ERA5-Land dataset, SRTM DEM and soil texture. Among them, in situ measurements are used as independent variables, and other data are used as dependent variables. In order to improve the spatio-temporal completeness of SSM, the missing value in SMAP soil moisture products were reconstructed using the Discrete Cosine Transformation-penalized Partial Least Square (DCT-PLS) method to provide spatially complete background field information for soil moisture retrieval. The results show that the reconstructed soil moisture value has high quality, and the DCT-PLS method can fully utilize the three-dimensional spatiotemporal information to fill the data gaps. Subsequently, the performance of four ensemble learning models of random forest (RF), extremely randomized trees (ERT), extreme gradient boosting (XGBoost), and light gradient boosting machine (LightGBM) for soil moisture retrieval was evaluated. The LightGBM outperformed the other three machine learning models, with a correlation coefficient (R2) of 0.88, a bias of 0.0004 m³/m³, and an unbiased root mean square error (ubRMSE) of 0.0366 m³/m³. The high correlation between the in situ soil moisture and the predicted values at each meteorological station further indicate that LightGBM can well capture the temporal variation of soil moisture. Finally, the model was used to map the 1 km daily SSM in China on the first day of each month from May to October 2018. This study can provide some reference and help for future long-term daily 1 km surface soil moisture mapping in China. © 2023 by the authors.,Accurate high-resolution soil moisture mapping is critical for surface studies as well as climate change research. Currently, regional soil moisture retrieval primarily focuses on a spatial resolution of 1 km, which is not able to provide effective information for environmental science research and agricultural water resource management. In this study, we developed a quantitative retrieval framework for high-resolution (250 m) regional soil moisture inversion based on machine learning, multisource data fusion, and in situ measurement data. Specifically, we used various data sources, including the normalized vegetation index, surface temperature, surface albedo, soil properties data, precipitation data, topographic data, and soil moisture products from passive microwave data assimilation as input parameters. The soil moisture products simulated based on ground model simulation were used as supplementary data of the in situ measurements, together with the measured data from the Maqu Observation Network as the training target value. The study was conducted in the Zoige region of the Tibetan Plateau during the nonfreezing period (May–October) from 2009 to 2018, using random forests for training. The random forest model had good accuracy, with a correlation coefficient of 0.885, a root mean square error of 0.024 m³/m³, and a bias of ?0.004. The ground-measured soil moisture exhibited significant fluctuations, while the random forest prediction was more accurate and closely aligned with the field soil moisture compared to the soil moisture products based on ground model simulation. Our method generated results that were smoother, more stable, and with less noise, providing a more detailed spatial pattern of soil moisture. Based on the permutation importance method, we found that topographic factors such as slope and aspect, and soil properties such as silt and sand have significant impacts on soil moisture in the southeastern Tibetan Plateau. This highlights the importance of fine-scale topographic and soil property information for generating high-precision soil moisture data. From the perspective of inter-annual variation, the soil moisture in this area is generally high, showing a slow upward trend, with small spatial differences, and the annual average value fluctuates between 0.3741 m3/m3 and 0.3943 m3/m3. The intra-annual evolution indicates that the monthly mean average soil moisture has a large geographical variation and a small multi-year linear change rate. These findings can provide valuable insights and references for regional soil moisture research. © 2023 by the authors."
124,123,36,123_Protein sequence representation and prediction,Protein sequence representation and prediction,"O-linked ?-N-acetylglucosamine (O-GlcNAc) is a distinct monosaccharide modification of serine (S) or threonine (T) residues of nucleocytoplasmic and mitochondrial proteins. O-GlcNAc modification (i.e., O-GlcNAcylation) is involved in the regulation of diverse cellular processes, including transcription, epigenetic modifications, and cell signaling. Despite the great progress in experimentally mapping O-GlcNAc sites, there is an unmet need to develop robust prediction tools that can effectively locate the presence of O-GlcNAc sites in protein sequences of interest. In this work, we performed a comprehensive evaluation of a framework for prediction of protein O-GlcNAc sites using embeddings from pre-trained protein language models. In particular, we compared the performance of three protein sequence-based large protein language models (pLMs), Ankh, ESM-2, and ProtT5, for prediction of O-GlcNAc sites and also evaluated various ensemble strategies to integrate embeddings from these protein language models. Upon investigation, the decision-level fusion approach that integrates the decisions of the three embedding models, which we call LM-OGlcNAc-Site, outperformed the models trained on these individual language models as well as other fusion approaches and other existing predictors in almost all of the parameters evaluated. The precise prediction of O-GlcNAc sites will facilitate the probing of O-GlcNAc site-specific functions of proteins in physiology and diseases. Moreover, these findings also indicate the effectiveness of combined uses of multiple protein language models in post-translational modification prediction and open exciting avenues for further research and exploration in other protein downstream tasks. LM-OGlcNAc-Site’s web server and source code are publicly available to the community. © 2023 by the authors.,In living organisms, proteins are considered as the executants of biological functions. Owing to its pivotal role played in protein folding patterns, comprehension of protein structure is a challenging issue. Moreover, owing to numerous protein sequence exploration in protein data banks and complication of protein structures, experimental methods are found to be inadequate for protein structural class prediction. Hence, it is very much advantageous to design a reliable computational method to predict protein structural classes from protein sequences. In the recent few years there has been an elevated interest in using deep learning to assist protein structure prediction as protein structure prediction models can be utilized to screen a large number of novel sequences. In this regard, we propose a model employing Energy Profile for atom pairs in conjunction with the Legion-Class Bayes function called Energy Profile Legion-Class Bayes Protein Structure Identification model. Followed by this, we use a Thompson Optimized convolutional neural network to extract features between amino acids and then the Thompson Optimized SoftMax function is employed to extract associations between protein sequences for predicting secondary protein structure. The proposed Energy Profile Bayes and Thompson Optimized Convolutional Neural Network (EPB-OCNN) method tested distinct unique protein data and was compared to the state-of-the-art methods, the Template-Based Modeling, Protein Design using Deep Graph Neural Networks, a deep learning-based S-glutathionylation sites prediction tool called a Computational Framework, the Deep Learning and a distance-based protein structure prediction using deep learning. The results obtained when applied with the Biopython tool with respect to protein structure prediction time, protein structure prediction accuracy, specificity, recall, F-measure, and precision, respectively, are measured. The proposed EPB-OCNN method outperformed the state-of-the-art methods, thereby corroborating the objective. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Well understanding protein function and structure in computational biology helps in the understanding of human beings. To face the limited proteins that are annotated structurally and functionally, the scientific community embraces the self-supervised pretraining methods from large amounts of unlabeled protein sequences for protein embedding learning. However, the protein is usually represented by individual amino acids with limited vocabulary size (e.g. 20 type proteins), without considering the strong local semantics existing in protein sequences. In this work, we propose a novel pre-training modeling approach SPRoBERTa. We first present an unsupervised protein tokenizer to learn protein representations with local fragment pattern. Then, a novel framework for deep pretraining model is introduced to learn protein embeddings. After pre-training, our method can be easily fine-tuned for different protein tasks, including amino acid-level prediction task (e.g. secondary structure prediction), amino acid pair-level prediction task (e.g. contact prediction) and also protein-level prediction task (remote homology prediction, protein function prediction). Experiments show that our approach achieves significant improvements in all tasks and outperforms the previous methods. We also provide detailed ablation studies and analysis for our protein tokenizer and training framework. © The Author(s) 2022. Published by Oxford University Press. All rights reserved."
125,124,36,124_Reservoir Logging and Lithofacies Prediction,Reservoir Logging and Lithofacies Prediction,"The accurate estimation of reservoir porosity plays a vital role in estimating the amount of hydrocarbon reserves and evaluating the economic potential of a reservoir. It also aids decision making during the exploration and development phases of oil and gas fields. This study evaluates the integration of artificial intelligence techniques, conventional well logs, and core analysis for the accurate prediction of porosity in carbonate reservoirs. In general, carbonate reservoirs are characterized by their complex pore systems, with the wide spatial variation and highly nonlinear nature of their petrophysical properties. Therefore, they require detailed well-log interpretations to accurately estimate their properties, making them good candidates for the application of machine learning techniques. Accordingly, a large database of (2100) well-log records and core-porosity measurements were integrated with four state-of-the-art machine learning techniques (multilayer perceptron artificial neural network, MLP-ANN; Gaussian process regression, GPR; least squares gradient boosting ensemble, LS-Boost; and radial basis function neural network, RBF-NN) for the prediction of reservoir porosity. The well-log data used in this study include sonic acoustic travel time, Gamma-ray, and bulk density log records, which were carefully collected from five wells in a carbonate reservoir. This study revealed that all the artificial intelligence models achieved high accuracy, with R-squared values exceeding 90% during both the training and blind-testing phases. Among the AI models examined, the GPR model outperformed the others in terms of the R-squared values, root-mean-square error (RMSE), and coefficient of variation of the root-mean-square error (CVRMSE). Furthermore, this study introduces an artificially intelligent AI-based correlation for the estimation of reservoir porosity from well-log data; this correlation was developed using an in-house, Fortran-coded MLP-ANN model presented herein. This AI-based correlation gave a promising level of accuracy, with R-squared values of 92% and 90% for the training and blind-testing datasets, respectively. This correlation can serve as an accurate and easy-to-use tool for porosity prediction without any prior experience in utilizing or implementing machine learning models. © 2023 by the authors.,The target formation in the study area of the Pearl River Mouth Basin is characterized by complex lithology and thin interbedded layers, with a large pore-permeability distribution range and strongly heterogeneous characteristics, which makes the reservoir pore structure and production capacity significantly different and brings research difficulties for reservoir logging evaluation and desert identification. The conventional reservoir classification method is mainly based on physical research, which requires developing extremely accurate formulas for calculating porosity and permeability; the calculation accuracy of pore permeability of low-porosity and low-permeability reservoirs is difficult to guarantee; and the conventional logging data cannot be comprehensively applied in reservoir classification. In this paper, taking Zhujiang and Zhuhai Formation reservoirs in the Huizhou M oilfield as an example, we integrated core analysis data such as core cast thin section, pore permeability data, rock electrical parameters, grain size, and relative permeability curves and combined with petrophysical parameters and pore structure characteristics to classify the reservoirs. The artificial neural network is used to predict the resistivity of saturated pure water (R0) to remove the influence of oil and gas on reservoir resistivity. The natural gamma ray (GR) “fluctuation” is used to calculate the variance root of variation (GS) to reflect the lithological variability and sedimentary heterogeneity of the reservoir, and then the conventional logging preferences, R0 and Gs (based on GR), are classified based on the automatic clustering MRGC algorithm to classify the logging facies. To classify the petrophysical phase reservoirs under the constraint of pore structure classification, we proposed a petrophysical classification logging model based on the natural gamma curve “fluctuation” intensity for strongly heterogeneous reservoirs. The learning model is extended to the whole area for training and prediction of desert identification, and the prediction results of the model are in good agreement with the actual results, which is important for determining favorable reservoirs in the area and the adjustment of oilfield development measures. Copyright © 2023 Zhao, Miao, Zhao, Liang, Wang and Tian.,Recently, the petroleum industry has focused on deeply buried reservoir discoveries and exploring potential CO2 storage sites close to existing infrastructure to increase the life span of already operating installations to save time and cost. It is therefore essential for the petroleum industry to find an innovative approach that exploits the existing core- and well log data to be successful in their endeavor of effectively characterizing and predicting reservoir quality. Continuous data sources (e.g. wireline logs) have a huge potential compared with expensive, time inefficient and sporadic data from cores in determining reservoir quality for use in a regional context. However, whereas core analysis offers in-depth knowledge about rock properties and diagenetic processes, continuous data sources can be difficult to interpret without a formation-specific framework. Here, we demonstrated how the pre-existing core data could be effectively used by integrating petrographic- and facies data with a pure predictive machine learning (ML) based porosity predictor. The inclusion of detailed core analysis is important for determining which reservoir parameter(s) that should be modeled and for the interpretation of model outputs. By applying this methodology, a framework for deducing lithological and diagenetic attributes can be established to aid reservoir quality delineation from wireline logs that can be used in frontier areas. With the ML porosity model, a Random Forest Regressor, the square of the correlation was 0.84 between predicted- and helium porosity test data over a large dataset consisting of 38 wells within the Stø Formation across the SW Barents Sea. By integrating the continuous ML porosity logs and core data, it was possible to differentiate three distinct bed types on wireline log responses within the Stø Formation. Particularly, the relationship between Gamma ray (GR) and porosity was effective in separating high porosity clean sand-, low porosity cemented clean sand and more clay and silt rich intervals. Additionally, in the P-wave velocity (VP) - density domain, separation of high porosity clean sand- and heavily cemented low porosity clean sand intervals were possible. The results also show that the ML derived porosity curves coincide with previously published and independent facies data from a selection of the wells included in the study. This demonstrates the applicability of the model in the region, because the Stø Formation has been described to exhibit similar lithological- and mineralogical properties over large parts of the Western Barents Sea area. Even though, continuous porosity data could be estimated from other sources like VP, neutron or density logs, this would generally require matrix and fluid information. This study demonstrated the effectiveness of the ML model in generating continuous porosity logs that are useful for characterizing and predicting reservoir properties in new wells. This methodology offers a workflow for exploiting already acquired core and well log data for frontier exploration that can be adapted to other formations and exploration scenarios worldwide. © 2022 The Authors"
126,125,35,125_Generative Adversarial Networks for Medical Image Synthesis and Data Augmentation,Generative Adversarial Networks for Medical Image Synthesis and Data Augmentation,"Deep learning-based segmentation methods provide an effective and automated way for assessing the structure and function of the heart in cardiac magnetic resonance (CMR) images. However, despite their state-of-the-art performance on images acquired from the same source (same scanner or scanner vendor) as images used during training, their performance degrades significantly on images coming from different domains. A straightforward approach to tackle this issue consists of acquiring large quantities of multi-site and multi-vendor data, which is practically infeasible. Generative adversarial networks (GANs) for image synthesis present a promising solution for tackling data limitations in medical imaging and addressing the generalization capability of segmentation models. In this work, we explore the usability of synthesized short-axis CMR images generated using a segmentation-informed conditional GAN, to improve the robustness of heart cavity segmentation models in a variety of different settings. The GAN is trained on paired real images and corresponding segmentation maps belonging to both the heart and the surrounding tissue, reinforcing the synthesis of semantically-consistent and realistic images. First, we evaluate the segmentation performance of a model trained solely with synthetic data and show that it only slightly underperforms compared to the baseline trained with real data. By further combining real with synthetic data during training, we observe a substantial improvement in segmentation performance (up to 4% and 40% in terms of Dice score and Hausdorff distance) across multiple data-sets collected from various sites and scanner. This is additionally demonstrated across state-of-the-art 2D and 3D segmentation networks, whereby the obtained results demonstrate the potential of the proposed method in tackling the presence of the domain shift in medical data. Finally, we thoroughly analyze the quality of synthetic data and its ability to replace real MR images during training, as well as provide an insight into important aspects of utilizing synthetic images for segmentation. © 2022 The Author(s),Deep neural networks have achieved excellent cell or nucleus quantification performance in microscopy images, but they often suffer from performance degradation when applied to cross-modality imaging data. Unsupervised domain adaptation (UDA) based on generative adversarial networks (GANs) has recently improved the performance of cross-modality medical image quantification. However, current GAN-based UDA methods typically require abundant target data for model training, which is often very expensive or even impossible to obtain for real applications. In this paper, we study a more realistic yet challenging UDA situation, where (unlabeled) target training data is limited and previous work seldom delves into cell identification. We first enhance a dual GAN with task-specific modeling, which provides additional supervision signals to assist with generator learning. We explore both single-directional and bidirectional task-augmented GANs for domain adaptation. Then, we further improve the GAN by introducing a differentiable, stochastic data augmentation module to explicitly reduce discriminator overfitting. We examine source-, target-, and dual-domain data augmentation for GAN enhancement, as well as joint task and data augmentation in a unified GAN-based UDA framework. We evaluate the framework for cell detection on multiple public and in-house microscopy image datasets, which are acquired with different imaging modalities, staining protocols and/or tissue preparations. The experiments demonstrate that our method significantly boosts performance when compared with the reference baseline, and it is superior to or on par with fully supervised models that are trained with real target annotations. In addition, our method outperforms recent state-of-the-art UDA approaches by a large margin on different datasets. © 2023 Elsevier B.V.,In the recent past, deep learning-based models have achieved tremendous success in computer vision-related tasks with the help of large-scale annotated datasets. An interesting application of deep learning is synthetic data generation, especially in the domain of medical image analysis. The need for such a task arises due to the scarcity of original data. Class imbalance is another reason for applying data augmentation techniques. Generative Adversarial Networks (GANs) are beneficial for synthetic image generation in various fields. However, stand-alone GANs may only fetch the localized features in the latent representation of an image, whereas combining different GANs might understand the distributed features. To this end, we have proposed AGGrGAN, an aggregation of three base GAN models—two variants of Deep Convolutional Generative Adversarial Network (DCGAN) and a Wasserstein GAN (WGAN) to generate synthetic MRI scans of brain tumors. Further, we have applied the style transfer technique to enhance the image resemblance. Our proposed model efficiently overcomes the limitation of data unavailability and can understand the information variance in multiple representations of the raw images. We have conducted all the experiments on the two publicly available datasets - the brain tumor dataset and the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 dataset. Results show that the proposed model can generate fine-quality images with maximum Structural Similarity Index Measure (SSIM) scores of 0.57 and 0.83 on the said two datasets. © 2022, The Author(s)."
127,126,35,126_Wildfire risk assessment and prediction,Wildfire risk assessment and prediction,"Analysing wildfire initiation patterns and identifying their primary drivers is essential for the development of more efficient fire prevention strategies. However, such analyses have traditionally been conducted at local or national scales, hindering cross-border comparisons and the formulation of broad-scale policy initiatives. In this study, we present an analysis of the spatial variability of wildfire initiations across Europe, focusing specifically on moderate to large fires (> 100 ha), and examining the influence of both human and climatic factors on initiation areas. We estimated drivers of fire initiation using machine learning algorithms, specifically Random Forest (RF), covering the majority of the European territory (referred to as the “ET scale”). The models were trained using data on fire initiations extracted from a satellite burned area product, comprising fires occurring from 2001 to 2019. We developed six RF models: three considering all fires larger than 100 ha, and three focused solely on the largest events (> 1000 ha). Models were developed using climatic and human predictors separately, as well as both types of predictors mixed together. We found that both climatic and mixed models demonstrated moderate predictive capacity, with AUC values ranging from 79 % to 81 %; while models based only on human variables have had poor predictive capacity (AUC of 60 %). Feature importance analysis, using Shapley Additive Explanations (SHAP), allowed us to assess the primary drivers of wildfire initiations across the European Territory. Aridity and evapotranspiration had the strongest effect on fire initiation. Among human variables, population density and aging had considerable effects on fire initiation, the former with a strong effect in mixed models estimating large fires, while the latter had a more important role in the prediction of very large fires. Distance to roads and forest-agriculture interfaces were also relevant in some initiation models. A better understanding of drivers of main fire events should help designing European forest fire management strategies, particularly in the light of growing importance of climate change, as it would affect both fire severity and areas at risk. Factors of fire initiation should also be part of a comprehensive approach for fire risk assessment, reduction and adaption, contributing to more effective wildfire management and mitigation across the continent. © 2024 The Authors,Wildfires are a growing management concern in western US rangelands, where invasive annual grasses have altered fire regimes and contributed to an increased incidence of catastrophic large wildfires. Fire activity in arid, nonforested ecosystems is thought to be largely controlled by interannual variation in fuel amount, which in turn is controlled by antecedent weather. Thus, long-range forecasting of fire activity in rangelands should be feasible given annual estimates of fuel quantity. Using a 32-yr time series of spatial data, we employed machine learning algorithms to predict the relative probability of large (> 405 ha) wildfire in the Great Basin based on fine-scale annual and 16-d estimates of cover and production of vegetation functional groups, weather, and multitemporal scale drought indices. We evaluated the predictive utility of these models with a leave-1-yr-out cross-validation, building spatial hindcasts of fire probability for each year that we compared against actual footprints of large wildfires. Herbaceous aboveground biomass production, bare ground cover, and long-term drought indices were the most important predictors of burning. Across 32 fire seasons, 88% of the area burned in large wildfires coincided with the upper 3 deciles of predicted fire probabilities. At the scale of the Great Basin, several metrics of fire activity were moderately to strongly correlated with average fire probability, including total area burned in large wildfires, number of large wildfires, and maximum fire size. Our findings show that recent years of exceptional fire activity in the Great Basin were predictable based on antecedent weather-driven growth of fine fuels and reveal a significant increasing trend in fire probability over the past 3 decades driven by widespread changes in fine fuel characteristics. © 2022 The Author(s),Considerable economic losses and ecological damage can be caused by forest fires, and compared to suppression, prevention is a much smarter strategy. Accordingly, this study focuses on developing a novel framework to assess forest fire risks and policy decisions on forest fire management in China. This framework integrated deep learning algorithms, geographic information, and multisource data. Compared to conventional approaches, our framework featured timesaving, easy implementation, and importantly, the use of deep learning that vividly integrates various factors from the environment and human activities. Information on 96,594 forest fire points from 2001 to 2019 was collected on Moderate Resolution Imaging Spectroradiometer (MODIS) fire hotspots from 2001 to 2019 from NASA's Fire Information Resource Management System. The information was classified into factors such as topography, climate, vegetation, and society. The prediction of forest fire risk was generated using a fully connected network model, and spatial autocorrelation used to analyze the spatial aggregation correlation of active fire hotspots in the whole area of China. The results show that high accuracy prediction of fire risks was achieved (accuracy 87.4%, positive predictive value 87.1%, sensitivity 88.9%, area under curve (AUC) 94.1%). Based on this, it was found that Chinese forest fire risk shows significant autocorrelation and agglomeration both in seasons and regions. For example, forest fire risk usually raises dramatically in spring and winter, and decreases in autumn and summer. Compared to the national average, Yunnan Province, Guangdong Province, and the Greater Hinggan Mountains region of Heilongjiang Province have higher fire risks. In contrast, a large region in central China has been recognized as having a long-term, low risk of forest fires. All forest risks in each region were recorded into the database and could contribute to the forest fire prevention. The successful assessment of forest fire risks in this study provides a comprehensive knowledge of fire risks in China over the last 20 years. Deep learning showed its advantage in integrating multiple factors in predicting forest fire risks. This technical framework is expected to be a feasible evaluation tool for the occurrence of forest fires in China. © 2022, Northeast Forestry University."
128,127,35,127_Temporal Action Recognition and Localization in Untrimmed Videos,Temporal Action Recognition and Localization in Untrimmed Videos,"Weakly-supervised temporal action localization aims to identify and localize action instances in untrimmed videos using only video-level action labels. Due to the lack of frame-level annotation information, correctly distinguishing foreground and background snippets in a video is crucial for temporal action localization. However, alongside foreground and background snippets, a large number of semantically similar snippets exist within the video. Such snippets share the same semantic information with foreground or background, leading to less fine-grained boundary localization of action instances. Inspired by the success of multimodal learning, we have extracted high-quality semantic features from multimodal inputs and constructed contrast loss to enhance the ability of the model to distinguish semantically similar snippets. In this paper, we propose a fusion detection network with discriminative enhancement(De-FDN). Specifically, we design a fusion detection model (FDM) that fully leverages the complementarity and correlation among multimodal features to extract high-quality semantic features from videos. We then construct multimodal class activation sequences to accomplish accurate identification and localization of action instances. Additionally, we design a discriminative enhancement mechanism (DEM), which increases the gap between semantically similar segments by calculating the semantic contrast loss. Extensive experiments on the THUMOS14, ActivityNet1.2, and ActivityNet1.3 datasets demonstrate the effectiveness of our method. © 2023 Elsevier Ltd,As the cornerstone of human-behavior analysis in video understanding, temporal action proposal generation aims to predict the starting and ending time of human action instances in untrimmed videos. Although large achievements in temporal action proposal generation have been achieved, most previous studies ignore the variability of action frequency in raw videos, leading to unsatisfying performances on high-action-frequency videos. In fact, there exists two main issues which should be well addressed: data imbalance between high and low action-frequency videos, and inferior detection of short actions in high-action-frequency videos. To address the above issues, we propose an effective framework by adapting to the variability of action frequency, namely Action Frequency Adaptive Network (AFAN), which can be flexibly built upon any temporal action proposal generation method. AFAN consists of two modules: Learning From Experts (LFE) and Fine-Grained Processing (FGP). The LFE first trains a series of action proposal generators on different subsets of imbalanced data as experts and then teaches a unified student model via knowledge distillation. To better detect short actions, FGP first finds out high-action-frequency videos and then performs fine-grained detection. Extensive experimental results on four benchmark datasets (ActivityNet-1.3, HACS, THUMOS14 and FineAction) demonstrate the effectiveness and generalizability of the proposed AFAN, especially for high-action-frequency videos.  © 1999-2012 IEEE.,Temporal action proposal (TAP) aims to detect the action instances’ starting and ending times in untrimmed videos, which is fundamental and critical for large-scale video analysis and human action understanding. The main challenge of the temporal action proposal lies in modeling representative temporal relations in long untrimmed videos. Existing state-of-the-art methods achieve temporal modeling by building local-level, proposal-level, or global-level temporal dependencies. Local methods lack a wider receptive field, while proposal and global methods lack the focalization of learning action frames and contain background distractions. In this paper, we propose that learning semantic-level affinities can capture more practical information. Specifically, by modeling semantic associations between frames and action units, action segments (foregrounds) can aggregate supportive cues from other co-occurring actions, and nonaction clips (backgrounds) can learn the discriminations between them and action frames. To this end, we propose a novel framework named the Mask-Guided Network (MGNet) to build semantic-level temporal associations for the TAP task. Specifically, we first propose a Foreground Mask Generation (FMG) module to adaptively generate the foreground mask, representing the locations of the action units throughout the video. Second, we design a Mask-Guided Transformer (MGT) by exploiting the foreground mask to guide the self-attention mechanism to focus on and calculate semantic affinities with the foreground frames. Finally, these two modules are jointly explored in a unified framework. MGNet models the intra-semantic similarities for foregrounds, extracting supportive action cues for boundary refinement; it also builds the inter-semantic distances for backgrounds, providing the semantic gaps to suppress false positives and distractions. Extensive experiments are conducted on two challenging datasets, ActivityNet-1.3 and THUMOS14, and the results demonstrate that our method achieves superior performance. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
129,128,33,128_Fault detection in solar photovoltaic (PV) systems using machine learning and deep learning techniques.,Fault detection in solar photovoltaic (PV) systems using machine learning and deep learning techniques.,"Solar energy production has significantly increased in recent years in the European Union (EU), accounting for 12% of the total in 2022. The growth in solar energy production can be attributed to the increasing adoption of solar photovoltaic (PV) panels, which have become cost-effective and efficient means of energy production, supported by government policies and incentives. The maturity of solar technologies has also led to a decrease in the cost of solar energy, making it more competitive with other energy sources. As a result, there is a growing need for efficient methods for detecting and mapping the locations of PV panels. Automated detection can in fact save time and resources compared to manual inspection. Moreover, the resulting information can also be used by governments, environmental agencies and other companies to track the adoption of renewable sources or to optimize energy distribution across the grid. However, building effective models to support the automated detection and mapping of solar photovoltaic (PV) panels presents several challenges, including the availability of high-resolution aerial imagery and high-quality, manually-verified labels and annotations. In this study, we address these challenges by first constructing a dataset of PV panels using very-high-resolution (VHR) aerial imagery, specifically focusing on the region of Piedmont in Italy. The dataset comprises 105 large-scale images, providing more than 9,000 accurate and detailed manual annotations, including additional attributes such as the PV panel category. We first conduct a comprehensive evaluation benchmark on the newly constructed dataset, adopting various well-established deep-learning techniques. Specifically, we experiment with instance and semantic segmentation approaches, such as Rotated Faster RCNN and Unet, comparing strengths and weaknesses on the task at hand. Second, we apply ad-hoc modifications to address the specific issues of this task, such as the wide range of scales of the installations and the sparsity of the annotations, considerably improving upon the baseline results. Last, we introduce a robust and efficient post-processing polygonization algorithm that is tailored to PV panels. This algorithm converts the rough raster predictions into cleaner and more precise polygons for practical use. Our benchmark evaluation shows that both semantic and instance segmentation techniques can be effective for detecting and mapping PV panels. Instance segmentation techniques are well-suited for estimating the localization of panels, while semantic solutions excel at surface delineation. We also demonstrate the effectiveness of our ad-hoc solutions and post-processing algorithm, which can provide an improvement up to +10% on the final scores, and can accurately convert coarse raster predictions into usable polygons.  © 2013 IEEE.,The widespread adoption of photovoltaic (PV) technology for renewable energy necessitates accurate segmentation of PV panels to estimate installation capacity. However, achieving highly efficient and precise segmentation methods remains a pressing challenge. Recent advancements in artificial intelligence and remote sensing techniques have shown promise in PV segmentation. Nevertheless, real-world scenarios introduce complexities such as diverse sensing platforms, sensors, panel categories, and testing regions. These factors contribute to resolution, size, and foreground-background class imbalances, impeding accurate and generalized PV panel segmentation over large areas. To address these challenges, we propose GenPV, a deep learning model that leverages data distribution analysis and PV panel characteristics to enhance segmentation accuracy and generalization. GenPV employs a multi-scale feature learning approach, utilizing an enhanced feature pyramid network to fuse data features from multiple resolutions, effectively addressing resolution imbalance. Moreover, inductive learning is employed through a multitask approach, facilitating the detection and identification of both small and large-sized PV panels to mitigate size imbalance. To address significant class imbalance in PV panel recognition tasks, we integrate the Focal loss function for effective hard sample mining. Through experimental evaluation conducted in Heilbronn, Germany, our proposed method demonstrates superior performance compared to state-of-the-art approaches in PV panel segmentation. The results exhibit progressively higher accuracy and improved generalization capability. These findings highlight the potential of our method to serve as an advanced and practical tool for PV segmentation in the renewable energy field. © 2023,Photovoltaic (PV) boards are a perfect way to create eco-friendly power from daylight. The defects in the PV panels are caused by various conditions; such defective PV panels need continuous monitoring. The recent development of PV panel monitoring systems provides a modest and viable approach to monitoring and managing the condition of the PV plants. In general, conventional procedures are used to identify the faulty modules earlier and to avoid declines in power generation. The existing deep learning architectures provide the required output to predict the faulty PV panels with less accuracy and a more time-consuming process. To increase the accuracy and to reduce the processing time, a new Convolutional Neural Network (CNN) architecture is required. Hence, in the present work, a new Real-time Multi Variant Deep learning Model (RMVDM) architecture is proposed, and it extracts the image features and classifies the defects in PV panels quickly with high accuracy. The defects that arise in the PV panels are identified by the CNN based RMVDM using RGB images. The biggest difference between CNN and its predecessors is that CNN automatically extracts the image features without any help from a person. The technique is quantitatively assessed and compared with existing faulty PV board identification approaches on the large real-time dataset. The results show that 98% of the accuracy and recall values in the fault detection and classification process. © 2023 Tech Science Press. All rights reserved."
130,129,32,129_Photonic Optimization for All-Optical Neural Networks,Photonic Optimization for All-Optical Neural Networks,"Diffractive optical neural networks have shown promising advantages over electronic circuits for accelerating modern machine learning (ML) algorithms. However, it is challenging to achieve fully programmable all-optical implementation and rapid hardware deployment. Here, a large-scale, cost-effective, complex-valued, and reconfigurable diffractive all-optical neural networks system in the visible range is demonstrated based on cascaded transmissive twisted nematic liquid crystal spatial light modulators. The employment of categorical reparameterization technique creates a physics-aware training framework for the fast and accurate deployment of computer-trained models onto optical hardware. Such a full stack of hardware and software enables not only the experimental demonstration of classifying handwritten digits in standard datasets, but also theoretical analysis and experimental verification of physics-aware adversarial attacks onto the system, which are generated from a complex-valued gradient-based algorithm. The detailed adversarial robustness comparison with conventional multiple layer perceptrons and convolutional neural networks features a distinct statistical adversarial property in diffractive optical neural networks. The developed full stack of software and hardware provides new opportunities of employing diffractive optics in a variety of ML tasks and in the research on optical adversarial ML. © 2022 Wiley-VCH GmbH.,Traditional computers are limited by the separation of memory and processor units, is difficult to achieve fast, efficient, and low-power computing. While photonic spiking neural networks (SNNs) can overcome these shortcomings, they encounter limitations in large-scale integration. Silicon photonics platform, compatible with mature Complementary Metal Oxide Semiconductor (CMOS) platforms, is a promising candidate for realizing large-scale photonic SNNs. In this work, we proposed an integrated photonic SNN by exploiting the photonic properties of phase-change material (PCM) Ge2Sb2Te5 (GST) and micro-ring resonators (MRR), and demonstrated its integrate-and-fire (IF) behavior. Based on a system-level behavioral model, we adopt an improved Tempotron-like ReSuMe supervised learning algorithm to train the proposed photonic SNNs and complete a pattern recognition task for the clock's 12 clockwise directions. Then the influence of different noise levels is considered, and the accuracy is close to 1 when the noise level is less than 0.2. We propose a photonic implementation of such an SNN system, and use wavelength division multiplexing to achieve a scalable architecture for the pattern recognition task. The collaborative design and optimization of hardware architecture and algorithm are realized, providing a theoretical basis for the realization of photonic SNN based on MRRs and PCM. © 2023 Elsevier B.V.,In light of recent achievements in optical computing and machine learning, we consider the conditions under which all-optical computing may surpass electronic and optoelectronic computing in terms of energy efficiency and scalability. When considering the performance of a system as a whole, the cost of memory access and data acquisition is likely to be one of the main efficiency bottlenecks not only for electronic, but also for optoelectronic and all-optical devices. However, we predict that all-optical devices will be at an advantage in the case of inference in large neural network models, and the advantage will be particularly large in the case of generative models. We also consider the limitations of all-optical neural networks, including footprint, strength of nonlinearity, optical signal degradation, limited precision of computations, and quantum noise.  © 2024 American Physical Society."
131,130,32,130_Image Dehazing and Rain Streaks Removal,Image Dehazing and Rain Streaks Removal,"Traditional dehazing approaches that rely on prior knowledge exhibit limited efficacy when confronted with the intricacies of real-world hazy environments. While learning-based dehazing techniques necessitate large-scale datasets for effective model training, the acquisition of these datasets is time-consuming and laborious, and the resulting models may encounter a domain shift when processing real-world hazy images. To overcome the limitations of prior-based and learning-based dehazing methods, we propose a self-supervised remote sensing (RS) image-dehazing network based on zero-shot learning, where the self-supervised process avoids dense dataset requirements and the learning-based structures refine the artifacts in extracted image priors caused by complex real-world environments. The proposed method has three stages. The first stage involves pre-processing the input hazy image by utilizing a prior-based dehazing module; in this study, we employed the widely recognized dark channel prior (DCP) to obtain atmospheric light, a transmission map, and the preliminary dehazed image. In the second stage, we devised two convolutional neural networks, known as RefineNets, dedicated to enhancing the transmission map and the initial dehazed image. In the final stage, we generated a hazy image using the atmospheric light, the refined transmission map, and the refined dehazed image by following the haze imaging model. The meticulously crafted loss function encourages cycle-consistency between the regenerated hazy image and the input hazy image, thereby facilitating a self-supervised dehazing model. During the inference phase, the model undergoes training in a zero-shot manner to yield the haze-free image. These thorough experiments validate the substantial improvement of our method over the prior-based dehazing module and the zero-shot training efficiency. Furthermore, assessments conducted on both uniform and non-uniform RS hazy images demonstrate the superiority of our proposed dehazing technique. © 2023 by the authors.,Single image dehazing is a critical problem in computer vision. However, most recently proposed learning-based dehazing methods achieve unsatisfactory quality with dehazed images due to inaccurate parametric estimation. The size of these models is also large to be applied with mobile devices' limited resources. Last, most models are tailored to image dehazing, achieving poor migration. Thus, we propose a compact multiscale attention feature fusion network with a model size of 2 MB called MSAFF-Net to perform end-to-end single image dehazing. In the proposed model, we design a simple and powerful feature extraction module to extract complex features from hazy images. We use a channel attention module and a multiscale spatial attention module to consider the regions with haze-relevant features. To our knowledge, this study is the first to directly apply the attention mechanism rather than to embed it into certain modules for single image dehazing. We compare MSAFF-Net with other approaches on the NTIRE18, RESIDE, and Middlebury Stereo datasets. We show that MSAFF-Net achieves comparable or better performance than other models. We also extend MSAFF-Net to single image deraining, and various experiments demonstrate its effectiveness. Results suggest that MSAFF-Net can directly restore clear images using channels with the most useful haze- or rain-relevant features and spatial locations.  © 1999-2012 IEEE.,Single-image rain streaks&#x2019; removal has attracted great attention in recent years. However, due to the highly visual similarity between the rain streaks and the line pattern image edges, the over-smoothing of image edges or residual rain streaks&#x2019; phenomenon may unexpectedly occur in the deraining results. To overcome this problem, we propose a direction and residual awareness network within the curriculum learning paradigm for the rain streaks&#x2019; removal. Specifically, we present a statistical analysis of the rain streaks on large-scale real rainy images and figure out that rain streaks in local patches possess principal directionality. This motivates us to design a direction-aware network for rain streaks&#x2019; modeling, in which the principal directionality property endows us with the discriminative representation ability of better differing rain streaks from image edges. On the other hand, for image modeling, we are motivated by the iterative regularization in classical image processing and unfold it into a novel residual-aware block (RAB) to explicitly model the relationship between the image and the residual. The RAB adaptively learns balance parameters to selectively emphasize informative image features and better suppress the rain streaks. Finally, we formulate the rain streaks&#x2019; removal problem into the curriculum learning paradigm which progressively learns the directionality of the rain streaks, rain streaks&#x2019; appearance, and the image layer in a coarse-to-fine, easy-to-hard guidance manner. Solid experiments on extensive simulated and real benchmarks demonstrate the visual and quantitative improvement of the proposed method over the state-of-the-art methods. IEEE"
132,131,32,131_Robotic Grasping and Tactile Sensing,Robotic Grasping and Tactile Sensing,"Industrial Cyber Physical Systems can use data and information gained from across a variety of different environments to enable robots that are reconfigurable. Custom grasping is a basic operation a robot must be able to carry out for a given task, i.e., finding the best grasping point for emergent behaviors. However, environmental disturbance and limited data degrade the precision and speed of many tailored machine learning models on robot grasping detection. This paper proposes a region-based method to enable fast custom grasping through fewer RGB-D data. The grasping detection problem is simplified as a two-stage prediction problem. At the first stage, a robust grasp candidate generation strategy is proposed based on the Sobel operator. At the second stage, a region-based predictor is designed to locate the best grasping point-pair for an emergent task. The predictor is trained by a modified consistency based self-training method to realize semi-supervised learning. Experimental results show that the success rate of custom grasping of new emergent object can be increased by 3.4% on average using the proposed method. By introducing data augmentation strategies in training, the success rate is further increased by 9.2% on average. A robot is able to grasp new object with 91.5% success rate using less than 100 training samples. The number of training samples required for the proposed method is less than to 1% of which for the previous works. Note to Practitioners - This research was motivated by the problem of robot reconfigurability for various industrial automation processes and focuses mainly on the recognition of grasping point-pair of emergent object for different task. Existing approaches on robotic grasping detection are tailored to a given object and require expensive training with large amount of labeled data. This paper presents a region-based few shot learning approach that enables the robot to detect the best grasping point-pair autonomously and quickly. We show how to generate candidate point-pairs with image distortion and background disturbance. We then demonstrate how the best grasping point-pair can be located with much less training cost. Experiments suggest that this approach is feasible in robot automation for handling a class of objects. In future research, we will construct behavior learning module to enable evolving cyber-physical robotic system for more purposes.  © 2004-2012 IEEE.,Robotic grasping is one of the most fundamental robotic manipulation tasks and has been the subject of extensive research. However, swiftly teaching a robot to grasp a novel target object in clutter remains challenging. This paper attempts to address the challenge by leveraging object attributes that facilitate recognition, grasping, and rapid adaptation to new domains. In this work, we present an end-to-end encoder-decoder network to learn attribute-based robotic grasping with data-efficient adaptation capability. We first pre-train the end-to-end model with a variety of basic objects to learn generic attribute representation for recognition and grasping. Our approach fuses the embeddings of a workspace image and a query text using a gated-attention mechanism and learns to predict instance grasping affordances. To train the joint embedding space of visual and textual attributes, the robot utilizes object persistence before and after grasping. Our model is self-supervised in a simulation that only uses basic objects of various colors and shapes but generalizes to novel objects in new environments. To further facilitate generalization, we propose two adaptation methods, adversarial adaption and one-grasp adaptation. Adversarial adaptation regulates the image encoder using augmented data of unlabeled images, whereas one-grasp adaptation updates the overall end-to-end model using augmented data from one grasp trial. Both adaptation methods are data-efficient and considerably improve instance grasping performance. Experimental results in both simulation and the real world demonstrate that our approach achieves over 81&#x0025; instance grasping success rate on unknown objects, which outperforms several baselines by large margins. Supplementary material is available at <uri>https://z.umn.edu/attr-grasp</uri>. IEEE,Damage-free grasping of deformable objects has been a long-standing difficult problem in the field of robotics. Humans can perceive the physical properties of objects and apply accurate force to complete dexterous and non-destructive operations when grasping vulnerable targets. In order to transfer this ability from humans to robots, a special neural network utilizing the improved Transformer structure is proposed to process the complete tactile time sequence during the grasping, considering the fabulous performance and extensive successful application of deep learning on large-scale datasets. Compared with computer vision, there are far from enough grasp datasets in the haptic domain. Tactile datasets for fruit grasping are almost unavailable. So we established a tactile dataset containing 9375 grasp of 15 fruits for experimental research. The proposed network has achieved a fruit recognition accuracy of 97.33% on this dataset, better than the traditional recurrent neural network (RNN) model. Furthermore, the performance of the proposed model is evaluated from various aspects, and the prediction of the subsequent grasp force is explored. Our work contributes to the realization of robotic haptic perception and damage-free grasping in the agricultural field, and can be helpful to fruit picking, handling, sorting and other related areas. Our method also provides a certain degree of technical reference for other researches on grasp tactile data. © 2023 Elsevier B.V."
133,132,31,132_Transport safety and crash prediction using machine learning and artificial intelligence.,Transport safety and crash prediction using machine learning and artificial intelligence.,"For both passenger and freight transportation, railroad operations must be dependable, accessible, maintained, and safe (RAMS). In many urban areas, railway stations risk and safety accidents represent an essential safety concern for daily operations. Moreover, the accidents lead to damage to market reputation, including injuries and anxiety among the people and costs. This stations under pressure caused by higher demand which consuming infrastructure and raised the safety administration consideration. To analysing these accidents and utilising the technology such AI methods to enhance safety, it is suggested to use unsupervised topic modelling for better understand the contributors to these extreme accidents. It is conducted to optimise Latent Dirichlet Allocation (LDA) for fatality accidents in the railway stations from textual data gathered RSSB including 1000 accidents in the UK railway station. This research describes using the machine learning topic method for systematic spot accident characteristics to enhance safety and risk management in the stations and provides advanced analysing. The study evaluates the efficacy of text by mining from accident history, gaining information, lesson learned and deeply coherent of the risk caused by assessing fatalities accidents for large and enduring scale. This Intelligent Text Analysis presents predictive accuracy for valuable accident information such as root causes and the hot spots in the railway stations. Further, the big data analytics ' improvement results in an understanding of the accidents' nature in ways not possible if a considerable amount of safety history and not through narrow domain analysis of the accident reports. This technology renders stand with high accuracy and a beneficial and extensive new era of AI applications in railway industry safety and other fields for safety applications.  © 2013 IEEE.,Traffic safety has been of great concern in recent years. The prediction of the severity of traffic accidents is an important part of it. The occurrence of traffic accidents shows the characteristics of uncertainty and nonlinearity because of the influence of random factors. However, most of the existing models are single machine learning (ML) models, which have limitations in accuracy and generalization. This study proposes a traffic accident severity prediction model based on a combination of XGBoost (eXtreme Gradient Boosting) and Backpropagation Neural Network (BPNN). Firstly, feature selection is performed using the XGBOOST model. Secondly, the selected feature is used as the input layer of BPNN. In addition, traffic accidents have class imbalance, so the total cost is minimized by using cost-sensitive algorithm. Finally, the precision, recall and area under the curve (AUC) are used to evaluate the prediction results of the model. The 2005-2014 UK traffic accident dataset is used for prediction and compared with other machine learning models. Experiments show that (1) the XGBoost-BPNN model outperformed the single XGBoost, logistic regression (LR), and Support vector machine (SVM) models in terms of AUC, recall, and precision. (2) The number of neurons, the number of hidden layers and the learning rate of a neural network model have a large impact on the prediction accuracy. Increasing the number of neurons appropriately can improve the convergence speed and prediction effect of the model. This study can provide a reference for traffic accident prevention and early warning. © 2023, Aracne Editrice. All rights reserved.,The rapid increase in traffic volume on urban roads, over time, has altered the global traffic scenario. Additionally, it has increased the number of road crashes, some of which are severe and fatal in nature. The identification of hazardous roadway sections using the spatial pattern analysis of crashes and recognition of the primary and contributing factors may assist in reducing the severity of road traffic crashes (R.T.C.s). For crash severity prediction, along with spatial patterns, various machine learning models are used, and the spatial relations of R.T.C.s with neighboring areas are evaluated. In this study, tree-based ensemble models (gradient boosting and random forest) and a logistic regression model are compared for the prediction of R.T.C. severity. Sample data of road crashes in Al-Ahsa, the eastern province of Saudi Arabia, were obtained from 2016 to 2018. Random forest (R.F.) identifies significant features strongly correlated with the severity of the R.T.C.s. The analysis findings showed that the cause of the crash and the type of collision are the most crucial elements affecting the severity of injuries in traffic crashes. Furthermore, the target-specific model interpretation results showed that distracted driving, speeding, and sudden lane changes significantly contributed to severe crashes. The random forest (R.F.) method surpassed other models in terms of injury severity, individual class accuracies, and collective prediction accuracy when using k-fold (k = 10) based on various performance metrics. In addition to taking into account the machine learning approach, this study also included spatial autocorrelation analysis based on G.I.S. for identifying crash hotspots, and Getis Ord (Formula presented.) statistics were devised to locate cluster zones with high- and low-severity crashes. The results demonstrated that the research area’s spatial dependence was very strong, and the spatial patterns were clustered with a distance threshold of 500 m. The analysis’s approaches, which included Getis Ord (Formula presented.), the crash severity index, and the spatial autocorrelation of accident incidents according to Moran’s I, were found to be a successful way of locating and rating crash hotspots and crash severity. The techniques used in this study could be applied to large-scale crash data analysis while providing a useful tool for policymakers looking to improve roadway safety. © 2022 by the authors."
134,133,31,133_Fire detection using deep learning and attention-based models,Fire detection using deep learning and attention-based models,"Fire accidents occur in every part of the world and cause a large number of casualties because of the risks involved in manually extinguishing the fire. In most cases, humans cannot detect and extinguish fire manually. Fire extinguishing robots with sophisticated functionalities are being rapidly developed nowadays, and most of these systems use fire sensors and detectors. However, they lack mechanisms for the early detection of fire, in case of casualties. To detect and prevent such fire accidents in its early stages, a deep learning-based automatic fire extinguishing mechanism was introduced in this work. Fire detection and human presence in fire locations were carried out using convolution neural networks (CNNs), configured to operate on the chosen fire dataset. For fire detection, a custom learning network was formed by tweaking the layer parameters of CNN for detecting fires with better accuracy. For human detection, Alex-net architecture was employed to detect the presence of humans in the fire accident zone. We experimented and analyzed the proposed model using various optimizers, activation functions, and learning rates, based on the accuracy and loss metrics generated for the chosen fire dataset. The best combination of neural network parameters was evaluated from the model configured with an Adam optimizer and softmax activation, driven with a learning rate of 0.001, providing better accuracy for the learning model. Finally, the experiments were tested using a mobile robotic system by configuring them in automatic and wireless control modes. In automatic mode, the robot was made to patrol around and monitor for fire casualties and fire accidents. It automatically extinguished the fire using the learned features triggered through the developed model. © 2023 by the authors.,Intense, large-scale forest fires are damaging and very challenging to control. Locations, where various types of fire behavior occur, vary depending on environmental factors. According to the burning site of forest fires and the degree of damage, this paper considers the classification and identification of surface fires and canopy fires. Deep learning-based forest fire detection uses convolutional neural networks to automatically extract multidimensional features of forest fire images with high detection accuracy. To accurately identify different forest fire types in complex backgrounds, an improved forest fire classification and detection model (FCDM) based on YOLOv5 is presented in this paper, which uses image-based data. By changing the YOLOv5 bounding box loss function to SIoU Loss and introducing directionality in the cost of the loss function to achieve faster convergence, the training and inference of the detection algorithm are greatly improved. The Convolutional Block Attention Module (CBAM) is introduced in the network to fuse channel attention and spatial attention to improve the classification recognition accuracy. The Path Aggregation Network (PANet) layer in the YOLOv5 algorithm is improved into a weighted Bi-directional Feature Pyramid Network (BiFPN) to fuse and filter forest fire features of different dimensions to improve the detection of different types of forest fires. The experimental results show that this improved forest fire classification and identification model outperforms the YOLOv5 algorithm in both detection performances. The mAP@0.5 of fire detection, surface fire detection, and canopy fire detection was improved by 3.9%, 4.0%, and 3.8%, respectively. Among them, the mAP@0.5 of surface fire reached 83.1%, and the canopy fire detection reached 90.6%. This indicates that the performance of our proposed improved model has been effectively improved and has some application prospects in forest fire classification and recognition. © 2022 by the authors.,Forest fires occur frequently around the world, causing serious economic losses and human casualties. Deep learning techniques based on convolutional neural networks (CNN) are widely used in the intelligent detection of forest fires. However, CNN-based forest fire target detection models lack global modeling capabilities and cannot fully extract global and contextual information about forest fire targets. CNNs also pay insufficient attention to forest fires and are vulnerable to the interference of invalid features similar to forest fires, resulting in low accuracy of fire detection. In addition, CNN-based forest fire target detection models require a large number of labeled datasets. Manual annotation is often used to annotate the huge amount of forest fire datasets; however, this takes a lot of time. To address these problems, this paper proposes a forest fire detection model, TCA-YOLO, with YOLOv5 as the basic framework. Firstly, we combine the Transformer encoder with its powerful global modeling capability and self-attention mechanism with CNN as a feature extraction network to enhance the extraction of global information on forest fire targets. Secondly, in order to enhance the model’s focus on forest fire targets, we integrate the Coordinate Attention (CA) mechanism. CA not only acquires inter-channel information but also considers direction-related location information, which helps the model to better locate and identify forest fire targets. Integrated adaptively spatial feature fusion (ASFF) technology allows the model to automatically filter out useless information from other layers and efficiently fuse features to suppress the interference of complex backgrounds in the forest area for detection. Finally, semi-supervised learning is used to save a large amount of manual labeling effort. The experimental results show that the average accuracy of TCA-YOLO improves by 5.3 compared with the unimproved YOLOv5. TCA-YOLO also outperformed in detecting forest fire targets in different scenarios. The ability of TCA-YOLO to extract global information on forest fire targets was much improved. Additionally, it could locate forest fire targets more accurately. TCA-YOLO misses fewer forest fire targets and is less likely to be interfered with by forest fire-like targets. TCA-YOLO is also more focused on forest fire targets and better at small-target forest fire detection. FPS reaches 53.7, which means that the detection speed meets the requirements of real-time forest fire detection. © 2023 by the authors."
135,134,31,134_Combustion Kinetics and Machine Learning,Combustion Kinetics and Machine Learning,"This study proposes a machine learning tabulation (MLT) method that employs deep neural networks (DNNs) to predict ignition delay and knock propensity in spark ignition (SI) engines. The commonly used Arrhenius model and Livengood-Wu integral for fast knock prediction are not accurate enough to account for residual gas species and may require adjustments or modifications to account for specific engine characteristics. Detailed kinetics modeling is computationally expensive, so the MLT approach is introduced to solve these issues. The MLT method uses precalculated thermochemical states of the mixture that are clustered based on a combustion progress variable. Hundreds of DNNs are trained with the stochastic Levenberg-Marquardt (SLM) optimization algorithm, reducing training time and memory requirements for large-scale problems. MLT has high interpolation accuracy, eliminates the need for table storage, and reduces memory requirements by three orders of magnitude. The proposed MLT approach can operate across a wider range of conditions and handle a variety of fuels, including those with complex reaction mechanisms. MLT computational time is independent of the reaction mechanism's size. It demonstrates a remarkable capability to reduce computation time by a factor of approximately 300 when dealing with complex reaction mechanisms comprising 621 species. MLT has the potential to significantly advance our understanding of complex combustion processes and aid in the design of more efficient and environmentally friendly combustion engines. In summary, the MLT approach has acceptable accuracy with less computation cost than detailed kinetics, making it ideal for fast model-based knock detection. This article presents a detailed description of the MLT method, including its workflow, challenges involved in data generation, pre-processing, data classification and regression, and integration into the engine cycle simulation. The results of the study are summarized, which includes validation against kinetics for ignition delay and engine simulation for knock angle prediction. The conclusions are presented along with future work.  © 2024 SAE International.,The development of skeletal mechanisms has become essential for multi-dimensional simulations of plasma-assisted combustion (PAC). However, reduction tools developed for traditional combustion applications are not always applicable to PAC, due to the complex interplay between non-equilibrium plasma and combustion kinetics. Plasma direct relation graph with error propagation (P-DRGEP) is a recent plasma-specific reduction method developed in order to incorporate plasma energy branching in the reduction. In the first part of this work, the applicability of P-DRGEP to large kinetic mechanisms is investigated. A detailed isooctane/air plasma mechanism containing 2805 species and 18457 reactions is reduced to 415 species and 4716 reactions, keeping errors on ignition time within 3% for a wide range of initial conditions: from 750 K to 1200 K, 10 atm and equivalence ratios from 0.75 to 1.50. The second part focuses on isomer lumping, which is another reduction technique widely used in combustion. When applied to PAC, it is shown that the resulting lumped mechanism produces poor results. A novel plasma-specific isomer lumping strategy using machine learning is proposed instead. With the supervised algorithm of gradient boosting, predictive regression models are generated, which describe rate coefficients of lumped reactions adequately. These models are trained with simulation data. Leveraging this newly proposed lumping approach on the reduced mechanism, allows for an additional 28% reduction in the number of species and 19% reduction in the number of reactions. Two different versions are presented: in the first one the models are trained using one input feature (1D), while in the second one, two input features are selected (2D). The resulting lumped mechanism is shown to produce accurate predictions of PAC over the entire parameter space of interest, while significantly decreasing the computational time. Indicatively, with the 1D version the maximum error on ignition time in this range of conditions is 6%. The 2D approach produces even lower errors, which do not exceed 3%. Novelty and significance statement In this work, a novel approach for isomer lumping, in plasma-assisted combustion mechanisms, is demonstrated. This plasma-specific approach, uses predictive machine learning regression models to describe the complex evolution of lumped reaction rate coefficients. Combining it with the plasma direct relation graph with error propagation, a powerful reduction framework is created, which is successfully demonstrated on a detailed isooctane/air plasma kinetic mechanism, via zero-dimensional ignition simulations. This framework constitutes a useful tool towards the creation of highly accurate skeletal mechanisms, which significantly reduce the computational costs of simulations. © 2023 The Combustion Institute,The numerical integration of the differential equations describing chemical kinetics consumes the majority of computational time in combustion simulations that involve direct coupling of chemistry and flow, such as transported probability density function (PDF) methods, direct numerical simulation (DNS), conditional moment closure (CMC), unsteady flamelet, multiple mapping closure (MMC), thickened flame model, linear eddy model (LEM), partially stirred reactor (PaSR) as in OpenFOAM and laminar flame computation. This step can be accelerated by tabulation, and artificial neural networks (ANNs) have recently emerged as a powerful technique in this domain. To be applicable to a wide family of problems, an ANN tabulation approach must be based on data generated by an abstract process, rather than from the turbulent flame to be simulated. In the present work, the hybrid flamelet/random data and multiple multilayer perceptrons (HFRD-MMLP) method (Ding et al., Combust. Flame 231, 111493, 2021) for non-premixed flames is taken as a basis to develop a thermochemistry tabulation method for premixed flames. In the spirit of maintaining an essentially random data set that still originates in meaningful composition states, a set of one-dimensional premixed flame simulations is employed to generate data that are used as starting points for a random data generation process and subsequently discarded. The approach is applied to large eddy simulations (LES) of the Cambridge/Sandia swirl burner in configurations five and six, with the transported PDF method employed to provide closure for the filtered reaction source terms and the stochastic fields method used for numerical solution. Very good agreement in both major and minor species is observed between the LES-PDF simulations using direct integration of the reaction source term and the ones with the ANNs. Furthermore, the average time taken for reaction source term computations is reduced by fourteen times, while memory requirements constitute only 1.4 MB. © 2023 The Author(s)"
136,135,31,"135_Document classification techniques for patents, citations, and scholarly papers.","Document classification techniques for patents, citations, and scholarly papers.","Citation and citation-based metrics are traditionally used to quantify the scholarly impact of scientific papers. However, for documents without citation data, i.e., newly published papers, the citation-based metrics are not available. By leveraging deep representation techniques, we propose a text-content based approach that may reveal the scholarly impact of papers without human domain-specific knowledge. Specifically, a large-scale Pre-Trained Model (PTM) with 110 million parameters is utilized to automatically encode the paper into the vector representation. Two indicators, ?(Topicality) and ?(Originality), are then proposed based on the learned representations. These two indicators leverage the spatial relations of paper representations in the semantic space to capture the impact-related characteristics of a scientific paper. Extensive experiments have been conducted on a COVID-19 open research dataset with 1,056,660 papers. The experimental results demonstrate that the deep representation learning method can better capture the scientific content in the published literature; and the proposed indicators are positively and significantly associated with a paper's potential scholarly impact. In the multivariate regression analysis for the potential impact of a paper, the coefficients of ?and ?are 5.4915 (P<0.001) and 6.6879 (P<0.001) for next 6 months prediction, 12.9964 (P<0.001) and 13.8678 (P<0.001) for next 12 months prediction. The proposed framework may facilitate the study of how scholarly impact is generated, from a textual representation perspective. © 2023 Elsevier Editora Ltda. All rights reserved.,With the rapid development of scientific research, a large number of scientific papers are produced every year. It is very important to find influential papers quickly from the massive literature resources, which can not only help researchers identify papers with reference value, but also help scientific research management departments to allocate resources. Among the quantification measures of academic impact, citation count stands out for its frequent use in the research community. Previous studies have either treated papers as independent individuals without considering their citation relationships in the citation network or have not adequately considered the long-time dependence of citation time series. In this paper, we consider the structural features of citation networks and propose a deep learning method AGSTA-NET from the perspective of spatio-temporal fusion, which models heterogeneous citation networks formed early in the publication of a paper and predicts the citation count for an article in the next few years. AGSTA-NET contains capturing module of spatial dependence and capturing module of time dependence. It could fully dig the complex spatio-temporal information from the dynamic heterogeneous citation network by only inputting the heterogeneous citation network to the model. Meanwhile, the sub-networks designed in this paper could adaptively determine the threshold of the loss function according to the samples for better training. Experiments validate that AGSTA-NET outperforms current state-of-the-art methods in citation count prediction. © 2022, Akadémiai Kiadó, Budapest, Hungary.,Given the exponential growth of patent documents, automatic patent summarization methods to facilitate the patent analysis process are in strong demand. Recently, the development of natural language processing (NLP), text-mining, and deep learning has greatly improved the performance of text summarization models for general documents. However, existing models cannot be successfully applied to patent documents, because patent documents describing an inventive technology and using domain-specific words have many differences from general documents. To address this challenge, we propose in this study a multi-patent summarization approach based on deep learning to generate an abstractive summarization considering the characteristics of a patent. Single patent summarization and multi-patent summarization were performed through a patent-specific feature extraction process, a summarization model based on generative adversarial network (GAN), and an inference process using topic modeling. The proposed model was verified by applying it to a patent in the drone technology field. In consequence, the proposed model performed better than existing deep learning summarization models. The proposed approach enables high-quality information summary for a large number of patent documents, which can be used by R&D researchers and decision-makers. In addition, it can provide a guideline for deep learning research using patent data. © 2022 Elsevier Ltd"
137,136,30,136_Machining efficiency and tool wear prediction in difficult-to-cut materials,Machining efficiency and tool wear prediction in difficult-to-cut materials,"Local Interpretable and Model-agnostic Explanation (LIME), an explainable artificial intelligence (XAI) approach is adapted to identify the globally important time-frequency bands for predicting average surface roughness (Ra) in a smart grinding process. The smart grinding setup consisted of a Supertech CNC precision surface grinding machine, instrumented with a Dytran piezoelectric accelerometer attached to the tailstock quill along the tangential direction (Y-axis). For every grinding pass, vibration signatures were captured, and the ground truth surface roughness values were recorded using a Mahr Marsurf M300C portable surface roughness profilometer. The roughness values ranged from 0.06 to 0.14 microns over the complete set of experiments. Time-frequency domain spectrogram frames were extracted for each of the vibration signals collected during the grinding process. Convolutional Neural Networks (CNNs) were modeled to predict the surface roughness based on these spectrogram frames and their image augmentations. The best CNN model was able to predict the roughness values with an overall R2-score of 0.95, training R2-score of 0.99, and testing R2-score of 0.81 with only 80 sets of vibration signals corresponding to 4 experiments with 20 trials each. Although the data size is not large enough to guarantee such performance metrics in real-world scenarios, one can extract statistically consistent explanations underlying the relationships these complex deep learning models capture. The LIME methodology was implemented on the developed surface roughness CNN model to identify the important time-frequency bands (i.e., the superpixels of a spectrogram) influencing the predictions. Based on the identified important regions on the spectrogram frames, the corresponding frequency characteristics were determined that influence the surface roughness predictions. The important frequency range based on LIME results was approximately 11.7 to 19.1 kHz. The power of XAI was demonstrated by cutting down the sampling rate from 160 kHz to 30, 20, 10, and 5 kHz based on the important frequency range and considering Nyquist criteria. Separate CNN models were developed for these ranges by only extracting time-frequency contents below their corresponding Nyquist cut-offs. A proper data acquisition strategy is proposed by comparing the model performances to argue the selection of a sufficient sampling rate to capture the grinding process successfully and robustly. © 2023 The Society of Manufacturing Engineers,Local Interpretable and Model-agnostic Explanation (LIME), an explainable artificial intelligence (XAI) approach is adapted to identify the globally important time-frequency bands for predicting average surface roughness (Ra) in a smart grinding process. The smart grinding setup consisted of a Supertech CNC precision surface grinding machine, instrumented with a Dytran piezoelectric accelerometer attached to the tailstock quill along the tangential direction (Y-axis). For every grinding pass, vibration signatures were captured, and the ground truth surface roughness values were recorded using a Mahr Marsurf M300C portable surface roughness profilometer. The roughness values ranged from 0.06 to 0.14 microns over the complete set of experiments. Time-frequency domain spectrogram frames were extracted for each of the vibration signals collected during the grinding process. Convolutional Neural Networks (CNNs) were modeled to predict the surface roughness based on these spectrogram frames and their image augmentations. The best CNN model was able to predict the roughness values with an overall R2-score of 0.95, training R2-score of 0.99, and testing R2-score of 0.81 with only 80 sets of vibration signals corresponding to 4 experiments with 20 trials each. Although the data size is not large enough to guarantee such performance metrics in real-world scenarios, one can extract statistically consistent explanations underlying the relationships these complex deep learning models capture. The LIME methodology was implemented on the developed surface roughness CNN model to identify the important time-frequency bands (i.e., the superpixels of a spectrogram) influencing the predictions. Based on the identified important regions on the spectrogram frames, the corresponding frequency characteristics were determined that influence the surface roughness predictions. The important frequency range based on LIME results was approximately 11.7 to 19.1 kHz. The power of XAI was demonstrated by cutting down the sampling rate from 160 kHz to 30, 20, 10, and 5 kHz based on the important frequency range and considering Nyquist criteria. Separate CNN models were developed for these ranges by only extracting time-frequency contents below their corresponding Nyquist cut-offs. A proper data acquisition strategy is proposed by comparing the model performances to argue the selection of a sufficient sampling rate to capture the grinding process successfully and robustly. © 2023 The Society of Manufacturing Engineers,Surface roughness and machining accuracy are essential indicators of the quality of parts in milling. With recent advancements in sensor technology and data processing, the cutting force signals collected during the machining process can be used for the prediction and determination of the machining quality. Deep-learning-based artificial neural networks (ANNs) can process large sets of signal data and can make predictions according to the extracted data features. During the final stage of the milling process of SUS304 stainless steel, we selected the cutting speed, feed per tooth, axial depth of cut, and radial depth of cut as the experimental parameters to synchronously measure the cutting force signals with a sensory tool holder. The signals were preprocessed for feature extraction using a Fourier transform technique. Subsequently, three different ANNs (a deep neural network, a convolutional neural network, and a long short-term memory network) were applied for training in order to predict the machining quality under different cutting conditions. Two training methods, namely whole-data training and training by data classification, were adopted. We compared the predictive accuracy and efficiency of the training process of these three models based on the same training data. The training results and the measurements after machining indicated that in predicting the surface roughness based on the feed per tooth classification, all the models had a percentage error within 10%. However, the convolutional neural network (CNN) and long short-term memory (LSTM) models had a percentage error of 20% based on the whole-data training, while that of the deep neural network (DNN) model was over 50%. The percentage error for the machining accuracy prediction based on the whole-data training of the DNN and CNN models was below 10%, while that of the LSTM model was as large as 20%. However, there was no significant improvement in the results of the classification training. In all the training processes, the CNN model had the best analytical efficiency, followed by the LSTM model. The DNN model performed the worst. © 2023 by the authors."
138,137,30,137_Speech Emotion Recognition Using Deep Learning and Multimodal Fusion,Speech Emotion Recognition Using Deep Learning and Multimodal Fusion,"Speech emotion recognition (SER), an important method of emotional human–machine interaction, has been the focus of much research in recent years. Motivated by powerful Deep Convolutional Neural Networks (DCNNs) to learn features and the landmark success of these networks in the field of image classification, the present study aimed to prepare a pre-trained DCNN model for SER and provide compatible input to these networks by converting a speech signal into a 3D tensor. First, using a reconstructed phase space, speech samples are reconstructed in a 3D phase space. Studies have shown that the patterns formed in this space contain meaningful emotional features of the speaker. To provide an input that is compatible with DCNN, a new speech signal representation called Chaogram was introduced as the projection of these patterns, and three channels similar to RGB images were obtained. In the next step, image enhancement techniques were used to highlight the details of Chaogram images. Then, the Visual Geometry Group (VGG) DCNN pre-trained on the large ImageNet dataset is utilized to learn Chaogram high-level features and corresponding emotion classes. Finally, transfer learning is performed on the proposed model, and the presented model is fine-tuned on our datasets. To optimize the hyper-parameter arrangement of architecture-determined CNNs, an innovative DCNN-GWO (gray wolf optimization) is also presented. The results of this study on two public datasets of emotions, i.e., EMO-DB and eNTERFACE05, show the promising performance of the proposed model, which can greatly improve SER applications. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,As deep learning technology research continues to progress, artificial intelligence technology is gradually empowering various fields. To achieve a more natural human-computer interaction experience, how to accurately recognize emotional state of speech interactions has become a new research hotspot. Sequence modeling methods based on deep learning techniques have promoted the development of emotion recognition, but the mainstream methods still suffer from insufficient multimodal information interaction, difficulty in learning emotion-related features, and low recognition accuracy. In this article, we propose a transformer-based deep-scale fusion network (TDFNet) for multimodal emotion recognition, solving the aforementioned problems. The multimodal embedding (ME) module in TDFNet uses pretrained models to alleviate the data scarcity problem by providing a priori knowledge of multimodal information to the model with the help of a large amount of unlabeled data. Furthermore, a mutual transformer (MT) module is introduced to learn multimodal emotional commonality and speaker-related emotional features to improve contextual emotional semantic understanding. In addition, we design a novel emotion feature learning method named the deep-scale transformer (DST), which further improves emotion recognition by aligning multimodal features and learning multiscale emotion features through GRUs with shared weights. To comparatively evaluate the performance of TDFNet, experiments are conducted with the IEMOCAP corpus under three reasonable data splitting strategies. The experimental results show that TDFNet achieves 82.08% WA and 82.57% UA in RA data splitting, which leads to 1.78% WA and 1.17% UA improvements over the previous state-of-the-art method, respectively. Benefiting from the attentively aligned mutual correlations and fine-grained emotion-related features, TDFNet successfully achieves significant improvements in multimodal emotion recognition.  © 2014 IEEE.,In recent times, there has been a lot of focus placed on speech emotion recognition, often known as SER. SER is a fundamental approach to emotional human-machine interaction. Utilising DCNNs has resulted in significant advancements being made, notably with regard to the learning of high-level characteristics for SER. However, one of the primary challenges that arises when applying deep neural networks to SER is the problem of overfitting, which occurs commonly when too many parameters are chosen using insufficiently large datasets. The majority of the research that has been done to attempt to solve this problem has focused on translating the audio input into a picture and employing transfer learning methods. According to research, the patterns generated in this region include significant emotional aspects of the speaker. A new speech signal format called Chaogram was designed to project these patterns, which would result in three channels like RGB images, in order to give an input that works with VGG-based Deep-CNNs. The original phase space is then used to reconstruct the voice samples in a three-dimensional space. In the subsequent stage, the Chaogram photos' finer details were accentuated by image enhancing techniques. To learn Chaogram's high-level features and emotion classifications, the Visual Geometry Group (VGG) deep convolutional neural network (DCNN) is employed after being pre-trained on the massive ImageNet dataset using intelligent sensors. We next apply transfer learning to our datasets to further improve the provided model, which we combine with the proposed model. To enhance the hyper-parameter layout of architecture-determined CNNs, a new Deep-CNN with BWO (Beluga Whale Optimization) is introduced. In order to apply Deep–CNN–KL and to the field of SER, thispropose a new method of representing speech signals, which we call Chaograms, by mapping the signal onto a 2D image. On two publicly available emotion datasets, these findings demonstrate the ability of the proposed approach, which has the potential to significantly improve SER applications: EMO-DB and eNTERFACE05. Image enhancement methods that place more emphasis on such features may lead to more precise classification in the performance analysis. Classification accuracy can be considerably improved by adapting these images to work with the pre-trained Deep-CNN inputs. © 2024 The Authors"
139,138,29,138_Pretrained Language Models and Prompt-Based Learning,Pretrained Language Models and Prompt-Based Learning,"Insufficiently labeled samples and low-generalization performance have become significant natural language processing problems, drawing significant concern for few-shot text classification (FSTC). Advances in prompt learning have significantly improved the performance of FSTC. However, prompt learning methods typically require the pre-trained language model and tokens of the vocabulary list for model training, while different language models have different token coding structures, making it impractical to build effective Chinese prompt learning methods from previous approaches related to English. In addition, a majority of current prompt learning methods do not make use of existing unlabeled data, thus often leading to unsatisfactory performance in real-world applications. To address the above limitations, we propose a novel Chinese FSTC method called CIPLUD that combines an improved prompt learning method and existing unlabeled data, which are used for the classification of a small amount of Chinese text data. We used the Chinese pre-trained language model to build two modules: the Multiple Masks Optimization-based Prompt Learning (MMOPL) module and the One-Class Support Vector Machine-based Unlabeled Data Leveraging (OCSVM-UDL) module. The former generates prompt prefixes with multiple masks and constructs suitable prompt templates for Chinese labels. It optimizes the random token combination problem during label prediction with joint probability and length constraints. The latter, by establishing an OCSVM model in the trained text vector space, selects reasonable pseudo-label data for each category from a large amount of unlabeled data. After selecting the pseudo-label data, we mixed them with the previous few-shot annotated data to obtain brand new training data and then repeated the steps of the two modules as an iterative semi-supervised optimization process. The experimental results on the four Chinese FSTC benchmark datasets demonstrate that our proposed solution outperformed other prompt learning methods with an average accuracy improvement of 2.3%. © 2023 by the authors.,Large language models (LLMs) have revolutionized natural language processing, but they require significant data and hardware resources. Prompt learning offers a solution by enabling a single model for multiple downstream tasks. However, current prompt learning methods rely on costly prompt templates for training. This is a challenge for tasks like sentiment classification, where high-quality templates are hard to create and pseudo-token composed templates can be expensive to train. Recent studies on the chain of thought (COT) have shown that enhancing the presentation of certain aspects of the reasoning process can improve the performance of LLMs. With this in mind, this research introduces the auto-generated COT and verbalizer templates (AGCVT-Prompt) technique, which clusters unlabeled texts according to their identified topic and sentiment. Subsequently, it generates dual verbalizers and formulates both topic and sentiment prompt templates, utilizing the categories discerned within the text and verbalizers. This method significantly improves the transparency and interpretability of the model's decision-making processes. The AGCVT-Prompt technique was evaluated against conventional prompt learning and advanced sentiment classification methods, using state-of-the-art LLMs on both Chinese and English datasets. The results showed superior performance in all evaluations. Specifically, the AGCVT-Prompt method outperformed previous prompt learning techniques in few-shot learning scenarios, providing higher zero-shot and few-shot learning capabilities. Additionally, AGCVT-Prompt was utilized to analyze network comments about Corona Virus Disease 2019, providing valuable insights. These findings indicate that AGCVT-Prompt is a promising alternative for sentiment classification tasks, particularly in situations where labeled data is scarce. © 2024,In recent years, large-scale pretrained language models have become widely used in natural language processing tasks. On this basis, prompt learning has achieved excellent performance in specific few-shot classification scenarios. The core idea of prompt learning is to convert a downstream task into a masked language modelling task. However, different prompt templates can greatly affect the results, and finding an appropriate template is difficult and time-consuming. To this end, this study proposes a novel hybrid prompt approach, which combines discrete prompts and continuous prompts, to motivate the model to learn more semantic knowledge from a small number of training samples. By comparing the performance difference between discrete prompts and continuous prompts, we find that hybrid prompts achieve the best results, reaching a 73.82% F1 value in the test set. In addition, we analyze the effect of different virtual token lengths in continuous prompts and hybrid prompts in a few-shot cross-language topic classification scenario. The results demonstrate that there is a threshold for the length of virtual tokens, and too many virtual tokens decrease the performance of the model. It is better not to exceed the average length of the training set corpus. Finally, this paper designs a method based on vector similarity to explore the real meanings represented by virtual tokens. The experimental results show that the prompt automatically learnt from the virtual token has a certain correlation with the input text. © 2023 by the authors."
140,139,29,139_Automated High-Resolution Crop Mapping and Yield Estimation,Automated High-Resolution Crop Mapping and Yield Estimation,"Crop-type mapping is the foundation of grain security and digital agricultural management. Accuracy, efficiency and large-scale scene consistency are required to perform crop classification from remote sensing images. Many current remote-sensing crop extraction methods based on deep learning cannot account for adaptation effects in large-scale, complex scenes. Therefore, this study proposes a novel adaptive feature-fusion network for crop classification using single-temporal Sentinel-2 images. The selective patch module implemented in the network can adaptively integrate the features of different patch sizes to assess complex scenes better. TabNet was used simultaneously to extract spectral information from the center pixels of the patches. Multitask learning was used to supervise the extraction process to improve the weight of the spectral characteristics while mitigating the negative impact of a small sample size. In the network, superpixel optimization was applied to post-process the classification results to improve the crop edges. By conducting the crop classification of peanut, rice, and corn based on Sentinel-2 images in 2022 in Henan Province, China, the novel method proposed in this paper was more accurate, indicated by an F1 score of 96.53%, than other mainstream methods. This indicates our model’s potential for application in crop classification in large scenes. © 2023 by the authors.,The timely and accurate mapping of crops over large areas is essential for alleviating food crises and formulating agricultural policies. However, most existing classical crop mapping methods usually require the whole-year historical time-series data that cannot respond quickly to the current planting information, let alone for future prediction. To address this issue, we propose a novel spatial–temporal feature and deep integration strategy for crop growth pattern prediction and early mapping (STPM). Specifically, the STPM first learns crop spatial–temporal evolving patterns from historical data to generate future remote sensing images based on the current observations. Then, a robust crop type recognition model is applied by combining the current early data with the predicted images for early crop mapping. Compared to existing spatial–temporal prediction models, our proposed model integrates local, global, and temporal multi-modal features comprehensively. Not only does it achieve the capability to predict longer sequence lengths (exceeding 100 days), but it also demonstrates a significant improvement in prediction accuracy for each time step. In addition, this paper analyses the impact of feature dimensionality and initial data length on prediction and early crop mapping accuracy, demonstrating the necessity of multi-modal feature fusion for spatial–temporal prediction of high-resolution remote sensing data and the benefits of longer initial time-series (i.e., longer crop planting time) for crop identification. In general, our method has the potential to carry out early crop mapping on a large scale and provide information to formulate changes in agricultural conditions promptly. © 2023 by the authors.,Crop mapping provides information on crop types and cropland spatial distribution. Therefore, accurate and timely crop mapping serves as the fundamental step to higher-level agricultural applications, such as crop yield prediction. Recently, deep learning (DL) classification models have been explored for crop mapping. However, most existing methods are still limited in practical applications for large-scale crop mapping due to the great need for representative labeled data that cover diverse geographical conditions. To address this issue, we proposed a novel deep active learning (AL) method, named Bayesian Semi-Supervised Active Learning (BSSAL), using time-series Sentinel-2 imagery for large-scale crop mapping. This framework consists of three parts: 1) a Bayesian neural network (BNN) for crop classification, 2) a Semi-Supervised task to leverage massive satellite imagery without crop type information, and 3) an AL strategy to select the most informative samples to be labeled. The proposed framework was validated on five crop classes over six sites with different geographical conditions across the U.S. in testing years 2019–2021. The BSSAL framework produced a mean OA of 98.69% in the end-of-season experiment and significantly outperformed other comparison methods. Moreover, it achieved an average OA of 97.00% with in-season satellite imagery from the day of the year (DOY) 140 (early April) to 200 (mid-July) and greatly improved the timeliness of crop mapping. Furthermore, the BSSAL framework had the best performance in all study sites, showing the robustness of the global model on local testing. By monitoring the accuracy of each crop type along with the number of samples selected, the results showed the effectiveness of the proposed active data query strategy to random choice. Overall, this study provided a robust framework for large-scale crop mapping with less labeling budget and higher classification accuracy. The source code is available at: https://github.com/YXu556/BSSAL. © 2024 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)"
141,140,29,140_Chemical Synthesis and Materials Discovery with Machine Learning,Chemical Synthesis and Materials Discovery with Machine Learning,"Machine learning models are increasingly being utilized to predict outcomes of organic chemical reactions. A large amount of reaction data is used to train these models, which is in stark contrast to how expert chemists discover and develop new reactions by leveraging information from a small number of relevant transformations. Transfer learning and active learning are two strategies that can operate in low-data situations, which may help fill this gap and promote the use of machine learning for tackling real-world challenges in organic synthesis. This Perspective introduces active and transfer learning and connects these to potential opportunities and directions for further research, especially in the area of prospective development of chemical transformations. © 2023 American Chemical Society,The molecular structures synthesizable by organic chemists dictate the molecular functions they can create. The invention and development of chemical reactions are thus critical for chemists to access new and desirable functional molecules in all disciplines of organic chemistry. This work seeks to expedite the exploration of emerging areas of organic chemistry by devising a machine-learning-guided workflow for reaction discovery. Specifically, this study uses machine learning to predict competent electrochemical reactions. To this end, we first develop a molecular representation that enables the production of general models with limited training data. Next, we employ automated experimentation to test a large number of electrochemical reactions. These reactions are categorized as competent or incompetent mixtures, and a classification model was trained to predict reaction competency. This model is used to screen 38,865 potential reactions in silico, and the predictions are used to identify a number of reactions of synthetic or mechanistic interest, 80% of which are found to be competent. Additionally, we provide the predictions for the 38,865-member set in the hope of accelerating the development of this field. We envision that adopting a workflow such as this could enable the rapid development of many fields of chemistry. © 2022 American Chemical Society. All rights reserved.,Conspectus In the domain of reaction development, one aims to obtain higher efficacies as measured in terms of yield and/or selectivities. During the empirical cycles, an admixture of outcomes from low to high yields/selectivities is expected. While it is not easy to identify all of the factors that might impact the reaction efficiency, complex and nonlinear dependence on the nature of reactants, catalysts, solvents, etc. is quite likely. Developmental stages of newer reactions would typically offer a few hundreds of samples with variations in participating molecules and/or reaction conditions. These “observations” and their “output” can be harnessed as valuable labeled data for developing molecular machine learning (ML) models. Once a robust ML model is built for a specific reaction under development, it can predict the reaction outcome for any new choice of substrates/catalyst in a few seconds/minutes and thus can expedite the identification of promising candidates for experimental validation. Recent years have witnessed impressive applications of ML in the molecular world, most of them aimed at predicting important chemical or biological properties. We believe that an integration of effective ML workflows can be made richly beneficial to reaction discovery. As with any new technology, direct adaptation of ML as used in well-developed domains, such as natural language processing (NLP) and image recognition, is unlikely to succeed in reaction discovery. Some of the challenges stem from ineffective featurization of the molecular space, unavailability of quality data and its distribution, in making the right choice of ML model and its technically robust deployment. It shall be noted that there is no universal ML model suitable for an inherently high-dimensional problem such as chemical reactions. Given these backgrounds, rendering ML tools conducive for reactions is an exciting as well as challenging endeavor at the same time. With the increased availability of efficient ML algorithms, we focused on tapping their potential for small-data reaction discovery (a few hundreds to thousands of samples). In this Account, we describe both feature engineering and feature learning approaches for molecular ML as applied to diverse reactions of high contemporary interest. Among these, catalytic asymmetric hydrogenation of imines/alkenes, ?-C(sp3)-H bond functionalization, and relay Heck reaction employed a feature engineering approach using the quantum-chemically derived physical organic descriptors as the molecular features?all designed to predict the enantioselectivity. The selection of molecular features to customize it for a reaction of interest is described, along with emphasizing the chemical insights that could be gathered through the use of such features. Feature learning methods for predicting the yield of Buchwald-Hartwig cross-coupling, deoxyfluorination of alcohols, and enantioselectivity of N,S-acetal formation are found to offer excellent predictions. We propose a transfer learning protocol, wherein an ML model such as a language model is trained on a large number of molecules (105-106) and fine-tuned on a focused library of target task reactions, as an effective alternative for small-data reaction discovery (102-103 reactions). The exploitation of deep neural network latent space as a method for generative tasks to identify useful substrates for a reaction is demonstrated as a promising strategy. © 2023 American Chemical Society."
142,141,29,"141_Spatial analysis of urban factors influencing housing prices, jogging behavior, migration balance, and poverty governance in Russia and Australia using machine learning techniques.","Spatial analysis of urban factors influencing housing prices, jogging behavior, migration balance, and poverty governance in Russia and Australia using machine learning techniques.","Property valuation plays a significant role in urban economics and is of great importance to various stakeholders who interact and shape the city, including property owners, buyers, banks, land developers, real estate agents, local councils and government planning authorities. In the literature, various predictive models have been proposed to automate the calculation of property value, most of which endeavour to factor in the combination of property characteristics, market factors and location-based attributes associated with individual properties use large citywide databases. At the same time, it has been widely acknowledged that regional sub-areas have impacts on property price prediction. Therefore, this paper aims to investigate the performance of various techniques on sub-areas using the Greater Sydney Region as the study area. The sub-area in this paper is defined as the statistical areas (SAs) as defined by the Australian Bureau of Statistics. In particular, two different SA geographies (SA4, SA3) along with the City Level are adopted to understand the spatial dependence which occurs at different levels. With real-world transaction records and data collected from a diverse range of sources, various methods including the traditional hedonic price model (HPM) and popular machine learning (ML) approaches are implemented and evaluated for property price prediction. Two different property markets for residential property are modelled, being for housing stock and apartment (unit) stock. Experimental results show that Random Forest and Gradient Boosting-based methods outperform other approaches in most scenarios and that the high spatial resolution property sub-area (SA3) improved the performance in terms of overall model accuracy. This research provides insights into how sub-area machine learning models can be employed in real estate to characterize property price, and helps understand the influential factors in different local geographical areas for policy-making. © 2022 Elsevier Ltd,In the United States, the rise in hypertension prevalence has been connected to neighborhood characteristics. While various studies have found a link between neighborhood and health, they do not evaluate the relative dependence of each component in the growth of hypertension and, more significantly, how this value differs geographically (i.e., across different neighborhoods). This study ranks the contribution of ten socioeconomic neighborhood factors to hypertension prevalence in Chicago, Illinois, using multiple global and local machine learning models at the census tract level. First, we use Geographical Random Forest, a recently proposed non-linear machine learning regression method, to assess each predictive factor’s spatial variation and contribution to hypertension prevalence. Then we compare GRF performance to Geographically Weighted Regression (local model), Random Forest (global model), and OLS (global model). The results indicate that GRF outperforms all models and that the importance of variables varies by census tract. Household composition is the most important factor in the Chicago tracts, while on the other hand, Housing type and Transportation is the least important factor. While the household composition is the most important determinant around north Lake Michigan, the socioeconomic condition of the neighborhood in Chicago’s mid-north has the most importance on hypertension prevalence. Understanding how the importance of socioeconomic factors associated with hypertension prevalence varies spatially aids in the design and implementation of health policies based on the most critical factors identified at the local level (i.e., tract), rather than relying on broad city-level guidelines (i.e., for entire Chicago and other large cities). © The Author(s) 2023.,The Turkish Housing Market has experienced a steep increase in prices. Individual and corporate investors now possess tools to estimate the real estate evaluation while using smaller amounts of data with traditional techniques. Not having an analytical approach to evaluate the price of real estate could cause the investor to lose considerable amounts of money, especially in the case of individual investors. This study aims to determine how different machine learning algorithms with real market data can improve this process. To be able to test this, over 30000 lines of housing market data with over 13 variables is scraped. Data is cleansed, manipulated and visualized, while predictive models such as linear regression, polynomial regression, decision trees, random forests, and XGboost are created and compared according to the CRISP-DM framework. The results show that using complex techniques to create machine learning models could improve the accuracy in predicting the listing prices of houses. This paper aims to: - analyze the effects of using a real and relatively large amount of data, - determine the main variables that contribute to the evaluation of an estate, - compare different machine learning models to find the optimal one for the real estate market, - create an accurate model to predict the value of any house on the Istanbul market.  © 2022 Mert Tekin et al."
143,142,28,142_Credit Card Fraud Detection,Credit Card Fraud Detection,"Research into machine learning methods for fraud detection is of paramount importance, largely due to the substantial financial implications associated with fraudulent activities. Our investigation is centered around the Credit Card Fraud Dataset and the Medicare Part D dataset, both of which are highly imbalanced. The Credit Card Fraud Detection Dataset is large data and contains actual transactional content, which makes it an ideal benchmark for credit card fraud detection. The Medicare Part D dataset is big data, providing researchers the opportunity to examine national trends and patterns related to prescription drug usage and expenditures. This paper presents a detailed comparison of One-Class Classification (OCC) and binary classification algorithms, utilizing eight distinct classifiers. OCC is a more appealing option, since collecting a second label for binary classification can be very expensive and not possible to obtain within a reasonable time frame. We evaluate our models based on two key metrics: the Area Under the Precision-Recall Curve (AUPRC)) and the Area Under the Receiver Operating Characteristic Curve (AUC). Our results show that binary classification consistently outperforms OCC in detecting fraud within both datasets. In addition, we found that CatBoost is the most performant among the classifiers tested. Moreover, we contribute novel results by being the first to publish a performance comparison of OCC and binary classification specifically for fraud detection in the Credit Card Fraud and Medicare Part D datasets. © 2023, Springer Nature Switzerland AG.,The rise in technology, particularly the increase in online shopping, has made it easier for cybercriminals to obtain and exploit stolen payment card information. Traditional fraud detection systems are finding it increasingly challenging to keep up with the rapid pace of technological advancement, leading to a surge in payment card fraud. Hence, it is essential for companies to continually update their fraud detection methods to keep up with the latest tactics employed by fraudsters. Machine learning algorithms have the ability to analyze large datasets and quickly identify anomalies or deviations from normal behaviour, making them a highly effective tool for payment card fraud detection. By detecting fraud early, organizations can minimize their financial losses and prevent further damage. In this study, we generated a credit card fraud dataset that comprises three types of fraud cases. The dataset is imbalanced, with a ratio of fraudulent transactions at 0.004, making it close to real-world data. To handle the imbalance in the dataset related to credit card fraud detection, we employed popular machine learning models such as Random Forest, Decision Tree, Logistic Regression, and XGBoost. The results showed that XGBoost and Random Forest outperformed the other models on both the training and test sets. However, the Decision Tree algorithm with unlimited depth had the highest average accuracy on the training set and the lowest average accuracy on the test set, indicating that this algorithm should be avoided due to overfitting. In conclusion, our study highlights the significance of using machine learning algorithms for payment card fraud detection. The results demonstrate that XGBoost and Random Forest are the most effective models for detecting credit card fraud in imbalanced datasets. By employing these models, organizations can improve their fraud detection capabilities and minimize the financial impact of payment card fraud. © 2023 Little Lion Scientific.,In recent years, with the rapid development of Internet technology, the number of credit card users has increased significantly. Subsequently, credit card fraud has caused a large amount of economic losses to individual users and related financial enterprises. At present, traditional machine learning methods (such as SVM, random forest, Markov model, etc.) have been widely studied in credit card fraud detection, but these methods are often have difficulty in demonstrating their effectiveness when faced with unknown attack patterns. In this paper, a new Unsupervised Attentional Anomaly Detection Network-based Credit Card Fraud Detection framework (UAAD-FDNet) is proposed. Among them, fraudulent transactions are regarded as abnormal samples, and autoencoders with Feature Attention and GANs are used to effectively separate them from massive transaction data. Extensive experimental results on Kaggle Credit Card Fraud Detection Dataset and IEEE-CIS Fraud Detection Dataset demonstrate that the proposed method outperforms existing fraud detection methods. © 2023 by the authors."
144,143,28,143_Landslide and Flood Susceptibility Modeling,Landslide and Flood Susceptibility Modeling,"Detecting and mapping landslides are crucial for effective risk management and planning. With the great progress achieved in applying optimized and hybrid methods, it is necessary to use them to increase the accuracy of landslide susceptibility maps. Therefore, this research aims to compare the accuracy of the novel evolutionary methods of landslide susceptibility mapping. To achieve this, a unique method that integrates two techniques from Machine Learning and Neural Networks with novel geomorphological indices is used to calculate the landslide susceptibility index (LSI). The study was conducted in western Azerbaijan, Iran, where landslides are frequent. Sixteen geology, environment, and geomorphology factors were evaluated, and 160 landslide events were analyzed, with a 30:70 ratio of testing to training data. Four Support Vector Machine (SVM) algorithms and Artificial Neural Network (ANN)-MLP were tested. The study outcomes reveal that utilizing the algorithms mentioned above results in over 80% of the study area being highly sensitive to large-scale movement events. Our analysis shows that the geological parameters, slope, elevation, and rainfall all play a significant role in the occurrence of landslides in this study area. These factors obtained 100%, 75.7%, 68%, and 66.3%, respectively. The predictive performance accuracy of the models, including SVM, ANN, and ROC algorithms, was evaluated using the test and train data. The AUC for ANN and each machine learning algorithm (Simple, Kernel, Kernel Gaussian, and Kernel Sigmoid) was 0.87% and 1, respectively. The Classification Matrix algorithm and Sensitivity, Accuracy, and Specificity variables were used to assess the models' efficacy for prediction purposes. Results indicate that machine learning algorithms are more effective than other methods for evaluating areas' sensitivity to landslide hazards. The Simple SVM and Kernel Sigmoid algorithms performed well, with a performance score of one, indicating high accuracy in predicting landslide-prone areas. © 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,This research aimed to assess landslide susceptibility in the Badakhshan province of Afghanistan, an area highly susceptible to landslides due to its complex topography and geological conditions. Three distinct machine learning (ML) models, namely the Generalized Linear Model (GLM), Maximum Entropy (ME), and Random Forest (RF), were employed to identify the key contributing factors to landslide occurrences in the study region. The dataset used in this study consisted of landslide conditioning factors and a landslide inventory map. The conditioning factors encompassed lithology, soil type, plane curvature, profile curvature, elevation, slope, aspect, precipitation, land use/land cover (LULC), distance to fault, river, road, Normalized Difference Vegetation Index (NDVI), Topographic Wetness Index (TWI), Terrain Ruggedness Index (TRI), and Standardized Precipitation Index (SPI). The landslide inventory map contained 177 landslide locations and 65 non-landslide points obtained from Google Earth. Each machine learning (ML) model was trained and implemented independently using 70% of the training data, with the results validated against the remaining 30% of the landslide inventory dataset. Ensemble results from GLM, ME, and RF were obtained using the median approach. All three models exhibited consistent performance and identified similar landslide-prone areas. Among the various factors studied, proximity to rivers emerged as the most influential factor contributing to landslides, followed by the distance to roads and slope gradient. The study revealed that the districts of Argo and Yaftali Sufla, near Faizabad, were identified as particularly susceptible to landslides, especially in the vicinity of large valleys. Out of the total study area of 3086.4 km2, ?2162 km2 were deemed relatively safe from landslides, while 149 km2 (representing 4.8% of the study region) were identified as highly susceptible to landslides. Area Under the Receiver Operating Characteristic Curve (AUC) and Root Mean Square Error (RMSE) statistics were used to evaluate the performance of the machine learning (ML) algorithms. The RF and ME models demonstrated the highest performance levels. This research contributes to our understanding of landslide susceptibility in Badakhshan province and can aid in implementing effective landslide risk management strategies in the region. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.,Landslide is a serious natural disaster next only to earthquake and flood, which will cause a great threat to people's lives and property safety. The traditional research of landslide disaster based on experience-driven or statistical model and its assessment results are subjective, difficult to quantify, and no pertinence. As a new research method for landslide susceptibility assessment, machine learning can greatly improve the landslide susceptibility model's accuracy by constructing statistical models. Taking Western Henan for example, the study selected 16 landslide influencing factors such as topography, geological environment, hydrological conditions, and human activities, and 11 landslide factors with the most significant influence on the landslide were selected by the recursive feature elimination (RFE) method. Five machine learning methods [Support Vector Machines (SVM), Logistic Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Linear Discriminant Analysis (LDA)] were used to construct the spatial distribution model of landslide susceptibility. The models were evaluated by the receiver operating characteristic curve and statistical index. After analysis and comparison, the XGBoost model (AUC 0.8759) performed the best and was suitable for dealing with regression problems. The model had a high adaptability to landslide data. According to the landslide susceptibility map of the five models, the overall distribution can be observed. The extremely high and high susceptibility areas are distributed in the Funiu Mountain range in the southwest, the Xiaoshan Mountain range in the west, and the Yellow River Basin in the north. These areas have large terrain fluctuations, complicated geological structural environments and frequent human engineering activities. The extremely high and highly prone areas were 12043.3 km2 and 3087.45 km2, accounting for 47.61% and 12.20% of the total area of the study area, respectively. Our study reflects the distribution of landslide susceptibility in western Henan Province, which provides a scientific basis for regional disaster warning, prediction, and resource protection. The study has important practical significance for subsequent landslide disaster management. ©2023 China Geology Editorial Office. © 2023 Editorial Office of China Geology. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd."
145,144,28,144_Seismic Response and Structural Assessment of Buildings,Seismic Response and Structural Assessment of Buildings,"Nonlinear response history analysis (NLRHA) is generally considered to be a reliable and robust method to assess the seismic performance of buildings under strong ground motions. While NLRHA is fairly straightforward to evaluate individual structures for a select set of ground motions at a specific building site, it becomes less practical for performing large numbers of analyses to evaluate either (1) multiple models of alternative design realizations with a site-specific set of ground motions, or (2) individual archetype building models at multiple sites with multiple sets of ground motions. In this regard, surrogate models offer an alternative to running repeated NLRHAs for variable design realizations or ground motions. In this paper, a recently developed surrogate modeling technique, called probabilistic learning on manifolds (PLoM), is presented to estimate structural seismic response. Essentially, the PLoM method provides an efficient stochastic model to develop mappings between random variables, which can then be used to efficiently estimate the structural responses for systems with variations in design/modeling parameters or ground motion characteristics. The PLoM algorithm is introduced and then used in two case studies of 12-story buildings for estimating probability distributions of structural responses. The first example focuses on the mapping between variable design parameters of a multidegree-of-freedom analysis model and its peak story drift and acceleration responses. The second example applies the PLoM technique to estimate structural responses for variations in site-specific ground motion characteristics. In both examples, training data sets are generated for orthogonal input parameter grids, and test data sets are developed for input parameters with prescribed statistical distributions. Validation studies are performed to examine the accuracy and efficiency of the PLoM models. Overall, both examples show good agreement between the PLoM model estimates and verification data sets. Moreover, in contrast to other common surrogate modeling techniques, the PLoM model is able to preserve correlation structure between peak responses. Parametric studies are conducted to understand the influence of different PLoM tuning parameters on its prediction accuracy. © 2023 The Authors. Earthquake Engineering & Structural Dynamics published by John Wiley & Sons Ltd.,The process of ground motion selection and scaling is an integral part of hazard- and risk-consistent seismic demand analysis of structures. Due to the lack of ground motion records that naturally possess high amplitude and intensity, the research community generally relies on scaling the records to match a target hazard intensity level. The scaling factors used are frequently as high as 10. Due to the criticism received in previous research studies, the extent of amplitude scaling and its process has become a matter of debate, and various constraints on the scaling factors have been proposed. The primary argument against unrestricted amplitude scaling is the unrealistic nature of the scaled records and the possible biases caused in the engineering demand parameters (EDPs) of structures. This study presents a framework to utilize machine-learning and statistical techniques for the assessment of ground motion amplitude scaling for nonlinear time-history analysis (NTHA) of structures. The framework utilizes Bayesian non-parametric Gaussian process regressions (GPRs) as surrogate models to obtain statistical estimates of EDPs for scaled and unscaled ground motions. The GPR surrogate models are developed based on a large-scale analysis of five steel moment frames (SMFs) using 200 unscaled as-recorded ground motions for ten spectral acceleration levels, ((Formula presented.)) (ranging from 0 g to maximum considered earthquake, MCE) and 2500 scaled ground motions representing 50 scale factors ((Formula presented.)), and the 10 (Formula presented.) levels for each SMF. For each building, two types of EDPs are considered: i) peak inter-story drift ratio (PIDR) and ii) peak floor acceleration (PFA). To provide a better interpretation of the GPR surrogate models, the concept of explainable artificial intelligence (i.e., Shapley additive explanation, SHAP) is used to obtain insights into the decision-making process of the GPR models with respect to the (Formula presented.) and (Formula presented.). Then, for the 10 (Formula presented.) levels, the GPR-based EDP estimates under scaled ground motions corresponding to 50 different SFs are compared with the EDP estimates of unscaled ground motions. The comparison is conducted using Kolmogorov–Smirnov (KS) statistical hypothesis test. Results indicate that the range of allowable (Formula presented.) s depends on two factors: i) intensity level (characterized by (Formula presented.)), and ii) the dynamic properties of the building. In general, it is noticed that allowable (Formula presented.) s range between 0.5 and 3.0 for PIDRs, and from 0.6 to 2.0 for PFAs. Finally, the EDP between the unscaled and scaled ground motions are adhered to various discrepancies observed in different intensity measures representing amplitude-, duration-, energy-, and frequency- content of the two sets of ground motions. © 2023 The Authors. Earthquake Engineering & Structural Dynamics published by John Wiley & Sons Ltd.,The seismic design and assessment of steel moment resisting frames (SMRFs) rely heavily on drifts. It is unsurprising, therefore, that several simplified methods have been proposed to predict lateral deformations in SMRFs, ranging from the purely mechanics-based to the wholly data-driven, which aim to alleviate the structural engineer's burden of conducting detailed nonlinear analyses either as part of preliminary design iterations or during regional seismic assessments. While many of these methods have been incorporated in design codes or are commonly used in research, they all suffer from a lack of consideration of the causal link between the seismic hazard level and the ground-motion suite used for their formulation. In this paper, we propose hybrid data-driven models that preserve this critical relationship of hazard-consistency. To this end, we assemble a large database of non-linear response history analyses (NRHA) on 24 SMRFs of different structural characteristics. These structural models are subjected to 816 ground-motion records whose occurrence rates and spectral shapes are selected to ensure the hazard consistency of our outputs. Two sites with different seismic hazards are examined to enable comparisons under different seismic demands. An initial examination of the resulting drift hazard curves allows us to re-visit the influence of salient structural modelling assumptions such as plastic resistance, geometric configurations and joint deterioration modelling. This is followed by a machine learning (ML)-guided feature selection process that considers structural and seismic parameters as well as key static response features, hence the hybrid nature of our models. New models for inter-storey drift and roof displacements are then developed. A comparison with currently available formulations highlights the significant levels of overestimation associated with previously proposed non-hazard consistent models. © 2023 The Authors. Earthquake Engineering & Structural Dynamics published by John Wiley & Sons Ltd."
146,145,28,145_Power system transient stability assessment and cascading outage analysis with machine learning,Power system transient stability assessment and cascading outage analysis with machine learning,"Simulation analysis is critical for identifying possible hazards and ensuring secure operation of power systems. In practice, large-disturbance rotor angle stability and voltage stability are two frequently intertwined stability problems. Accurately identifying the dominant instability mode (DIM) between them is important for directing power system emergency control action formulation. However, DIM identification has always relied on human expertise. This article proposes an intelligent DIM identification framework that can discriminate among stable status, rotor angle instability, and voltage instability based on active deep learning (ADL). To reduce human expert efforts required to label the DIM dataset when building DL models, a two-stage batch-mode integrated ADL query strategy (preselection and clustering) is designed for the framework. It samples only the most helpful samples to label in each iteration and considers both information contents and diversity in them to improve query efficiency, significantly reducing the required number of labeled samples. Case studies conducted on a benchmark power system (China Electric Power Research Institute (CEPRI) 36-bus system) and a practical large-area power system (Northeast China Power System) reveal that the proposed approach outperforms conventional methods in terms of accuracy, label efficiency, scalability, and adaptability to operational variability. IEEE,In this paper, a hybrid machine learning model is applied to evaluate the relationship between random initial states and the power system's vulnerability to cascading outages. A cascading outage simulator (CS), which uses off-line AC power flows, is proposed for generating training data. The initial states are randomly selected and the CS model is deployed for each initial state, where power system generation and loads are adjusted dynamically and power flows are redistributed to quantify the vulnerability metric. Furthermore, the proposed hybrid machine learning model deploys a combined Support Vector Machine (SVM) classification and Gradient Boosting Regression (GBR) to improve the learning precision. The classification model is trained by SVM, which divides the data into two categories with and without load shedding. Then, GBR is adopted only for the data with load shedding to determine the relationship between input power outage states and the vulnerability metric. The proposed vulnerability analysis approach is applied to several test systems and the results are analyzed. Note to Practitioners - The power system vulnerability can be quantified by cascading outage simulations. However, there are two challenges: i) there are a huge number of possible initial states and we cannot enumerate all these initial states for the cascading outage simulation. Neither can we precisely quantify the bus vulnerability. ii) The cascading outage simulation may be time-consuming for large-scale power systems, which is challenging for the online application. To address the above challenges, we expect to design a machine learning technique to predict the power system vulnerability, which can train the model in an offline way and then use it for the online application. Firstly, since there is not enough operation data from practical power systems, we develop a cascading outage simulator, using off-line AC power flows, for generating synthetic training data. Secondly, we observe that the training precision by directly applying the regression model may be very poor because the output of the machine learning model may take on an uneven distribution concerning input parameters. Thus, we propose a hybrid machine learning model with a combined classification and regression method, where the classification model is employed to remove the data without the load shedding, and the regression model then determines the relationship between input power outage states and the vulnerability metric. The proposed model and method have been tested on several systems including a practical large-scale Polish power system to show the effectiveness. © 2004-2012 IEEE.,The topological configuration of a bulk power grid is often altered by network investment upgrades, forecasted disasters and random faults, as well as planned operator-triggered transmission line maintenance and controlled switching actions. Such topological variations can drastically change the measurement data distribution from phasor measurement units (PMUs), which may in turn compromise the accuracy of the artificial intelligence (AI)-aided monitoring and control applications using the measurements. For instance, data-driven transient stability assessment (TSA) models that were trained with static network topologies may no longer be accurate for monitoring power grid stability as the network topology changes. Not only would the number of possible topology changes be too vast to train all possible scenarios, but also the training process will render computationally intensive. This paper proposes a model-based transfer learning (TL) approach that integrates a convolutional neural network and a long short-term memory network (ConvLSTM), to efficiently train a new stability prediction model that predicts the system operating states (SOSs) and identify critical generators (CGs) in case of instability when the system undergoes enduring topological changes. Numerical analyses on three test cases including the IEEE 39-bus test system, the IEEE 118-bus test system, and the large-scale 2000-bus synthetic power grid in the state of Texas verify the efficiency of the proposed approach and highlight benefits in training time and accuracy, when compared to the state-of-the-art alternatives. <italic>Note to Practitioners</italic>&#x2014;As the national power grid goes through transitions towards digitalization and modernization with emerging technologies, maintaining its reliability and resiliency against environmental stressors and cyber attacks remains an urgent need. The power system topology is expected to change more frequently, sometimes to accommodate the proliferation of heterogeneous distributed energy resources and tackle their intermittence, sometimes event-driven due to disruptive events (e.g., faults), and sometimes operator-triggered for maintenance activities and responsive control to return the system back to its normal operating condition. This article is motivated by the need for an efficient and computationally-attractive approach for online situational awareness and real-time transient stability monitoring and assessment of the power grid under an enduring topological change, where the main goal is to identify the power system operating states (SOSs) and critical generators (CGs) in case of instability. Instead of training a new model for each topological change, this paper proposes an adaptive power system transient stability assessment (TSA) method that uses transfer learning (TL) which in return reduces the training time yet with less data than a newly-trained model for each topology change scenario. IEEE"
147,146,28,146_Stochastic Scheduling in Production with Genetic-Simulated Annealing Algorithm,Stochastic Scheduling in Production with Genetic-Simulated Annealing Algorithm,"Inspired by real-life applications, mainly in hand-intensive manufacturing, the incorporation of learning effects into scheduling problems has garnered attention in recent years. This paper deals with the flowshop scheduling problem with a learning effect, when minimising the makespan. Four approaches to model the learning effect, well-known in the literature, are considered. Mathematical models are providing for each case. A solver allows us to find the optimal solution in small problem instances, while a Simulated Annealing algorithm is proposed to deal with large problem instances. In the latter, the initial solution is obtained using the well-known Nawaz-Enscore-Ham algorithm, and two local search operators are evaluated. Computational experiments are carried out using benchmark datasets from the literature. The Simulated Annealing algorithm shows a better result for learning approaches with fast learning effects as compared to slow learning effects. Finally, for industrial decision makers, some insights about how the learning effect model might affect the makespan minimisation flowshop scheduling problem are presented. © 2023 Informa UK Limited, trading as Taylor & Francis Group.,Distributed manufacturing involving heterogeneous factories presents significant challenges to enterprises. Furthermore, the need to prioritize various jobs based on order urgency and customer importance further complicates the scheduling process. Consequently, this study addresses the practical issue by tackling the distributed heterogeneous hybrid flow shop scheduling problem with multiple priorities of jobs (DHHFSP-MPJ). The primary objective is to simultaneously minimize the total weighted tardiness and total energy consumption. To solve DHHFSP-MPJ, a double deep Q-network-based co-evolution (D2QCE) is developed with four features: i) The global and local searches are allocated into two populations to balance computational resources; ii) A hybrid heuristic strategy is proposed to obtain an initialized population with great convergence and diversity; iii) Four knowledge-based neighborhood structures are proposed to accelerate converging. Next, the double deep Q-Network is applied to learn operator selection; and iv) An energy-efficient strategy is presented to save energy. To verify the effectiveness of D2QCE, five state-of-the-art algorithms are compared on 20 instances and a real-world case. The results of numerical experiments indicate that: i) The D2QN can learn fast by only consuming a few computation resources and can select the best operator. ii) Combining D2QN and co-evolution can vastly improve the performance of evolutionary algorithms for solving distributed shop scheduling. iii) The proposed D2QCE has better performance than state-of-the-arts for DHHFSP-MPJ <italic>Note to Practitioners</italic>&#x2014;This paper is inspired by a real-world problem encountered in blanking workshop systems within the manufacturing of large engineering equipment. In this practical scenario, jobs come with varying priorities and distinct due dates. Balancing these priority and due date constraints while efficiently scheduling a considerable volume of jobs to enhance enterprise profitability poses a significant challenge. Thus, this scheduling problem is abstracted to the distributed heterogeneous hybrid flow shop scheduling problem with multiple priorities of jobs. The objectives are minimizing weighted due date delay and total energy consumption. Notably, this model has never been studied before. To address this, we&#x2019;ve formulated a mixed-integer linear programming model and developed a novel co-evolutionary algorithm based on double deep Q-networks (DQN). Our approach introduces several key components. First, we present a co-evolutionary framework to strike a balance between global and local search aspects. Additionally, we&#x2019;ve devised three problem-specific enhancement strategies to expedite convergence, which include hybrid initialization, local search techniques, and energy-saving measures. To accelerate the learning process of selecting the optimal operator with minimal computational resources, we employ the double DQN. Experimental results demonstrate the superior performance of our approach, outperforming state-of-the-art algorithms when applied to a real-world case. In summary, this work proposes an extended DHHFSP and provides a case of designing the deep learning-assisted evolutionary algorithm. However, online deep reinforcement learning (DRL) consumes additional time, and the generalization of online DRL needs to be improved. In future research, we will consider the dynamic events such as new jobs insert and due date change for the blanking workshop. Moreover, the end-to-end model will be considered to save energy and realize sustainable DRL. IEEE,The ever-changing and dynamic market environment requires applying job shop systems based on real-time data. The establishment of physical-virtual systems in the production process has led to the emergence of intelligent factories. Compared with those employing traditional production methods, such factories manufacture products with higher quality, higher production speed, and other economic benefits. Regarding the virtual connections of factories, events such as the arrival of new jobs, and machine breakdowns, are identified by Radio Frequency Identification System between different production units, and related decisions are made quickly and carefully. In this intelligent job shop scheduling, it is assumed that independent factories create virtual production networks in which, each factory focuses on its own interests. Regarding the importance of this issue in today's industry, this research explores the real-time scheduling problem in multi-agent production networks distributed in smart factories. Since this problem is a combination of static scheduling and real-time scheduling, a bi-objective model of mixed-integer linear programming is first developed. An approach is then proposed to solve the dynamic real-time scheduling problem. In addition, a learning-based memetic algorithm for solving large-size bi-objective instances is proposed due to the NP-hardness of the considered problem. Afterward, the results of the proposed algorithm are compared with the hybrid Pareto-based tabu search algorithm. The computational results show that in large-size instances, the proposed algorithm outperforms the competing algorithm. © 2023 Elsevier Ltd"
148,147,28,147_Image and Video Quality Assessment in the Presence of Distortions,Image and Video Quality Assessment in the Presence of Distortions,"Ultra-high-definition (UHD) video has brought new challenges to objective video quality assessment (VQA) due to its high resolution and high frame rate. Most existing VQA methods are designed for non-UHD videos—when they are employed to deal with UHD videos, the processing speed will be slow and the global spatial features cannot be fully extracted. In addition, these VQA methods usually segment the video into multiple segments, predict the quality score of each segment, and then average the quality score of each segment to obtain the quality score of the whole video. This breaks the temporal correlation of the video sequences and is inconsistent with the characteristics of human visual perception. In this paper, we present a no-reference VQA method, aiming to effectively and efficiently predict quality scores for UHD videos. First, we construct a spatial distortion feature network based on a super-resolution model (SR-SDFNet), which can quickly extract the global spatial distortion features of UHD videos. Then, to aggregate the spatial distortion features of each UHD frame, we propose a time fusion network based on a reinforcement learning model (RL-TFNet), in which the actor network continuously combines multiple frame features extracted by SR-SDFNet and outputs an action to adjust the current quality score to approximate the subjective score, and the critic network outputs action values to optimize the quality perception of the actor network. Finally, we conduct large-scale experiments on UHD VQA databases and the results reveal that, compared to other state-of-the-art VQA methods, our method achieves competitive quality prediction performance with a shorter runtime and fewer model parameters. © 2023 by the authors.,No-reference (NR) video quality assessment (VQA) is a challenging problem due to the difficulty in model training caused by insufficient annotation samples. Previous work commonly utilizes transfer learning to directly migrate pre-trained models on the image database, which suffers from domain inadaptation. Recently, self-supervised representation learning has become a hot spot for the independence of large-scale labeled data. However, existing self-supervised representation learning method only considers the distortion types and contents of the video, there needs to investigate the intrinsic properties of videos for the VQA task. To amend this, here we propose a novel multi-task self-supervised representation learning framework to pre-train a video quality assessment model. Specifically, we consider the effects of distortion degrees, distortion types, and frame rates on the perceived quality of videos, and utilize them as guidance to generate self-supervised samples and labels. Then, we optimize the ability of the VQA model in capturing spatio-temporal differences between the original video and the distorted version using three pretext tasks. The resulting framework not only eases the requirements for the quality of the original video but also benefits from the self-supervised labels as well as the Siamese network. In addition, we propose a Transformer-based VQA model, where short-term spatio-temporal dependencies of videos are modeled by 3D-CNN and 2D-CNN, and then the long-term spatio-temporal dependencies are modeled by Transformer because of its excellent long-term modeling capability. We evaluated the proposed method on four public video quality assessment databases and found that it is competitive with all compared VQA algorithms.  © 1963-12012 IEEE.,To monitor objects of interest, such as wildlife and people, image-capturing devices are used to collect a large number of images with and without objects of interest. As we are recording valuable information about the behavior and activity of objects, the quality of images containing objects of interest should be better than that of images without objects of interest, even if the former exhibits more severe distortion than the latter. However, according to current methods, quality assessments produce the opposite results. In this study, we propose an end-to-end model, named DETR-IQA (detection transformer image quality assessment), which extends the capability to perform object detection and blind image quality assessment (IQA) simultaneously by adding IQA heads comprising simple multi-layer perceptrons at the top of the DETRs (detection transformers) decoder. Using IQA heads, DETR-IQA carried out blind IQAs based on the weighted fusion of the distortion degree of the region of objects of interest and the other regions of the image; the predicted quality score of images containing objects of interest was generally greater than that of images without objects of interest. Currently, the subjective quality score of all public datasets is in accordance with the distortion of images and does not consider objects of interest. We manually extracted the images in which the five predefined classes of objects were the main contents of the largest authentic distortion dataset, KonIQ-10k, which was used as the experimental dataset. The experimental results show that with slight degradation in object detection performance and simple IQA heads, the values of PLCC and SRCC were 0.785 and 0.727, respectively, and exceeded those of some deep learning-based IQA models that are specially designed for only performing IQA. With the negligible increase in the computation and complexity of object detection and without a decrease in inference speeds, DETR-IQA can perform object detection and IQA via multi-tasking and substantially reduce the workload. © 2023 by the authors."
149,148,28,148_Online Portfolio Optimization with Transaction Costs Using Machine Learning and ADMM,Online Portfolio Optimization with Transaction Costs Using Machine Learning and ADMM,"Stochastic gradient descent method is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, there have been many explicit variance reduction methods for stochastic descent, such as SVRG Johnson and Zhang [Advances in neural information processing systems, (2013), pp. 315–323], SAG Roux et al. [Advances in neural information processing systems, (2012), pp. 2663–2671], SAGA Defazio et al. [Advances in neural information processing systems, (2014), pp. 1646–1654] and so on. Conjugate gradient method, which has the same computation cost with gradient descent method, is considered. In this paper, in the spirit of SAGA, we propose a stochastic conjugate gradient algorithm which we call SCGA. With the Fletcher and Reeves type choices, we prove a linear convergence rate for smooth and strongly convex functions. We experimentally demonstrate that SCGA converges faster than the popular SGD type algorithms for four machine learning models, which may be convex, nonconvex or nonsmooth. Solving regression problems, SCGA is competitive with CGVR, which is the only one stochastic conjugate gradient algorithm with variance reduction so far, as we know. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Due to their faster convergence rate than gradient descent algorithms and less computational cost than second order algorithms, conjugate gradient (CG) algorithms have been widely used in machine learning. This paper considers conjugate gradient in the mini-batch setting. Concretely, we propose a stable adaptive stochastic conjugate gradient (SCG) algorithm via incorporating both the stochastic recursive gradient algorithm (SARAH) and second order information into the CG-type algorithm. Unlike most of existing CG algorithms that spend a lot of time in determining the step size by using line search and may fail in stochastic optimization, the proposed algorithms use a local quadratic model to estimate the step size sequence, but do not require computing the Hessian information, which make the proposed algorithms attain a low computational cost as first-order algorithms. We establish the linear convergence rate of a class of SCG algorithms, when the loss function is the strongly convex. Moreover, we show that the complexity of the proposed algorithm matches modern stochastic optimization algorithms. As a by-product, we develop a practical variant of the proposed algorithm by setting a stopping criterion for the number of inner loop iterations. Various numerical experiments on machine learning problems demonstrate the efficiency of the proposed algorithms. © 2022 Elsevier Ltd,As a classical machine learning model, support vector machine (SVM) has attracted much attention due to its rigorous theoretical foundation and powerful discriminative performance. The doubly regularized SVM (DRSVM) is an important variant of SVM based on elastic-net regularization, which considers both the sparsity and stability of the model. To tackle the problems of explosive increases in data dimensions and data volume, the alternating direction method of multipliers (ADMM) algorithm can be used to train the DRSVM model. ADMM is an effective iterative algorithm for solving convex optimization problems by decomposing a large issue into a series of solvable subproblems, which is also well suited for distributed computing. However, lack of guaranteed convergence and slow convergence rate are two critical limitations of ADMM. In this paper, a 3-block ADMM algorithm based on the over-relaxation technique is proposed to accelerate DRSVM training, namely, the over-relaxed DRSVM (O-RDRSVM). The main strategy of the over-relaxation technique is to further append the information from the previous iteration to the next iteration to improve the convergence of ADMM. We also propose a distributed version of O-RDRSVM to handle parallel and distributed computing faster, termed DO-RDRSVM. Moreover, we develop a fast O-RDRSVM algorithm (FO-RDRSVM) and a fast DO-RDRSVM algorithm (FDO-RDRSVM), which further reduce the computational cost of O-RDRSVM and DO-RDRSVM by employing the matrix inversion lemma. The convergence analyses ensure the effectiveness of our algorithms for DRSVM training. Finally, extensive experiments on public datasets demonstrate the advantages of our algorithms in terms of convergence rate and training time while maintaining accuracy and sparsity comparable to those of previous works. © 2023 Elsevier B.V."
150,149,28,149_Anomaly detection in system logs and microservices,Anomaly detection in system logs and microservices,"Log anomaly is a manifestation of a software system error or security threat. Detecting such unusual behaviours across logs in real-time is the driving force behind large-scale autonomous monitoring technology that can rapidly alert zero-day attacks. Increasingly, AI methods are being used to process voluminous log datasets and reveal patterns of correlated anomaly. In this paper, we propose an enhanced approach to learning semantic-aware embeddings for logs called the Subword Encoder Neural network (SEN). Solving upon a key limitation of previous semantic log parsing works, the proposed work introduces the concept of learning word vectors from subword-level granularity using an attention encoder strategy. The learnt embeddings reflect the contextual/lexical relationships at the word level. As a result, the learnt word representations precisely capture new log messages previously not seen by the model. Furthermore, we develop a novel feature distillation algorithm termed Naive Bayes Feature Selector (NBFS) to extract useful log events. This probabilistic technique examines the occurrence pattern of events to only select the salient ones that can aid anomaly detection. To our best knowledge, this is the first attempt to associate affinity to log events based on the target task. Since the predictions can be traced to the log messages, the AI is inherently explainable too. The model outperforms state-of-the-art methods by a fair margin. It achieves a 0.99 detection F1-score on the benchmarked BGL, HDFS and OpenStack log datasets. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Large-scale services are generating massive logs, which trace the runtime states and critical events. Anomaly detection via logs is critical for service maintenance and reliability assurance. Existing log-based anomaly detection methods make use of the limited information in log data, resulting in their incapability of detecting diverse anomalies related to unused log features. In this paper, we propose AllInfoLog, a robust log-based anomaly detection method taking advantage of all log information, to detect diverse types of anomalies. To capture all log features, AllInfoLog utilizes four encoders to extract semantic, parameter, time, and other feature embeddings, respectively. The embeddings of all log features are then combined to train an attention-based Bi-LSTM model to detect diverse anomalies. The experimental evaluations on real-world log datasets, synthetic datasets, and unstable log datasets demonstrate AllInfoLog outperforms the state-of-the-art log-based anomaly detection methods from aspects of performance and robustness, and has effectiveness to detect diverse types of anomalies.  © 2004-2012 IEEE.,Now-a-days, failure detection and prediction have become a significant research focus on enhancing the reliability and availability of IT infrastructure components. Log analysis is an emerging domain aimed at diminishing downtime caused by IT infrastructure components' failure. However, it can be challenging due to poor log quality and large data sizes. The proposed system automatically classifies logs based on log level and semantic analysis, allowing for a precise understanding of the meaning of log entries. Using the BERT pre-trained model, semantic vectors are generated for various IT infrastructures, such as Server Applications, Cloud Systems, Operating Systems, Supercomputers, and Mobile Systems. These vectors are then used to train machine learning (ML) classifiers for log categorization. The trained models are competent in classifying logs by comprehending the context of different types of logs. Additionally, semantic analysis outperforms sentiment analysis when dealing with unobserved log records. The proposed system significantly reduces engineers' day-to-day error-handling work by automating the log analysis process. © 2023, International Journal of Advanced Computer Science and Applications. All Rights Reserved."
151,150,28,"150_Deep Learning Methods for Steganalysis, Steganography, and Watermarking","Deep Learning Methods for Steganalysis, Steganography, and Watermarking","Because the JPEG recompression in social networks changes the DCT coefficients of uploaded images, applying image steganography in popular image-sharing social networks requires robustness. Currently, most robust steganography algorithms rely on the resistance of embedding to the general JPEG recompression process. The operations in a specific compression channel are usually ignored, which reduces the robustness performance. Besides, to acquire the robust cover image, the state-of-the-art robust steganography needs to upload the cover image to social networks several times, which may be insecure regarding behavior security. In this paper, a robust steganography method based on the softmax outputs of a trained classifier and protocol message embedding is proposed. In the proposed method, a deep learning-based robustness classifier is trained to model the specific process of the JPEG recompression channel. The prediction result of the classifier is used to select the robust DCT blocks to form the embedding domain. The selection information is embedded as the protocol messages into the middle-frequency coefficients of DCT blocks. To further improve the recovery possibility of the protocol message, a robustness enhancement method is proposed. It decreases the predicted non-robust possibility of the robustness classifier by modifying low-frequency coefficients of DCT blocks. The experimental results show that the proposed method has better robustness performance compared with state-of-the-art robust steganography and does not have the disadvantage regarding behavior security. The method is universal and can be implemented in different JPEG compression channels after fine-tuning the classifier. Moreover, it has better security performance compared with the state-of-the-art method when embedding large-sized secret messages. © 2023, The Author(s).,Steganalysis methods have developed to attack steganography, a technique used to hide secret information in a digital media. The traditional way of steganalysis is performed as feature extraction followed by classification. With the popularity of Deep Learning (DL) in the field of computer vision, researchers started applying deep learning for steganalysis problems also. Soon they found promising results with DL as it automates the feature extraction step and classification results can be used to better learn the features. Thus, the tedious task of manual extraction of features with a separate classification step is unified in deep learning giving optimistic results. This work provides a better insight into steganalysis evolution using deep learning and provides a broad review on how researchers have successfully applied Convolutional Neural Network (CNN) by using steganalysis specific activation functions, different convolutional layers and others. Researchers have compared their results with each other as well as state-of-the-art before deep learning (Rich Models + Ensemble Classifier). Initially, CNNs were created from scratch in the field of steganalysis but later researchers moved to highly efficient pretrained networks such as SRNet, ResNet and EfficientNet and found significant improvement in results on more challenging datasets such as ALASKA-I and ALASKA-II. The reason for such improvement is that pretrained networks are already trained on a very large dataset of images for some classification tasks and thus can be finetuned easily to other classification tasks with improved results. © 2023 University of Bahrain. All rights reserved.,With the rapid proliferation of urbanization, massive data in social networks are collected and aggregated in real time, making it possible for criminals to use images as a cover to spread secret information on the Internet. How to determine whether these images contain secret information is a huge challenge for multimedia computing security. The steganalysis method based on deep learning can effectively judge whether the pictures transmitted on the Internet in urban scenes contain secret information, which is of great significance to safeguarding national and social security. Image steganalysis based on deep learning has powerful learning ability and classification ability, and its detection accuracy of steganography images has surpassed that of traditional steganalysis based on manual feature extraction. In recent years, it has become a hot topic of the information hiding technology. However, the detection accuracy of existing deep learning based steganalysis methods still needs to be improved, especially when detecting arbitrary-size and multi-source images, their detection efficientness is easily affected by cover mismatch. In this manuscript, we propose a steganalysis method based on Inverse Residuals structured Siamese network (abbreviated as SiaIRNet method, Siamese-Inverted-Residuals-Network Based method). The SiaIRNet method uses a siamese convolutional neural network (CNN) to obtain the residual features of subgraphs, including three stages of preprocessing, feature extraction, and classification. Firstly, a preprocessing layer with high-pass filters combined with depth-wise separable convolution is designed to more accurately capture the correlation of residuals between feature channels, which can help capture rich and effective residual features. Then, a feature extraction layer based on the Inverse Residuals structure is proposed, which improves the ability of the model to obtain residual features by expanding channels and reusing features. Finally, a fully connected layer is used to classify the cover image and the stego image features. Utilizing three general datasets, BossBase-1.01, BOWS2, and ALASKA#2, as cover images, a large number of experiments are conducted comparing with the state-of-the-art steganalysis methods. The experimental results show that compared with the classical SID method and the latest SiaStegNet method, the detection accuracy of the proposed method for 15 arbitrary-size images is improved by 15.96% and 5.86% on average, respectively, which verifies the higher detection accuracy and better adaptability of the proposed method to multi-source and arbitrary-size images in urban scenes. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM."
152,151,27,151_Metabolomics and Compound Identification using Mass Spectrometry and Machine Learning,Metabolomics and Compound Identification using Mass Spectrometry and Machine Learning,"Tandem mass spectrometry (MS/MS) shows great promise in the research of metabolomics, providing an abundance of information on compounds. Due to the rapid development of mass spectrometric techniques, a large number of MS/MS spectral data sets have been produced from different experimental environments. The massive data brings great challenges into the spectral analysis including compound identification and spectra clustering. The core challenge in MS/MS spectral analysis is how to describe a spectrum more quantitatively and effectively. Recently, emerging deep-learning-based technologies have brought new opportunities to handle this challenge in which high-quality descriptions of MS/MS spectra can be obtained. In this study, we propose a novel contrastive learning-based method for the representation of MS/MS spectra, called CLERMS, which is based on transformer architecture. Specifically, an optimized model architecture equipped with a sinusoidal embedder and a novel loss function composed of InfoNCE loss and MSE loss has been proposed for the attainment of good embedding from the peak information and the metadata. We evaluate our method using a GNPS data set, and the results demonstrate that the learned embedding can not only distinguish spectra from different compounds but also reveal the structural similarity between them. Additionally, the comparison between our method and other methods on the performance of compound identification and spectra clustering shows that our method can achieve significantly better results.  © 2023 American Chemical Society.,Metabolite annotation continues to be the widely accepted bottleneck in nontargeted metabolomics workflows. Annotation of metabolites typically relies on a combination of high-resolution mass spectrometry (MS) with parent and tandem measurements, isotope cluster evaluations, and Kendrick mass defect (KMD) analysis. Chromatographic retention time matching with standards is often used at the later stages of the process, which can also be followed by metabolite isolation and structure confirmation utilizing nuclear magnetic resonance (NMR) spectroscopy. The measurement of gas-phase collision cross-section (CCS) values by ion mobility (IM) spectrometry also adds an important dimension to this workflow by generating an additional molecular parameter that can be used for filtering unlikely structures. The millisecond timescale of IM spectrometry allows the rapid measurement of CCS values and allows easy pairing with existing MS workflows. Here, we report on a highly accurate machine learning algorithm (CCSP 2.0) in an open-source Jupyter Notebook format to predict CCS values based on linear support vector regression models. This tool allows customization of the training set to the needs of the user, enabling the production of models for new adducts or previously unexplored molecular classes. CCSP produces predictions with accuracy equal to or greater than existing machine learning approaches such as CCSbase, DeepCCS, and AllCCS, while being better aligned with FAIR (Findable, Accessible, Interoperable, and Reusable) data principles. Another unique aspect of CCSP 2.0 is its inclusion of a large library of 1613 molecular descriptors via the Mordred Python package, further encoding the fine aspects of isomeric molecular structures. CCS prediction accuracy was tested using CCS values in the McLean CCS Compendium with median relative errors of 1.25, 1.73, and 1.87% for the 170 [M - H]-, 155 [M + H]+, and 138 [M + Na]+adducts tested. For superclass-matched data sets, CCS predictions via CCSP allowed filtering of 36.1% of incorrect structures while retaining a total of 100% of the correct annotations using a ?CCSthreshold of 2.8% and a mass error of 10 ppm. © 2022 American Chemical Society. All rights reserved.,The majority of tandem mass spectrometry (MS/MS) spectra in untargeted metabolomics and exposomics studies lack any annotation. Our deep learning framework, Integrated Data Science Laboratory for Metabolomics and Exposomics—Mass INTerpreter (IDSL_MINT) can translate MS/MS spectra into molecular fingerprint descriptors. IDSL_MINT allows users to leverage the power of the transformer model for mass spectrometry data, similar to the large language models. Models are trained on user-provided reference MS/MS libraries via any customizable molecular fingerprint descriptors. IDSL_MINT was benchmarked using the LipidMaps database and improved the annotation rate of a test study for MS/MS spectra that were not originally annotated using existing mass spectral libraries. IDSL_MINT may improve the overall annotation rates in untargeted metabolomics and exposomics studies. The IDSL_MINT framework and tutorials are available in the GitHub repository at https://github.com/idslme/IDSL_MINT . Scientific contribution statement. Structural annotation of MS/MS spectra from untargeted metabolomics and exposomics datasets is a major bottleneck in gaining new biological insights. Machine learning models to convert spectra into molecular fingerprints can help in the annotation process. Here, we present IDSL_MINT, a new, easy-to-use and customizable deep-learning framework to train and utilize new models to predict molecular fingerprints from spectra for the compound annotation workflows. © 2024, The Author(s)."
153,152,27,152_Deep Learning for Modulation Recognition and Emitter Identification,Deep Learning for Modulation Recognition and Emitter Identification,"Automatic modulation classification (AMC), which plays a significant role in wireless communication, can recognize the modulation type of the received signal without large amounts of transmitted data and parameter information. Supported by deep learning, which is a powerful tool for functional expression and feature extraction, the development of AMC can be greatly promoted. In this paper, we propose a deep learning-based modulation classification method with 2D time-frequency signal representation. In our proposed method, signals which have been received are first analyzed by time-frequency based on continuous wavelet transform (CWT). Then, CWT images of received signals are obtained and input to the deep learning model for classifying. We create a new CWT image dataset including 12 modulation types of signals under various signal-to-noise ratio (SNR) environment to verify the effectiveness of the proposed method. The experimental results demonstrate that our proposed method can reach to a high classification accuracy over the SNR of ?11 dB. © 2022 by the authors.,With the advancement of 5G technology, wireless communication resources such as channels and spectrum become scarce. This necessitates ensuring the efficiency and security of signal modulation and demodulation, which imposes higher requirements for wireless communication systems. However, signal modulation has the problems of large amount of data, low recognition accuracy and various types. In this study, a classification network of automatic modulation classification recognition algorithm for signal-to-noise ratio is proposed to solve the problem that traditional noise reduction algorithms will damage signals with high signal-to-noise ratio, consequently reducing the accuracy of signal recognition. In order to solve the problem of high complexity of network model algorithm, in particular, a signal automatic modulation classification and recognition algorithm based on neural network autoencoder is proposed. Experimental results show that the accuracy of signal automatic modulation classification recognition in the algorithm increases as the increase of modulation signals and tends to be stable. When the modulation signal is 0dB, the recognition accuracy gradually converges to the highest, and reaches 81.6% when the modulation signal is 18 dB. In contrast, the DenseNet algorithm has the lowest recognition accuracy, with only 77.5% recognition accuracy when the signal modulation classification is 18dB, a difference of 4.1%. This indicates that the algorithm performs exceptionally well in automatic signal modulation classification, and its complexity is lower than other comparative network models, providing certain advantages.  © 2013 IEEE.,Specific emitter identification (SEI) is a technique of identifying individual emitters via unique characteristics of different emitters. In this paper, we consider a SEI problem with transmitter changing modulations scenario. There have been few previous studies on this type of scenario. To cope with the daunting challenge, a variable-modulation SEI framework with domain adaptation is proposed. The components characteristics of transmitter are analyzed and the distortion models are established for simulation dataset generation. The received in-phase/quadrature (I/Q) signals are demodulated and reconstructed to obtain baseband ideal modulation signals. The received signals and the ideal modulation signals corresponding to demodulation and reconstruction are merged and embedded into the feature extraction network. Domain adversarial neural network (DANN) is added into the SEI framework to generate domain-invariant fingerprint features, thus realizing variable-modulation SEI. To better align the distortion features of emitters with variable modulations, Gaussian Encoder is designed to project fingerprint features into Gaussian distribution space. Numerous experiments show that the proposed SEI framework can improve recognition accuracy of individual emitter for single modulation and variable transfer greatly, and outperform the existing transfer learning methods. The ablation study demonstrates the components of framework are complementary. The complexity of framework is acceptable and it can extend to large-scale use. The robustness of framework is verified through modulation transfer among PSK and QAM.  © 2005-2012 IEEE."
154,153,26,"153_Cell microscopy image analysis tools and techniques for segmentation, classification, and tracking of cells, nuclei, and organelles.","Cell microscopy image analysis tools and techniques for segmentation, classification, and tracking of cells, nuclei, and organelles.","To produce abundant cell culture samples to generate large, standardized image datasets of human induced pluripotent stem (hiPS) cells, we developed an automated workflow on a Hamilton STAR liquid handler system. This was developed specifically for culturing hiPS cell lines expressing fluorescently tagged proteins, which we have used to study the principles by which cells establish and maintain robust dynamic localization of cellular structures. This protocol includes all details for the maintenance, passage and seeding of cells, as well as Matrigel coating of 6-well plastic plates and 96-well optical-grade, glass plates. We also developed an automated image-based hiPS cell colony segmentation and feature extraction pipeline to streamline the process of predicting cell count and selecting wells with consistent morphology for high-resolution three-dimensional (3D) microscopy. The imaging samples produced with this protocol have been used to study the integrated intracellular organization and cell-to-cell variability of hiPS cells to train and develop deep learning-based label-free predictions from transmitted-light microscopy images and to develop deep learning-based generative models of single-cell organization. This protocol requires some experience with robotic equipment. However, we provide details and source code to facilitate implementation by biologists less experienced with robotics. The protocol is completed in less than 10 h with minimal human interaction. Overall, automation of our cell culture procedures increased our imaging samples’ standardization, reproducibility, scalability and consistency. It also reduced the need for stringent culturist training and eliminated culturist-to-culturist variability, both of which were previous pain points of our original manual pipeline workflow. © 2023, Crown.,Background: High-throughput live-cell imaging is a powerful tool to study dynamic cellular processes in single cells but creates a bottleneck at the stage of data analysis, due to the large amount of data generated and limitations of analytical pipelines. Recent progress on deep learning dramatically improved cell segmentation and tracking. Nevertheless, manual data validation and correction is typically still required and tools spanning the complete range of image analysis are still needed. Results: We present Cell-ACDC, an open-source user-friendly GUI-based framework written in Python, for segmentation, tracking and cell cycle annotations. We included state-of-the-art deep learning models for single-cell segmentation of mammalian and yeast cells alongside cell tracking methods and an intuitive, semi-automated workflow for cell cycle annotation of single cells. Using Cell-ACDC, we found that mTOR activity in hematopoietic stem cells is largely independent of cell volume. By contrast, smaller cells exhibit higher p38 activity, consistent with a role of p38 in regulation of cell size. Additionally, we show that, in S. cerevisiae, histone Htb1 concentrations decrease with replicative age. Conclusions: Cell-ACDC provides a framework for the application of state-of-the-art deep learning models to the analysis of live cell imaging data without programming knowledge. Furthermore, it allows for visualization and correction of segmentation and tracking errors as well as annotation of cell cycle stages. We embedded several smart algorithms that make the correction and annotation process fast and intuitive. Finally, the open-source and modularized nature of Cell-ACDC will enable simple and fast integration of new deep learning-based and traditional methods for cell segmentation, tracking, and downstream image analysis. Source code: https://github.com/SchmollerLab/Cell_ACDC © 2022, The Author(s).,Massive, parallelized 3D stem cell cultures for engineering in vitro human cell types require imaging methods with high time and spatial resolution to fully exploit technological advances in cell culture technologies. Here, we introduce a large-scale integrated microfluidic chip platform for automated 3D stem cell differentiation. To fully enable dynamic high-content imaging on the chip platform, we developed a label-free deep learning method called Bright2Nuc to predict in silico nuclear staining in 3D from confocal microscopy bright-field images. Bright2Nuc was trained and applied to hundreds of 3D human induced pluripotent stem cell cultures differentiating toward definitive endoderm on a microfluidic platform. Combined with existing image analysis tools, Bright2Nuc segmented individual nuclei from bright-field images, quantified their morphological properties, predicted stem cell differentiation state, and tracked the cells over time. Our methods are available in an open-source pipeline, enabling researchers to upscale image acquisition and phenotyping of 3D cell culture. © 2023 The Author(s)"
155,154,26,154_Deep Learning-Based Image Registration Methods for Multimodal Medical Images,Deep Learning-Based Image Registration Methods for Multimodal Medical Images,"Learning-based image registration approaches typically learn to map from input images to a transformation matrix. Regarding the current deep-learning-based image rigid registration approaches learn a transformation matrix in a one-shot way. Our purpose is to present a deep reinforcement learning (DRL) based method for image registration to explicitly model the step-wise nature of the human registration process. We cast an image registration process as a Markov Decision Process (MDP) where actions are defined as global image adjustment operations. Then we train our proxy to learn the optimal action sequences to achieve a good registration. More specifically, we propose a DRL proxy incorporating an attention mechanism to address the challenge of large differences in appearance between images from different modalities. Registration experiments on 3D CT-MR image pairs of patients with nasopharyngeal carcinoma and on publicly available 3D PET-MR image pairs show that our approach significantly outperforms other methods, and achieves state-of-the-art performance in multi-m-modal medical image registration.  © 2013 IEEE.,In recent years, deep learning (DL)-based registration technology has significantly improved the calculation speed of medical image registration. Existing DL-based registration methods generally use raw data features to predict the deformation field. However, this strategy may not be very effective for difficult registration tasks. Hence, in this study, we propose a similarity attention-based convolutional neural network (CNN) for accurate and robust three-dimensional medical image registration. We first introduce a similarity-based local attention model as an auxiliary module for building a displacement searching space, instead of a direct displacement prediction based on raw data. The proposed model can help the network focus on spatial correspondences with high similarities and ignore those with low similarities. A multi-scale CNN is then integrated with the similarity-based local attention for providing non-local attention, lightweight network, and coarse-to-fine registration. We evaluated the proposed method for various applications, such as the registration of large-scope abdominal computerized tomography (CT) images and chest CT images acquired at different respiratory phases, and atlas registration in magnetic resonance imaging. The experimental results demonstrate that the proposed method can provide a more accurate and robust registration performance than state-of-the-art registration methods. © 2022 Elsevier Ltd,Medical image registration is an essential task in researching and applying medical images. Doctors can observe and extract relevant pathological features to quickly analyze the disease by registered images to diagnose the infection. After more than ten years of research and development, medical image registration has achieved good research results in traditional and deep learning methods. However, most existing methods only focus on unidirectional medical image registration research and rarely consider bidirectional medical image registration research. This paper proposes a new, unsupervised bidirectional medical image registration method based on this aspect. This method guarantees the registration effect in the forward and reverses directions and adds a cascade connection-based channel attention network to the registration model to enable better automatic learning of the registration model, optimizes feature weights, and extracts essential information from images to improve registration performance. We verified the effectiveness of our method by conducting experiments on large-scale 3D brain MRI images and achieved a comparable registration speed and effect with most existing medical image registration methods. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
156,155,26,155_Pruning Techniques for Deep Neural Networks,Pruning Techniques for Deep Neural Networks,"Network-based transfer learning allows the reuse of deep learning features with limited data, but the resulting models can be unnecessarily large. Although network pruning can improve inference efficiency, existing algorithms usually require fine-tuning that may not be suitable for small datasets. In this paper, using the singular value decomposition, we decompose a convolutional layer into two layers: a convolutional layer with the orthonormal basis vectors as the filters, and a “BasisScalingConv” layer which is responsible for rescaling the features and transforming them back to the original space. As the filters in each decomposed layer are linearly independent, when using the proposed basis scaling factors with the Taylor approximation of importance, pruning can be more effective and fine-tuning individual weights is unnecessary. Furthermore, as the numbers of input and output channels of the original convolutional layer remain unchanged after basis pruning, it is applicable to virtually all architectures and can be combined with existing pruning algorithms for double pruning to further increase the pruning capability. When transferring knowledge from ImageNet pre-trained models to different target domains, with less than 1% reduction in classification accuracies, we can achieve pruning ratios up to 74.6% for CIFAR-10 and 98.9% for MNIST in model parameters. © 2023 Elsevier B.V.,Deep learning technology has found a promising application in lightweight model design, for which pruning is an effective means of achieving a large reduction in both model parameters and float points operations (FLOPs). The existing neural network pruning methods mostly start from the consideration of the importance of model parameters and design parameter evaluation metrics to perform parameter pruning iteratively. These methods were not studied from the perspective of network model topology, so they might be effective but not efficient, and they require completely different pruning for different datasets. In this article, we study the graph structure of the neural network and propose a regular graph pruning (RGP) method to perform a one-shot neural network pruning. Specifically, we first generate a regular graph and set its node-degree values to meet the preset pruning ratio. Then, we reduce the average shortest path-length (ASPL) of the graph by swapping edges to obtain the optimal edge distribution. Finally, we map the obtained graph to a neural network structure to realize pruning. Our experiments demonstrate that the ASPL of the graph is negatively correlated with the classification accuracy of the neural network and that RGP has a strong precision retention capability with high parameter reduction (more than 90%) and FLOPs reduction (more than 90%) (the code for quick use and reproduction is available at https://github.com/Holidays1999/Neural-Network-Pruning-through-its-RegularGraph-Structure). IEEE,Network pruning has been a hot topic in recent years, and many popular pruning methods rely on network design expertise. However, the pruning process usually involves manual intervention and can be difficult for users who lack prior knowledge. Automatic pruning using evolutionary algorithms shows great promise, but it must address the challenge of performing time-consuming model evaluations and searching through a large solution space. Dataset distillation is a technique that compresses the original dataset to decrease the cost of fine-tuning models. In this paper, we explore the potential of using the distilled dataset to exhibit a similar role as the real dataset in network pruning, and proposed the evolutionary pruning framework using distilled dataset. Specifically, the network pruning pipeline is carried out on the distilled dataset to significantly reduce the model evaluation cost, and the number of filters in the convolutional layer is directly coded to narrow the search space. In addition, a tailored evolutionary algorithm is proposed that takes the form of constrained optimization to search the most suitable pruned network. The experiments conducted on VGG16, VGG19, ResNet56, and ResNet110 demonstrate that the proposed method reduces at least 41.56% of the flops and achieves competitive results with little compromising accuracy. © 2023 Elsevier Inc."
157,156,25,156_Milk Quality and Animal Health in Dairy Cows,Milk Quality and Animal Health in Dairy Cows,"In the beverages industry, milk foaming is done to enhance the flavor, texture, and visual appeal of milk-based beverages. It is very crucial to study milk foam properties not just to create visually appealing and rich in taste beverages but also to estimate the adulterants present in it. Machine learning is being used in every field nowadays as it can analyze large datasets quickly and help in making data-driven decisions. This paper is a demonstration of how a futuristic apparatus will detect the best type of milk for beverages and identify milk adulteration using machine learning. In the current study, machine learning methods are employed to assess milk foam properties. This study aims to choose the best type of milk for foam-based milk beverages preparations and detect surfactants often used in low concentrations for foaming but act as adulterants at high concentrations. Surfactants alter the foaming properties of milk in different ways depending on their charge and are therefore used in the dairy industry. By using machine learning techniques, the impact of three different surfactants, having distinct ionic properties, on three distinct types of milk have been analyzed. It was found that foaming properties of milk were highly correlated to each other. “Random forest classifier” turned out to be the most effective among all the machine learning models in both the tasks. Heating and addition of sodium dodecyl sulfate (SDS) improved foaming. The findings of this study can be used for deriving valuable insights about the dairy industry. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Cheese whey addition to milk is a type of fraud with high prevalence and severe economic effects, resulting in low yield for dairy products, nutritional reduction of milk and milk-derived products, and even some safety concerns. Nevertheless, methods to detect fraudulent addition of cheese whey to milk are expensive and time consuming, and are thus ineffective as screening methods. The Fourier-transform infrared (FTIR) spectroscopy technique is a promising alternative to identify this type of fraud because a large number of data are generated, and useful information might be extracted to be used by machine learning models. The objective of this work was to evaluate the use of FTIR with machine learning methods, such as classification tree and multilayer perceptron neural networks to detect the addition of cheese whey to milk. A total of 520 samples of raw milk were added with cheese whey in concentrations of 1, 2, 5, 10, 15, 20, 25, and 30%; and 65 samples were used as control. The samples were stored at 7, 20, and 30°C for 0, 24, 48, 72, and 168 h, and analyzed using FTIR equipment. Complementary results of 520 samples of authentic raw milk were used. Selected components (fat, protein, casein, lactose, total solids, and solids nonfat) and freezing point (°C) were predicted using FTIR and then used as input features for the machine learning algorithms. Performance metrics included accuracy as high as 96.2% for CART (classification and regression trees) and 97.8% for multilayer perceptron neural networks, with precision, sensitivity, and specificity above 95% for both methods. The use of milk composition and freezing point predicted using FTIR, associated with machine learning techniques, was highly efficient to differentiate authentic milk from samples added with cheese whey. The results indicate that this is a potential method to be used as a high-performance screening process to detected milk adulterated with cheese whey in milk quality laboratories. © 2022 American Dairy Science Association,In this research communication we compare three different approaches for developing dry matter intake (DMI) prediction models based on milk mid-infrared spectra (MIRS), using data collected from a research herd over five years. In dairy production, knowledge of individual DMI could be important and useful, but DMI can be difficult and expensive to measure on most commercial farms as cows are commonly group-fed. Instead, this parameter is often estimated based on the age, body weight, stage of lactation and body condition score of the cow. Recently, milk MIRS have also been used as a tool to estimate DMI. There are different methods available to create prediction models from large datasets. The main data used were total DMI calculated as a 3-d average, coupled with milk MIRS data available fortnightly. Data on milk yield and lactation stage parameters were also available for each animal. We compared the performance of three prediction approaches: partial least-squares regression, support vector machine regression and random forest regression. The full milk MIRS alone gave low to moderate prediction accuracy (R2 = 0.07-0.40), regardless of prediction modelling approach. Adding more variables to the model improved R2 and decreased the prediction error. Overall, partial least-squares regression proved to be the best method for predicting DMI from milk MIRS data, while MIRS data together with milk yield and concentrate DMI at 3-30 d in milk provided good prediction accuracy (R2 = 0.52-0.65) regardless of the prediction tool used.  Copyright © The Author(s), 2023. Published by Cambridge University Press on behalf of Hannah Dairy Research Foundation."
158,157,25,157_Skeleton-based Action Recognition with Temporal Feature Cross-Extraction and Multimodal Fusion,Skeleton-based Action Recognition with Temporal Feature Cross-Extraction and Multimodal Fusion,"In recent years, spatial-temporal graph convolutional networks have played an increasingly important role in skeleton-based human action recognition. However, there are still three major limitations to most ST-GCN-based approaches: (1) They only use a single joint scale to extract action features, or process joint and skeletal information separately. As a result, action features cannot be extracted dynamically through the mutual directivity between the scales. (2) These models treat the contributions of all joints equally in training, which neglects the problem that some joints with difficult loss-reduction are critical joints in network training. (3) These networks rely heavily on a large amount of labeled data, which remains costly. To address these problems, we propose a Tohjm-trained multiscale spatial-temporal graph convolutional neural network for semi-supervised action recognition, which contains three parts: encoder, decoder and classifier. The encoder’s core is a correlated joint–bone–body-part fusion spatial-temporal graph convolutional network that allows the network to learn more stable action features between coarse and fine scales. The decoder uses a self-supervised training method with a motion prediction head, which enables the network to extract action features using unlabeled data so that the network can achieve semi-supervised learning. In addition, the network is also capable of fully supervised learning with the encoder, decoder and classifier. Our proposed time-level online hard joint mining strategy is also used in the decoder training process, which allows the network to focus on hard training joints and improve the overall network performance. Experimental results on the NTU-RGB + D dataset and the Kinetics-skeleton dataset show that the improved model achieves good performance for action recognition based on semi-supervised training, and is also applicable to the fully supervised approach. © 2022 by the authors.,Current self-supervised approaches for skeleton action representation learning often focus on constrained scenarios, where videos and skeleton data are recorded in laboratory settings. When dealing with estimated skeleton data in real-world videos, such methods perform poorly due to the large variations across subjects and camera viewpoints. To address this issue, we introduce ViA, a novel View-Invariant Autoencoder for self-supervised skeleton action representation learning. ViA leverages motion retargeting between different human performers as a pretext task, in order to disentangle the latent action-specific ‘Motion’ features on top of the visual representation of a 2D or 3D skeleton sequence. Such ‘Motion’ features are invariant to skeleton geometry and camera view and allow ViA to facilitate both, cross-subject and cross-view action classification tasks. We conduct a study focusing on transfer-learning for skeleton-based action recognition with self-supervised pre-training on real-world data (e.g., Posetics). Our results showcase that skeleton representations learned from ViA are generic enough to improve upon state-of-the-art action classification accuracy, not only on 3D laboratory datasets such as NTU-RGB+D 60 and NTU-RGB+D 120, but also on real-world datasets where only 2D data are accurately estimated, e.g., Toyota Smarthome, UAV-Human and Penn Action. Code and models will be publicly available at https://walker-a11y.github.io/ViA-project . © 2024, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,In recent years, skeleton-based action recognition has received extensive attention, and a large number of researches have achieved excellent performance. This paper investigates on unsupervised domain adaptation (UDA) method (STT-DA) used in skeleton-based action recognition tasks, which is challenging in real scenes. In domain adaptation tasks, the labels are only available on source domain but unavailable on target domain. Different from other traditional approaches for UDA like the adversarial learning-based methods, this paper adapts a Transformer mechanism based on cross-attention to align different domains. It learns from both source and target domain to reduce the domain shift between different skeleton datasets, thus reducing the effect of pseudo-labels errors which is generated in domain adaptation process. Taking the particularity of skeleton data into account, this paper proposes bidirectional normalized alignment algorithm to align skeleton sequences from source and target domain and explore the feature representation in both spatial and temporal dimensions. Especially, it focuses on the adjacency dependency of skeleton joints, that is, each joint is a weight summary of adjacent joints. It enables the network to pay attention to the global characteristics of skeleton data and consider the local characteristics of joint connections. Meanwhile, skeleton sequences are divided into several parts, called subs , to reduce the time cost of the model. We conduct experiments on five datasets for skeleton-based action recognition, including two large-scale datasets (NTU RGB+D, NW-UCLA). Extensive results demonstrate that the accuracy of the proposed method (STT-DA) reaches 82.5% on NTU UCLA domain adaptation task and it also performs better on other datasets. With the application of skeleton sequences alignment algorithm and local attention weights, the accuracy improves largely.  © 2013 IEEE."
159,158,25,158_Supply Chain Demand Forecasting and Inventory Management,Supply Chain Demand Forecasting and Inventory Management,"This study aims to identify emerging topics, themes, and potential areas for applying large language models (LLMs) in supply chain management through data triangulation. This study involved the synthesis of 33 published articles and a total of 3421 social media documents, including tweets, posts, expert opinions, and industry reports on utilizing LLMs in supply chain management. By employing BERT models, four core themes were derived: Supply chain optimization, supply chain risk and security management, supply chain knowledge management, and automated contract intelligence, which provides the present status of LLM in the supply chain. The results of this study will empower managers to identify prospective applications and areas for improvement, affording them a comprehensive understanding of the antecedents, decisions, and outcomes detailed in the framework. The insights garnered from this study are highly valuable to both researchers and managers, equipping them to harness the latest advancements in LLM technology and its role within supply chain management. © 2024 IGI Global. All rights reserved.,To address the problem of the large subjective error of expert evaluation methods in supply chain management, the supply chain system is comprehensively analyzed, and a deep learning backpropagation (BP) neural network-based supply chain risk assessment model is constructed. First, the basic theories of supply chain and risk assessment are described, and the process of supply chain risk management is explained. Then, the ANN (artificial neural network) is discussed in detail. On this basis, the feasibility of the BP neural network applied in the risk assessment of the supply chain is analyzed. In addition, the risks of the supply chain system are analyzed under the support of the Internet of Things (IoT), and the indices for risk assessment of the supply chain are determined. The reliability analysis, validity analysis, and factor analysis of the evaluation indices are implemented using a questionnaire survey, based on which the risk assessment indices of the supply chain are determined as 7 first-level indices and 20 sesond-level indices. Finally, a BP neural network-based supply chain risk assessment model is established, and the simulation results are analyzed in MATLAB. The maximum relative error of the proposed BP neural network model for supply chain risk assessment is as low as 0.03076923%, and that calculated by the AHP (analytic hierarchy process) is 57.41%. Compared with that of AHP, the fitting degree of the BP neural network-based supply chain risk assessment model is much higher. Meanwhile, the simulation experiment indicates that the established risk assessment model has strong generalization ability and learning ability. This work not only provides technical support for the development of remanufacturing closed-loop supply chain systems but also contributes to the improvement of the accuracy of supply chain risk assessment. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The purposes are to improve the accuracy of inventory demand forecast, balance the indexes of enterprises, and reduce the costs of human, material and financial resources of enterprises and suppliers, thus reducing the supply chain costs and meeting the actual needs of enterprises. In terms of training a large amount of data, deep learning is better than traditional machine learning. The sales demand time series data and the previous material demand time series data are input and trained by back propagation (BP) neural network, and then the material demand value is output. Therefore, the historical data of sales demand forecast and material information are input, and the model is established by BP neural network, which not only takes into account the decisive factor of sales demand forecast, but also considers the material consumption, achieving more accurate forecast. The material demand budget of enterprises is analyzed and a material forecast demand model based on deep learning algorithm is proposed. The model uses a neural network to input the sales demand forecast data, material inventory information and material attribute information into the model, and then the model is trained by the training set in accordance with the error back propagation algorithm. Finally, the training effect of the model is tested by the test set. The results show that when the independent variables include sales demand forecast, material consumption forecast and material attribute information, the forecast error of both models is lower and the effect is better, compared with the material consumption data only as an independent variable. The forecast method based on neural network proposed increases the lead time of the forecast, give the supplier a longer time to prepare goods, and reduce the shortage or surplus of supply caused by the short lead time. Therefore, the material demand forecast model based on convolution neural network (CNN) algorithm provides an important reference for the enterprises, helps them improve their work efficiency and promotes the development of enterprises. This model achieves a great improvement on the accuracy of material demand forecast, and has a certain guiding significance in relevant theory and practice. © 2021, The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden."
160,159,25,159_Remote sensing change detection methods using deep learning techniques,Remote sensing change detection methods using deep learning techniques,"In the field of remote sensing, change detection is a crucial study area. Deep learning has made significant strides in the study of remote sensing image change detection during the past few years. Deep learning techniques still have some drawbacks. The global context cannot be modeled by convolutional neural networks due to the receptive field's restrictions. When extracting visual characteristics, the neural network does not concentrate more on the change region, which results in poor distinction between change and no-change regions. To address these problems, we propose networks with large receptive fields (LRFs) and difference image enhancement. First, we design the LRF strategy. It employs a long kernel shape in one spatial dimension for obtaining a long range of relations. Keeping a narrow kernel size in the other spatial dimension can extract local context information while avoiding interference from irrelevant regions. To focus on the changing features, we design the image difference enhancement (IDE) method, which decreases the distance between invariant features and enlarges the distance between changing features. In addition, we design the cross-channel interaction (CNI) strategy, which models the relationship between feature map channels and extracts feature representations through local CNI. On the CDD, WHU-CD, and LEVIR-CD public datasets, we conducted comprehensive experiments. According to the experimental results, our proposed LRDE-Net performs better than other state-of-The-Art change detection techniques, and the change regions are more precisely identified. It can better cope with seasonal changes, light intensity, and other pseudochange disturbances. © 2008-2012 IEEE.,Deep learning instantiated by convolutional neural networks has achieved great success in high-resolution remote-sensing image change detection. However, such networks have a limited receptive field, being unable to extract long-range dependencies in a scene. As the transformer model with self-attention can better describe long-range dependencies, we introduce a hierarchical transformer model to improve the precision of change detection in high-resolution remote sensing images. First, the hierarchical transformer extracts abstract features from multitemporal remote sensing images. To effectively minimize the model’s complexity and enhance the feature representation, we limit the self-attention calculation of each transformer layer to local windows with different sizes. Then, we combine the features extracted by the hierarchical transformer and input them into a nested U-Net to obtain the change detection results. Furthermore, a simple but effective model fusion strategy is adopted to improve the change detection accuracy. Extensive experiments are carried out on two large-scale data sets for change detection, LEVIR-CD and SYSU-CD. The quantitative and qualitative experimental results suggest that the proposed method outperforms the advanced methods in terms of detection performance. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.,High-resolution remote sensing image change detection technology compares and analyzes bi-temporal or multitemporal high-resolution remote sensing images to determine the change areas. It plays an important role in land cover/use monitoring, natural disaster monitoring, illegal building investigation, military target strike effect analysis, and land and resource investigation. The change detection of high-resolution remote sensing images has developed rapidly from data accumulation to algorithm models because of the rapid development of technologies such as deep learning and earth observation in recent years. However, the current deep learning-based change detection methods are strongly dependent on large sample data, and the training model has insufficient cross-domain generalization ability. As a result, a prior semantic information-guided change detection framework (PSI-CD), which alleviates the change detection model’s dependence on datasets by making full use of prior semantic information, is proposed in this paper. The proposed method mainly includes two parts: one is a prior semantic information generation network that uses the semantic segmentation dataset to extract robust and reliable prior semantic information; the other is the prior semantic information guided change detection network that makes full use of prior semantic information to reduce the sample size of the change detection. To verify the effectiveness of the proposed method, we produced pixel-level semantic labels for the bi-temporal images of the public change detection dataset (LEVIR-CD). Then, we performed extensive experiments on the WHU and LEVIR-CD datasets, including comparisons with existing methods, experiments with different amounts of data, and ablation study, to show the effectiveness of the proposed method. Compared with other existing methods, our method has the highest IoU for all training samples and different amounts of training samples on WHU and LEVIR-CD, reaching a maximum of 83.25% and 83.80%, respectively. © 2023 by the authors."
161,160,24,"160_QSAR modeling for Chemicals, Drugs, and Compounds","QSAR modeling for Chemicals, Drugs, and Compounds","Accurate prediction of human pharmacokinetics (PK) remains one of the key objectives of drug metabolism and PK (DMPK) scientists in drug discovery projects. This is typically performed by using in vitro-in vivo extrapolation (IVIVE) based on mechanistic PK models. In recent years, machine learning (ML), with its ability to harness patterns from previous outcomes to predict future events, has gained increased popularity in application to absorption, distribution, metabolism, and excretion (ADME) sciences. This study compares the performance of various ML and mechanistic models for the prediction of human IV clearance for a large (645) set of diverse compounds with literature human IV PK data, as well as measured relevant in vitro end points. ML models were built using multiple approaches for the descriptors: (1) calculated physical properties and structural descriptors based on chemical structure alone (classical QSAR/QSPR); (2) in vitro measured inputs only with no structure-based descriptors (ML IVIVE); and (3) in silico ML IVIVE using in silico model predictions for the in vitro inputs. For the mechanistic models, well-stirred and parallel-tube liver models were considered with and without the use of empirical scaling factors and with and without renal clearance. The best ML model for the prediction of in vivo human intrinsic clearance (CLint) was an in vitro ML IVIVE model using only six in vitro inputs with an average absolute fold error (AAFE) of 2.5. The best mechanistic model used the parallel-tube liver model, with empirical scaling factors resulting in an AAFE of 2.8. The corresponding mechanistic model with full in silico inputs achieved an AAFE of 3.3. These relative performances of the models were confirmed with the prediction of 16 Pfizer drug candidates that were not part of the original data set. Results show that ML IVIVE models are comparable to or superior to their best mechanistic counterparts. We also show that ML IVIVE models can be used to derive insights into factors for the improvement of mechanistic PK prediction. © 2023 American Chemical Society.,In vitro receptor binding assays for estrogen and androgen systems are widely used for assessing the endocrine disruption potential of chemicals. These assays have generated large amounts of data regularly used for building predictive quantitative structure-activity relationship (QSAR) models. At the same time, in vivo screening assays such as uterotrophic and Hershberger are very valuable because they reflect organ level changes as a result of the interactions of xenobiotics with the endocrine system in physiological conditions. However, such in vivo tests are expensive, time consuming, and require a large number of animals. As a result, very little data are available from these assays, and it is difficult to build useful predictive QSAR models using conventional techniques. In this study, we developed a method to predict in vivo endocrine disruption potential of chemicals using naive Bayes classification models parameterized on the outcomes of QSAR models of in vitro endpoints. The method reduces the need for large amounts of in vivo assay data. In fact, toxicity data of only 25 to 42 compounds were used from uterotrophic, Hershberger agonist, and Hershberger antagonist assays. The model's internal validation performance metrics are in the range of 50%-91% sensitivity, 73%-100% specificity, and 69%-88% accuracy in predicting in vivo outcomes. Balanced accuracies are 87%, 75%, and 70% for the models of uterotrophic, Hershberger agonist, and Hershberger antagonist effects, respectively. On a small external uterotrophic data set of nine compounds with only one negative, the method predicted with 100% accuracy. Copyright © 2022, Mary Ann Liebert, Inc.,Background: Despite their large numbers and widespread use, very little is known about the extent to which per- and polyfluoroalkyl substances (PFAS) can cross the placenta and expose the developing fetus. Objective: The aim of our study is to develop a computational approach that can be used to evaluate the of extend to which small molecules, and in particular PFAS, can cross to cross the placenta and partition to cord blood. Methods: We collected experimental values of the concentration ratio between cord and maternal blood (RCM) for 260 chemical compounds and calculated their physicochemical descriptors using the cheminformatics package Mordred. We used the compiled database to, train and test an artificial neural network (ANN). And then applied the best performing model to predict RCM for a large dataset of PFAS chemicals (n = 7982). We, finally, examined the calculated physicochemical descriptors of the chemicals to identify which properties correlated significantly with RCM. Results: We determined that 7855 compounds were within the applicability domain and 127 compounds are outside the applicability domain of our model. Our predictions of RCM for PFAS suggested that 3623 compounds had a log RCM > 0 indicating preferable partitioning to cord blood. Some examples of these compounds were bisphenol AF, 2,2-bis(4-aminophenyl)hexafluoropropane, and nonafluoro-tert-butyl 3-methylbutyrate. Significance: These observations have important public health implications as many PFAS have been shown to interfere with fetal development. In addition, as these compounds are highly persistent and many of them can readily cross the placenta, they are expected to remain in the population for a long time as they are being passed from parent to offspring. Impact: Understanding the behavior of chemicals in the human body during pregnancy is critical in preventing harmful exposures during critical periods of development. Many chemicals can cross the placenta and expose the fetus, however, the mechanism by which this transport occurs is not well understood. In our study, we developed a machine learning model that describes the transplacental transfer of chemicals as a function of their physicochemical properties. The model was then used to make predictions for a set of 7982 per- and polyfluorinated alkyl substances that are listed on EPA’s CompTox Chemicals Dashboard. The model can be applied to make predictions for other chemical categories of interest, such as plasticizers and pesticides. Accurate predictions of RCM can help scientists and regulators to prioritize chemicals that have the potential to cause harm by exposing the fetus. © 2022, The Author(s), under exclusive licence to Springer Nature America, Inc."
162,161,24,161_Groundwater quality assessment using machine learning,Groundwater quality assessment using machine learning,"Groundwater quality is typically measured through water sampling and lab analysis. The field-based measurements are costly and time-consuming when applied over a large domain. In this study, we developed a machine learning-based framework to map groundwater quality in an unconfined aquifer in the north of Iran. Groundwater samples were provided from 248 monitoring wells across the region. The groundwater quality index (GWQI) in each well was measured and classified into four classes: very poor, poor, good, and excellent, according to their cut-off values. Factors affecting groundwater quality, including distance to industrial centers, distance to residential areas, population density, aquifer transmissivity, precipitation, evaporation, geology, and elevation, were identified and prepared in the GIS environment. Six machine learning classifiers, including extreme gradient boosting (XGB), random forest (RF), support vector machine (SVM), artificial neural networks (ANN), k-nearest neighbor (KNN), and Gaussian classifier model (GCM), were used to establish relationships between GWQI and its controlling factors. The algorithms were evaluated using the receiver operating characteristic curve (ROC) and statistical efficiencies (overall accuracy, precision, recall, and F-1 score). Accuracy assessment showed that ML algorithms provided high accuracy in predicting groundwater quality. However, RF was selected as the optimum model given its higher accuracy (overall accuracy, precision, and recall = 0.92; ROC = 0.95). The trained RF model was used to map GWQI classes across the entire region. Results showed that the poor GWQI class is dominant in the study area (covering 66% of the study area), followed by good (19% of the area), very poor (14% of the area), and excellent (< 1% of the area) classes. An area of very poor GWQI was observed in the north. Feature analysis indicated that the distance to industrial locations is the main factor affecting groundwater quality in the region. The study provides a cost-effective methodology in groundwater quality modeling that can be duplicated in other regions with similar hydrological and geological settings. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Sustainable groundwater management requires an accurate characterization of aquifer-storage change over time. This process begins with an analysis of historical water levels at observation wells. However, water-level records can be sparse, particularly in developing areas. To address this problem, we developed an imputation method to approximate missing monthly averaged groundwater-level observations at individual wells since 1948. To impute missing groundwater levels at individual wells, we used two global data sources: Palmer Drought Severity Index (PDSI), and the Global Land Data Assimilation System (GLDAS) for regression. In addition to the meteorological datasets, we engineered four additional features and encoded the temporal data as 13 parameters that represent the month and year of an observation. This extends previous similar work by using inductive bias to inform our models on groundwater trends and structure from existing groundwater observations, using prior estimates of groundwater behavior. We formed an initial prior by estimating the long-term ground trends and developed four additional priors by using smoothing. These prior features represent the expected behavior over the long term of the missing data and allow the regression approach to perform well, even over large gaps of up to 50 years. We demonstrated our method on the Beryl-Enterprise aquifer in Utah and found the imputed results follow trends in the observed data and hydrogeological principles, even over long periods with no observed data. © 2022 by the authors.,The Mississippi Alluvial Plain, located in the south-central United States, is undergoing long-term groundwater-level declines within the surficial Mississippi River Valley alluvial aquifer (hereinafter referred to as “alluvial aquifer”), which has raised concerns about future groundwater availability. In some parts of the alluvial aquifer, groundwater availability for common uses such as irrigation, public supply, and domestic use is limited by quality (for example, high salinity) rather than quantity of water stored in the aquifer. The Mississippi Alluvial Plain region has an abundance of water-quality measurements in the alluvial aquifer and deeper aquifers; however, large areas lack direct measurements of salinity to evaluate regional groundwater availability. Statistical models can interpolate between wells to fill in spatial data gaps. In 2021, the U.S. Geological Survey trained two boosted regression tree (BRT) machine-learning models on specific conductance data available between 1942 and 2020 to predict spatially continuous surfaces of groundwater salinity at multiple depths for the alluvial aquifer and deeper aquifers. Well construction information, water levels, and surficial variables such as geomorphology and soils were included as explanatory variables in this baseline model. Additionally, subsurface electrical resistivity data from the first aquifer-wide aerial electromagnetic (AEM) survey for the region were incorporated to create a geophysical model. This work expands on prior BRT salinity predictions of the alluvial aquifer and extends predictions south to the Gulf of Mexico, where groundwater salinity is high. AEM survey data were not available for the southern extent of the alluvial aquifer at the time of modeling. A BRT model was trained without (baseline) and with (geophysical) AEM variables to test the ability of the models to predict salinity where explanatory data are missing and response data are sparse. Additionally, model sensitivity to AEM survey data was evaluated to better understand how AEM variables influence specific conductance predictions. Model performance was improved with the addition of geophysical data, which added three-dimensional information, thereby improving salinity predictions at depth. Groundwater specific conductance predictions can help inform other geophysical investigations in the southern extent of the study area, where high groundwater specific conductance can obfuscate changes in aquifer sediment resistivity and could limit groundwater resources for agricultural, public supply, and domestic uses. © 2023, US Geological Survey. All rights reserved."
163,162,24,162_Heart disease prediction algorithms,Heart disease prediction algorithms,"Background: Heart disease prediction model helps physicians to identify patients who are at high risk of developing heart disease and target prevention strategies accordingly. These models use patient demographics, medical history, lifecycle factors, and clinical measurements to calculate the risk of heart disease within a certain time frame. In identifying important features of heart disease, a popular approach is using Machine learning (ML) models. ML models can analyse a large amount of data and find patterns that are difficult for humans to detect. Methods: In this proposed work, Random Forest classifier is used to identify the most important features that contribute to heart disease and increase the prediction accuracy of the model by tuning the hyperparameters using grid search approach. Results: The proposed system was evaluated and compared in terms of accuracy, error rate and recall with the traditional system. As the traditional system achieved accuracies between 81.97% and 90.16%., the proposed hyperparameter tuning model achieved accuracies in the range increased between 84.22% and 96.53%. Conclusion: These evaluations demonstrated that the proposed prediction approach is capable of achieving more accurate results compared with the traditional approach in predicting heart disease by finding optimum features. © 2023 Saranya and Pravin.,The use of machine learning (ML) within medical field is on the rise, notably as a means to enhance both the speed and precision of diagnosis. Through evaluating large volumes of patient information, machine learning is able to provide disease prediction, giving both patients and doctors more control over their health. Predicting and preventing heart disease has become a major area of study in medical data processing as a result of the increased expense of therapy. Since there are so many factors that come into play, estimating one's heart disease risk manually is a challenging task. Moreover, there are very few methods which provide better accuracy for the prediction of the heart disease. Hence, by using openly accessible cleveland heart disease dataset, this research aims to design and evaluate several advanced technologies constructed employing machine leaning algorithms for diagnosing if an individual is going to get heart disease or not. In this paper, we propose an ensemble feature optimized (EFO) learning method which uses an enhanced extreme gradient boosting tree and feature level cross validation scheme for effective heart disease (EHD) prediction. The presented EFO prediction algorithm and other existing machine learning algorithms have been used for the prediction of the heart diseases. The performance of the existing algorithms (XGB-based, ensemble tree hyper optimization (ETHO), and MLP-PSO) and proposed EFO algorithm has been evaluated using the classification metrics. When compared with the XGB-based, ensemble tree hyper optimization (ETHO), and MLP-PSO algorithm, the EFO algorithm has attained an accuracy of 98.61%. The EFO algorithm provides the doctors to able to predict the heart disease more efficiently and effectively © 2023, International Journal of Intelligent Engineering and Systems.All Rights Reserved.,For the design and implementation of Clinical decision support system, computation time and prognostic accuracy are very important. To analyze the large collection of a dataset for detecting and diagnosis disease machine learning techniques are used. According to the reports of World Health Organizations, heart disease is a major cause of death and killer in urban and rural areas or worldwide. The main reason for this is a shortage of doctors and delay in the diagnosis. The outcome presaging of this disease is a very challenging job. This proposed work used the approach of self-diagnosis algorithm, fuzzy artificial neural network, and NCA and PCA and imputation methods. By the use of this technique reduces the computation time for prediction of Coronary heart disease. For the implementation of this the two datasets are using such as Cleveland and Statlog datasets. In this research work, heart disease is a diagnosis by used the clinical parameters of patients for early stages. Classifiers used for that random forest algorithm, ANN, and K-NN algorithm. The datasets for the disease prediction measure are used to accurately calculate the difference between variables and to determine whether they are correlated or not. For this classification model, the performance measure is calculated in requisites of their accuracy, precision, recall, and specificity. This approach is evaluated on the heart disease datasets for improving the accuracy performance results obtained. The experimental results obtained by NCA provide greater performance in terms of performance metrics for the multiple classifiers like RF, DT, NB, SVM. The aim of this study to develop the classification model as well as show a comparative analysis between existing systems is discussed. In this paper for Cleveland dataset has taken 303 instances and 14 attributes are used. This dataset has preprocessed dataset. This paper depicts the higher accuracy score with the multiple classifiers such as Random forest,SVM, and NB using NCA is 99.34% and for DT is 98%. Overall NCA is best in terms of classification accuracy. The result shows not only accuracy for the proposed method but obtains better results for multiple classifiers that exhibit their reliability.in medical science, this approach is used to predict heart disease at its early stages.The outcome for KNN + SDA + NCA + Fuzzy ANN for Cleveland dataset accuracy achieved 98.56% and for Statlog dataset 98.66%. This result describes the hybrid method used for prediction of heart disease and achieved better results. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
164,163,24,163_Privacy-preserving data sharing and secure federated learning in blockchain-based IoT systems,Privacy-preserving data sharing and secure federated learning in blockchain-based IoT systems,"Federated Learning (FL) has emerged as a privacy-preserving distributed Machine Learning paradigm, which collaboratively trains a shared global model across a number of end devices (clients) without exposing their raw data. However, FL typically assumes that all clients are benign and trust the coordinating central server, which is unrealistic for many real-world scenarios. In practice, clients can harm the FL process by sharing poisonous model updates while the server could malfunction or misbehave. Moreover, the deployment of FL for real-world applications is hindered by the high communication overhead between the server and clients that are often at the network edge with limited bandwidth. To address these key challenges, we propose a lightweight Blockchain-Empowered secure and efficient Federated Learning (BEFL) system. BEFL is built by integrating a communication-efficient and mutual-information guarded training scheme, a cost-effective Verifiable Random Function (VRF)-based consensus mechanism, and Inter-Planetary File System (IPFS)-enabled scalable blockchain architecture. Extensive simulation experiments using two benchmark FL datasets demonstrate that BEFL is resistant against byzantine clients launching data poisoning and model poisoning attacks, fault-tolerant against colluded malicious blockchain nodes, scalable to a large number of blockchain nodes, and communication-efficient at the network edge.  © 1968-2012 IEEE.,With the wide range of Internet of Things (IoT) applications, federated learning (FL) is commonly adopted to protect the privacy of IoT data. FL enables privacy-preserving model training while keeping the data locally available. To alleviate the additional load caused by FL, an improved hierarchical aggregation framework is presented in this article to decentralize the model aggregation tasks based on end-device clusters. However, when applying FL to IoT networks, how to keep high efficiency and reliability remains open challenges due to a large number and vulnerability of IoT end devices. In this article, we propose a blockchain-assisted aggregation scheme for FL in IoT networks, where the aggregation node selection is applied for efficiency improvement as well as blockchain for performance verification. During model aggregation, a selection strategy is obtained by the deep deterministic policy gradient (DDPG) algorithm and aims to select the optimal subset of IoT end devices based on multiple metrics. Furthermore, a new performance verification based on the characteristics of blockchain is applied to achieve mutual verification among a number of untrustworthy nodes with the optimal stopping theory, which provides reliable model performance proofs. Simulation results show that the proposed scheme can maintain FL efficiency and reduce the system latency while protecting data privacy. © 2014 IEEE.,Cloud storage is widely used by large companies to store vast amounts of data and files, offering flexibility, financial savings, and security. However, information shoplifting poses significant threats, potentially leading to poor performance and privacy breaches. Blockchain-based cognitive computing can help protect and maintain information security and privacy in cloud platforms, ensuring businesses can focus on business development. To ensure data security in cloud platforms, this research proposed a blockchain-based Hybridized Data Driven Cognitive Computing (HD2C) model. However, the proposed HD2C framework addresses breaches of the privacy information of mixed participants of the Internet of Things (IoT) in the cloud. HD2C is developed by combining Federated Learning (FL) with a Blockchain consensus algorithm to connect smart contracts with Proof of Authority. The “Data Island” problem can be solved by FL’s emphasis on privacy and lightning-fast processing, while Blockchain provides a decentralized incentive structure that is impervious to poisoning. FL with Blockchain allows quick consensus through smart member selection and verification. The HD2C paradigm significantly improves the computational processing efficiency of intelligent manufacturing. Extensive analysis results derived from IIoT datasets confirm HD2C superiority. When compared to other consensus algorithms, the Blockchain PoA’s foundational cost is significant. The accuracy and memory utilization evaluation results predict the total benefits of the system. In comparison to the values 0.004 and 0.04, the value of 0.4 achieves good accuracy. According to the experiment results, the number of transactions per second has minimal impact on memory requirements. The findings of this study resulted in the development of a brand-new IIoT framework based on blockchain technology. © 2023 Tech Science Press. All rights reserved."
165,164,24,"164_Urban Street Assessments using Street View Images: Walkability, Urban Renewal, Household Vulnerability, Spatial Quality, Restorative Environments, Acoustic Environments, Urban Predictions, Physical Disorder, Community Perceptions, Housing Prices, Fear of Crime, Street Greening","Urban Street Assessments using Street View Images: Walkability, Urban Renewal, Household Vulnerability, Spatial Quality, Restorative Environments, Acoustic Environments, Urban Predictions, Physical Disorder, Community Perceptions, Housing Prices, Fear of Crime, Street Greening","Evaluating the spatial quality of a living street entails identifying and assessing the outdoor space that influences residents’ leisure and recreation, which may contribute to urban renewal. The application of multi-source data and deep learning technology enables an objective evaluation of large-scale spatial quality as opposed to the traditional questionnaire survey or experts’ subjective evaluation. Based on street view images, points of interest, and road network data, this study developed subjective and objective evaluation indicators for the central city of Hengyang using semantic segmentation and ArcGIS spatial analysis. This study then assigned weights to each indicator and calculated the spatial quality score for living streets. In addition, the subjective evaluations of the street view images were compared to test and verify the validation of the objective evaluation model. Finally, the study analyzed the accessibility within 500 m of the study area using Spatial Syntax and ArcGIS to overlay the low spatial quality score with the highest accessibility to identify the streets with the highest priority in the subsequent urban plan. The results indicate that the spatial quality of living in the west of Hengyang is higher than that in its northeast region. In addition, Xiao Xia Street, Guanghui Street, and Hengqi Road comprised the majority of the areas that required a priority update. Correspondingly, our research is expected to be a useful management tool for identifying urban street space issues and guiding urban renewal. © 2023 by the authors.,The physical presence of a street, called the “street view”, is a medium through which people perceive the urban form. A street’s spatial ratio is the main feature of the street view, and its measurement and quality are the core issues in the field of urban design. The traditional method of studying urban aspect ratios is manual on-site observation, which is inefficient, incomplete and inaccurate, making it difficult to reveal overall patterns and influencing factors. Street view images (SVI) provide large-scale urban data that, combined with deep learning algorithms, allow for studying street spatial ratios from a broader space-time perspective. This approach can reveal an urban forms’ aesthetics, spatial quality, and evolution process. However, current streetscape research mainly focuses on the creation and maintenance of spatial data infrastructure, street greening, street safety, urban vitality, etc. In this study, quantitative research of the Beijing street spatial ratio was carried out using street view images, a convolution neural network algorithm, and the classical street spatial ratio theory of urban morphology. Using the DenseNet model, the quantitative measurement of Beijing’s urban street location, street aspect ratio, and the street symmetry was realized. According to the model identification results, the law of the gradual transition of the street spatial ratio was depicted (from the open and balanced type to the canyon type and from the historical to the modern). Changes in the streets’ spatiotemporal characteristics in the central area of Beijing were revealed. Based on this, the clustering and distribution phenomena of four street aspect ratio types in Beijing are discussed and the relationship between the street aspect ratio type and symmetry is summarized, selecting a typical lot for empirical research. The classical theory of street spatial proportion has limitations under the conditions of high-density development in modern cities, and the traditional urban morphology theory, combined with new technical methods such as streetscape images and deep learning algorithms, can provide new ideas for the study of urban space morphology. © 2023 by the authors.,Restorative environments help people recover from mental fatigue and negative emotional and physical reactions to stress. Excellent restorative environments in urban streets help people focus and improve their daily behavioral performance, allowing them to regain efficient information processing skills and cognitive levels. High-density urban spaces create obstacles in resident interactions with the natural environment. For urban residents, the restorative function of the urban space is more important than that of the natural environment in the suburbs. An urban street is a spatial carrier used by residents on a daily basis; thus, the urban street has considerable practical value in terms of improving the urban environment to have effective restorative function. Thus, in this study, we explored a method to determine the perceived restorability of urban streets using street view data, deep learning models, and the Ordinary Least Squares (OLS), the multiscale geographically weighted regression (MGWR) model. We performed an empirical study in the Nanshan District of Shenzhen, China. Nanshan District is a typical high-density city area in China with a large population and limited urban resources. Using the street view images of the study area, a deep learning scoring model was developed, the SegNet algorithm was introduced to segment and classify the visual street elements, and a random forest algorithm based on the restorative factor scale was employed to evaluate the restorative perception of urban streets. In this study, spatial heterogeneity could be observed in the restorative perception data, and the MGWR models yielded higher R2 interpretation strength in terms of processing the urban street restorative data compared to the ordinary least squares and geographically weighted regression (GWR) models. The MGWR model is a regression model that uses different bandwidths for different visual street elements, thereby allowing additional detailed observation of the extent and relevance of the impact of different elements on restorative perception. Our research also supports the exploration of the size of areas where heterogeneity exists in space for each visual street element. We believe that our results can help develop informed design guidelines to enhance street restorative and help professionals develop targeted design improvement concepts based on the restorative nature of the urban street. Copyright © 2023 Han, Wang, He and Jung."
166,165,24,165_Machine Learning for High Entropy Alloys (HEAs),Machine Learning for High Entropy Alloys (HEAs),"High-entropy alloys (HEAs) represent a promising class of materials with exceptional structural and functional properties. However, their design and optimization pose challenges due to the large composition-phase space coupled with the complex and diverse nature of the phase formation dynamics. In this study, a data-driven approach that utilizes machine learning (ML) techniques to predict HEA phases and their composition-dependent phases is proposed. By employing a comprehensive dataset comprising 5692 experimental records encompassing 50 elements and 11 phase categories, we compare the performance of various ML models. Our analysis identifies the most influential features for accurate phase prediction. Furthermore, the class imbalance is addressed by employing data augmentation methods, raising the number of records to 1500 in each category, and ensuring a balanced representation of phase categories. The results show that XGBoost and Random Forest consistently outperform the other models, achieving 86% accuracy in predicting all phases. Additionally, this work provides an extensive analysis of HEA phase formers, showing the contributions of elements and features to the presence of specific phases. We also examine the impact of including different phases on ML model accuracy and feature significance. Notably, the findings underscore the need for ML model selection based on specific applications and desired predictions, as feature importance varies across models and phases. This study significantly advances the understanding of HEA phase formation, enabling targeted alloy design and fostering progress in the field of materials science. © 2023, Crown.,High entropy alloys (HEAs) are an important material class in the development of next-generation structural materials, but the astronomically large composition space cannot be efficiently explored by experiments or first-principles calculations. Machine learning (ML) methods might address this challenge, but ML of HEAs has been hindered by the scarcity of HEA property data. In this work, the EMTO-CPA method was used to generate a large HEA dataset (spanning a composition space of 14 elements) containing 7086 cubic HEA structures with structural properties, 1911 of which have the complete elastic tensor calculated. The elastic property dataset was used to train a ML model with the Deep Sets architecture. The Deep Sets model has better predictive performance and generalizability compared to other ML models. Association rule mining was applied to the model predictions to describe the compositional dependence of HEA elastic properties and to demonstrate the potential for data-driven alloy design. © 2022, The Author(s).,High Entropy Alloys (HEAs) are a novel category of materials with unique physical and mechanical properties, which makes them a promising candidate for various engineering applications. However, the accurate identification of different phases present in HEAs, controlling its superb properties, is still a challenging task. To address this issue, this study proposes a comprehensive approach based on machine learning (ML) and deep learning (DL) to detect the phases present in HEAs. A deep neural network (DNN) was successfully developed that uses unique feature values of a number of HEAs to accurately predict different phases present in the alloy system. The DNN architecture consists of multiple layers that allow it to learn and extract the relevant features from the data, and then classify them into their respective phases. To train and validate the DNN model, a large dataset of HEAs (with known phases and their corresponding feature values) was collected from the literature. This dataset was then used to train the DNN model using a supervised learning approach. The DNN model was also validated using a separate set of HEAs with unknown phases to test its accuracy and reliability. Comparison with known results demonstrated that the DNN model can accurately predict the phases present in HEAs with high accuracy. After implementing all ML and DL algorithms, a voting ensemble (containing six best performing algorithms) was constructed. The final accuracy was obtained as high as 84% which is quite reasonable for classification problem. Further, it was also observed how significant each feature is in determining the phase(s) of HEAs. The proposed approach offers a promising method for phase detection in HEAs, which can lead to a deeper understanding of their properties and facilitate the design of new materials for various applications. © 2023 Elsevier Ltd"
167,166,24,166_Wind Power Forecasting,Wind Power Forecasting,"In order to solve the security threat brought by the volatility and randomness of large-scale distributed wind power, this paper proposed a wind power prediction model which integrates two-layer decomposition and deep learning, effectively realizing the accurate prediction of wind power series with non-stationary characteristics. Initially, pearson correlation coefficient (PCC) is employed to identify primary meteorological variables as input series. Second, the wind power series are smoothed by implementing complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), and then all subseries are decomposed and obtained by utilizing empirical wavelet transform (EWT) for the components with the highest complexity. Subsequently, hidden information related to wind speed, wind direction, and wind power series are extracted through the bidirectional temporal convolutional network (BiTCN), and the obtained information is fed into a bidirectional long short-term memory network (BiLSTM) optimized by attention mechanism for prediction. Finally, the predicted values of all components are summed to derive the final prediction results. In addition, the significant advantages of the prediction model in this paper are verified by five comparison experiments. The mean absolute error (MAE) and root mean square error (RMSE) of the model's one-step prediction in the January dataset are 2.1647 and 2.8456, respectively. © 2023,Accurate and effective short-term wind power forecasting is vital for the large-scale integration of wind power generation into the power grid. However, due to the intermittence and volatility of wind resources, short-term wind power forecasting is challenging. To address the issue that the existing decomposition forecasting methods ignore the coupling relationship between wind power series and multiple meteorological series, this study proposes a short-term wind power forecasting method based on multivariate signal decomposition and variable selection. First, multivariate variational mode decomposition (MVMD) is used to perform time-frequency synchronous analysis on wind power and multidimensional meteorological series, thereby decomposing them into the same predefined number of frequency-aligned intrinsic mode functions (IMFs). Secondly, elastic net (EN) is used for supervised variable selection on all IMFs to provide a high-quality training set for the forecasting model, thereby enhancing precision and interpretability. Next, a hybrid deep neural network combining convolutional neural network (CNN), bidirectional long-short term memory (BiLSTM) neural network, and multi-head attention (MHA) mechanism is employed to model the output curve of a group of wind turbines in a wind farm. Finally, the proposed method is comprehensively evaluated through four sets of comparative experiments and multiple evaluation metrics on data gathered from the Mahuangshan first wind farm in China with four forecasting horizons: 15-min ahead, 30-min ahead, 45-min ahead, and 1-h ahead. The experimental results show that the proposed method significantly outperforms fifteen existing deep learning methods in terms of precision and stability. © 2024 Elsevier Ltd,Large-scale wind power grid connection increases the uncertainty of the power system, which reduces the economy and security of power system operations. Wind power prediction technology provides the wind power sequence for a period of time in the future, which provides key technical support for the reasonable development of the power generation plan and the arrangement of spare capacity. For large-scale wind farm groups, we propose a cluster model of wind power prediction based on multi-task learning, which can directly output the power prediction results of multiple wind farms. Firstly, the spatial and temporal feature matrix is constructed based on the meteorological forecast data provided by eight wind farms, and the dimensionality of each attribute is reduced by the principal component analysis algorithm to form the spatial fusion feature set. Then, a network structure with bidirectional gated cycle units is constructed, and a multi-output network structure is designed based on the Multi-gate Mixture-of-Experts (MMoE) framework to design the wind power group prediction model. Finally, the data provided by eight wind farms in Jilin, China, was used for experimental analysis, and the predicted average normalized root mean square error is 0.1754, meaning the prediction precision meets the scheduling requirement, which verifies the validity of the wind power prediction model. © 2023 by the authors."
168,167,24,167_Automated Dental Radiograph Analysis,Automated Dental Radiograph Analysis,"Objective: This study aims to investigate the effect of number of data on model performance, for the detection of tooth numbering problem on dental panoramic radiographs, with the help of image processing and deep learning algorithms. Study Design: The data set consists of 3000 anonymous dental panoramic X-rays of adult individuals. Panoramic X-rays were labeled on the basis of 32 classes in line with the FDI tooth numbering system. In order to examine the relationship between the number of data used in image processing algorithms and model performance, four different datasets which include 1000, 1500, 2000 and 2500 panoramic X-rays, were used. The training of the models was carried out with the YOLOv4 algorithm and trained models were tested on a fixed test dataset with 500 data and compared based on F1 score, mAP, sensitivity, precision and recall metrics. Results: The performance of the model increased as the number of data used during the training of the model increased. Therefore, the last model trained with 2500 data showed the highest success among all the trained models. Conclusion: Dataset size is important for dental enumeration, and large samples should be considered as more reliable. © 2023, The Author(s) under exclusive licence to Japanese Society for Oral and Maxillofacial Radiology.,Neural networks and artificial intelligence find more applications in dentistry. It treats dental caries, the most prevalent type of dental illness worldwide. Even though dental caries can be prevented and treated, they typically cause dental discomfort and tooth loss. For dental caries to be treated quickly and effectively, comprehensive detection may be needed, a combination of techniques that include eye inspection, probing, using a dental probe, and using a hand-held mirror, and the individual application of each of these techniques, can quickly identify large caries cavities. Long-established caries detection techniques help to locate only partially hidden but still accessible holes. Deep learning (DL) techniques have produced remarkable diagnostic results in radiology. This study aimed to classify various radiographic extensions on panoramic films using DL techniques, identify caries lesions using these techniques, and compare the results to those of dentists with extensive training. Faster region-based convolutional neural networks (R-CNN) is a newly discovered field of medical research that is rapidly expanding and has produced outstanding results in diagnosing and prognosis of pathology and radiology conditions. In this study, dental cavities were detected and analysed using periapical radiographs to evaluate the accuracy of the Faster R-CNN algorithm. Because these three caries were derived from the oral panoramic images, we designed You Only Look Once Version 3 (YOLOv3) as a U-shaped network with a large-scale axial attention module. We also compare the effectiveness of YOLOv3's segmentation to that of other industrial standards. Experiments show that our proposed method, Fast R-CNN–YOLOv3, achieves higher accuracy in segmenting the three distinct caries level. The proposed model (R-CNN–YOLOv3) achieved an effective result with a precision of 97.183%. © 2023 John Wiley & Sons Ltd.,Objectives: This study developed and validated a deep learning-based method to automatically segment and number teeth in panoramic radiographs across primary, mixed, and permanent dentitions. Methods: A total of 6,046 panoramic radiographs were collected and annotated. The dataset encompassed primary, mixed and permanent dentitions and dental abnormalities such as tooth number anomalies, dental diseases, dental prostheses, and orthodontic appliances. A deep learning-based algorithm consisting of a U-Net-based region of interest extraction model, a Hybrid Task Cascade-based teeth segmentation and numbering model, and a post-processing procedure was trained on 4,232 images, validated on 605 images, and tested on 1,209 images. Precision, recall and Intersection-over-Union (IoU) were used to evaluate its performance. Results: The deep learning-based teeth identification algorithm achieved good performance on panoramic radiographs, with precision and recall for teeth segmentation and numbering exceeding 97%, and the IoU between predictions and ground truths reaching 92%. It generalized well across all three dentition stages and complex real-world cases. Conclusions: By utilizing a two-stage training framework with a large-scale heterogeneous dataset, the automatic teeth identification algorithm achieved a performance level comparable to that of dental experts. Clinical Significance: Deep learning can be leveraged to aid clinical interpretation of panoramic radiographs across primary, mixed, and permanent dentitions, even in the presence of real-world complexities. This robust teeth identification algorithm could contribute to the future development of more advanced, diagnosis- or treatment-oriented dental automation systems. © 2023"
169,168,23,168_Model-Based Systems Engineering (MBSE) for Natural Language Requirements Conversion,Model-Based Systems Engineering (MBSE) for Natural Language Requirements Conversion,"Machine reading comprehension (MRC) is one of the most challenging tasks and active fields in natural language processing (NLP). MRC systems aim to enable a machine to understand a given context in natural language and to answer a series of questions about it. With the advent of bi-directional deep learning algorithms and large-scale datasets, MRC achieved improved results. However, these models are still suffering from two research issues: textual ambiguities and semantic vagueness to comprehend the long passages and generate answers for abstractive MRC systems. To address these issues, this paper proposes a novel Extended Generative Pretrained Transformers-based Question Answering (ExtGPT-QA) model to generate precise and relevant answers to questions about a given context. The proposed architecture comprises two modified forms of encoder and decoder as compared to GPT. The encoder uses a positional encoder to assign a unique representation with each word in the sentence for reference to address the textual ambiguities. Subsequently, the decoder module involves a multi-head attention mechanism along with affine and aggregation layers to mitigate semantic vagueness with MRC systems. Additionally, we applied syntax and semantic feature engineering techniques to enhance the effectiveness of the proposed model. To validate the proposed model's effectiveness, a comprehensive empirical analysis is carried out using three benchmark datasets including SQuAD, Wiki-QA, and News-QA. The results of the proposed ExtGPT-QA outperformed state of art MRC techniques and obtained 93.25% and 90.52% F1-score and exact match, respectively. The results confirm the effectiveness of the ExtGPT-QA model to address textual ambiguities and semantic vagueness issues in MRC systems. © 2023 Ahmed et al.,A man-made machine-reading comprehension (MRC) dataset is necessary to train the answer extraction part of existing Question Answering (QA) systems. However, a high-quality and well-structured dataset with question-paragraph-answer pairs is not usually found in the real world. Furthermore, updating or building an MRC dataset is a challenging and costly affair. To address these shortcomings, we propose a QA system that uses a large-scale English Community Question Answering (CQA) dataset (i.e., Stack Exchange) composed of 3,081,834 question-answer pairs. The QA system adopts a classifier-retriever-summarizer structure design. The question classifier and the answer retriever part are based on a Bidirectional Encoder Representations from Transformers (BERT) Natural Language Processing (NLP) model by Google, and the summarizer part introduces a deep learning-based Text-to-Text Transfer Transformer (T5) model to summarize the long answers. We instantiated the proposed QA system with 140 topics from the CQA dataset (including topics such as biology, law, politics, etc.) and conducted human and automatic evaluations. Our system presented encouraging results, considering that it provides high-quality answers to the questions in the test set and satisfied the requirements to develop a QA system without MRC datasets. Our results show the potential of building automatic and high-performance QA systems without being limited by man-made datasets, a significant step forward in the research of open-domain or specific-domain QA systems. © 2023 Elsevier B.V.,With the fast growth of information science and engineering, a large number of textual data generated are valuable for natural language processing and its applications. Particularly, finding correct answers to natural language questions or queries requires spending tremendous time and effort in human life. While using search engines to discover information, users manually determine the answer to a given question on a range of retrieved texts or documents. Question answering relies heavily on the capability to automatically comprehend questions in human language and extract meaningful answers from a single text. In recent years, such question–answering systems have become increasingly popular using machine reading comprehension techniques. On the other hand, high-resource languages (e.g., English and Chinese) have witnessed tremendous growth in question-answering methodologies based on various knowledge sources. Besides, powerful BERTology-based language models only encode texts with a limited length. The longer texts contain more distractor sentences that affect the QA system performance. Vietnamese has a variety of question words in the same question type. To address these challenges, we propose ViQAS, a new question–answering system with multi-stage transfer learning using language models based on BERTology for a low-resource language such as Vietnamese. Last but not least, our QA system is integrated with Vietnamese characteristics and transformer-based evidence extraction techniques into an effective contextualized language model-based QA system. As a result, our proposed system outperforms our forty retriever-reader QA configurations and seven state-of-the-art QA systems such as DrQA, BERTserini, BERTBM25, XLMRQA, ORQA, COBERT, and NeuralQA on three Vietnamese benchmark question answering datasets. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
170,169,23,169_Deep Learning for Aneurysm Detection and Intracranial Hemorrhage Segmentation on CT and CTA Scans,Deep Learning for Aneurysm Detection and Intracranial Hemorrhage Segmentation on CT and CTA Scans,"Intracranial hemorrhage (ICH) brain hemorrhages are life-threatening medical conditions that require a prompt and accurate diagnosis to ensure timely intervention and treatment. Computed tomography (CT) scans play a vital role in diagnosing brain hemorrhages due to their ability to provide detailed cross-sectional images of the brain. However, the manual interpretation of CT scans is time-consuming and subjects to human error. To address these challenges, this study proposes a stacked deep model-based classification approach for the automated identification and differentiation of multiclass brain hemorrhages in CT scans. The stacked deep model architecture consists of multiple layers of neural networks, each designed to learn specific levels of features, leading to enhanced representation learning and improved classification performance. Later, the stacked models are connected with the weighted approach that helps to get the ensemble models in the proper order based on their power of predicting correct samples. To build the stacked deep model, a large dataset of 7,52 000 annotated CT scans is utilized to train the model and optimize its parameters. Furthermore, transfer learning is employed to leverage pretrained models for feature extraction, which aids in handling limited data scenarios. Experimental results demonstrate that the stacked deep model achieves superior accuracy (98.56%), and other classification performance measures such as sensitivity (95.89%), specificity (99.58%), AUC-ROC (98.47%), and weighted log loss (0.04967) as compared to conventional methods. The automated classification of multiclass brain hemorrhages in CT scans using the proposed model not only significantly reduces the time required for diagnosis but also enhances the reliability and consistency of results. This model has the potential to assist medical professionals in making more informed decisions and improve patient outcomes by enabling rapid and accurate diagnosis of brain hemorrhages. © 2023 Wiley Periodicals LLC.,The accuracy of computed tomography angiography (CTA) image interpretation depends on the radiologist. This study aims to develop a new method for automatically detecting intracranial aneurysms from CTA images using deep learning, based on a convolutional neural network (CNN) implemented on the DeepMedic platform. Ninety CTA scans of patients with intracranial aneurysms are collected and divided into two datasets: training (80 subjects) and test (10 subjects) datasets. Subsequently, a deep learning architecture with a three-dimensional (3D) CNN model is implemented on the DeepMedic platform for the automatic segmentation and detection of intracranial aneurysms from the CTA images. The samples in the training dataset are used to train the CNN model, and those in the test dataset are used to assess the performance of the established system. Sensitivity, positive predictive value (PPV), and false positives are evaluated. The overall sensitivity and PPV of this system for detecting intracranial aneurysms from CTA images are 92.3% and 100%, respectively, and the segmentation sensitivity is 92.3%. The performance of the system in the detection of intracranial aneurysms is closely related to their size. The detection sensitivity for small intracranial aneurysms (? 3 mm) is 66.7%, whereas the sensitivity of detection for large (> 10 mm) and medium-sized (3–10 mm) intracranial aneurysms is 100%. The deep learning architecture with a 3D CNN model on the DeepMedic platform can reliably segment and detect intracranial aneurysms from CTA images with high sensitivity. © 2022, The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine.,Background and purpose: Multiple attempts at intracranial hemorrhage (ICH) detection using deep-learning techniques have been plagued by clinical failures. We aimed to compare the performance of a deep-learning algorithm for ICH detection trained on strongly and weakly annotated datasets, and to assess whether a weighted ensemble model that integrates separate models trained using datasets with different ICH improves performance. Methods: We used brain CT scans from the Radiological Society of North America (27,861 CT scans, 3,528 ICHs) and AI-Hub (53,045 CT scans, 7,013 ICHs) for training. DenseNet121, InceptionResNetV2, MobileNetV2, and VGG19 were trained on strongly and weakly annotated datasets and compared using independent external test datasets. We then developed a weighted ensemble model combining separate models trained on all ICH, subdural hemorrhage (SDH), subarachnoid hemorrhage (SAH), and small-lesion ICH cases. The final weighted ensemble model was compared to four well-known deep-learning models. After external testing, six neurologists reviewed 91 ICH cases difficult for AI and humans. Results: InceptionResNetV2, MobileNetV2, and VGG19 models outperformed when trained on strongly annotated datasets. A weighted ensemble model combining models trained on SDH, SAH, and small-lesion ICH had a higher AUC, compared with a model trained on all ICH cases only. This model outperformed four deep-learning models (AUC [95% C.I.]: Ensemble model, 0.953[0.938–0.965]; InceptionResNetV2, 0.852[0.828–0.873]; DenseNet121, 0.875[0.852–0.895]; VGG19, 0.796[0.770–0.821]; MobileNetV2, 0.650[0.620–0.680]; p < 0.0001). In addition, the case review showed that a better understanding and management of difficult cases may facilitate clinical use of ICH detection algorithms. Conclusion: We propose a weighted ensemble model for ICH detection, trained on large-scale, strongly annotated CT scans, as no model can capture all aspects of complex tasks. Copyright © 2023 Kang, Park, Ryu, Schellingerhout, Kim, Kim, Park, Lee, Han, Jeong and Kim."
171,170,23,170_Text Summarization,Text Summarization,"Journaling is a widely adopted technique, known to improve mental health and well-being by enabling reflection on past events. Large amounts of text in digital journaling applications could hinder the reflection process due to information overload. Abstractive summarization can solve this problem by generating short summaries to quickly glance at and reminisce. In this paper, we present an investigation of the utility of large language models in the context of autobiographical text summarization. We study two approaches to adapt a self-supervised learning (SSL) model to the domain of autobiographical text. One model employs transfer learning using our new autobiographical text summary dataset to fine-tune the SSL model. The second model leverages existing news datasets for high-quality text summarization mixed with our autobiographical summary dataset. We conducted mixed methods research to analyze the performance of these two models. Through objective evaluation using ROUGE and BART scores, we find that both these approaches perform significantly better than the SSL model fine-tuned with only high-quality news datasets, showing the importance of domain adaptation and autobiographical text summary dataset for this task. Secondly, through a subjective evaluation on a crowd-sourcing platform, we evaluated the summaries generated from these models on various quality criteria such as grammar, non-redundancy, structure, and coherence. We found that on all criteria, these summaries score >4 out of 5, and the two models show comparable results. We deployed a proof-of-concept web-based journaling application to assess the practical real-world implications of incorporating abstractive summarization in a digital journaling context. We found that the participants showed a high consensus that the summaries generated by the system captured the main idea of their journal entry (80% of the 75 participants gave a Likert scale rating of (Formula presented.) out of 7.0, with the overall mean rating of 5.56 ± 1.32) while being factually correct, and they found it to be a useful feature of a journaling application. Finally, we conducted human evaluation studies to compare the quality of the summaries generated from a commercial tool ChatGPT and mixed distribution fine-tuned SSL model, and present insights into these systems in the context of autobiographical abstractive text summarization. We have made our model, dataset, and subjective evaluation questionnaire openly available to the research community. © 2023 Taylor & Francis Group, LLC.,With the development of pre-trained language models and large-scale datasets, automatic text summarization has attracted much attention from the community of natural language processing, but the progress of automatic summarization evaluation has stagnated. Although there have been efforts to improve automatic summarization evaluation, ROUGE has remained one of the most popular metrics for nearly 20 years due to its competitive evaluation performance. However, ROUGE is not perfect, there are studies have shown that it is suffering from inaccurate evaluation of abstractive summarization and limited diversity of generated summaries, both caused by lexical bias. To avoid the bias of lexical similarity, more and more meaningful embedding-based metrics have been proposed to evaluate summaries by measuring semantic similarity. Due to the challenge of accurately measuring semantic similarity, none of them can fully replace ROUGE as the default automatic evaluation toolkit for text summarization. To address the aforementioned problems, we propose a compromise evaluation framework (ROUGE-SEM) for improving ROUGE with semantic information, which compensates for the lack of semantic awareness through a semantic similarity module. According to the differences in semantic similarity and lexical similarity, summaries are classified into four categories for the first time, including good-summary, pearl-summary, glass-summary, and bad-summary. In particular, the back-translation technique is adopted to rewrite pearl-summary and glass-summary that are inaccurately evaluated by ROUGE to alleviate lexical bias. Through this pipeline framework, summaries are first classified by candidate summary classifier, then rewritten by categorized summary rewriter, and finally scored by rewritten summary scorer, which are efficiently evaluated in a manner consistent with human behavior. When measured using Pearson, Spearman, and Kendall rank coefficients, our proposal achieves comparable or higher correlations with human judgments than several state-of-the-art automatic summarization evaluation metrics in dimensions of coherence, consistency, fluency, and relevance. This also suggests that improving ROUGE with semantics is a promising direction for automatic summarization evaluation. © 2023 Elsevier Ltd,With the rapid and unprecedented growth of textual data in recent years, there is a remarkable need for automatic text summarization models to retrieve useful information from these large numbers of textual documents without human intervention within a reasonable time. Text summarization is commonly performed based on extractive and abstractive paradigms. Although different machine learning and deep learning based methods have been proposed for the task of text summarization during the last decades, they are still in their early steps of development and their potential has yet to be fully explored. Accordingly, a new summarization model is proposed in this paper which takes advantage of both extractive and abstractive text summarization models as a single unified model based on the strategy gradient of reinforcement learning. The proposed model also employs the combination of convolutional neural network and gated recurrent unit in both extraction and abstraction modules besides attention mechanism. Moreover, language models, namely Word2Vec and BERT, are used as the backbone of the proposed model to better express sentence semantics as a word vector. We conducted our experiments on widely-studied text summarization datasets (CNN\Daily Mail and DUC-2004) and according to the empirical results, not only the proposed model achieved higher accuracy compared to both extractive and abstractive summarization models in terms of ROUGE metric but also its generated summaries presented higher saliency and readability based on human evaluation. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
172,171,23,171_Fuzzy Cognitive Maps for Time Series Forecasting,Fuzzy Cognitive Maps for Time Series Forecasting,"As a soft computing method, applying fuzzy cognitive map (FCM) to time series prediction has become a timely issue pursued by numerous researchers. Although many FCM construction methods have emerged, most of them exhibit obvious limitations in weight learning especially for long-term or complex time series. Either the weight calculation is computationally expensive, or it cannot achieve gratifying accuracy. In this paper, a new method for constructing FCM is proposed which extracts concepts from data by exploiting triangular membership function, and the weights of high-order FCM are subtly obtained by transforming the learning problem of FCM into a convex optimization problem with constraints. Since then, FCM with optimized weights is used to represent fuzzy logical relationships of time series and implement prediction further. Fifteen benchmark time series,such as Soybean Price time series, Yahoo stock time series, Condition monitoring of hydraulic systems time series etc. are applied to verify prediction performance of the proposed method. Accordingly, experiment results show that the proposed numerical prediction method of time series is effective and can acquire better prediction accuracy with lower computation time than other recent advanced methods. In addition, the influence of parameters of the method is analyzed individually.  © 2013 IEEE.,Complex fuzzy sets are an extension of type-1 fuzzy sets with complex-valued membership functions. Over the last 20 years, time-series forecasting has emerged as the most important application of complex fuzzy sets, with neuro-fuzzy systems employing them shown to be accurate and compact forecasting models. In the complex fuzzy sets literature, two dominant approaches to designing forecasters can be observed: sinusoidal membership functions versus complex-valued Gaussian membership functions. To date, however, there has never been a systematic investigation that compares the performance of these two membership types (or their combination) within a common architecture. We propose a new neuro-fuzzy architecture using complex fuzzy sets that has been designed for large-scale learning problems. This architecture employs randomized learning to speed up network training. In designing this architecture, we empirically compared sinusoidal complex fuzzy sets and complex Gaussian fuzzy sets. Across multiple variations of the architecture, we find that the complex Gaussian fuzzy sets lead to significantly more accurate forecasts on moderate-to-large time series datasets, while still keeping the overall size of the network compact. © 2023 Elsevier B.V.,Among various soft computing approaches for time series forecasting, fuzzy cognitive maps (FCMs) have shown remarkable results as a tool to model and analyze the dynamics of complex systems. FCMs have similarities to recurrent neural networks and can be classified as a neuro-fuzzy method. In other words, FCMs are a mixture of fuzzy logic, neural network, and expert system aspects, which act as a powerful tool for simulating and studying the dynamic behavior of complex systems. The most interesting features are knowledge interpretability, dynamic characteristics and learning capability. The goal of this survey paper is mainly to present an overview on the most relevant and recent FCM-based time series forecasting models proposed in the literature. In addition, this article considers an introduction on the fundamentals of FCM model and learning methodologies. Also, this survey provides some ideas for future research to enhance the capabilities of FCM in order to cover some challenges in the real-world experiments such as handling non-stationary data and scalability issues. Moreover, equipping FCMs with fast learning algorithms is one of the major concerns in this area. © 2022, The Author(s), under exclusive licence to Springer Nature B.V."
173,172,23,172_Reliability analysis using Kriging and surrogate models,Reliability analysis using Kriging and surrogate models,"Estimating the small failure probability of highly reliable structures is often computationally expensive in reliability analysis. The adaptive kriging-based reliability analysis methods have been widely used to solve this issue. However, the kriging refinement phase of these methods may not achieve adequate efficiency due to a large candidate sample pool (CSP) size and unnecessary limit state function (LSF) evaluations. In this work, an efficient adaptive kriging refinement method (EAKRM) is proposed to alleviate the computational burden. First, a CSP generation strategy is developed using the radius sequence to gradually generate uniform samples along the direction vector until the failure region appears. Considering points with a high risk of misjudgment and located outside of CSP, a two-stage training point selection strategy is then proposed based on the learning function U to determine the most valuable training point. Finally, a correlation-based stopping criterion is presented by quantifying the consistency of the kriging predictive signs between two successive refinement processes. Three mathematical examples and three engineering examples are employed to illustrate the effectiveness of the proposed EAKRM. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,The reliability analysis methods based on the Kriging model have been explored to update the design of experiments (DoE), but the number of calls to the performance function is high. In this paper, a new active learning method combining the weight information entropy function (WH) and the adaptive candidate sample pool is proposed to improve the reliability analysis efficiency. Based on the information entropy function (H), the learning function WH is constructed to consider Kriging variance and the joint probability density function (PDF). In this proposed method, the sample points can be assigned different weight by the learning function WH according to the important degrees of sample points. The point which not only nears the limit state function (LSF), but also has a high probability density function value and large Kriging variance is assigned more weight than others. To select the sample points with lower confidence level, the adaptive candidate sample pool is generated by Markov Chain Monte Carlo (MCMC) simulation. The Kriging model can be updated efficiently by the proposed method. Four numerical examples and an engineering example with implicit performance function are used to verify the efficiency and accuracy of the proposed method. The results show that the proposed method can significantly improve the computational efficiency of the reliability analysis without losing accuracy. © IMechE 2022.,With increasing complexity of engineering problems, various traditional reliability analysis methods are facing rising challenges in terms of computational efficiency and accuracy. Surrogate models, especially Kriging model, have received growing attention and been widely used in reliability analyses by the virtue of their advantages for achieving high computational efficiency and ensuring high numerical accuracy. Nevertheless, there have been still two significant problems in the Kriging model-assisted reliability analyses due to the absence of prior knowledge: i.e. (1) the size of candidate sample pool tends to be quite large in order to ensure prediction of a convergent failure probability; and (2) local prediction accuracy of limiting state surface by Kriging model is generally excessive. These above two issues can often result in high computational cost for Kriging-based reliability analyses. To enhance computational efficiency, a new method that combines adaptive Kriging and n-hypersphere rings, named an AK-HRn method, is proposed in this study. First, the n-hypersphere rings, which can update its position and radius adaptively, is adopted to divide the design space into potential safety domains and potential failure domains. Second, these potential failure domains are used as the sampling domains for implementing importance sampling method to generate a suitably-sized candidate sample pool. Third, a novel learning function is presented to enrich the design of experiment (DoE), which avoids excessive local prediction accuracy of Kriging models by establishing the rejection domains. Finally, the efficiency and robustness of AK-HRn is compared with other Kriging-based reliability analysis methods through four illustrative numerical examples and one 6-DOF industrial robot case study. Comparison shows that the proposed AK-HRn method has high efficiency and robustness to solve complex reliability analysis problems. © 2023 Elsevier B.V."
174,173,23,173_Health Monitoring with Wearable Biosensors and Machine Learning,Health Monitoring with Wearable Biosensors and Machine Learning,"Background and objective: Work-related stress affects a large part of today's workforce and is known to have detrimental effects on physical and mental health. Continuous and unobtrusive stress detection may help prevent and reduce stress by providing personalised feedback and allowing for the development of just-in-time adaptive health interventions for stress management. Previous studies on stress detection in work environments have often struggled to adequately reflect real-world conditions in controlled laboratory experiments. To close this gap, in this paper, we present a machine learning methodology for stress detection based on multimodal data collected from unobtrusive sources in an experiment simulating a realistic group office environment (N=90). Methods: We derive mouse, keyboard and heart rate variability features to detect three levels of perceived stress, valence and arousal with support vector machines, random forests and gradient boosting models using 10-fold cross-validation. We interpret the contributions of features to the model predictions with SHapley Additive exPlanations (SHAP) value plots. Results: The gradient boosting models based on mouse and keyboard features obtained the highest average F1 scores of 0.625, 0.631 and 0.775 for the multiclass prediction of perceived stress, arousal and valence, respectively. Our results indicate that the combination of mouse and keyboard features may be better suited to detect stress in office environments than heart rate variability, despite physiological signal-based stress detection being more established in theory and research. The analysis of SHAP value plots shows that specific mouse movement and typing behaviours may characterise different levels of stress. Conclusions: Our study fills different methodological gaps in the research on the automated detection of stress in office environments, such as approximating real-life conditions in a laboratory and combining physiological and behavioural data sources. Implications for field studies on personalised, interpretable ML-based systems for the real-time detection of stress in real office environments are also discussed. © 2023 The Author(s),Stress is an increasingly prevalent mental health condition that can have serious effects on human health. The development of stress prediction tools would greatly benefit public health by allowing policy initiatives and early stress-reducing interventions. The advent of mobile health technologies including smartphones and smartwatches has made it possible to collect objective, real-time, and continuous health data. We sought to pilot the collection of heart rate variability data from the Apple Watch electrocardiograph (ECG) sensor and apply machine learning techniques to develop a stress prediction tool. Random Forest (RF) and Support Vector Machines (SVM) were used to model stress based on ECG measurements and stress questionnaire data collected from 33 study participants. Data were stratified into socio-demographic classes to further explore our prediction model. Overall, the RF model performed slightly better than SVM, with results having an accuracy within the low end of state-of-the-art. Our models showed specificity in their capacity to assess “no stress” states but were less successful at capturing “stress” states. Overall, the results presented here suggest that, with further development and refinement, Apple Watch ECG sensor data could be used to develop a stress prediction tool. A wearable device capable of continuous, real-time stress monitoring would enable individuals to respond early to changes in their mental health. Furthermore, large-scale data collection from such devices would inform public health initiatives and policies. 2022 Velmovitsky, Alencar, Leatherdale, Cowan and Morita.,Introduction: Advances in wearable sensor technology have enabled the collection of biomarkers that may correlate with levels of elevated stress. While significant research has been done in this domain, specifically in using machine learning to detect elevated levels of stress, the challenge of producing a machine learning model capable of generalizing well for use on new, unseen data remain. Acute stress response has both subjective, psychological and objectively measurable, biological components that can be expressed differently from person to person, further complicating the development of a generic stress measurement model. Another challenge is the lack of large, publicly available datasets labeled for stress response that can be used to develop robust machine learning models. In this paper, we first investigate the generalization ability of models built on datasets containing a small number of subjects, recorded in single study protocols. Next, we propose and evaluate methods combining these datasets into a single, large dataset to study the generalization capability of machine learning models built on larger datasets. Finally, we propose and evaluate the use of ensemble techniques by combining gradient boosting with an artificial neural network to measure predictive power on new, unseen data. In favor of reproducible research and to assist the community advance the field, we make all our experimental data and code publicly available through Github at https://github.com/xalentis/Stress. This paper's in-depth study of machine learning model generalization for stress detection provides an important foundation for the further study of stress response measurement using sensor biomarkers, recorded with wearable technologies. Methods: Sensor biomarker data from six public datasets were utilized in this study. Exploratory data analysis was performed to understand the physiological variance between study subjects, and the complexity it introduces in building machine learning models capable of detecting elevated levels of stress on new, unseen data. To test model generalization, we developed a gradient boosting model trained on one dataset (SWELL), and tested its predictive power on two datasets previously used in other studies (WESAD, NEURO). Next, we merged four small datasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of 99 subjects, and applied feature engineering to generate additional features utilizing statistical summaries, with sliding windows of 25 s. We name this large dataset, StressData. In addition, we utilized random sampling on StressData combined with another dataset (EXAM) to build a larger training dataset consisting of 200 synthesized subjects, which we name SynthesizedStressData. Finally, we developed an ensemble model that combines our gradient boosting model with an artificial neural network, and tested it using Leave-One-Subject-Out (LOSO) validation, and on two additional, unseen publicly available stress biomarker datasets (WESAD and Toadstool). Results: Our results show that previous models built on datasets containing a small number (<50) of subjects, recorded in single study protocols, cannot generalize well to new, unseen datasets. Our presented methodology for generating a large, synthesized training dataset by utilizing random sampling to construct scenarios closely aligned with experimental conditions demonstrate significant benefits. When combined with feature-engineering and ensemble learning, our method delivers a robust stress measurement system capable of achieving 85% predictive accuracy on new, unseen validation data, achieving a 25% performance improvement over single models trained on small datasets. The resulting model can be used as both a classification or regression predictor for estimating the level of perceived stress, when applied on specific sensor biomarkers recorded using a wearable device, while further allowing researchers to construct large, varied datasets for training machine learning models that closely emulate their exact experimental conditions. Conclusion: Models trained on small, single study protocol datasets do not generalize well for use on new, unseen data and lack statistical power. Machine learning models trained on a dataset containing a larger number of varied study subjects capture physiological variance better, resulting in more robust stress detection. Feature-engineering assists in capturing these physiological variance, and this is further improved by utilizing ensemble techniques by combining the predictive power of different machine learning models, each capable of learning unique signals contained within the data. While there is a general lack of large, labeled public datasets that can be utilized for training machine learning models capable of accurately measuring levels of acute stress, random sampling techniques can successfully be applied to construct larger, varied datasets from these smaller sample datasets, for building robust machine learning models. © 2023 The Author(s)"
175,174,23,174_Improved Tracking Methods and Algorithms,Improved Tracking Methods and Algorithms,"Unmanned aerial vehicle (UAV)-based tracking has shown large potential in various domains such as transportation, logistics, public safety, and more. However, deploying deep learning (DL)-based tracking algorithms on UAVs is challenging because of limitations in computing resources, battery capacity, and maximum load. Discriminative correlation filter (DCF)-based trackers have become a popular choice in the UAV tracking community owing to their ability to provide superior efficiency while consuming fewer resources. However, the limited representation learning ability of DCF-based trackers leads to lower precision in complex scenarios compared to DL-based methods. Filter pruning is a prevalent practice for deploying deep neural networks on edge devices with constrained resources, and it may be an effective way to solve problems encountered when deploying deep learning trackers on UAVs. However, the application of filter pruning to UAV tracking is underexplored, and a straightforward and useful pruning standard is desirable. This paper proposes using Fisher pruning to reduce the SiamFC++ model for UAV tracking, resulting in the F-SiamFC++ tracker. The proposed tracker achieves a remarkable balance between precision and efficiency, as demonstrated through exhaustive experiments on four popular UAV benchmarks: UAVDT, DTB70, UAV123@10fps, and Vistrone2018, showing state-of-the-art performance. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Over the last decade, Siamese network architectures have emerged as dominating tracking paradigms, which have led to significant progress. These architectures are made up of a backbone network and a head network. The backbone network comprises two identical feature extraction sub-branches, one for the target template and one for the search candidate. The head network takes both the template and candidate features as inputs and produces a local similarity score for the target object in each location of the search candidate. Despite promising results that have been attained in visual tracking, challenges persist in developing efficient and lightweight models due to the inherent complexity of the task. Specifically, manually designed tracking models that rely heavily on the knowledge and experience of relevant experts are lacking. In addition, the existing tracking approaches achieve excellent performance at the cost of large numbers of parameters and vast amounts of computations. A novel Siamese tracking approach called TrackNAS based on neural architecture search is proposed to reduce the complexity of the neural architecture applied in visual tracking. First, according to the principle of the Siamese network, backbone and head network search spaces are constructed, constituting the search space for the network architecture. Next, under the given resource constraints, the network architecture that meets the tracking performance requirements is obtained by optimizing a hybrid search strategy that combines distributed and joint approaches. Then, an evolutionary method is used to lighten the network architecture obtained from the search phase to facilitate deployment to devices with resource constraints (FLOPs). Finally, to verify the performance of TrackNAS, comparison and ablation experiments are conducted using several large-scale visual tracking benchmark datasets, such as OTB100, VOT2018, UAV123, LaSOT, and GOT-10k. The results indicate that the proposed TrackNAS achieves competitive performance in terms of accuracy and robustness, and the number of network parameters and computation volume are far smaller than those of other advanced Siamese trackers, meeting the requirements for lightweight deployment to resource-constrained devices. © 2023 by the authors.,Pedestrian tracking is a challenging task in the area of visual object tracking research and it is a vital component of various vision-based applications such as surveillance systems, human-following robots, and autonomous vehicles. In this paper, we proposed a single pedestrian tracking (SPT) framework for identifying each instance of a person across all video frames through a tracking-by-detection paradigm that combines deep learning and metric learning-based approaches. The SPT framework comprises three main modules: detection, re-identification, and tracking. Our contribution is a significant improvement in the results by designing two compact metric learning-based models using Siamese architecture in the pedestrian re-identification module and combining one of the most robust re-identification models for data associated with the pedestrian detector in the tracking module. We carried out several analyses to evaluate the performance of our SPT framework for single pedestrian tracking in the videos. The results of the re-identification module validate that our two proposed re-identification models surpass existing state-of-the-art models with increased accuracies of 79.2% and 83.9% on the large dataset and 92% and 96% on the small dataset. Moreover, the proposed SPT tracker, along with six state-of-the-art (SOTA) tracking models, has been tested on various indoor and outdoor video sequences. A qualitative analysis considering six major environmental factors verifies the effectiveness of our SPT tracker under illumination changes, appearance variations due to pose changes, changes in target position, and partial occlusions. In addition, quantitative analysis based on experimental results also demonstrates that our proposed SPT tracker outperforms the GOTURN, CSRT, KCF, and SiamFC trackers with a success rate of 79.7% while beating the DiamSiamRPN, SiamFC, CSRT, GOTURN, and SiamMask trackers with an average of 18 tracking frames per second. © 2023 by the authors."
176,175,23,175_Reward learning and behavioral changes in adolescence,Reward learning and behavioral changes in adolescence,"Many real-world situations require navigating decisions for both reward and threat. While there has been significant progress in understanding mechanisms of decision-making and mediating neurocircuitry separately for reward and threat, there is limited understanding of situations where reward and threat contingencies compete to create approach-avoidance conflict (AAC). Here, we leverage computational learning models, independent component analysis (ICA), and multivariate pattern analysis (MVPA) approaches to understand decision-making during a novel task that embeds concurrent reward and threat learning and manipulates congruency between reward and threat probabilities. Computational modeling supported a modified reinforcement learning model where participants integrated reward and threat value into a combined total value according to an individually varying policy parameter, which was highly predictive of decisions to approach reward vs avoid threat during trials where the highest reward option was also the highest threat option (i.e., approach-avoidance conflict). ICA analyses demonstrated unique roles for salience, frontoparietal, medial prefrontal, and inferior frontal networks in differential encoding of reward vs threat prediction error and value signals. The left frontoparietal network uniquely encoded degree of conflict between reward and threat value at the time of choice. MVPA demonstrated that delivery of reward and threat could accurately be decoded within salience and inferior frontal networks, respectively, and that decisions to approach reward vs avoid threat were predicted by the relative degree to which these reward vs threat representations were active at the time of choice. This latter result suggests that navigating AAC decisions involves generating mental representations for possible decision outcomes, and relative activation of these representations may bias subsequent decision-making towards approaching reward or avoiding threat accordingly. © 2022 The Authors,Background: Neural activation during reward processing is thought to underlie critical behavioral changes that take place during the transition to adolescence (e.g., learning, risk-taking). Though literature on the neural basis of reward processing in adolescence is booming, important gaps remain. First, more information is needed regarding changes in functional neuroanatomy in early adolescence. Another gap is understanding whether sensitivity to different aspects of the incentive (e.g., magnitude and valence) changes during the transition into adolescence. We used fMRI from a large sample of preadolescent children to characterize neural responses to incentive valence vs. magnitude during anticipation and feedback, and their change over a period of two years. Methods: Data were taken from the Adolescent Cognitive and Brain DevelopmentSM (ABCD®) study release 3.0. Children completed the Monetary Incentive Delay task at baseline (ages 9–10) and year 2 follow-up (ages 11–12). Based on data from two sites (N = 491), we identified activation-based Regions of Interest (ROIs; e.g., striatum, prefrontal regions, etc.) that were sensitive to trial type (win $5, win $0.20, neutral, lose $0.20, lose $5) during anticipation and feedback phases. Then, in an independent subsample (N = 1470), we examined whether these ROIs were sensitive to valence and magnitude and whether that sensitivity changed over two years. Results: Our results show that most ROIs involved in reward processing (including the striatum, prefrontal cortex, and insula) are specialized, i.e., mainly sensitive to either incentive valence or magnitude, and this sensitivity was consistent over a 2-year period. The effect sizes of time and its interactions were significantly smaller (0.002??2?0.02) than the effect size of trial type (0.06??2?0.30). Interestingly, specialization was moderated by reward processing phase but was stable across development. Biological sex and pubertal status differences were few and inconsistent. Developmental changes were mostly evident during success feedback, where neural reactivity increased over time. Conclusions: Our results suggest sub-specialization to valence vs. magnitude within many ROIs of the reward circuitry. Additionally, in line with theoretical models of adolescent development, our results suggest that the ability to benefit from success increases from pre- to early adolescence. These findings can inform educators and clinicians and facilitate empirical research of typical and atypical motivational behaviors during a critical time of development. © 2023 The Author(s),Tobacco use and its harmful health-related problems have become one of the largest modern preventable public health issues. Current research strongly suggests that smoking during adolescence enhances addictive smoking behaviors during life, which can be related to adolescence as a critical ontogenetic period characterized by behaviors that can increase the probability of risk-related behaviors such as sensation and novelty seeking. Adolescent development is also a period of maturation of frontal and subcortical neural systems, brain changes that underlie higher impulsivity tendencies to promote adequate learning and adaptations necessary to succeed the novel challenges of the adult life, but those changes also enhance vulnerabilities to the addictive effects of drugs. Consistent with this, tobacco use affects brain development processes which underlie longterm psychobiological alterations and the enhanced risks for tobacco addiction during adult life. Thus, the present review describes current psychobiological approaches to understand general addiction processes and tobacco addiction, highlighting the behavioral and neural short-term effects of tobacco use during adolescence and its long-term effects during adulthood. Current research has advanced on four aspects for the understanding of both the psychobiology of adolescent development and the effects of drugs of abuse during this time. The first aspect is behavioral, as adolescence is related to important changes on motivational and emotional behaviors such as sensation seeking. Other important behavioral changes are social approach, a higher variety of opportunity for personal choices, and development of personal independence. Research on a second aspect has focused on cognition. A review of research is presented showing enhanced abilities during adolescence development for reading, abstract and logical thinking, and novel problem solving. Stress reactivity is the third aspect of reviewed psychobiological mechanisms. The stress biological system undergoes important changes during adolescence, including changes on stress-related hormones and neural architecture. An important issue is that exposure to early and/or chronic stressful circumstances during adolescence could be related to higher risk to the start and maintenance of addiction states, as suggested by research assessing the disruptive effects of stress on psychobiological homeostatic processes needed to maintain stable biological and emotional regulation. The fourth aspect is psychobiology. In this section research is reviewed related to the development of monoaminergic brain circuits underlying motivation, novelty-seeking, impulsivity, and addiction processes. Using as model the previous review integration, the effects of nicotine are discussed, the essential addictive component of tobacco, on the neurochemical systems underlying tobacco addiction. Following this, important research is introduced that describes psychobiological changes during adolescence and evidence of vulnerability to addiction during this life stage. Then, current research on both short-term and long-term effects of tobacco or nicotine administration during adolescence on the brain, behavior, and cognition is introduced. The current research advances and discussions on the psychobiology of addictions in general, and tobacco addiction in particular, have been possible to a large extent from the use of animal models and preclinical research, since animal models have become crucial to identify learning, motivational, emotional, and cognitive mechanisms that underlie addictive processes, and making possible to perform experimental procedures to discover the functioning and participation of biological components. One example of such components is the cholinergic system, which is activated by nicotine and is part of the neurochemical machinery on different brain areas important for both tobacco addiction and adolescence development such as the dorsal striatum, amygdala, ventral tegmental area, nucleus accumbens, prefrontal cortex, and hippocampus. The present review and research divulgation written in Spanish are expected to clarify modern research on addiction and encourage current scientific education on the vulnerabilities and predispositions for tobacco abuse in Latin-American countries © 2023, Interdisciplinaria.All Rights Reserved."
177,176,22,176_Bird sound classification and monitoring using acoustic recordings,Bird sound classification and monitoring using acoustic recordings,"There is a need for monitoring biodiversity at multiple spatial and temporal scales to aid conservation efforts. Autonomous recording units (ARUs) can provide cost-effective, long-term and systematic species monitoring data for sound-producing wildlife, including birds, amphibians, insects and mammals over large areas. Modern deep learning can efficiently automate the detection of species occurrences in these sound data with high accuracy. Further, citizen science can be leveraged to scale up the deployment of ARUs and collect reference vocalizations needed for training and validating deep learning models. In this study we develop a convolutional neural network (CNN) acoustic classification pipeline for detecting 54 bird species in Sonoma County, California USA, with sound and reference vocalization data collected by citizen scientists within the Soundscapes to Landscapes project (www.soundscapes2landscapes.org). We trained three ImageNet-based CNN architectures (MobileNetv2, ResNet50v2, ResNet100v2), which function as a Mixture of Experts (MoE), to evaluate the usefulness of several methods to enhance model accuracy. Specifically, we: 1) quantify accuracy with fully-labeled 1-min soundscapes for an assessment of real-world conditions; 2) assess the effect on precision and recall of additional pre-training with an external sound archive (xeno-canto) prior to fine-tuning with vocalization data from our study domain; and, 3) assess how detections and errors are influenced by the presence of coincident biotic and non-biotic sounds (i.e., soundscape components). In evaluating accuracy with soundscape data (n = 37 species) across CNN probability thresholds and models, we found acoustic pre-training followed by fine-tuning improved average precision by 10.3% relative to no pre-training, although there was a small average 0.8% reduction in recall. In selecting an optimal CNN architecture for each species based on maximum F(? = 0.5), we found our MoE approach had total precision of 84.5% and average species precision of 85.1%. Our data exhibit multiple issues arising from applying citizen science and acoustic monitoring at the county scale, including deployment of ARUs with relatively low fidelity and recordings with background noise and overlapping vocalizations. In particular, human noise was significantly associated with more incorrect species detections (false positives, decreased precision), while physical interference (e.g., recorder hit by a branch) and geophony (e.g., wind) was associated with the classifier missing detections (false negatives, decreased recall). Our process surmounted these obstacles, and our final predictions allowed us to demonstrate how deep learning applied to acoustic data from low-cost ARUs paired with citizen science can provide valuable bird diversity data for monitoring and conservation efforts. © 2023,Automated bioacoustic analysis aids understanding and protection of both marine and terrestrial animals and their habitats across extensive spatiotemporal scales, and typically involves analyzing vast collections of acoustic data. With the advent of deep learning models, classification of important signals from these datasets has markedly improved. These models power critical data analyses for research and decision-making in biodiversity monitoring, animal behaviour studies, and natural resource management. However, deep learning models are often data-hungry and require a significant amount of labeled training data to perform well. While sufficient training data is available for certain taxonomic groups (e.g., common bird species), many classes (such as rare and endangered species, many non-bird taxa, and call-type) lack enough data to train a robust model from scratch. This study investigates the utility of feature embeddings extracted from audio classification models to identify bioacoustic classes other than the ones these models were originally trained on. We evaluate models on diverse datasets, including different bird calls and dialect types, bat calls, marine mammals calls, and amphibians calls. The embeddings extracted from the models trained on bird vocalization data consistently allowed higher quality classification than the embeddings trained on general audio datasets. The results of this study indicate that high-quality feature embeddings from large-scale acoustic bird classifiers can be harnessed for few-shot transfer learning, enabling the learning of new classes from a limited quantity of training data. Our findings reveal the potential for efficient analyses of novel bioacoustic tasks, even in scenarios where available training data is limited to a few samples. © 2023, The Author(s).,Open audio databases such as Xeno-Canto are widely used to build datasets to explore bird song repertoire or to train models for automatic bird sound classification by deep learning algorithms. However, such databases suffer from the fact that bird sounds are weakly labelled: a species name is attributed to each audio recording without timestamps that provide the temporal localization of the bird song of interest. Manual annotations can solve this issue, but they are time consuming, expert-dependent, and cannot run on large datasets. Another solution consists in using a labelling function that automatically segments audio recordings before assigning a label to each segmented audio sample. Although labelling functions were introduced to expedite strong label assignment, their classification performance remains mostly unknown. To address this issue and reduce label noise (wrong label assignment) in large bird song datasets, we introduce a data-centric novel labelling function composed of three successive steps: 1) time-frequency sound unit segmentation, 2) feature computation for each sound unit, and 3) classification of each sound unit as bird song or noise with either an unsupervised DBSCAN algorithm or the supervised BirdNET neural network. The labelling function was optimized, validated, and tested on the songs of 44 West-Palearctic common bird species. We first showed that the segmentation of bird songs alone aggregated from 10% to 83% of label noise depending on the species. We also demonstrated that our labelling function was able to significantly reduce the initial label noise present in the dataset by up to a factor of three. Finally, we discuss different opportunities to design suitable labelling functions to build high-quality animal vocalizations with minimum expert annotation effort. © 2022 Elsevier B.V."
178,177,22,177_Machine Learning for Magnetic Torque Design and Optimization,Machine Learning for Magnetic Torque Design and Optimization,"A novel multiobjective optimization model is presented for the interior permanent magnet synchronous motors (IPMSMs). First of all, in the model initialization stage, appropriate design variables are determined for the actual topology. Also, with reference to the engineering needs, the key electromagnetic characteristics relating to the no-load and on-load magnetic fields are selected as the main optimization objectives in order to achieve global performance improvement. Next, in the model prediction stage, two performance prediction models are proposed, studied, and implemented in parallel. One is an analytical model (AM) based on the improved subdomain approach and the magnetic equivalent circuit. It is utilized to predict the no-load electromagnetic characteristics of IPMSM. The complex structure and core nonlinearity of IPMSM are also reasonably accounted for. The other is a surrogate model (SM) based on the intelligent machine learning language (support vector regression). It is utilized to predict the on-load electromagnetic characteristics of IPMSM. The reliance on finite-element analysis is further also minimized. The proposed AM and SM have commendable behavior in terms of analysis speed, prediction accuracy, and storage consumption. Meanwhile, AM screens credible samples as well as provides robust support for the construction of SM. All of these signs build a solid foundation for a substantial boost in optimization efficiency. Afterward, in the model optimization stage, the advanced nondominated sorting genetic algorithm III is investigated to complete the final multiobjective optimization. Ultimately, a large number of calculations, simulations, and experiments have highlighted the effectiveness, rationality, and engineering practicality of this research. IEEE,Purpose: Under the condition of small data set, a prediction model of motor magnetic field is established based on deep learning method. This paper aims to complete the magnetic field prediction quickly and accurately. Design/methodology/approach: An improved Linknet model is proposed to predict the motor magnetic field. This is a digital twin technology, which can predict the function values of other points according to the function values of typical sampling points. The results of magnetic field distribution are represented by color images. By predicting the pixels of the image, the corresponding magnetic field distribution is obtained. The model not only considers the correlation between pixels but also retains the spatial information in the original input image and can well learn the mapping relationship between motor structure and magnetic field. Findings: The model can speed up the calculation while ensuring the accuracy and has obvious advantages in large-scale calculation and real-time simulation. Originality/value: Under the condition of small data set, the model can well learn the mapping relationship between motor structure and magnetic field, so as to complete the magnetic field prediction quickly and accurately. In the future, according to the characteristics of magnetic field distribution, it will lay a foundation for solving the problems of rapid optimization, real-time simulation and physical field control of electrical equipment. © 2022, Emerald Publishing Limited.,A saturated iron-core type superconducting fault current limiter (SI-SFCL) can effectively restrict the magnitude of the fault current and alleviate the strain on circuit breakers in DC power systems. Design of a superconducting coil (SC), which is one of the key tasks in the SI-SFCL design, requires guaranteeing a sufficient magnetic field, ensuring optimization of the shape and size, minimizing the wire cost, and satisfying the safety and stability of operation. Generally, finite element method (FEM) is used to calculate and evaluate the operating characteristics of SCs, from which it is possible to determine their optimal design parameters. When the coil is complex and large, the simulation time may range from hours to days, and if input parameters change even slightly, the simulations have to be redone from scratch. Recent advances in deep learning represent the ability to be effective for modeling and optimizing complex problems from training data or in real-Time. In this paper, we presented a combination of the FEM simulation and deep Q-network (DQN) algorithm to optimize the SC design of a lab-scale SI-SFCL for a DC power system. The detailed design process and options for the SC of SI-SFCL were proposed. In order to analyze the characteristics related to the electromagnetic properties and operational features of the SC, a 3D FEM model was developed. Then, a DQN model was constructed and integrated with the FEM simulation for training and optimizing the design parameters of the SC in real-Time. The obtained results of this study have the potential to effectively optimize the design parameters of large-scale SI-SFCL development for high-voltage DC power systems.  © 2023 Kim et al."
179,178,22,178_Neural Machine Translation (NMT) and Data Evaluation for Low-Resource Languages,Neural Machine Translation (NMT) and Data Evaluation for Low-Resource Languages,"With more and more active international connections, the complex scenes-aware machine translation has been a novel concern in the area of natural language processing. Although various machine translation methods have been proposed during the past few years, automatic and intelligent quality detection for translation results failed to receive sufficient attention. Actually, the real-time quality evaluation for machine translation results remains important, because it can facilitate constant debugging and optimization of machine translation products. Existing approaches mostly focused on the offline written contents rather than real-time extensive oral contents. To bridge current gap, a sentence-level machine translation quality estimation method is deployed in this paper. In particular, a specifical recurrent neural network with double directions (Double-RNN) is proposed as the backbone network structure. The feature extraction process utilizes the Double-RNN translation model, which makes full use of a large amount of parallel corpus. The evaluations show that Double-RNN method proposed in this paper is the closest to the standard quality assessment, and thus can also evaluate the quality of Chinese and English translations more fairly.  © 2013 IEEE.,Neural Machine Translation (NMT) brings promising improvements in translation quality, but until recently, these models rely on large-scale parallel corpora. As such corpora only exist on a handful of language pairs, the translation performance is far from the desired effect in the majority of low-resource languages. Thus, developing low-resource language translation techniques is crucial and it has become a popular research field in neural machine translation. In this article, we make an overall review of existing deep learning techniques in low-resource NMT. We first show the research status as well as some widely used low-resource datasets. Then, we categorize the existing methods and show some representative works detailedly. Finally, we summarize the common characters among them and outline the future directions in this field.  © 2022 Association for Computing Machinery.,The development of the machine translation field was driven by people’s need to communicate with each other globally by automatically translating words, sentences, and texts from one language into another. The neural machine translation approach has become one of the most significant in recent years. This approach requires large parallel corpora not available for low-resource languages, such as the Kazakh language, which makes it difficult to achieve the high performance of the neural machine translation models. This article explores the existing methods for dealing with low-resource languages by artificially increasing the size of the corpora and improving the performance of the Kazakh–English machine translation models. These methods are called forward translation, backward translation, and transfer learning. Then the Sequence-to-Sequence (recurrent neural network and bidirectional recurrent neural network) and Transformer neural machine translation architectures with their features and specifications are concerned for conducting experiments in training models on parallel corpora. The experimental part focuses on building translation models for the high-quality translation of formal social, political, and scientific texts with the synthetic parallel sentences from existing monolingual data in the Kazakh language using the forward translation approach and combining them with the parallel corpora parsed from the official government websites. The total corpora of 380,000 parallel Kazakh–English sentences are trained on the recurrent neural network, bidirectional recurrent neural network, and Transformer models of the OpenNMT framework. The quality of the trained model is evaluated with the BLEU, WER, and TER metrics. Moreover, the sample translations were also analyzed. The RNN and BRNN models showed a more precise translation than the Transformer model. The Byte-Pair Encoding tokenization technique showed better metrics scores and translation than the word tokenization technique. The Bidirectional recurrent neural network with the Byte-Pair Encoding technique showed the best performance with 0.49 BLEU, 0.51 WER, and 0.45 TER. © Copyright 2023 Karyukin et al."
180,179,22,179_Parkinson's Disease Diagnosis and Assessment with Machine Learning,Parkinson's Disease Diagnosis and Assessment with Machine Learning,"Parkinson's disease (PD) is a common neurodegenerative movement disorder among older individuals. As one of the typical symptoms of PD, tremor is a critical reference in the PD assessment. A widely accepted clinical approach to assessing tremors in PD is based on part III of the Movement Disorder Society-Unified Parkinson's Disease Rating Scale (MDS-UPDRS). However, expert assessment of tremor is a time-consuming and laborious process that poses considerable challenges to the medical evaluation of PD. In this paper, we proposed a novel model, Global Temporal-difference Shift Network (GTSN), to estimate the MDS-UPDRS score of PD tremors based on video. The PD tremor videos were scored according to the majority vote of multiple raters. We used Eulerian Video Magnification (EVM) pre-processing to enhance the representations of subtle PD tremors in the videos. To make the model better focus on the tremors in the video, we proposed a special temporal difference module, which stacks the current optical flow to the result of inter-frame difference. The prediction scores were obtained from the Residual Networks (ResNet) embedded with a novel module, the Global Shift Module (GSM), which allowed the features of the current segment to include the global segment features. We carried out independent experiments using PD tremor videos of different body parts based on the scoring content of the MDS-UPDRS. On a fairly large dataset, our method achieved an accuracy of 90.6% for hands with rest tremors, 85.9% for tremors in the leg, and 89.0% for the jaw. An accuracy of 84.9% was obtained for postural tremors. Our study demonstrated the effectiveness of computer-assisted assessment for PD tremors based on video analysis. The latest version of the code is available at https://github.com/199507284711/PD-GTSN. © 2023 Elsevier B.V.,Background: Since both essential tremor (ET) and Parkinson’s disease (PD) are movement disorders and share similar clinical symptoms, it is very difficult to recognize the differences in the presentation, course, and treatment of ET and PD, which leads to misdiagnosed commonly. Purpose: Although neuroimaging biomarker of ET and PD has been investigated based on statistical analysis, it is unable to assist the clinical diagnosis of ET and PD and ensure the efficiency of these biomarkers. The aim of the study was to identify the neuroimaging biomarkers of ET and PD based on structural magnetic resonance imaging (MRI). Moreover, the study also distinguished ET from PD via these biomarkers to validate their classification performance. Methods: This study has developed and implemented a three-level machine learning framework to identify and distinguish ET and PD. First of all, at the model-level assessment, the searchlight-based machine learning method has been used to identify the group differences of patients (ET/PD) with normal controls (NCs). And then, at the feature-level assessment, the stability of group differences has been tested based on structural brain atlas separately using the permutation test to identify the robust neuroimaging biomarkers. Furthermore, the identified biomarkers of ET and PD have been applied to classify ET from PD based on machine learning techniques. Finally, the identified biomarkers have been compared with the previous findings of the biology-level assessment. Results: According to the biomarkers identified by machine learning, this study has found widespread alterations of gray matter (GM) for ET and large overlap between ET and PD and achieved superior classification performance (PCA + SVM, accuracy = 100%). Conclusions: This study has demonstrated the significance of a machine learning framework to identify and distinguish ET and PD. Future studies using a large data set are needed to confirm the potential clinical application of machine learning techniques to discern between PD and ET. © 2022, The Author(s).,Although many studies have been conducted on machine learning (ML) models for Parkinson’s disease (PD) prediction using neuroimaging and movement analyses, studies with large population-based datasets are limited. We aimed to propose PD prediction models using ML algorithms based on the National Health Insurance Service-Health Screening datasets. We selected individuals who participated in national health-screening programs > 5 times between 2002 and 2015. PD was defined based on the ICD-code (G20), and a matched cohort of individuals without PD was selected using a 1:1 random sampling method. Various ML algorithms were applied for PD prediction, and the performance of the prediction models was compared. Neural networks, gradient boosting machines, and random forest algorithms exhibited the best average prediction accuracy (average area under the receiver operating characteristic curve (AUC): 0.779, 0.766, and 0.731, respectively) among the algorithms validated in this study. The overall model performance metrics were higher in men than in women (AUC: 0.742 and 0.729, respectively). The most important factor for predicting PD occurrence was body mass index, followed by total cholesterol, glucose, hemoglobin, and blood pressure levels. Smoking and alcohol consumption (in men) and socioeconomic status, physical activity, and diabetes mellitus (in women) were highly correlated with the occurrence of PD. The proposed health-screening dataset-based PD prediction model using ML algorithms is readily applicable, produces validated results, and could be a useful option for PD prediction models. © 2022, The Author(s)."
181,180,22,180_Hardware Design and Electronic Design Automation (EDA),Hardware Design and Electronic Design Automation (EDA),"Support Vector Machines (SVM) are widely used techniques in the field of classification problems because of their ability to effectively deal with datasets that have complex non-linear structures and a high dimensionality. The compute-intensive training algorithm associated with SVM makes it challenging to keep an up-to-date model that accurately reflects the characteristics of newly arriving data points in real-time systems. This paper proposes a novel training algorithm for incremental learning from large datasets, based on a variant of Sequential Minimal Optimization (SMO). High-Level Synthesis (HLS) was used for implementing the Field Programmable Gate Array (FPGA) based Intellectual Property (IP) Core, which includes the computationally intensive kernel computation portion of the training algorithm. In addition to the kernel computation, the inference phase of the SVM classifier is built into the IP core, and its use can be switched on the fly. The computational latency and memory bandwidth of an IP core are optimized using loop pipelining and DMA burst data transfer. With the help of hardware/software co-design, the IP core is integrated into the design of a flexible and re-usable System on Chip (SoC) called PYNQ Overlay. The experiments show that the overlay outperforms the embedded processor, multiple hardware SVM classifiers, and hardware accelerated Convolutional Neural Networks (CNN) in terms of real-time efficiency. The Overlay makes much less use of the resources available on the chip in comparison to the majority of the CNN accelerators. The overlay achieves an average classification accuracy that is only 1% lower than that of an ARM Cortex-A9 processor, according to experimental results on six datasets. Furthermore, it can increase training speed by an average of 31.82x and inference speed by an average of 31.74x. In addition, the proposed Overlay design achieves a 2.3x improvement in average training speed, as measured in Mega bits per second, compared to existing SVM training implementations, along with incremental learning and multi-class classification support. © 2023 Elsevier B.V.,Field-programmable gate arrays (FPGAs) have grown to be an important platform for integrated circuit design and hardware emulation. However, with the dramatic increase in design scale, it has become a key challenge to partition very large scale integration into multi-FPGA systems. Fast estimation of FPGA on-chip resource usage for individual sub-circuit blocks early in the circuit design flow will provide an essential basis for reasonable circuit partition. It will also help FPGA designers to tune the circuits in hardware description language. In this article, we propose a framework for fast estimation of the on-chip resources consumed by register transfer level (RTL) designs with machine learning methods. We extensively collect RTL designs as a dataset, extract features from the result of a parser tool and analyze their roles, and train a targeted three-stage ensemble learning model. A 5,513× speedup is achieved while having 27% relative absolute error. Although the effect is sufficient to support RTL circuit partition, we discuss how the estimation quality continues to be improved.  © 2022 Association for Computing Machinery.,With the recent advances in hardware technologies like advanced CPUs and GPUs and the large availability of open-source libraries, machine learning has penetrated various domains, including Electronics Design Automation (EDA). EDA consists of multiple stages, from high-level synthesis and logic synthesis to placement and routing. Traditionally, estimating resources and areas from one level of design abstraction to the next level uses mathematical, statistical, and analytical approaches. However, as the technology node decreases and the number of cells inside the chip increases, the traditional estimation methods fail to correlate with the actual post-route values. Machine-learning (ML) based methodologies pave a strong path towards accurately estimating post-route values. In this paper, we present a comprehensive survey of the existing literature in the ML application field in EDA, emphasizing FPGA design automation tools. We discuss how ML is applied in different stages to predict congestion, power, performance, and area (PPA), both for High-Level Synthesis (HLS) and Register Transfer Level (RTL)-based FPGA designs, application of design space exploration and application in Computer-Aided Design (CAD) tool parameter settings to optimize timing and area requirements. Reinforcement learning is widely applied in both FPGA and ASIC physical design flow, a topic of discussion in this paper. We also discuss various ML models like classical regression and classification ML, convolution neural networks, reinforcement learning, and graph convolution network and their application in EDA.  © 2013 IEEE."
182,181,22,181_Active Learning for Efficient Labeling,Active Learning for Efficient Labeling,"Active learning is usually used in scenarios where few labels are available and manual labeling is expensive. To improve model performance, it is necessary to find the most valuable instance among all instances and label it to maximize the benefits of labeling. In practical scenarios, it is often more efficient to query a group of instances instead of a individual instance during each iteration. To achieve this goal, we need to explore the similarities between instances to ensure the informativeness and diversity. Many ad-hoc algorithms are proposed for batch mode active learning, and there are generally two major issues. One is that similarity measurement among instances often only relies on the expression of features but it is not well integrated with the classification algorithm model. This will cut down the precise measurement of diversity. The other is that in order to explore the decision boundary, these algorithms often choose the instance near the boundary. It is difficult to get the true boundary when there are few labeled instances. As a large number of instances continue to be labeled, information between instances is less used, and the performance will be greatly improved if it is properly used. In our work, we propose an adaptive algorithm based on deep neural networks to solve the two problems mentioned above. During the training phase, we established a paired network to improve the accuracy of the classification model, and the network can project the instance to a new feature space for more accurate similarity measurement. When batch labeling instances, we use the adaptive algorithm to select the instance by balancing the maximum uncertainty (exploration) and diversity (exploitation). Our algorithm has been validated for heart failure prediction tasks in real-world EHR datasets. Due to the no public of EHR data, we also conducted validation on two other classic classification tasks. Our algorithm is superior to the baseline method in both accuracy and convergence rate. © 2023,Active learning is an important technology to solve the lack of data in crack detection model training. However, the sampling strategies of most existing active learning methods for crack detection are based on the uncertainty or representation of the samples, which cannot effectively balance the exploitation and exploration of active learning. To solve this problem, this study proposes an active learning method for crack detection based on subset searching and weighted sampling. First, a new active learning framework is established to successively search subsets with large uncertainty from the candidate dataset, and select training samples with large diversity from the subsets to update the crack detection model. Second, to realize the active learning process, a subset searching method based on sample relative error is proposed to adaptively select subsets with large uncertainty, and a weighted sampling method based on flow-based deep generative network is introduced to select training samples with large diversity form the subsets. Third, a termination criterion for active learning directly based on the prediction accuracy of the trained model is proposed to adaptively determine the maximum number of iterations. Finally, the proposed method is tested using two open-source crack datasets. The experimental comparison results on the Bridge Crack Library dataset show that the proposed method has higher calculation efficiency and prediction accuracy in crack detection than the uncertainty-based and representation-based active learning methods. The test results on the DeepCrack dataset show that the crack detection model trained by the proposed method has good transferability on different datasets with multi-scale concrete cracks and scenes. © The Author(s) 2023.,Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. However, the growing availability of data streams has led to an increase in the number of approaches that focus on online active learning, which involves continuously selecting and labeling observations as they arrive in a stream. This work aims to provide an overview of the most recently proposed approaches for selecting the most informative observations from data streams in real time. We review the various techniques that have been proposed and discuss their strengths and limitations, as well as the challenges and opportunities that exist in this area of research. © 2023, The Author(s)."
183,182,22,182_Chemical language models and graph neural networks for drug discovery and molecular property prediction.,Chemical language models and graph neural networks for drug discovery and molecular property prediction.,"Although substantial efforts have been made using graph neural networks (GNNs) for artificial intelligence (AI)-driven drug discovery, effective molecular representation learning remains an open challenge, especially in the case of insufficient labeled molecules. Recent studies suggest that big GNN models pre-trained by self-supervised learning on unlabeled datasets enable better transfer performance in downstream molecular property prediction tasks. However, the approaches in these studies require multiple complex self-supervised tasks and large-scale datasets, which are time-consuming, computationally expensive and difficult to pre-train end-to-end. Here, we design a simple yet effective self-supervised strategy to simultaneously learn local and global information about molecules, and further propose a novel bi-branch masked graph transformer autoencoder (BatmanNet) to learn molecular representations. BatmanNet features two tailored complementary and asymmetric graph autoencoders to reconstruct the missing nodes and edges, respectively, from a masked molecular graph. With this design, BatmanNet can effectively capture the underlying structure and semantic information of molecules, thus improving the performance of molecular representation. BatmanNet achieves state-of-the-art results for multiple drug discovery tasks, including molecular properties prediction, drug–drug interaction and drug–target interaction, on 13 benchmark datasets, demonstrating its great potential and superiority in molecular representation learning. © The Author(s) 2023. Published by Oxford University Press. All rights reserved.,Models based on machine learning can enable accurate and fast molecular property predictions, which is of interest in drug discovery and material design. Various supervised machine learning models have demonstrated promising performance, but the vast chemical space and the limited availability of property labels make supervised learning challenging. Recently, unsupervised transformer-based language models pretrained on a large unlabelled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, we present molecular embeddings obtained by training an efficient transformer encoder model, MoLFormer, which uses rotary positional embeddings. This model employs a linear attention mechanism, coupled with highly distributed training, on SMILES sequences of 1.1 billion unlabelled molecules from the PubChem and ZINC datasets. We show that the learned molecular representation outperforms existing baselines, including supervised and self-supervised graph neural networks and language models, on several downstream tasks from ten benchmark datasets. They perform competitively on two others. Further analyses, specifically through the lens of attention, demonstrate that MoLFormer trained on chemical SMILES indeed learns the spatial relationships between atoms within a molecule. These results provide encouraging evidence that large-scale molecular language models can capture sufficient chemical and structural information to predict various distinct molecular properties, including quantum-chemical properties. © 2022, The Author(s), under exclusive licence to Springer Nature Limited.,Molecular property prediction is an essential task in drug discovery. Recently, deep neural networks have accelerated the discovery of compounds with improved molecular profiles for effective drug development. In particular, graph neural networks (GNNs) have played a pivotal role in identifying promising drug candidates with desirable molecular properties. However, it is common for only a few molecules to share the same set of properties, which presents a low-data problem unanswered by regular machine learning (ML) approaches. Transformer networks have also emerged as a promising solution to model the long-range dependence in molecular embeddings and achieve encouraging results across a wide range of molecular property prediction tasks. Nonetheless, these methods still require a large number of data points per task to achieve acceptable performance. In this study, we propose a few-shot GNN-Transformer architecture, FS-GNNTR to face the challenge of low-data in molecular property prediction. The proposed model accepts molecules in the form of molecular graphs to model the local spatial context of molecular graph embeddings while preserving the global information of deep representations. Furthermore, we introduce a two-module meta-learning framework to iteratively update model parameters across few-shot tasks and predict new molecular properties with limited available data. Finally, we conduct multiple experiments on small-sized biological datasets for molecular property prediction, Tox21 and SIDER, and our results demonstrate the superior performance of FS-GNNTR compared to simpler graph-based baselines. The code and data underlying this article are available in the repository, https://github.com/ltorres97/FS-GNNTR. © 2023 The Author(s)"
184,183,22,183_Geochemical Mineral Prospectivity Mapping,Geochemical Mineral Prospectivity Mapping,"Although mineral prospectivity modeling (MPM) has undergone decades of development, it has not yet been widely adopted in the global mineral exploration industry. Exploration geoscientists encounter challenges in understanding the internal working of many mineral prospectivity models due to their black box nature. Besides, their predictive results usually delineate undesirably large high-prospectivity areas, which are biased toward existing deposits, making MPM impractical. However, there are only a few data-driven methods for MPM that address both the interpretability of black box models and the issue of bias in high prospective areas, which may result from the intrinsic stochastic uncertainty of training samples, particularly toward well-known deposits. In this study, we construct and demonstrate a framework to improve the performance and reliability of data-driven MPM in the Qulong–Jiama mineral district of Tibet. Firstly, the mineral systems concept was applied to select appropriate targeting criteria and to derive corresponding evidential features. Secondly, model-agnostic methods, such as permutation feature importance, partial dependence plot, individual conditional expectation plot, and Shapely values, were applied to interpret the machine learning models. Finally, modulated prediction models and the spatial pattern of linked uncertainties were generated by an ensemble method that combines bootstrapping and the Random Forest algorithm. The final exploration targets, which were demarcated by cells with high modulated values and low uncertainties obtained by 50 predictive models, account for just ~ 3% of the study area. © 2023, International Association for Mathematical Geosciences.,Several large-scale porphyry copper deposits (PCDs) with high economic value have been excavated in the Duolong ore district, Tibet, China. However, the high altitudes and harsh conditions in this area make traditional exploration difficult. Hydrothermal alteration minerals related to PCDs with diagnostic spectral absorption features in the visible–near-infrared–shortwave-infrared ranges can be effectively identified by remote sensing imagery. Mainly based on hyperspectral imagery supplemented by multispectral imagery and geochemical element data, the Duolong ore district was selected to conduct data-driven PCD prospectivity modelling. A total of 11 known deposits and 17 evidential layers of multisource geoscience information related to Cu mineralization constitute the input datasets of the predictive models. A deep learning convolutional neural network (CNN) model was applied to mineral prospectivity mapping, and its applicability was tested by comparison to conventional machine learning models, such as support vector machine and random forest. CNN achieves the greatest classification performance with an accuracy of 0.956. This is the first trial in Duolong to conduct mineral prospectivity mapping combined with remote imagery and geochemistry based on deep learning methods. Four metallogenic prospective sites were delineated and verified through field reconnaissance, indicating that the application of deep learning-based methods in PCD prospecting proposed in this paper is feasible by utilizing geoscience big data such as remote sensing datasets and geochemical elements. © 2023 by the authors.,A mineral prospectivity map (MPM) focusing on gold mineralization in the Larder Lake region of Northern Ontario, Canada, has been produced in this study. We have used the Random Forest (RF) algorithm to use 32 predictor maps integrating geophysical, geochemical, and geological datasets from various sources that represent vectors to gold mineralization. It is evident from the efficiency of classification curves that MPMs generated are robust. The unsupervised algorithms, K-means and principal component analysis (PCA) were used to investigate and visualize the clustering nature of large geochemical and geophysical datasets. We used RQ-mode PCA to compute variable and object loadings simultaneously, which allows the displays of observations and the variables at the same scale. PCA biplots of the Larder Lake geochemical data show that Au is strongly correlated with W, S, Pb and K, but inversely correlated with Fe, Mn, Co, Mg, Ca, and Ni. The known gold mineralization locations were well classified by RF with the accuracy of 95.63 %. Furthermore, partial least squares-discriminant analysis (PLS-DA) model combines 3D geophysical clusters and geochemical compositions, which indicates the Au-rich areas are characterized with low to mid resistivity – low susceptibility properties. We conclude that the Larder Lake-Cadillac deformation zone (LLCDZ) is relatively more fertile than the Lincoln-Nipissing shear zone (LNSZ) with respect to gold mineralization due to deeper penetrating faults. The intersection of the LLCDZ and network of high-angle NE-trending cross faults acts as key conduits for gold endowments in the Larder Lake area. This study innovatively combined multivariate geological, geochemical, and geophysical datasets via machine learning algorithms, which improves identification of geochemical anomalies and interpretation of spatial features associated with gold mineralization. © 2023 Elsevier B.V."
185,184,22,184_Energy Management in Renewable Power Systems,Energy Management in Renewable Power Systems,"Future electric grids will face severe uncertainties with the unprecedented penetration of renewable energy sources, which may cause problems in grid exploitation. It is essential to evaluate the uncertainty of system performance in this grid; therefore, traditional exploitation methods may not be suitable for distributions grid such as multi-carrier microgrids when considering the electricity and gas grid constraints. This paper proposes an effective method for simultaneously optimizing different types of energy infrastructure in an environment with various uncertainties while considering grid constraints. To address the multi-energy synergy supply optimization problem in the micro energy grid, a coordinated two-stage programming approach is also suggested. A day-ahead and real-time phases are separated in the scheduling cycle to overcome the effects of the unpredictability of wind energy and solar power. The findings of the day-ahead forecast are then taken into account as uncertain variables for the higher-layer model. The demand response programming model and the energy storage revision model are lower-layer models considering the actual value as the realization of uncertain variables. The competitive swarm optimization algorithm is used to solve the problem mentioned above The Cumulative search optimization (CSO) uses global and local systems in particle swarm optimization and adopts a novel learning mechanism for creating competition between particle pairs. Comprehensive numeric examinations show that convergence speed and precision are critical, especially in solving large-scale problems. Therefore, we use chaos theory to improve its performance. Finally, the proposed method is tested and discussed on a system. The results showed that: (1) power-to-gas could convert extra electricity into natural gas; (2) the proposed two-stage optimization model and algorithm achieved different energy optimizations; (3) price-based demand response (DR) can level the energy load curve and maximize multi energy grid (MEG) revenue by using complementary specifications of energy price; and (4) the micro energy grid could communicate with the high-degree energy grid in order to gain more economic benefits. Accordingly, the incentive-based demand response (IBDR) output is reduced. The electricity and heating provided by convention gas turbine (CGT) is increased, which is fed to upper power grid (UPG) and upper heating grid (UHG). Since MEG's purchasing power from UPG during the peak period generates more total revenue than island operations. Based on the obtained results, it can be seen that the proposed algorithm was about 50% faster compared to other methods and its standard deviation was about 0.0013 with different implementations, which was much better compared to other models. © 2023,Battery storage is an important factor for power systems made up of renewable energy sources. Technologies for battery storage are crucial to accelerating the transition from fossil fuels to renewable energy. Between responding to electricity demand and using renewable energy sources, battery storage devices will become increasingly important. The aim of this study is to examine how battery storage affects a power system consisting of solar and hydroelectric energy and to draw conclusions about whether energy storage recommends a power system. The method involves designing a model of eight real cascade hydropower power plants and solving an optimization problem. This power system model is based on existing hydroelectric power plants powered by solar energy and batteries in the Turkish cities of Yozgat and Tokat. A case study with four different battery capacities in the system was carried out to assess the implications of energy storage in the power system. The stochastic nonlinear optimization problem was modeled for 72 h and solved with the MATLAB programming tool. The stochastic Quasi-Newton method performs very well in hybrid renewable problems arising from large-scale machine learning. When solar energy and batteries were added to the system, the maximum installed wind power was found to be 2 MW and 3.6 MW, respectively. In terms of profit and hydropower planning, a medium-proportion battery was found to be the most suitable. Increased variability in hydropower generation results from the installation of an energy storage system. © 2023 by the author.,Using renewable energies such as wind and solar energy, two types of renewable energy, we may adjust the structure of the energy system, addressing both energy and environmental challenges at the same time. As a result of its impact on the environment, wind and solar energy generation are inherently unreliable sources of energy. Lithium batteries, a technology that is becoming increasingly mature in terms of energy storage, are a critical component of the answer to the problem of instability. In order to avoid waste and expense increases, the capacity should not be too large or too small, respectively. Power consumption restricts the amount of energy that may be stored, but industrial power usage is unpredictable and non-periodic. This is a significant task that needs the development of a model that can dispatch while still providing a reasonable amount of storage. In this paper, we develop a KNN classification model that considers the test cyclic of photovoltaic (PV) generation that includes battery installation, data on electricity consumption and data on PV generation in India. These metrics are used to develop an energy management model. The model aims at the reduction of operation cost and optimal storage of energy that should satisfy the grid demands. The results of simulation and the comparison of the theoretical results shows that the proposed model has higher optimisation of energy in the storage devices in case of distributed systems. © 2022"
186,185,22,185_Phishing websites and spam emails detection,Phishing websites and spam emails detection,"Phishing is a persistent and major threat on the internet that is growing steadily and dangerously. It is a type of cyber-attack, in which phisher mimics a legitimate website page to harvest victim’s sensitive information, such as usernames, emails, passwords and bank or credit card details. To prevent such attacks, several phishing detection techniques have been proposed such as AI based, 3rd party, heuristic and content based. However, these approaches suffer from a number of limitations that needs to be addressed in order to detect phishing URLs. Firstly, features extracted in the past are extensive, with a limitation that it takes a considerable amount of time to extract such features. Secondly, several approaches selected important features using statistical methods, while some propose their own features. Although both methods have been implemented successfully in various approaches, however, these methods produce incorrect results without amplification of domain knowledge. Thirdly, most of the literature has used pre-classified and smaller datasets, which fail to produce exact efficiency and precision on large and real world datasets. Fourthly, the previous proposed approaches lack in advanced evaluation measures. Hence, in this paper, effective machine learning framework is proposed, which predicts phishing URLs without visiting the webpage nor utilizing any 3rd party services. The proposed technique is based on URL and uses full URL, protocol scheme, hostname, path area of the URL, entropy feature, suspicious words and brand name matching using TF-IDF technique for the classification of phishing URLs. The experiments are carried out on six different datasets using eight different machine learning classifiers, in which Random Forest achieved a significant higher accuracy than other classifiers on all the datasets. The proposed framework with only 30 features achieved a higher accuracy of 96.25% and 94.65% on the Kaggle datasets. The comparative results show that the proposed model achieved an accuracy of 92.2%, 91.63%, 94.80, 96.85% on benchmark datasets, which is higher than the existing approaches. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Phishing attacks are one of the most challenging social engineering cyberattacks due to the large amount of entities involved in online transactions and services. In these attacks, criminals deceive users to hijack their credentials or sensitive data through a login form which replicates the original website and submits the data to a malicious server. Many anti-phishing techniques have been developed in recent years, using different resource such as the URL and HTML code from legitimate index websites and phishing ones. These techniques have some limitations when predicting legitimate login websites, since, usually, no login forms are present in the legitimate class used for training the proposed model. Hence, in this work we present a methodology for phishing website detection in real scenarios, which uses URL, HTML, and web technology features. Since there is not any updated and multipurpose dataset for this task, we crafted the Phishing Index Login Websites Dataset (PILWD), an offline phishing dataset composed of 134,000 verified samples, that offers to researchers a wide variety of data to test and compare their approaches. Since approximately three-quarters of collected phishing samples request the introduction of credentials, we decided to crawl legitimate login websites to match the phishing standpoint. The developed approach is independent of third party services and the method relies on a new set of features used for the very first time in this problem, some of them extracted from the web technologies used by the on each specific website. Experimental results show that phishing websites can be detected with 97.95% accuracy using a LightGBM classifier and the complete set of the 54 features selected, when it was evaluated on PILWD dataset. © 2022,Phishing attacks are evolving with more sophisticated techniques, posing significant threats. Considering the potential of machine-learning-based approaches, our research presents a similar modern approach for web phishing detection by applying powerful machine learning algorithms. An efficient layered classification model is proposed to detect websites based on their URL structure, text, and image features. Previously, similar studies have used machine learning techniques for URL features with a limited dataset. In our research, we have used a large dataset of 20,000 website URLs, and 22 salient features from each URL are extracted to prepare a comprehensive dataset. Along with this, another dataset containing website text is also prepared for NLP-based text evaluation. It is seen that many phishing websites contain text as images, and to handle this, the text from images is extracted to classify it as spam or legitimate. The experimental evaluation demonstrated efficient and accurate phishing detection. Our layered classification model uses support vector machine (SVM), XGBoost, random forest, multilayer perceptron, linear regression, decision tree, naïve Bayes, and SVC algorithms. The performance evaluation revealed that the XGBoost algorithm outperformed other applied models with maximum accuracy and precision of 94% in the training phase and 91% in the testing phase. Multilayer perceptron also worked well with an accuracy of 91% in the testing phase. The accuracy results for random forest and decision tree were 91% and 90%, respectively. Logistic regression and SVM algorithms were used in the text-based classification, and the accuracy was found to be 87% and 88%, respectively. With these precision values, the models classified phishing and legitimate websites very well, based on URL, text, and image features. This research contributes to early detection of sophisticated phishing attacks, enhancing internet user security. © 2023 by the authors."
187,186,21,186_Soil Organic Carbon (SOC) Mapping and Prediction,Soil Organic Carbon (SOC) Mapping and Prediction,"Soil organic carbon (SOC) in Arctic coastal polygonal tundra is vulnerable to climate change, especially in soils with occurrence of large amounts of ground ice. Pan-arctic studies of mapping SOC exist, yet they fail to describe the high spatial variability of SOC storage in permafrost landscapes. An important factor is the landscape history which determines landform development and consequently the spatial variability of SOC. Our aim was to map SOC stocks, and which environmental variables that determine SOC, in two adjacent coastal areas along Canadian Beaufort Sea coast with different glacial history. We used the machine learning technique random forest and environmental variables to map the spatial distribution of SOC stocks down to 1 m depth at a spatial resolution of 2 m for depth increments of 0–5, 5–15, 15–30, 30–60 and 60–100 cm. The results show that the two study areas had large differences in SOC stocks in the depth 60–100 cm due to high amounts of ground ice in one of the study areas. There are also differences in variable importance of the explanatory variables between the two areas. The area low in ground ice content had with 66.6 kg C/m?2 more stored SOC than the area rich in ground ice content with 40.0 kg C/m?2. However, this SOC stock could be potentially more vulnerable to climate change if ground ice melts and the ground subsides. The average N stock of the area low in ground ice is 3.77 kg m?2 and of the area rich in ground ice is 3.83 kg m?2. These findings support that there is a strong correlation between ground ice and SOC, with less SOC in ice-rich layers on a small scale. In addition to small scale studies of SOC mapping, detailed maps of ground ice content and distribution are needed for a validation of large-scale quantifications of SOC stocks and transferability of models. © 2023 The Author(s),Insights into the controlling factors of soil organic carbon (SOC) stock variation are necessary both for our scientific understanding of the terrestrial carbon balance and to support policies that intend to promote carbon storage in soils to mitigate climate change. In recent years, complex statistical and algorithmic tools from the field of machine learning have become popular for modelling and mapping SOC stocks over large areas. In this paper, we report on the development of a statistical method for interpreting complex models, which we implemented for the study of SOC stock variation. We fitted a random forest machine learning model with 2206 measurements of SOC stocks for the 0-50 cm depth interval from mainland France and used a set of environmental covariates as explanatory variables. We introduce Shapley values, a method from coalitional game theory, and use them to understand how environmental factors influence SOC stock prediction: what is the functional form of the association in the model between SOC stocks and environmental covariates, and how does the covariate importance vary locally from one location to another and between carbon-landscape zones? Results were validated both in light of the existing and well-described soil processes mediating soil carbon storage and with regards to previous studies in the same area. We found that vegetation and topography were overall the most important drivers of SOC stock variation in mainland France but that the set of most important covariates varied greatly among locations and carbon-landscape zones. In two spatial locations with equivalent SOC stocks, there was nearly an opposite pattern in the individual covariate contribution that yielded the prediction - in one case climate variables contributed positively, whereas in the second case climate variables contributed negatively - and this effect was mitigated by land use. We demonstrate that Shapley values are a methodological development that yield useful insights into the importance of factors controlling SOC stock variation in space. This may provide valuable information to understand whether complex empirical models are predicting a property of interest for the right reasons and to formulate hypotheses on the mechanisms driving the carbon sequestration potential of a soil.  © 2023 Alexandre M. J.-C. Wadoux et al.,Calculating the amount of soil organic carbon (SOC) stored in coastal environments, including salt marshes, is needed to determine their role in mitigating the Climate Crisis. Several techniques exist to calculate the SOC content of a unit of land from the upscaling of soil cores. However, no comprehensive assessment has been made on the performance of commonly used SOC upscaling techniques until now. We measured the SOC content of soil cores gathered from two Scottish salt marshes. Two SOC values were used for upscaling; SOC content for a 1 m standardised depth (as recommended by the IPCC), and SOC content of the modern marsh deposit (identified in the stratigraphy as a transition from organic-rich (marsh) to mineral-rich (intertidal flat) soil. Twenty-two upscaling techniques were used (SOC content × area, interpolative, and regression-based extrapolative calculations). Leave-one-out cross-validation procedures and prediction interval widths were used to assess the accuracy of each technique. Digital Terrain Models and Normalized Difference Vegetation Indices were used as covariate surfaces in the regression models. We found that marsh-scale SOC stocks varied by as much as fifty-two times depending on which sampling depth and upscaling technique was used. The largest differences emerged when comparing SOC stocks upscaled from 1 m deep and modern marsh deposits. Using the IPCC recommended 1 m sampling depth inflated the SOC stocks of salt marshes, as intertidal flat environments were included in the calculation. Ensemble regression models from the weighted average of seven machine learning algorithm outputs produced the highest upscaling accuracies across marshes and sampling depths. Simple SOC content × area calculations produced marsh-scale SOC stocks that were comparable to stock values produced by more advanced ensemble regression models. However, regression models produced detailed maps of SOC distribution across a marsh, and the associated uncertainty in the SOC values. Our findings are broadly applicable for other environments where large-scale SOC stock assessments and uncertainty are needed. © 2022"
188,187,21,187_Prediction models for suicide risk and substance abuse among adolescents and adults using machine learning techniques,Prediction models for suicide risk and substance abuse among adolescents and adults using machine learning techniques,"Suicide is a major global health concern and a prominent cause of death in adolescents. Previous research on suicide prediction has mainly focused on clinical or adult samples. To prevent suicides at an early stage, however, it is important to screen for risk factors in a community sample of adolescents. We compared the accuracy of logistic regressions, elastic net regressions, and gradient boosting machines in predicting suicide attempts by 17-year-olds in the Millennium Cohort Study (N = 7,347), combining a large set of self- and other-reported variables from different categories. Both machine learning algorithms outperformed logistic regressions and achieved similar balanced accuracies (.76 when using data 3 years before the self-reported lifetime suicide attempts and.85 when using data from the same measurement wave). We identified essential variables that should be considered when screening for suicidal behavior. Finally, we discuss the usefulness of complex machine learning models in suicide prediction. © The Author(s) 2023.,Introduction: False positives in retrospective binary suicide attempt classification models are commonly attributed to sheer classification error. However, when machine learning suicide attempt classification models are trained with a multitude of psycho-socio-environmental factors and achieve high accuracy in suicide risk assessment, false positives may turn out to be at high risk of developing suicidal behavior or attempting suicide in the future. Thus, they may be better viewed as “true alarms,” relevant for a suicide prevention program. In this study, using large population-based longitudinal dataset, we examine three hypotheses: (1) false positives, compared to the true negatives, are at higher risk of suicide attempt in future, (2) the suicide attempts risk for the false positives increase as a function of increase in specificity threshold; and (3) as specificity increases, the severity of risk factors between false positives and true positives becomes more similar. Methods: Utilizing the Gradient Boosting algorithm, we used a sample of 11,369 Norwegian adolescents, assessed at two timepoints (1992 and 1994), to classify suicide attempters at the first time point. We then assessed the relative risk of suicide attempt at the second time point for false positives in comparison to true negatives, and in relation to the level of specificity. Results: We found that false positives were at significantly higher risk of attempting suicide compared to true negatives. When selecting a higher classification risk threshold by gradually increasing the specificity cutoff from 60% to 97.5%, the relative suicide attempt risk of the false positive group increased, ranging from minimum of 2.96 to 7.22 times. As the risk threshold increased, the severity of various mental health indicators became significantly more comparable between false positives and true positives. Conclusion: We argue that the performance evaluation of machine learning suicide classification models should take the clinical relevance into account, rather than focusing solely on classification error metrics. As shown here, the so-called false positives represent a truly at-risk group that should be included in suicide prevention programs. Hence, these findings should be taken into consideration when interpreting machine learning suicide classification models as well as planning future suicide prevention interventions for adolescents. Copyright © 2023 Haghish, Laeng and Czajkowski.,Background: Suicide is the second leading cause of death in adolescents, and self-harm is one of the strongest predictors of death by suicide. The rates of adolescents presenting to emergency departments (EDs) for suicidal thoughts and behaviors (STBs) have increased. Still, existing follow-up after ED discharge is inadequate, leaving a high-risk period for reattempts and suicide. There is a need for innovative evaluation of imminent suicide risk factors in these patients, focusing on continuous real-time evaluations with low assessment burden and minimal reliance on patient disclosure of suicidal intent. Objective: This study examines prospective longitudinal associations between observed real-time mobile passive sensing, including communication and activity patterns, and clinical and self-reported assessments of STB over 6 months. Methods: This study will include 90 adolescents recruited on their first outpatient clinic visit following their discharge from the ED due to a recent STB. Participants will complete brief weekly assessments and be monitored continuously for their mobile app usage, including mobility, activity, and communication patterns, over 6 months using the iFeel research app. Participants will complete 4 in-person visits for clinical assessment at baseline and at the 1-, 3-, and 6-month follow-ups. The digital data will be processed, involving feature extraction, scaling, selection, and dimensionality reduction. Passive monitoring data will be analyzed using both classical machine learning models and deep learning models to identify proximal associations between real-time observed communication, activity patterns, and STB. The data will be split into a training and validation data set, and predictions will be matched against the clinical evaluations and self-reported STB events (ie, labels). To use both labeled and unlabeled digital data (ie, passively collected), we will use semisupervised methods in conjunction with a novel method that is based on anomaly detection notions. Results: Participant recruitment and follow-up started in February 2021 and are expected to be completed by 2024. We expect to find prospective proximal associations between mobile sensor communication, activity data, and STB outcomes. We will test predictive models for suicidal behaviors among high-risk adolescents. Conclusions: Developing digital markers of STB in a real-world sample of high-risk adolescents presenting to ED can inform different interventions and provide an objective means to assess the risk of suicidal behaviors. The results of this study will be the first step toward large-scale validation that may lead to suicide risk measures that aid psychiatric follow-up, decision-making, and targeted treatments. This novel assessment could facilitate timely identification and intervention to save young people’s lives. ©Shira Barzilay, Shai Fine, Shannel Akhavan, Liat Haruvi-Catalan, Alan Apter, Anat Brunstein-Klomek, Lior Carmi, Mishael Zohar, Inbar Kinarty, Talia Friedman, Silvana Fennig."
189,188,21,188_Federated Intrusion Detection and Privacy in IoT,Federated Intrusion Detection and Privacy in IoT,"Detecting anomalies, intrusions, and security threats in the network (including Internet of Things) traffic necessitates the processing of large volumes of sensitive data, which raises concerns about privacy and security. Federated learning, a distributed machine learning approach, enables multiple parties to collaboratively train a shared model while preserving data decentralization and privacy. In a federated learning environment, instead of training and evaluating the model on a single machine, each client learns a local model with the same structure but is trained on different local datasets. These local models are then communicated to an aggregation server that employs federated averaging to aggregate them and produce an optimized global model. This approach offers significant benefits for developing efficient and effective intrusion detection system (IDS) solutions. In this research, we investigated the effectiveness of federated learning for IDSs and compared it with that of traditional deep learning models. Our findings demonstrate that federated learning, by utilizing random client selection, achieved higher accuracy and lower loss compared to deep learning, particularly in scenarios emphasizing data privacy and security. Our experiments highlight the capability of federated learning to create global models without sharing sensitive data, thereby mitigating the risks associated with data breaches or leakage. The results suggest that federated averaging in federated learning has the potential to revolutionize the development of IDS solutions, thus making them more secure, efficient, and effective. © 2023 by the authors.,The Internet of Things (IoT) is a network of electrical devices that are connected to the Internet wirelessly. This group of devices generates a large amount of data with information about users, which makes the whole system sensitive and prone to malicious attacks eventually. The rapidly growing IoT-connected devices under a centralized ML system could threaten data privacy. The popular centralized machine learning (ML)-assisted approaches are difficult to apply due to their requirement of enormous amounts of data in a central entity. Owing to the growing distribution of data over numerous networks of connected devices, decentralized ML solutions are needed. In this paper, we propose a Federated Learning (FL) method for detecting unwanted intrusions to guarantee the protection of IoT networks. This method ensures privacy and security by federated training of local IoT device data. Local IoT clients share only parameter updates with a central global server, which aggregates them and distributes an improved detection algorithm. After each round of FL training, each of the IoT clients receives an updated model from the global server and trains their local dataset, where IoT devices can keep their own privacy intact while optimizing the overall model. To evaluate the efficiency of the proposed method, we conducted exhaustive experiments on a new dataset named Edge-IIoTset. The performance evaluation demonstrates the reliability and effectiveness of the proposed intrusion detection model by achieving an accuracy (92.49%) close to that offered by the conventional centralized ML models’ accuracy (93.92%) using the FL method. © 2023 by the authors.,The number of Internet of Things (IoT) devices has increased considerably in the past few years, resulting in a large growth of cyber attacks on IoT infrastructure. As part of a defense in depth approach to cybersecurity, intrusion detection systems (IDSs) have acquired a key role in attempting to detect malicious activities efficiently. Most modern approaches to IDS in IoT are based on machine learning (ML) techniques. The majority of these are centralized, which implies the sharing of data from source devices to a central server for classification. This presents potentially crucial issues related to privacy of user data as well as challenges in data transfers due to their volumes. In this article, we evaluate the use of federated learning (FL) as a method to implement intrusion detection in IoT environments. FL is an alternative, distributed method to centralized ML models, which has seen a surge of interest in IoT intrusion detection recently. In our implementation, we evaluate FL using a shallow artificial neural network (ANN) as the shared model and federated averaging (FedAvg) as the aggregation algorithm. The experiments are completed on the ToN_IoT and CICIDS2017 datasets in binary and multiclass classification. Classification is performed by the distributed devices using their own data. No sharing of data occurs among participants, maintaining data privacy. When compared against a centralized approach, results have shown that a collaborative FL IDS can be an efficient alternative, in terms of accuracy, precision, recall and F1-score, making it a viable option as an IoT IDS. Additionally, with these results as baseline, we have evaluated alternative aggregation algorithms, namely FedAvgM, FedAdam and FedAdagrad, in the same setting by using the Flower FL framework. The results from the evaluation show that, in our scenario, FedAvg and FedAvgM tend to perform better compared to the two adaptive algorithms, FedAdam and FedAdagrad. © 2023 by the authors."
190,189,21,189_Autonomous Driving and Safety-Critical Trajectory Planning and Behavior Prediction,Autonomous Driving and Safety-Critical Trajectory Planning and Behavior Prediction,"The traffic environment and driving behaviors are of great complexity and uncertainty in our physical world. Therefore, training in the digital world with low cost and diverse complexities become popular for autonomous driving in recent years. However, the current training methods tend to be limited to static data sets and deterministic models that do not sufficiently take into account the uncertainty and diversity prevalent in real traffic scenarios. These approaches also limit more possibilities for the comprehensive development and optimization of vision systems. In this paper, we develop a parallel training method based on artificial systems, computational experiments, and parallel execution (ACP) for the intelligent optimization and learning of the aforementioned agents in uncertain driving spaces. Parallel training creates a virtual driving space following the instruction of the ACP approach and conducts large-scale rehearsal experiments for possible scenarios. By enhancing the diversity of virtual scenarios, intelligent vehicles are trained to respond and adapt to the diverse uncertainties in the physical real-world driving space. Specifically, parallel training first proposes a standard operating procedure for intelligent driving systems, namely the projection-emergence-convergence-operation (PECO) loop. Digital quadruplets for parallel training, i.e., physical, descriptive, predictive, and prescriptive coaches, are also proposed. With the guidance of parallel training, virtual and real-world driving spaces are set up in parallel and interact frequently. They are closely linked and unified in opposition to each other, ultimately building a parallel driving system that fulfills safety, security, sustainability, sensitivity, service, and smartness (6S).  © 2016 IEEE.,Planning appropriate driving trajectory for route following is an important function for autonomous driving. Behavioral cloning, which allows automatic trajectory learning and improvement, has been effectively used in driving trajectory planning. However, existing behavioral cloning methods always rely on large scales of time-consuming, laborious, and reliable labels. To address this problem, this paper proposes a new off-policy imitation learning method for autonomous driving using task knowledge distillation. This novel method clones human driving behavior and effectively transfers the driving strategies to domain shift scenarios. The experiment results indicate that our method can lead to satisfactory route-following performance in realistic urban driving scenes and can transfer the driving strategies to new unknown scenes under various illumination and weather scenarios for autonomous driving.  © 2016 IEEE.,An open problem in autonomous vehicle safety validation is building reliable models of human driving behavior in simulation. This work presents an approach to learn neural driving policies from real world driving demonstration data. We model human driving as a sequential decision making problem that is characterized by non-linearity and stochasticity, and unknown underlying cost functions. Imitation learning is an approach for generating intelligent behavior when the cost function is unknown or difficult to specify. Building upon work in inverse reinforcement learning (IRL), Generative Adversarial Imitation Learning (GAIL) aims to provide effective imitation even for problems with large or continuous state and action spaces, such as modeling human driving. This article describes the use of GAIL for learning-based driver modeling. Because driver modeling is inherently a multi-agent problem, where the interaction between agents needs to be modeled, this paper describes a parameter-sharing extension of GAIL called PS-GAIL to tackle multi-agent driver modeling. In addition, GAIL is domain agnostic, making it difficult to encode specific knowledge relevant to driving in the learning process. This paper describes Reward Augmented Imitation Learning (RAIL), which modifies the reward signal to provide domain-specific knowledge to the agent. Finally, human demonstrations are dependent upon latent factors that may not be captured by GAIL. This paper describes Burn-InfoGAIL, which allows for disentanglement of latent variability in demonstrations. Imitation learning experiments are performed using NGSIM, a real-world highway driving dataset. Experiments show that these modifications to GAIL can successfully model highway driving behavior, accurately replicating human demonstrations and generating realistic, emergent behavior in the traffic flow arising from the interaction between driving agents.  © 2000-2011 IEEE."
191,190,21,190_Soil Spectral Analysis using VNIR-SWIR and MIR Spectroscopy,Soil Spectral Analysis using VNIR-SWIR and MIR Spectroscopy,"Large and publicly available soil spectral libraries, such as the USDA National Soil Survey Center–Kellogg Soil Survey Laboratory (NSSC-KSSL) mid-infrared (MIR) spectral library, are enormously valuable resources enabling laboratories around the world to make rapid low-cost estimates of a number of soil properties. A limitation to widespread sharing of soil spectral data is the need to ensure that spectra collected on a secondary spectrometer are compatible with the spectra in the primary or reference library. Various spectral preprocessing and calibration transfer techniques have been proposed to overcome this limitation. We tested the transferability of models developed using the USDA NSSC-KSSL MIR library to a secondary instrument. For the soil properties, total carbon (TC), pH, and clay content, we found that good performance (ratio of performance to deviation [RPD] = 4.9, 2.0, and 3.6, respectively) could be achieved on an independent test set with Savitzky-Golay smoothing and first derivative preprocessing of the secondary spectra using a memory-based learning chemometric approach. We tested three calibration transfer techniques (direct standardization [DS], piecewise direct standardization [PDS], and spectral space transformation [SST]) using different size transfer sets selected to be representative of the entire NSSC-KSSL library. Among the transfer methods, SST consistently outperformed DS and PDS with 50 transfer samples being an optimal number for transfer model development. For TC and pH, performance was improved using the SST transfer (RPD = 7.7 and 2.2, respectively) primarily through the elimination of bias. Calibration transfer could not improve predictions for clay. These findings suggest that calibration transfer may not always be necessary, but users should test to confirm this assumption using a small set of representative samples scanned at both laboratories. © 2022 The Authors. Soil Science Society of America Journal published by Wiley Periodicals LLC on behalf of Soil Science Society of America.,Soil bulk density (BD) is a key physical parameter in soil quality control and in the calculation from soil organic carbon (SOC) mass (g/kg) content to area stock (kg/ha). However, BD laboratory analysis is time-consuming, labour intensive and expensive, especially for a national-scale soil assessment. Hence, how to fill the omissions of BD values for all or some records in soil databases is widely discussed. This study employed different chemometric and machine learning algorithms to estimate BD in Irish soil from 671 horizon-based samples from MIR spectral libraries by partial least square regression (PLSR), random forest, Cubist and support vector machine (SVM). The best performance was observed for the SVM model with a higher ratio of performance to interquartile distance (RPIQ = 3.61) and R2 (0.81) values and lower root mean square error of prediction (RMSEP = 0.132). Moreover, BD highly correlated wavenumber bands were determined by principal components analysis (PCA) and variable importance analysis. Soil organic matter (SOM) was identified as the primary factor in the spectral soil BD model. The generalisation error of predicting unknown samples using a spectral soil bulk density (BD) model was calculated by employing leave-one-out cross-validation (LOO-CV) on SVM. Estimation of BD by the spectral BD model was compared with published traditional pedo-transfer functions (PTFs), results were then compared for the overall models, different horizon types and specific depth categories. The spectral soil BD model is significantly better than traditional PTFs overall, with RMSEP equalling 0.132 g/cm3 and 0.196 g/cm3 respectively. The spectral soil BD model showed a similar accuracy on the A horizon, but considerable performance improvements were found on the other types of horizon. As for different depth categories, there is no significant accuracy difference between shallow (A-Samples: 5–20 cm) and deep (S-Samples: 35–50 cm) topsoil for the spectral soil BD model, which differs from traditional PTFs. Hence, high accuracy and the homogeneity of performance on different depth layers above 50 cm could be noteworthy strengths of spectral modelling techniques when carrying out national soil surveys and large-scale carbon stock assessments. © 2023,Global pressures to improve soil organic carbon sequestration and soil health in general amongst the world's agricultural soils are creating a demand for improved practice to drive positive and sustainable changes in the natural capital of soils. Incentive programs aimed to promote this must be informed by accurate observations of the state of soils, both temporally and spatially. Soil spectral inference is a useful method for capturing the state of soils cost-effectively, but the price of standard laboratory grade visible and near-infrared (Vis-NIR) sensors can limit its application. Further, the acquisition of spectra by these laboratory grade sensors is performed primarily in air-dried and ground condition, adding a time lag to information retrieval. Recently, low-cost, portable miniaturised near-infrafred (NIR) spectrometers have become available and have shown to be a viable alternative for the measurement of several agronomically important soil properties, which are also vital to the maintenance of soil health, including soil organic carbon (SOC), and cation exchange capacity (CEC). However, the implementation of new spectrometers, to new locations requires the creation of new spectral libraries, an expensive and labour-intensive process requiring large amounts of soil analytical and spectral data gathering. Thus, existing, laboratory grade Vis-NIR spectral libraries present a high-quality and high-resolution resource to leverage. This work demonstrates how existing spectral library resources can be accessed with cheaper, portable miniaturised NIR spectrometers with appropriate spectral filtering, and appropriate transformation matrices. In addition, the work shows that by correcting for the influences of spectral differences between soils scanned in field condition, and those prepared for analysis in the laboratory, greater uptake of spectral inference as a tool to evaluate the state of soils can be enabled. This work also demonstrates how large existing laboratory grade spectral libraries such as the CSIRO national Australian Vis-NIR soil spectral library can be queried and using memory-based learning or similar methods, such as RS-Local, and the most appropriate samples may be identified to be used for the prediction of soil properties. This work builds off an existing framework for the use of soil spectral inference for monitoring the state of soil, the Australian 2021 Soil Organic Carbon Credits Methodology Determination. Methods are demonstrated for the prediction of nine agronomically important soil properties, SOC, pH in water, pH in CaCl2, electrical conductivity, CEC, and exchangeable Ca, K, Mg and Na. For SOC a model using only 20 local samples was produced in this work with a Lin's concordance correlation coefficient (LCCC) of 0.72, surpassing both the minimum requirement under the carbon credits methodology determination (LCCC 0.6), and a 50 sample local only model (LCCC 0.61). This example demonstrates that a significant further potential cost saving in laboratory analysis across soil monitoring projects can be achieved through selectively leveraging a large spectral library resource. © 2023"
192,191,21,191_Deep reinforcement learning for scheduling and routing problems in manufacturing and logistics,Deep reinforcement learning for scheduling and routing problems in manufacturing and logistics,"Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this article presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to construct production-adaptive operation and machine features to support high-quality decision-making. Experimental results using synthetic data as well as public benchmarks corroborate that the proposed approach outperforms both traditional PDRs and the state-of-the-art DRL method. Moreover, it achieves results comparable to exact methods in certain cases and demonstrates favorable generalization ability to large-scale and real-world unseen FJSP tasks. IEEE,The flexible job shop scheduling (FJSS) is important in real-world factories due to the wide applicability. FJSS schedules the operations of jobs to be executed by specific machines at the appropriate time slots based on two decision steps, namely, the job sequencing (i.e., the sequence of jobs executed on a machine) and the job routing (i.e., the route of a job to a machine). Most current studies utilize either deep reinforcement learning (DRL) or multi-agent reinforcement learning (MARL) for FJSS with a large search space. However, these studies suffer from two major limitations: no integration between DRL and MARL, and independent agents without cooperation. To this end, we propose a new model for FJSS, called DeepMAG based on Deep reinforcement learning with Multi-Agent Graphs. DeepMAG has two key contributions. (1) Integration between DRL and MARL. DeepMAG integrates DRL with MARL by associating a different agent to each machine and job. Each agent exploits DRL to find the best action on the job sequencing and routing. After a job-associated agent chooses the best machine, the job becomes a job candidate for the machine to proceed to its next operation, while a machine-associated agent selects the next job from its job candidate set to be processed. (2) Cooperative agents. A multi-agent graph is built based on the operation relationships among machines and jobs. An agent cooperates with its neighboring agents to take one cooperative action. Finally, we conduct experiments to evaluate the performance of DeepMAG and experimental results show that it outperforms the state-of-the-art techniques. © 2022 Elsevier B.V.,Parallel machine scheduling (PMS) is a common setting in many manufacturing facilities, in which each job is allowed to be processed on one of the machines of the same type. It involves scheduling n jobs on m machines to minimize certain objective functions. For preemptive scheduling, most problems are not only NP-hard but also difficult in practice. Moreover, many unexpected events, such as machine failure and requirement change, are inevitable in the practical production process, meaning that rescheduling is required for static scheduling methods. Deep reinforcement learning (DRL), which combines deep learning and reinforcement learning, has achieved promising results in several domains and has shown the potential to solve large Markov decision process (MDP) optimization tasks. Moreover, PMS problems can be formulated as an MDP problem, inspiring us to devise a DRL method to deal with PMS problems in a dynamic environment. We develop a novel DRL-based PMS method, called DPMS, in which the developed model considers the characteristics of PMS to design states and the reward. The actions involve dispatching rules, so DPMS can be considered a meta-dispatching-rule system that can efficiently select a sequence of dispatching rules based on the current environment or unexpected events. The experimental results demonstrate that DPMS can yield promising results in a dynamic environment by learning from the interactions between the agent and the environment. Furthermore, we conduct extensive experiments to analyze DPMS in the context of developing a DRL to deal with dynamic PMS problems. © 2013 IEEE."
193,192,20,192_Adaptive Routing and Delivery Algorithm for Logistics,Adaptive Routing and Delivery Algorithm for Logistics,"With the development of the logistics economy, problems such as the timeliness of logistics distribution and the high cost of distribution have emerged. A new adaptive genetic algorithm is proposed to solve these problems. The pc and pm values of the algorithm are related to the number of iterations and the individual fitness values. To improve the local optimization ability of the algorithm, a large neighborhood search algorithm is proposed. In addition, this study establishes a soft time window town logistics distribution model with constraints. The model considers the optimal cost as the objective function and customer satisfaction as the influencing factor. In the experiment, the proposed adaptive genetic algorithm is compared with the traditional genetic algorithm, validating the effectiveness of the proposed algorithm. © 2022,The diversification of customers’ geographic locations forces companies’ delivery systems to travel long-distances increasing distribution costs. In this regard, customers in an inconvenient location for one distribution company could be ideal for another company. Therefore, a set of distribution companies may shorten the delivery distance by combining urban distribution networks. The feeder vehicle routing problem (FVRP) is a new type of VRP to provide fast services in urban transportation. Unlike VRP, FVRP includes a fleet of heterogeneous vehicles (i.e., trucks and motorcycles) in which trucks and motorcycles move from the depot to serve customers. In this problem, motorcycles pass easily in crowded areas, and the traffic of urban logistics is distributed easily. Since returning to the depot and re-shipment to the customers increase cost and the distance traveled, one strategy to deal with this problem is applying the “joint” mechanism. In this mechanism, motorcycles visit the trucks rather than return to the depot at the joint points. Also, the feeder approach lowers the number of times the vehicle returns to the central depot for loading, resulting in cost and time savings. This study introduces a collaborative feeder vehicle routing problem with flexible time windows (CFVRPFlexTW) as a bi-objective model that simultaneously minimizes routing costs and maximizes customer satisfaction with a flexible time window. After modeling CFVRPFlexTW through mixed-integer linear programming (MILP), the augmented epsilon constraint (AEC) approach is applied in the CPLEX solver to solve the problem. Also, multi-objective particle swarm optimization with dynamic inertia weigh (WMOPSO) and MOPSO with adaptive learning strategy (LAMOPSO) were developed regarding the complexity of the problem. Then, their performance is compared with that of the Pareto solutions produced by the non-dominated sorting genetic algorithm-II (NSGA-II). The computational outcomes indicate the outperformance of the WMOPSO based on some related metrics. Eventually, the AHP-TOPSIS method is applied to prioritize and analyze the algorithms. The results indicate the proposed LAMOPSO algorithm is more efficient in small-size instances. Furthermore, in large-size instances, the WMOPSO algorithm outperforms the MOPSO, LAMOPSO, LAWMOPSO, and NSGA-II algorithms. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.,To meet the demands of green logistics while considering the time-dependent effects caused by traffic congestion, we establish a time-dependent green vehicle routing problem with time windows model for cold chain logistics. This model aims to minimize the total cost, including the transportation cost, refrigeration cost, carbon emission cost, and labor cost. Vehicles are allowed to wait to avoid a bad traffic environment after completing their services to customers. To solve the model, we develop a two-stage hybrid search algorithm. In the first stage of this algorithm, an adaptive large neighborhood search technique is used to determine the vehicle route, while in the second stage, a shortest-path algorithm is used to determine the departure time of the vehicles from customer's node. Finally, numerical experiments are performed to verify the effectiveness and superiority of our model and the proposed hybrid search algorithm by comparing with the standard instances and large-scale instances. © 2022 Elsevier Ltd"
194,193,20,193_Multi-agent reinforcement learning for large-scale traffic signal control,Multi-agent reinforcement learning for large-scale traffic signal control,"The continuous development of intelligent traffic control systems has a profound influence on urban traffic planning and traffic management. Indeed, as big data and artificial intelligence continue to evolve, the traffic control strategy based on deep reinforcement learning (RL) has been proven to be a promising method to improve the efficiency of intersections and save people's travel time. However, the existing algorithms ignore the temporal and spatial characteristics of intersections. In this article, we propose a multiagent RL based on the deep spatiotemporal attentive neural network (MARL-DSTAN) to determine the traffic signal timing in a large-scale road network. In this model, the state information captures the spatial dependency of the entire road network by leveraging the graph convolutional network (GCN) and integrates the information based on the importance of intersections via the attention mechanism. Meanwhile, to accumulate more valuable samples and enhance the learning efficiency, the recurrent neural network (RNN) is introduced in the exploration stage to constrain the action search space instead of fully random exploration. MARL-DSTAN decomposes the large-scale area into multiple base environments, and the agents in each base environment use the idea of 'centralized training and decentralized execution' to learn to accelerate the algorithm convergence. The simulation results show that our algorithm significantly outperforms the fixed timing scheme and several other state-of-the-art baseline RL algorithms.  © 2022 IEEE.,The optimization of intersection signal control can improve traffic efficiency, reduce congestion degree, and improve traffic safety. Aiming at implementing the coordinated adaptive traffic signal control (ATSC) across large-scale arterial network, multi-agent reinforcement learning (MARL) has been widely concerned and lucubrated. Nevertheless, the existing MARL-based ATSC studies suffers from several limitations: (1) While most existing researches focused on the mobility performance of controlled corridor, there calls for a methodology that aims at combine multi-objective performance on traffic safety, efficiency, and network coordination simultaneously; (2) Most methods ignore the correlations between multiple agents, nor considers the spatial–temporal dependencies among the corelated neighboring intersections due to high communications requirements, which can hardly be achieved in real adaptive coordination control. To overcome the aforementioned difficulties, a multi-objective reinforcement learning model (NACRL) for network-wide coordinated signal control is proposed. Firstly, to enforce a coordinated network control with safety and efficiency considerations, a reward mechanism inspecting both traffic safety and traffic efficiency indicators was designed to achieve ideal performance in terms of mobility, safety and smooth. Secondly, the proposed NACRL conducted a centralized training-decentralized execution framework, this overcomes the critical limitation of data transmission in the field implementation while explicitly analyzing the traffic state over the entire network instead of examining each isolated intersection. Last but not least, the proposed algorithm utilized the attention mechanism to dynamically capture the sophisticated spatial–temporal dependencies over the complex arterial network, which aids the better coordinated control over multi-agents deployed at the intersections across the corridor. To testify the effeteness of the proposed algorithm, extensive experiments were implemented in both large-scale synthetic traffic grid and real-world arterial network. The experiment demonstrated that the proposed NACRL algorithm outperforms other state-of-the-art baselines with simultaneously improved performance in terms of traffic safety, traffic efficiency and network coordination, as well as improved algorithm convergence and interpretability. © 2023 Elsevier Ltd,Cooperation between intersections in large-scale road networks is critical in traffic congestion. Currently, most traffic signals cooperate via pre-defined timing phases, which is extremely inefficient in real-time traffic scenarios. Most existing studies on multi-agent reinforcement learning (MARL) traffic signal control have focused on designing efficient communication methods, but have ignored the importance of how agents interact in cooperative communication. To achieve more efficient cooperation among traffic signals and alleviate urban traffic congestion, this study constructs a Graph Cooperation Q-learning Network Traffic Signal Control (GCQN-TSC) model, which is a graph cooperation network with an embedded self-attention mechanism that enables agents to adjust their attention in real time according to the dynamic traffic flow information, perceive the traffic environment quickly and effectively in a larger range, and help agents achieve more effective collaboration. Moreover, the Deep Graph Q-learning (DGQ) algorithm is proposed in this model to optimize the traffic signal control strategy according to the spatio-temporal characteristics of different traffic scenes and provide the optimal signal phase for each intersection. This study also integrates the ecological traffic concept into MARL traffic signal control, which aims to reduce traffic exhaust emissions. Finally, the proposed GCQN-TSC is experimentally validated both in a synthetic traffic grid and a real-world traffic network using the SUMO simulator. The experimental results show that GCQN-TSC outperforms other traffic signal control methods in almost all performance metrics, including average queue length and waiting time, as it can aggregate information acquired from collaborative agents and make network-level signal optimization decisions. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
195,194,20,194_Drug-Target Affinity Prediction and Drug Discovery,Drug-Target Affinity Prediction and Drug Discovery,"Motivation: Large-scale prediction of drug-target affinity (DTA) plays an important role in drug discovery. In recent years, machine learning algorithms have made great progress in DTA prediction by utilizing sequence or structural information of both drugs and proteins. However, sequence-based algorithms ignore the structural information of molecules and proteins, while graph-based algorithms are insufficient in feature extraction and information interaction. Results: In this article, we propose NHGNN-DTA, a node-adaptive hybrid neural network for interpretable DTA prediction. It can adaptively acquire feature representations of drugs and proteins and allow information to interact at the graph level, effectively combining the advantages of both sequence-based and graph-based approaches. Experimental results have shown that NHGNN-DTA achieved new state-of-the-art performance. It achieved the mean squared error (MSE) of 0.196 on the Davis dataset (below 0.2 for the first time) and 0.124 on the KIBA dataset (3% improvement). Meanwhile, in the case of cold start scenario, NHGNN-DTA proved to be more robust and more effective with unseen inputs than baseline methods. Furthermore, the multi-head self-attention mechanism endows the model with interpretability, providing new exploratory insights for drug discovery. The case study on Omicron variants of SARS-CoV-2 illustrates the efficient utilization of drug repurposing in COVID-19.  © 2023 The Author(s). Published by Oxford University Press.,Background: Accurately predicting drug-target binding affinity (DTA) in silico plays an important role in drug discovery. Most of the computational methods developed for predicting DTA use machine learning models, especially deep neural networks, and depend on large-scale labelled data. However, it is difficult to learn enough feature representation from tens of millions of compounds and hundreds of thousands of proteins only based on relatively limited labelled drug-target data. There are a large number of unknown drugs, which never appear in the labelled drug-target data. This is a kind of out-of-distribution problems in bio-medicine. Some recent studies adopted self-supervised pre-training tasks to learn structural information of amino acid sequences for enhancing the feature representation of proteins. However, the task gap between pre-training and DTA prediction brings the catastrophic forgetting problem, which hinders the full application of feature representation in DTA prediction and seriously affects the generalization capability of models for unknown drug discovery. Results: To address these problems, we propose the GeneralizedDTA, which is a new DTA prediction model oriented to unknown drug discovery, by combining pre-training and multi-task learning. We introduce self-supervised protein and drug pre-training tasks to learn richer structural information from amino acid sequences of proteins and molecular graphs of drug compounds, in order to alleviate the problem of high variance caused by encoding based on deep neural networks and accelerate the convergence of prediction model on small-scale labelled data. We also develop a multi-task learning framework with a dual adaptation mechanism to narrow the task gap between pre-training and prediction for preventing overfitting and improving the generalization capability of DTA prediction model on unknown drug discovery. To validate the effectiveness of our model, we construct an unknown drug data set to simulate the scenario of unknown drug discovery. Compared with existing DTA prediction models, the experimental results show that our model has the higher generalization capability in the DTA prediction of unknown drugs. Conclusions: The advantages of our model are mainly attributed to two kinds of pre-training tasks and the multi-task learning framework, which can learn richer structural information of proteins and drugs from large-scale unlabeled data, and then effectively integrate it into the downstream prediction task for obtaining a high-quality DTA prediction in unknown drug discovery. © 2022, The Author(s).,The identification of drug-Target relations (DTRs) is substantial in drug development. A large number of methods treat DTRs as drug-Target interactions (DTIs), a binary classification problem. The main drawback of these methods are the lack of reliable negative samples and the absence of many important aspects of DTR, including their dose dependence and quantitative affinities. With increasing number of publications of drug-protein binding affinity data recently, DTRs prediction can be viewed as a regression problem of drug-Target affinities (DTAs) which reflects how tightly the drug binds to the target and can present more detailed and specific information than DTIs. The growth of affinity data enables the use of deep learning architectures, which have been shown to be among the state-of-The-Art methods in binding affinity prediction. Although relatively effective, due to the black-box nature of deep learning, these models are less biologically interpretable. In this study, we proposed a deep learning-based model, named AttentionDTA, which uses attention mechanism to predict DTAs. Different from the models using 3D structures of drug-Target complexes or graph representation of drugs and proteins, the novelty of our work is to use attention mechanism to focus on key subsequences which are important in drug and protein sequences when predicting its affinity. We use two separate one-dimensional Convolution Neural Networks (1D-CNNs) to extract the semantic information of drug's SMILES string and protein's amino acid sequence. Furthermore, a two-side multi-head attention mechanism is developed and embedded to our model to explore the relationship between drug features and protein features. We evaluate our model on three established DTA benchmark datasets, Davis, Metz, and KIBA. AttentionDTA outperforms the state-of-The-Art deep learning methods under different evaluation metrics. The results show that the attention-based model can effectively extract protein features related to drug information and drug features related to protein information to better predict drug target affinities. It is worth mentioning that we test our model on IC50 dataset, which provides the binding sites between drugs and proteins, to evaluate the ability of our model to locate binding sites. Finally, we visualize the attention weight to demonstrate the biological significance of the model. The source code of AttentionDTA can be downloaded from https://github.com/zhaoqichang/AttentionDTA_TCBB.  © 2004-2012 IEEE."
196,195,20,195_Automated Electron Microscopy and Nanoparticle Analysis,Automated Electron Microscopy and Nanoparticle Analysis,"The rise of automation and machine learning (ML) in electron microscopy has the potential to revolutionize materials research through autonomous data collection and processing. A significant challenge lies in developing ML models that rapidly generalize to large data sets under varying experimental conditions. We address this by employing a cycle generative adversarial network (CycleGAN) with a reciprocal space discriminator, which augments simulated data with realistic spatial frequency information. This allows the CycleGAN to generate images nearly indistinguishable from real data and provide labels for ML applications. We showcase our approach by training a fully convolutional network (FCN) to identify single atom defects in a 4.5 million atom data set, collected using automated acquisition in an aberration-corrected scanning transmission electron microscope (STEM). Our method produces adaptable FCNs that can adjust to dynamically changing experimental variables with minimal intervention, marking a crucial step towards fully autonomous harnessing of microscopy big data. © 2023, The Author(s).,The major role of many research studies in nanotechnology is to identify, count, and measure nanoparticles. Images with particles are often handled by hand, employing a computer program ruler. The lack of available algorithms and specialized software tools for analyzing microscope images are limited in this research area. So, this study aims to encourage the development of a comprehensive model to predict the nanoparticles type in scanning electron microscopy (SEM). We present a dataset of 750 ordered nanoscale materials. Palladium nanoparticles (Pd nanoparticles) are placed on a carbon surface using a scanning electron microscope. This paper proposes an intelligent optimization model to classify Nanoparticle types in SEM images (e.g., lines, intersections, networks, ellipses, and circles). The utilized dataset is unlabeled with the imbalanced distribution of classes. Moreover, it contains a large amount of redundant information. These kinds of problems affect the performance of the nanoparticle's classifier. The proposed model can be divided into four phases: preprocessing, feature extraction, feature selection, and classification. The first phase of the Nanoparticle classification model is concerned with image labeling, where some morphological operations and image filtration are proposed to help experts in labeling the SEM dataset. VGG-19 deep networks in combination with Grey Wolf Optimization (GWO) are used for feature extraction and selection phases. The imbalanced input data classification phase proposes a class weight balancing support vector machine (SVM) with different kernel functions. Detailed results of all experiments show that the proposed intelligent optimization model is promising in providing a high-performance classifier for nanoparticle types in SEM images. Overall performance measures are 97% and 98% for overall accuracy and F1-score, respectively. © 2023 Elsevier B.V.,Electron backscattering diffraction provides the analysis of crystalline phases at large scales (microns) while precession electron diffraction may be used to get 4D-STEM data to elucidate structure at nanometric resolution. Both are limited by the probe size and also exhibit some difficulties for the generation of large datasets, given the inherent complexity of image acquisition. The latter appoints the application of advanced machine learning techniques, such as deep learning adapted for several tasks, including pattern matching, image segmentation, etc. This research aims to show how Gabor filters provide an appropriate feature extraction technique for electron microscopy images that could prevent the need of large volumes of data to train deep learning models. The work presented herein combines an algorithm based on Gabor filters for feature extraction and an unsupervised learning method to perform particle segmentation of polyhedral metallic nanoparticles and crystal orientation mapping at atomic scale. Experimental results have shown that Gabor filters are convenient for electron microscopy images analysis, that even a nonsupervised learning algorithm can provide remarkable results in crystal segmentation of individual nanoparticles. This approach enables its application to dynamic analysis of particle transformation recorded with aberration-corrected microscopy, offering new possibilities of analysis at nanometric scale. © 2022 Wiley-VCH GmbH."
197,196,20,196_Crowd counting and density estimation in various scenarios using deep learning models and other techniques,Crowd counting and density estimation in various scenarios using deep learning models and other techniques,"Labeling is onerous for crowd counting as it should annotate each individual in crowd images. Recently, several methods have been proposed for semi-supervised crowd counting to reduce the labeling efforts. Given a limited labeling budget, they typically select a few crowd images and densely label all individuals in each of them. Despite the promising results, we argue the None-or-All labeling strategy is suboptimal as the densely labeled individuals in each crowd image usually appear similar while the massive unlabeled crowd images may contain entirely diverse individuals. To this end, we propose to break the labeling chain of previous methods and make the first attempt to reduce spatial labeling redundancy for semi-supervised crowd counting. First, instead of annotating all the regions in each crowd image, we propose to annotate the representative ones only. We analyze the region representativeness from both vertical and horizontal directions of initially estimated density maps, and formulate them as cluster centers of Gaussian Mixture Models. Additionally, to leverage the rich unlabeled regions, we exploit the similarities among individuals in each crowd image to directly supervise the unlabeled regions via feature propagation instead of the error-prone label propagation employed in the previous methods. In this way, we can transfer the original spatial labeling redundancy caused by individual similarities to effective supervision signals on the unlabeled regions. Extensive experiments on the widely-used benchmarks demonstrate that our method can outperform previous best approaches by a large margin.  © 1979-2012 IEEE.,Crowd localization is a new computer vision task, evolved from crowd counting. Different from the latter, it provides more precise location information for each instance, not just counting numbers for the whole crowd scene, which brings greater challenges, especially in extremely congested crowd scenes. In this paper, we focus on how to achieve precise instance localization in high-density crowd scenes, and to alleviate the problem that the feature extraction ability of the traditional model is reduced due to the target occlusion, the image blur, etc. To this end, we propose a Dilated Convolutional Swin Transformer (DCST) for congested crowd scenes. Specifically, a window-based vision transformer is introduced into the crowd localization task, which effectively improves the capacity of representation learning. Then, the well-designed dilated convolutional module is inserted into some different stages of the transformer to enhance the large-range contextual information. Extensive experiments evidence the effectiveness of the proposed methods and achieve the state-of-the-art performance on five popular datasets. Especially, the proposed model achieves F1-measure of 77.5% and MAE of 84.2 in terms of localization and counting performance, respectively. © 2022 Elsevier B.V.,Aiming at the problem that the existing crowd counting methods cannot achieve accurate crowd counting and map visualization in a large scene, a crowd density estimation and mapping method based on surveillance video and GIS (CDEM-M) is proposed. Firstly, a crowd semantic segmentation model (CSSM) and a crowd denoising model (CDM) suitable for high-altitude scenarios are constructed by transfer learning. Then, based on the homography matrix between the video and remote sensing image, the crowd areas in the video are projected to the map space. Finally, according to the distance from the crowd target to the camera, the camera inclination, and the area of the crowd polygon in the geographic space, a BP neural network for the crowd density estimation is constructed. The results show the following: (1) The test accuracy of the CSSM was 96.70%, and the classification accuracy of the CDM was 86.29%, which can achieve a high-precision crowd extraction in large scenes. (2) The BP neural network for the crowd density estimation was constructed, with an average error of 1.2 and a mean square error of 4.5. Compared to the density map method, the MAE and RMSE of the CDEM-M are reduced by 89.9 and 85.1, respectively, which is more suitable for a high-altitude camera. (3) The crowd polygons were filled with the corresponding number of points, and the symbol was a human icon. The crowd mapping and visual expression were realized. The CDEM-M can be used for crowd supervision in stations, shopping malls, and sports venues. © 2023 by the authors."
198,197,20,197_Continual Learning with Reduced Forgetting,Continual Learning with Reduced Forgetting,"Learning continually is a key aspect of intelligence and a necessary ability to solve many real-life problems. One of the most effective strategies to control catastrophic forgetting, the Achilles’ heel of continual learning, is storing part of the old data and replaying them interleaved with new experiences (also known as the replay approach). Generative replay, which is using generative models to provide replay patterns on demand, is particularly intriguing, however, it was shown to be effective mainly under simplified assumptions, such as simple scenarios and low-dimensional data. In this paper, we show that, while the generated data are usually not able to improve the classification accuracy for the old classes, they can be effective as negative examples (or antagonists) to better learn the new classes, especially when the learning experiences are small and contain examples of just one or few classes. The proposed approach is validated on complex class-incremental and data-incremental continual learning scenarios (CORe50 and ImageNet-1000) composed of high-dimensional data and a large number of training experiences: a setup where existing generative replay approaches usually fail. © 2023 Elsevier Ltd,One notable weakness of current machine learning algorithms is the poor ability of models to solve new problems without forgetting previously acquired knowledge. The Continual Learning paradigm has emerged as a protocol to systematically investigate settings where the model sequentially observes samples generated by a series of tasks. In this work, we take a task-agnostic view of continual learning and develop a hierarchical information-theoretic optimality principle that facilitates a trade-off between learning and forgetting. We derive this principle from a Bayesian perspective and show its connections to previous approaches to continual learning. Based on this principle, we propose a neural network layer, called the Mixture-of-Variational-Experts layer, that alleviates forgetting by creating a set of information processing paths through the network which is governed by a gating policy. Equipped with a diverse and specialized set of parameters, each path can be regarded as a distinct sub-network that learns to solve tasks. To improve expert allocation, we introduce diversity objectives, which we evaluate in additional ablation studies. Importantly, our approach can operate in a task-agnostic way, i.e., it does not require task-specific knowledge, as is the case with many existing continual learning algorithms. Due to the general formulation based on generic utility functions, we can apply this optimality principle to a large variety of learning problems, including supervised learning, reinforcement learning, and generative modeling. We demonstrate the competitive performance of our method on continual reinforcement learning and variants of the MNIST, CIFAR-10, and CIFAR-100 datasets. © 2022, The Author(s).,While continual learning has shown its impressive performance in addressing catastrophic forgetting of traditional neural networks and enabling them to learn multiple tasks continuously, it still requires a large amount of input data to train neural networks with satisfactory classification performance. Since collecting a large amount of training data is a time-consuming and expensive procedure, this study attempts to propose a novel data-free contrastive reversion method for continual learning (DFCRCL) to significantly reduce the number of training data for continual learning, while maintaining or even improving the classification performance of continual learning. In order to achieve such a goal, DFCRCL uses contrastive reversion to generate high-semantic pseudo samples from the previous task to guide the training of the current task. DFCRCL has three merits: (1) knowledge distillation from the previous task model to the current task model guarantees both the reduction of training data and the avoidance of catastrophic forgetting, and thus DFCRCL can effectively learn a sequence of tasks continuously (2) contrastive reversion enhances the semantic diversity of pseudo samples by learning the distinguishability between distinct pseudo samples in the feature space (3) contrastive reversion improves the performance of knowledge distillation in DFCRCL by enhancing the semantic diversity of the pseudo samples generated from the previous task model. Compared to six mainstream continual learning methods, the proposed DFCRCL achieves at least comparable or even better classification performance and stability in four benchmarking continual learning scenarios. In addition, the effectiveness of DFCRCL is demonstrated by ablation experiments. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
199,198,19,198_Unsupervised Topic Modeling and Text Analysis,Unsupervised Topic Modeling and Text Analysis,"Topic modelling is an important approach of unsupervised machine learning that allows automatically extracting the main ""topics""from large collections of documents. In addition, topic modelling is able to identify the topic proportions of each individual document, which can be helpful for organizing the collections. Many topic modelling algorithms have been proposed to date, including several that leverage advanced techniques such as variational inference and deep autoencoders. However, to date topic modelling has made limited use of reinforcement learning, a framework that has obtained vast success in many other unsupervised learning tasks. For this reason, in this article we propose training a neural topic model using a reinforcement learning objective and minimizing the objective with the recently-proposed REBAR gradient estimator. Experiments performed over two probing datasets have shown that the proposed model has achieved improvements over all the compared models in terms of both model perplexity and topic coherence, and produced topics that appear qualitatively informative and consistent.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.,Background: Topic models are a class of unsupervised machine learning models, which facilitate summarization, browsing and retrieval from large unstructured document collections. This study reviews several methods for assessing the quality of unsupervised topic models estimated using non-negative matrix factorization. Techniques for topic model validation have been developed across disparate fields. We synthesize this literature, discuss the advantages and disadvantages of different techniques for topic model validation, and illustrate their usefulness for guiding model selection on a large clinical text corpus. Design, setting and data: Using a retrospective cohort design, we curated a text corpus containing 382,666 clinical notes collected between 01/01/2017 through 12/31/2020 from primary care electronic medical records in Toronto Canada. Methods: Several topic model quality metrics have been proposed to assess different aspects of model fit. We explored the following metrics: reconstruction error, topic coherence, rank biased overlap, Kendall’s weighted tau, partition coefficient, partition entropy and the Xie-Beni statistic. Depending on context, cross-validation and/or bootstrap stability analysis were used to estimate these metrics on our corpus. Results: Cross-validated reconstruction error favored large topic models (K ? 100 topics) on our corpus. Stability analysis using topic coherence and the Xie-Beni statistic also favored large models (K = 100 topics). Rank biased overlap and Kendall’s weighted tau favored small models (K = 5 topics). Few model evaluation metrics suggested mid-sized topic models (25 ? K ? 75) as being optimal. However, human judgement suggested that mid-sized topic models produced expressive low-dimensional summarizations of the corpus. Conclusions: Topic model quality indices are transparent quantitative tools for guiding model selection and evaluation. Our empirical illustration demonstrated that different topic model quality indices favor models of different complexity; and may not select models aligning with human judgment. This suggests that different metrics capture different aspects of model goodness of fit. A combination of topic model quality indices, coupled with human validation, may be useful in appraising unsupervised topic models. © 2023, The Author(s).,Topic discovery involves identifying the main ideas within large volumes of textual data. It indicates recurring topics in documents, providing an overview of the text. Current topic discovery models receive the text, with or without pre-processing, including stop word removal, text cleaning, and normalization (lowercase conversion). A topic discovery process that receives general domain text with or without processing generates general topics. General topics do not offer detailed overviews of the input text, and manual text categorization is tedious and time-consuming. Extracting topics from text with an automatic classification task is necessary to generate specific topics enriched with top words that maintain semantic relationships among them. Therefore, this paper presents an approach that integrates text classification for topic discovery from large amounts of English textual data, such as 20-Newsgroups and Reuters Corpora. We rely on integrating automatic text classification before the topic discovery process to obtain specific topics for each class with relevant semantic relationships between top words. Text classification performs a word analysis that makes up a document to decide what class or category to identify; then, the proposed integration provides latent and specific topics depicted by top words with high coherence from each obtained class. Text classification accomplishes this with a convolutional neural network (CNN), incorporating an embedding model based on semantic relationships. Topic discovery over categorized text is realized with latent Dirichlet analysis (LDA), probabilistic latent semantic analysis (PLSA), and latent semantic analysis (LSA) algorithms. An evaluation process for topic discovery over categorized text was performed based on the normalized topic coherence metric. The 20-Newsgroups corpus was classified, and twenty topics with the ten top words were identified for each class. The normalized topic coherence obtained was 0.1723 with LDA, 0.1622 with LSA, and 0.1716 with PLSA. The Reuters Corpus was also classified, and twenty and fifty topics were identified. A normalized topic coherence of 0.1441 was achieved when applying the LDA algorithm, obtaining 20 topics for each class; with LSA, the coherence was 0.1360, and with PLSA, it was 0.1436. © 2023 by the authors."
200,199,19,199_Stroke Prognosis and Classification in Thrombectomy Patients,Stroke Prognosis and Classification in Thrombectomy Patients,"Objectives We aimed to develop and externally validate a generalisable risk prediction model for 30-day stroke mortality suitable for supporting quality improvement analytics in stroke care using large nationwide stroke registers in the UK and Sweden. Design Registry-based cohort study. Setting Stroke registries including the Sentinel Stroke National Audit Programme (SSNAP) in England, Wales and Northern Ireland (2013-2019) and the national Swedish stroke register (Riksstroke 2015-2020). Participants and methods Data from SSNAP were used for developing and temporally validating the model, and data from Riksstroke were used for external validation. Models were developed with the variables available in both registries using logistic regression (LR), LR with elastic net and interaction terms and eXtreme Gradient Boosting (XGBoost). Performances were evaluated with discrimination, calibration and decision curves. Outcome measures The primary outcome was all-cause 30-day in-hospital mortality after stroke. Results In total, 488 497 patients who had a stroke with 12.4% 30-day in-hospital mortality were used for developing and temporally validating the model in the UK. A total of 128 360 patients who had a stroke with 10.8% 30-day in-hospital mortality and 13.1% all mortality were used for external validation in Sweden. In the SSNAP temporal validation set, the final XGBoost model achieved the highest area under the receiver operating characteristic curve (AUC) (0.852 (95% CI 0.848 to 0.855)) and was well calibrated. The performances on the external validation in Riksstroke were as good and achieved AUC at 0.861 (95% CI 0.858 to 0.865) for in-hospital mortality. For Riksstroke, the models slightly overestimated the risk for in-hospital mortality, while they were better calibrated at the risk for all mortality. Conclusion The risk prediction model was accurate and externally validated using high quality registry data. This is potentially suitable to be deployed as part of quality improvement analytics in stroke care to enable the fair comparison of stroke mortality outcomes across hospitals and health systems across countries  © 2023 Author(s). Published by BMJ.,Mechanical thrombectomy (MT) is the standard of care for patients with acute ischemic stroke from large vessel occlusion (AIS-LVO). The association of blood pressure variability (BPV) during MT and outcomes are unknown. We leveraged a supervised machine learning algorithm to predict patient characteristics that are associated with BPV indices. We performed a retrospective review of our comprehensive stroke center’s registry of all adult patients undergoing MT between 01/01/2016 and 12/31/2019. The primary outcome was poor functional independence, defined as 90-day modified Rankin Scale (mRS) ? 3. We used probit analysis and multivariate logistic regressions to evaluate the association of patients’ clinical factors and outcomes. We applied a machine learning algorithm (random forest, RF) to determine predictive factors for the different BPV indices during MT. Evaluation was performed with root-mean-square error (RMSE) and normalized-RMSE (nRMSE) metrics. We analyzed 375 patients with mean age (± standard deviation [SD]) of 65 (15) years. There were 234 (62%) patients with mRS ? 3. Univariate probit analysis demonstrated that BPV during MT was associated with poor functional independence. Multivariable logistic regression showed that age, admission National Institutes of Health Stroke Scale (NIHSS), mechanical ventilation, and thrombolysis in cerebral infarction (TICI) score (OR 0.42, 95% CI 0.17–0.98, P = 0.044) were significantly associated with outcome. RF analysis identified that the interval from last-known-well time-to-groin puncture, age, and mechanical ventilation were among important factors significantly associated with BPV. BPV during MT was associated with functional outcome in univariate probit analysis but not in multivariable regression analysis, however, NIHSS and TICI score were. RF algorithm identified risk factors influencing patients’ BPV during MT. While awaiting further studies’ results, clinicians should still monitor and avoid high BPV during thrombectomy while triaging AIS-LVO candidates quickly to MT. Graphical abstract: [Figure not available: see fulltext.]. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Background There is high variability in the clinical outcomes of patients with acute ischemic stroke (AIS) after mechanical thrombectomy (MT). Methods 217 consecutive patients with anterior circulation large vessel occlusion who underwent MT between August 2018 and January 2022 were analysed. The primary outcome was functional independence defined as a modified Rankin Scale score of 0-2 at 3 months. In the derivation cohort (August 2018 to December 2020), 7 ensemble ML models were trained on 70% of patients and tested on the remaining 30%. The model's performance was further validated on the temporal validation cohort (January 2021 to January 2022). The SHapley Additive exPlanations (SHAP) framework was applied to interpret the prediction model. Results Derivation analyses generated a 9-item score (PFCML-MT) comprising age, National Institutes of Health Stroke Scale score, collateral status, and postoperative laboratory indices (albumin-to-globulin ratio, estimated glomerular filtration rate, blood neutrophil count, C-reactive protein, albumin and serum glucose levels). The area under the curve was 0.87 for the test set and 0.84 for the temporal validation cohort. SHAP analysis further determined the thresholds for the top continuous features. This model has been translated into an online calculator that is freely available to the public (https://zhelvyao-123-60-sial5s.streamlitapp.com). Conclusions Using ML and readily available features, we developed an ML model that can potentially be used in clinical practice to generate real-time, accurate predictions of the outcome of patients with AIS treated with MT.  © 2023 BMJ Publishing Group. All rights reserved."
201,200,19,200_Lung cancer detection and diagnosis using deep learning and CT scans,Lung cancer detection and diagnosis using deep learning and CT scans,"The morbidity and mortality of lung cancer are increasing rapidly in every country in the world, and pulmonary nodules are the main symptoms of lung cancer in the early stage. If we can diagnose pulmonary nodules in time at the early stage and follow up and treat suspicious patients, we can effectively reduce the incidence of lung cancer. CT (Computed Tomography) has been applied to the screening of many diseases because of its high resolution. Pulmonary nodules show white round shadows in CT images. With the popularity of CT equipment, doctors need to review a large number of imaging results every day. Doctors will misjudge and miss the lesions because of reviewing CT scanning results for a long time. At this time, the method of automatic detection of pulmonary nodules by computer can relieve the pressure of doctors in reviewing CT scan results. Traditional lung nodule detection methods, such as gray threshold method and region growing method, divide the detection process into two steps: extracting candidate regions and eliminating false regions. In addition, the traditional detection method can only operate on a single image, which leads to the inability of this method to detect the batch scanning results in real time. With the continuous development of computer equipment performance and artificial intelligence, the relationship between medical image processing and deep learning is getting closer and closer. In deep learning, object detection methods such as Faster R-CNN?YOLO can complete parallel detection of batch images, and deep structure can fully extract the features of input images. Compared with traditional lung nodule detection methods, it has the characteristics of high efficiency and high precision. Faster R-CNN is a classical and high-precision two-stage object detection method. In this paper, an improved Faster R-CNN model is proposed. On the basis of Faster R-CNN, multi-scale training strategy is used to fully mine the features of different scale spaces and perform path augmentation on lower-dimensional features, which improves the small object detection ability of the model. Through Online Hard Example Mining (OHEM), the loss value is used to quantify the difficulty of candidate region detection, and the training times of the region to be detected are adaptively adjusted. Make full use of prior information to customize the size and proportion of preset boundary anchor boxes. Using deformable convolution to improve the visual field to enhance the global features and enhance the ability to extract the feature information of pulmonary nodules in the same scale space. The new model was tested on LUNA16 (Lung Nodule Analysis 2016) dataset. The detection precision of the improved Faster R-CNN model for pulmonary nodules increased from 76.4% to 90.7%, and the recall rate increased from 40.1% to 56.8% Compared with the mainstream object detection algorithms YOLOv3 and Cascade R-CNN, the improved model is superior to the above models in every index. © 2022 Elsevier Ltd,The CT images of Lung illnesses or diseases that damage the lungs and weaken the respiratory system. Lung cancer is one of the topmost causes of death in humans around the world. Humans have a better chance of surviving if they are detected early. The average survival rate of persons with lung cancer increases from 14 to 49 percent if the disease is detected early.While computed tomography (CT) is significantly more effective than X-ray, a complete diagnosis requires a combination of imaging techniques that complement each other. But, because there are multiple phases of cancer that develop into different types of tumors with varying sizes and risks, finding lung cancer does not predict the risk of cancer. A deep neural network is constructed and tested for detecting lung cancer CT images. This research work analyses different types of tumor sizes such as large cell carcinoma, normal, squamous cell carcinoma, and adenocarcinoma. Also, the lung tumors are detected and predicted with the help of computer vision methods such as Residual neural network (ResNet), Convolutional neural network (CNN). Finally, the results of all the methods are compared and various parameters were calculated. Thus, the proposed method (ResNet) gives an optimal solution on comparison with respect to all the parameters. © 2022 Lavoisier. All rights reserved.,Lung cancer (LC) remains a leading cause of death worldwide. Early diagnosis is critical to protect innocent human lives. Computed tomography (CT) scans are one of the primary imaging modalities for lung cancer diagnosis. However, manual CT scan analysis is time-consuming and prone to errors/not accurate. Considering these shortcomings, computational methods especially machine learning and deep learning algorithms are leveraged as an alternative to accelerate the accurate detection of CT scans as cancerous, and non-cancerous. In the present article, we proposed a novel transfer learning-based predictor called, Lung-EffNet for lung cancer classification. Lung-EffNet is built based on the architecture of EfficientNet and further modified by adding top layers in the classification head of the model. Lung-EffNet is evaluated by utilizing five variants of EfficientNet i.e., B0–B4. The experiments are conducted on the benchmark dataset “IQ-OTH/NCCD” for lung cancer patients grouped as benign, malignant, or normal based on the presence or absence of lung cancer. The class imbalance issue was handled through multiple data augmentation methods to overcome the biases. The developed model Lung-EffNet attained 99.10% of accuracy and a score of 0.97 to 0.99 of ROC on the test set. We compared the efficacy of the proposed fine-tuned pre-trained EfficientNet with other pre-trained CNN architectures. The predicted outcomes demonstrate that EfficientNetB1 based Lung-EffNet outperforms other CNNs in terms of both accuracy and efficiency. Moreover, it is faster and requires fewer parameters to train than other CNN based models, making it a good choice for large-scale deployment in clinical settings and a promising tool for automated lung cancer diagnosis from CT scan images. © 2023 The Authors"
202,201,19,201_Energetic Materials and their Properties,Energetic Materials and their Properties,"The aim of the present work is to show a simple quantitative theoretical study on the relaxation mechanisms of the 4F3/2 levels of the Nd3+ ions in the widely used meta-phosphate laser glass system (P2O5–Al2O3–BaO–K2O) for the studied concentration range of the Nd3+ ions from 0.10 × 1020 to 8.15 × 1020 ions/cm3. Auzel's diffusion-limited model for energy transfer has been adopted in the present study for estimating zero concentration level lifetime, ?0 of 4F3/2 levels in the Nd3+ ions present in the studied glass matrix. Primarily, the relaxation mechanisms are dominated by radiative decay (krad) and energy transfer to hydroxyl groups (kOH) present in the glass network structure up to the dopant, Nd3+ concentration 0.78 × 1020 ions/cm3. Later on, with increasing dopant ion concentration, fluorescence from trivalent Nd3+ ions is unfortunately quenched by the interaction between Nd3+ ions, also known as concentration quenching. The concentration quenching of the fluorescence lifetime has been analysed using proposed Inokuti-Hirayama (I–H), Yokota-Tanimoto (Y-T) and Brushtein (B) models and found to be migration-assisted cross-relaxation mechanisms at or beyond 1.47 × 1020 ions/cm3 dopant concentration. The diffusion coefficient, D derived from the Y-T model and the energy migration rate, Wm attained from the B model has demonstrated diffusion-based energy migration initially at about >1.47 × 1020 ions/cm3 Nd3+ concentration and after that, it has switched to a hopping mechanism with further increasing ion concentration, >2.97 × 1020 ions/cm3. Critical quenching concentration, Q and critical radius, R0 for the studied glass composition have also been determined and correlated to the experimental results. No evidence for ion clustering has been found within the studied dopant ion concentration range (0.10 × 1020 - 8.15 × 1020) ions/cm3 with respect to the estimated R0. Finally, the contribution of the individual relaxation process to the experimentally recorded emission spectra and measured lifetime, ?exp has been investigated in order to identify the most suitable Nd3+ ion concentration for developing large-aperture high-energy/high-peak-power lasers and optical amplifiers. Therefore, the present work is expected to be significant in aiding to formulate an effective glass composition for construction of compact high-power lasers and optical amplifiers. © 2023 Elsevier B.V.,The current rise in performance prediction techniques for energetic compounds provides the possibility of prejudging the effectiveness of derived patterns. In this work, 2,4-diamino-6-chloropyrimidine was used as the precursor, and the three possible derived pathways were systematically explored (guest oxidant, nitration, and N-oxidation). Theoretical calculations were adopted to predict the energy and stability parameters of possible products. The calculations indicated that constructing bridged-ring energetic molecules with N-oxide and nitro group is an effective way to balance energy and safety. Based on this protocol, we synthesized a series of pyrimidine-based energetic molecules within three steps and tested and analyzed their physicochemical properties, verifying the consistency between experimental results and theoretical predictions. This work provides a research model for determining the feasibility and effectiveness of the derivative pathway based on a specific energetic compound precursor and can offer guidance for the directed and large-scale synthesis of high-energy and low-sensitivity explosive molecules. © 2023 Elsevier Ltd,The deuteration of energetic materials contributes to high signal-to-noise ratios (SNRs) in neutron diffraction, thus allowing the structures of energetic materials to be effectively investigated. This study developed the synthesis methods of deuterated energetic materials through chemical synthesis or newly developed one-pot H/D exchange. Using these methods, it synthesized nine deuterated energetic materials in a concise and low-cost manner: deuterated 1,3,5-triamino-2,4,6-trinitrobenzene (TATB-d6, 1), 1,3,5,7-tetranitro-1,3,5,7-tetraazacyclooctane (HMX-d8, 2), 1,3,5-trinitro-1,3,5-triazacyclohexane (RDX-d6, 3), dihydroxylammonium 5,5?-bis(tetrazole-1-oate) (TKX-50-d8, 4), nitroguanidine (NQ-d4, 5), 1,1-diamino-2,2-dinitroethylene (FOX-7-d4, 6), 2,6-diamino-3,5-dinitropyrazine-1-oxide (LLM-105-d4, 7), trinitrotoluene (TNT-d3, 8), and 3-nitro-1,2,4-triazol-5-one (NTO-d2, 9). Furthermore, the single crystals of HMX-d8 (2) and RDX-d6 (3) were obtained, and the ?-, ?-, ?-, and ?-polymorphs of HMX-d8 (2) were prepared accordingly. The deuterated energetic materials were characterized and analyzed using infrared spectroscopy (IR), nuclear magnetic resonance (NMR) spectroscopy, differential scanning calorimetry (DSC), thermogravimetry (TG), X-ray diffraction (XRD), and neutron diffraction. Besides, this study determined the decomposition activation energy (Ea), pre-exponential factor (A), decomposition rate constant (k), and critical explosion temperature (Tb) of TATB-d6 (1), HMX-d8 (2), and RDX-d6 (3) via DSC experiments at different heating rates. The NMR and neutron diffraction data show that these deuterated energetic materials have high deuteration rates of more than 95%. The DSC and TG analyses indicate that the deuterated energetic materials exhibit slightly higher decomposition temperatures than their nondeuterated counterparts. Furthermore, neutron diffraction shows that the deuterated energetic materials feature high SNRs. © 2023 The Authors"
203,202,19,202_Catalytic surfaces and computational approaches,Catalytic surfaces and computational approaches,"As a zero-carbon fuel, hydrogen can be produced via electrochemical water splitting using clean electric energy by the hydrogen evolution reaction (HER) process. The ultimate goal of HER catalyst is to replace the expensive Pt metal benchmark with a cheap one with equivalent activities. In this work, we investigated the possibility of HER process on single-atom catalysts (SACs) doped on two-dimensional (2D) GaPS4 materials, which have a large intrinsic band gap that can be regulated by doping and tensile strain. Based on the machine learning regression analysis, we can expand the prediction of HER performance to more catalysts without expensive DFT calculation. The electron affinity and first ionization energy are the two most important descriptors related to the HER behavior. Furthermore, constrain molecular dynamics with solvation models and constant potentials were applied to understand the dynamics barrier of HER process of Pt SAC on GaPS4 materials. These findings not only provide important insights into the catalytic properties of single-atom catalysts on GaPS4 2D materials, but also provides theoretical guidance paradigm for exploration of new catalysts. © 2023 SOCIETY,Few-atom catalysts, due to the unique coordination structure compared to metal particles and single-atom catalysts, have the potential to be applied for efficient electrochemical CO2 reduction (CRR). In this study, we designed a class of triple-atom A2B catalysts, with two A metal atoms and one B metal atom either horizontally or vertically embedded in the nitrogen-doped graphene plane. Metals A and B were selected from 17 elements across 3d to 5d transition metals. The structural stability and CRR activity of the 257 constructed A2B catalysts were evaluated. The active-learning approach was applied to predict the adsorption site of key reaction intermediate *CO, which only used 40% computing resources in comparison to “brute force” calculation and greatly accelerated the large amount of computation brought by the large number of A2B catalysts. Our results reveal that these triple atom catalysts can selectively produce more valuable hydrocarbon products while preserving high reactivity. Additionally, six triple-atom catalysts were proposed as potential CRR catalysts. These findings provide a theoretical understanding of the experimentally synthesized Fe3 and Ru3-N4 catalysts and lay a foundation for future discovery of few-atom catalysts and carbon materials in other applications. A new machine learning method, masked energy model, was also proposed which outperforms existing methods by approximately 5% when predicting low-coverage adsorption sites. © 2023 The Authors,Two-dimensional materials with active sites are expected to replace platinum as large-scale hydrogen production catalysts. However, the rapid discovery of excellent two-dimensional hydrogen evolution reaction catalysts is seriously hindered due to the long experiment cycle and the huge cost of high-throughput calculations of adsorption energies. Considering that the traditional regression models cannot consider all the potential sites on the surface of catalysts, we use a deep learning method with crystal graph convolutional neural networks to accelerate the discovery of high-performance two-dimensional hydrogen evolution reaction catalysts from two-dimensional materials database, with the prediction accuracy as high as 95.2%. The proposed method considers all active sites, screens out 38 high performance catalysts from 6,531 two-dimensional materials, predicts their adsorption energies at different active sites, and determines the potential strongest adsorption sites. The prediction accuracy of the two-dimensional hydrogen evolution reaction catalysts screening strategy proposed in this work is at the density-functional-theory level, but the prediction speed is 10.19 years ahead of the high-throughput screening, demonstrating the capability of crystal graph convolutional neural networks-deep learning method for efficiently discovering high-performance new structures over a wide catalytic materials space. © 2021 Zhengzhou University."
204,203,19,203_Deep learning methods for fault diagnosis in industrial production environments,Deep learning methods for fault diagnosis in industrial production environments,"The fault diagnosis method based on generative adversarial networks (GANs) has been successfully applied to the early fault detection of motor bearings, and it has effectively solved the problems of small samples, unlabeled sample features, and data imbalance in early faults. The existing methods, however, ignore the weak features of bearing vibration signals when solving the problem of missing early fault samples, which seriously affects the accuracy of diagnosis results. In addition, a large amount of human parameter adjustment work is required during the network model training process. This makes the whole diagnosis process lack objectivity and intelligence. To address these issues, this article proposes an intelligent diagnosis algorithm for early faults of motor bearings based on Wasserstein GAN meta learning (WGANML). First, the method performs feature extraction on the collected bearing vibration signals. Then, the generation of missing samples and the enhancement of weak features are completed by using the game between the generative and discriminative models. Second, to get rid of manual intervention, meta learning (ML) is applied to Wasserstein distance GANs (WGANs) parameter variation training for the first time. Finally, the feasibility and effectiveness of the proposed WGANML algorithm are proved by the open dataset of Case Western Reserve University (CWRU) and the data of the motor bearing fault experimental platform. In addition, compared with the existing advanced methods, the superiority of the proposed method in motor bearing early fault diagnosis is further verified.  © 1963-2012 IEEE.,Purpose: Due to the influence of external factors such as noise, the operating conditions of rotating machinery in actual operation are not constant, and it is difficult to obtain high diagnostic accuracy using differently distributed training data and test data for fault diagnosis. Methods: To solve the problem of fault diagnosis under variable working conditions, we come up with a fault diagnosis approach on the account of Inception V1 module with self-attention mechanism for domain adversarial transfer network (IS-DATN). First, a feature extractor based on the Inception V1 module and the self-attention mechanism is constructed to learn domain-invariant features from the training data of the source and target domains; then, the feature extractor and the domain discriminator are trained using an adversarial training strategy to optimize the performance of both, and the labeled classifier is trained to enable accurate fault identification; finally, the test data are fed into the model, and the labeled classifier can accurately assign the unlabeled target domain data to each category, which enables fault diagnosis under variable running requirements. Results and Conclusion: The experiments in this article use the Case Western Reserve University and Laboratory self-test rolling bearing dataset and show that the presented approach can reduce the domain distribution differences, and obtain 95.43% diagnostic accuracy in case of a large difference in working conditions and 100% diagnostic accuracy in case of similar working conditions, compared with other methods, the proposed method has better transfer effect and higher diagnostic accuracy. © Krishtel eMaging Solutions Private Limited 2022.,Data-driven fault diagnosis techniques utilizing deep learning have achieved widespread success. However, their diagnostic capability and application possibility are significantly reduced in real-world scenarios where fault modes are not fully covered and labels are lacking. Owing to potential conflicts of interest and legal risks, industrial equipment fault data usually exist in the form of isolated islands, making it difficult to carry out large-scale centralized model training. This paper proposes open-set federated adversarial domain adaptation (OS-FADA) to achieve collaborative evolution of fault diagnosis capabilities among cross-domain data owners while protecting privacy. The OS-FADA is a general fault diagnosis framework that employs two-phase adversarial learning. First, faced with the data distribution shift caused by variable working conditions, a generative adversarial feature extractor training strategy is designed to achieve domain-invariant fault feature extraction by approximating the feature distributions of clients to a unified generated distribution. Second, considering the label distribution shift of unknown faults occurring in the target client, an adversarial learning method is proposed to establish decision boundaries between known and unknown faults. Ultimately, the co-evolution of fault diagnosis models between clients is achieved by combining two-phase adversarial learning and federated aggregation. Results from an industrial gearbox case demonstrate that our proposed method achieves over 20% diagnostic accuracy improvement and has excellent potential for cross-domain fault diagnosis tasks with unknown faults when the data silos problem cannot be ignored. © 2023 IOP Publishing Ltd."
205,204,18,204_Optical Fiber Sensing and Communication with Deep Learning-Based Techniques.,Optical Fiber Sensing and Communication with Deep Learning-Based Techniques.,"The main difficulties of using the deep learning model to decouple the orbital angular momentum (OAM) in Free-Space Optical (FSO) communication are that the model requires a large number of training data sets and the model's convergence speed is low. In this paper, transfer learning and depthwise separable convolution are combined to improve the computational speed of the model and to reduce the requirement of training data set size. The recognition accuracies of 4-OAM and 8-OAM based on the measured data with noise are studied respectively and the OAM transmission in atmospheric turbulence are simulated to test the robustness of the model. In addition, the proposed method can be trained on the expanded data set of 38 experimental data collection, and the test classification results can reach 99.5%. Meanwhile, the minimum accuracy of the model in testing data of different transmission distances and turbulence intensities is 81.25%, indicating good robustness. The paper's work on the OAM pattern detection has great significance. © 2023 Elsevier B.V.,Orbital angular momentum (OAM) has recently obtained tremendous research interest in free-space optical communications (FSO). During signal transmission within the free-space link, atmospheric turbulence (AT) poses a significant challenge as it diminishes the signal strength and introduce intermodal crosstalk, significantly reducing OAM mode detection accuracy. This issue directly impacts the performance of OAM-based communication systems and leads to a reduction in received information. To address this critical bottleneck of low mode recognition accuracy in OAM-based FSO-communications, a deep learning method based on vision transformers (ViT) is proposed for what we believe is for the first time. Designed carefully by numerous experts, the advanced self-attention mechanism of ViT captures more global information from the input image. To train the model, pretraining on a large dataset, named IMAGENET is conducted. Subsequently, we performed fine-tuning on our specific dataset, consisting of OAM beams that have undergone varying AT strengths. The computer simulation shows that based on ViT method, the multiple OAM modes can be recognized with a high accuracy (nearly 100%) under weak-to-moderate turbulence and with almost 98% accuracy even under long transmission distance with strong turbulence (CN2 = 1 × 10?14). Our findings highlight that leveraging ViT enables robust detection of complex OAM beams, mitigating the adverse effects caused by atmospheric turbulence. © 2023 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.,Nonlinear impairment in a high-speed orbital angular momentum (OAM) mode-division multiplexing (MDM) optical fiber communication system presents high complexity and strong stochasticity due to the massive optoelectronic devices. In this paper, we propose an Affinity Network (AffinityNet) nonlinear equalizer for an OAM-MDM intensity-modulation direct-detection (IM/DD) transmission with four OAM modes. The labeled training and testing signals from the OAM-MDM system can be regarded as “small sample” and “large target”, respectively. AffinityNet can be used to build an accurate nonlinear model using “small sample” based on few-shot learning and can predict the stochastic characteristic nonlinearity of OAM-MDM with a high level of generalization. As a result, the AffinityNet nonlinear equalizer can effectively compensate the stochastic nonlinearity in the OAM-MDM system, despite the large difference between the training and testing signals due to the stochastic nonlinear impairment. An experiment was conducted on a 400 Gbit/s transmission with four OAM modes using a pulse amplitude modulation-8 (PAM-8) signal over a 2 km ring-core fiber (RCF). Our experimental results show that the proposed nonlinear equalizer outperformed the conventional Volterra equalizer with improvements in receiver sensitivity of 1.7, 1.8, 3, and 3.3 dB for the four OAM modes at the 15% forward error correction (FEC) threshold, respectively. In addition, the proposed equalizer outperformed a convolutional neural network (CNN) equalizer with improvements in receiver sensitivity of 0.8, 0.5, 0.9, and 1.4 dB for the four OAM modes at the 15% FEC threshold. In the experiment, a complexity reduction of 37% and 83% of the AffinityNet equalizer is taken compared to the conventional Volterra equalizer and CNN equalizer, respectively. The proposed equalizer is a promising candidate for a high-speed OAM-MDM optical fiber communication system. © 2023 OSA - The Optical Society. All rights reserved."
206,205,18,205_Monocular Camera Pose Estimation and Localization in Challenging Environments,Monocular Camera Pose Estimation and Localization in Challenging Environments,"Efficient localization plays a significant role in mobile autonomous robots' navigation systems. Traditional visual simultaneous localization systems based on point feature matching suffer from two shortcomings. First one is that the method of tracking features is not robust for environments with frequent changes in brightness. Another one is the large of consecutive visual keyframes can consume expensive computational and storage resources in complex environments. To solve these problems, an end-to-end real-time six degrees of freedom object pose estimation algorithm is proposed to solve the robust and efficient challenges through a deep learning model. First, preprocessing operations such as cropping, averaging, and timestamp alignment are performed on datasets to reduce computational cost and time. Second, the processed dataset is fed into our neural network model to extract the most effective features for matching. Finally, the robot's current 3D translation and 4D angle information are predicted and output to achieve an end-to-end localization system. A broad range of experiments are performed on both indoor and outdoor datasets. The experimental results demonstrate that the translation and orientation accuracy in outdoor scenes improved by 32.9% and 31.4%, respectively. The average improvement of localization accuracy in indoor scenes is 38.4%, and the angle improvement is 13.1%. Moreover, the effectiveness of predicting the global motion trajectories of sequential images algorithm has been verified and is superior to other convolutional neural network methods.  © 2023 Niansheng Chen et al.,— Estimating monocular depth and ego-motion via unsupervised learning has emerged as a promising approach in autonomous driving, mobile robots, and augmented reality (AR)/VR applications. It avoids intensive efforts to collect a large amount of ground truth and further improves the scene construction density and long-term tracking accuracy in simultaneous localization and mapping (SLAM) systems. However, existing approaches are susceptible to illumination variations and blurry pictures due to fast movements in real-world driving scenarios. In this article, we propose a novel unsupervised learning framework to fuse the complementary strength of visual and inertial measurements for monocular depth estimation. It learns both forward and backward inertial sequences at multiple subspaces to produce environment-independent and scale-consistent motion features and selectively weights inertial and visual modalities to adapt to various scenes and motion states. In addition, we explore a novel virtual stereo model to adopt such depth estimates in the monocular SLAM system, thus improving the system efficiency and accuracy. Extensive experiments on the KITTI, EuRoC, and TUM datasets have shown our effectiveness in terms of monocular depth estimation, SLAM initialization efficiency, and pose estimation accuracy compared with the state-of-the-art. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.,Camera pose estimation has long relied on geometry-based approaches and sparse 2D-3D keypoint correspondences. With the advent of deep learning methods, the estimation of camera pose parameters, i.e., the six parameters that describe position and rotation denoted by 6 Degrees of Freedom (6-DoF), has decreased from tens of meters to a few centimeters in median error for indoor applications. For outdoor applications, errors can be quite large and highly dependent on the variations in occlusion, contrast, brightness, repetitive structures, or blur introduced by camera motion. To address these limitations, we introduce, B-Pose, a Bayesian Convolutional deep network capable of not only automatically estimating the camera's pose parameters from a single RGB image but also provides a measure of uncertainty in the parameter estimation. Reported experiments on outdoor and indoor datasets demonstrate that B-Pose outperforms SOTA techniques and generalizes better to unseen RGB images. A strong correlation is shown between the prediction error and the model's uncertainty, indicating that the prediction is almost always incorrect whenever the model's uncertainty is high. © 2016 IEEE."
207,206,18,206_TBM Excavation and Tunnel Settlement,TBM Excavation and Tunnel Settlement,"Accurately predicting tunnel boring machine (TBM) performance is beneficial for excavation efficiency enhancement and risk mitigation of TBM tunneling. In this paper, we develop a long short-term memory (LSTM) based hybrid intelligent model to predict two key TBM performance parameters (advance rate and cutterhead torque). The model combines the LSTM, BN, Dropout and Dense layers to process the raw data and improve the fitting quality. The features, including the ground formation properties, tunnel route curvature, tunnel location and TBM operational parameters, are divided into historical/real-time time-varying parameters, time-invariant parameters and historical/real-time output prediction data. The effectiveness of the proposed model is verified based on a large monitoring database of the Baimang River Tunnel Project in Shenzhen, south China. We then discuss the influence of the prediction mode, neural network structure and time division interval length of historical data on the prediction accuracy. The significance evaluation of input features shows that the historical output prediction has the largest influence on the prediction accuracy, and the influence of ground properties is secondary. It is also found that the correlations between input features and the output prediction are coincident with their interrelationships with the ground properties and ease of TBM excavation. Finally, it is found that the prediction results are most affected by the total propulsion force followed by the rotation speed of the cutterhead. The established model can provide useful guidance for construction personnel to roughly grasp the possible TBM status from the prediction results when adjusting the operational parameters. © 2023 Tongji University,Tunnel Boring Machines (TBMs) are large-scale excavation tools used commonly in transportation tunnel construction. While tunnelling, TBMs generate data at large scales, often at levels difficult to parse using traditional statistical techniques. Utilising this data can be highly beneficial to obtain a better understanding of TBM excavation conditions, and such understanding can be applied to machine and technique selection. This paper presents a novel method for sequential estimation of geological composition using advanced machine learning algorithms and the data collected from TBMs. In this approach, we use Recurrent Neural Networks and Long Short-Term Memory (RNN-LSTM) models as a hybrid machine learning algorithm for processing sequential and time-series data. The results from an excavation case study demonstrate that the proposed method is an effective approach for sequential estimation of geological composition as encountered by TBM during operation. It is worth noting that TBM data captures the signature of the ground, and the developed model in this study was successful in predicting the ground geological composition even without using the borehole data. © 2023 Informa UK Limited, trading as Taylor & Francis Group.,Surface settlement due to tunnel excavation is affected by several factors. However, no explicit mapping correlation exists between surface settlement and the main influencing factors. In this study, three tree-based methodologies, including classification and regression tree (CART), random forest (RF), and gradient boosting decision tree (GBRT), were implemented to predict the tunneling-induced surface settlement of the South Hong-Mei Road tunnel in Shanghai, where a large mix-shield was used. Thirteen influencing factors within three categories (tunnel geometry, geological conditions, and shield operation factors) were employed as input variables. Results show that the ensemble methods (RF and GBDT) provide superior performance over the single-tree model (CART). Moreover, GBDT has the highest level of prediction accuracy among the three statistical learning methods. The importance of influencing factors on the tunneling-induced surface settlement was probed. The tunnel geometry had the greatest effect on surface settlement. It is followed by the influencing factors in shield operation factors. Moreover, geological conditions were not as influential as the other influencing factors. The outcomes of this study may provide a reference for evaluating tunneling-induced surface settlement in other similar tunnel projects. © 2023"
208,207,18,207_Industry 4.0 compliant biometric recognition techniques using CNN for various traits,Industry 4.0 compliant biometric recognition techniques using CNN for various traits,"Finger-vein biometrics has been extensively investigated for personal verification. Single sample per person (SSPP) finger-vein recognition is one of the open issues in finger-vein recognition. Despite recent advances in deep neural networks for finger-vein recognition, current approaches depend on a large number of training data. However, they lack the robustness of extracting robust and discriminative finger-vein features from a single training image sample. A deep ensemble learning method is proposed to solve the SSPP finger-vein recognition in this article. In the proposed method, multiple feature maps were generated from an input finger-vein image, based on various independent deep learning-based classifiers. A shared learning scheme is investigated among classifiers to improve their feature representation captivity. The learning speed of weak classifiers is also adjusted to achieve the simultaneously best performance. A deep learning model is proposed by an ensemble of all these adjusted classifiers. The proposed method is tested with two public finger vein databases. The result shows that the proposed approach has a distinct advantage over all the other tested popular solutions for the SSPP problem. Copyright © 2023 Liu, Qin, Song, Yan and Luo.,Finger vein recognition is an advanced biometric recognition technology that offers high precision and high security. It recognizes or authenticates individuals using irradiating vein texture images collected from fingers with near-infrared light. In this paper, we propose a new finger vein recognition model (MMRAN) based on a multiscale and multistage residual attention network. First, to fully adapt to the low-resolution, grayscale pixels, and linear patterns of finger vein images, we designed an architecture that combines a fusion residual attention block (FRAB) and a multistage residual attention connection (MRAC). The FRAB contains two distinct subpaths: the main vein path (MVP) and the guided attention path (GAP). The MVP extracts finger vein features at multiple scales by using a multibranch residual structure, while the GAP uses an hourglass network to generate weight maps to guide the setting of eigenvalues at corresponding locations in the main vein feature map. MRAC integrates venous features extracted at different learning stages through the above two pathways. The proposed multiscale and multistage extraction model is effective at extracting various types of digital vein features, including those whose shapes change in width, direction, curvature, and so on. We combine the various dimensions of vein features through multistage learning to further improve the model performance for extracting high-level abstract features. To evaluate the performance of our proposed model, we conducted a large number of experiments on five publicly available finger vein datasets. The experimental results show that the proposed model not only achieves a recognition accuracy above 98%, which is an improvement compared to the current state-of-the-art methods, but it can also be implemented with fewer parameters, which improves training and inference. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The usage of biometric identification has increased in recent years, with numerous public and commercial organizations incorporating biometric technologies into their infrastructures. One of the technologies is iris recognition which has been used as a biometric recognition compared to other modalities to combat identity abuse due to its ability to eliminate risk of collisions or false matches even when comparing large populations. The use of CNN is proven to provide high accuracy; however, this technology involves the need for a large dataset and higher computational cost. Therefore, this study uses a combined model of Convolutional Neural Network (CNN) and Vision Transformer (ViT) in identifying and verifying an iris image. By using the proposed learning rate, it proves that the novel hybrid model is capable to achieve up to 93.66% accuracy in recognizing iris images. The cross-entropy loss function was implemented to reduce the loss and it was able to predict the class label more correctly. In addition, the model was thoroughly tested on three publicly available iris databases, achieving satisfactory iris recognition results. Furthermore, this model has the potential to be used in other biometrics such as face and retina recognitions. © 2024, Semarak Ilmu Publishing. All rights reserved."
209,208,18,208_Remote Sensing Image Segmentation using Attention-based Methods,Remote Sensing Image Segmentation using Attention-based Methods,"Although deep learning-based methods for semantic segmentation have achieved prominent performance in the general image domain, semantic segmentation for high-resolution remote sensing images remains highly challenging. One challenge is the large image size. High-resolution remote sensing images can have very high spatial resolution, resulting in images with hundreds of millions of pixels. This makes it difficult for deep learning models to process the images efficiently, as they typically require large amounts of memory and computational resources. Another challenge is the complexity of the objects and scenes in the images. High-resolution remote sensing images often contain a wide variety of objects, such as buildings, roads, trees, and water bodies, with complex shapes and textures. This requires deep learning models to be able to capture a wide range of features and patterns to segment the objects accurately. Moreover, remote sensing images can suffer from various types of noise and distortions, such as atmospheric effects, shadows, and sensor noises, which can also increase difficulty in segmentation tasks. To deal with the aforementioned challenges, we propose a new, mixed deep learning model for semantic segmentation on high-resolution remote sensing images. Our proposed model adopts our newly designed local channel spatial attention, multi-scale attention, and 16-piece local channel spatial attention to effectively extract informative multi-scale features and improve object boundary discrimination. Experimental results with two public benchmark datasets show that our model can indeed improve overall accuracy and compete with several state-of-the-art methods. © 2023 by the authors.,Semantic segmentation of high-resolution remote sensing images has emerged as one of the foci of research in the remote sensing field, which can accurately identify objects on the ground and determine their localization. In contrast, the traditional deep learning-based semantic segmentation, on the other hand, requires a large amount of annotated data, which is unsuitable for high-resolution remote sensing tasks with limited resources. It is therefore important to build a semantic segmentation method for high-resolution remote sensing images. In this paper, it is proposed an improved U-Net model based on transfer learning to solve the semantic segmentation problem of high-resolution remote sensing images. The model is based on the symmetric encoder–decoder structure of U-Net. For the encoder, transfer learning is applied and VGG16 is used as the backbone of the feature extraction network, and in the decoder, after upsampling using bilinear interpolation, it is performed multiscale fusion with the feature maps of the corresponding layers of the encoder in turn and is finally obtained the predicted value of each pixel to achieve precise localization. To verify the efficacy of the proposed network, experiments are performed on the ISPRS Vaihingen dataset. The experiments show that the applied method has achieved high-quality semantic segmentation results on the high-resolution remote sensing dataset, and the MIoU is 1.70%, 2.20%, and 2.33% higher on the training, validation, and test sets, respectively, and the IoU is 4.26%, 6.89%, and 5.44% higher for the automotive category compared to the traditional U-Net. © 2023, The Author(s).,There are many problems with remote sensing images, such as large data scales, complex illumination conditions, occlusion, and dense targets. The existing semantic segmentation methods for remote sensing images are not accurate enough for small and irregular target segmentation results, and the edge extraction results are poor. The authors propose a remote sensing image segmentation method based on a DCNN and multiscale feature fusion. Firstly, an end-to-end remote sensing image segmentation model using complete residual connection and multiscale feature fusion was designed based on a deep convolutional encoder–decoder network. Secondly, weighted high-level features were obtained using an attention mechanism, which better preserved the edges, texture, and other information of remote sensing images. The experimental results on ISPRS Potsdam and Urban Drone datasets show that compared with the comparison methods, this method has better segmentation effect on small and irregular objects and achieves the best segmentation performance while ensuring the computation speed. © 2023 IGI Global. All rights reserved."
210,209,18,209_Depression detection and mental health support on social media.,Depression detection and mental health support on social media.,"The recent coronavirus disease (COVID-19) has become a pandemic and has affected the entire globe. During the pandemic, we have observed a spike in cases related to mental health, such as anxiety, stress, and depression. Depression significantly influences most diseases worldwide, making it difficult to detect mental health conditions in people due to unawareness and unwillingness to consult a doctor. However, nowadays, people extensively use online social media platforms to express their emotions and thoughts. Hence, social media platforms are now becoming a large data source that can be utilized for detecting depression and mental illness. However, the existing approaches often overlook data sparsity in tweets and the multimodal aspects of social media. In this article, we propose a novel multimodal framework that combines textual, user-specific, and image analysis to detect depression among social media users. To provide enough context about the user&#x2019;s emotional state, we propose the following: 1) an extrinsic feature by harnessing the URLs present in tweets and 2) extracting textual content present in images posted in tweets. We also extract five sets of features belonging to different modalities to describe a user. In addition, we introduce a deep learning model, the visual neural network (VNN), to generate embeddings of user-posted images, which are used to create the visual feature vector for prediction. We contribute a curated COVID-19 dataset of depressed and nondepressed users for research purposes and demonstrate the effectiveness of our model in detecting depression during the COVID-19 outbreak. Our model outperforms the existing state-of-the-art methods over a benchmark dataset by 2%&#x2013;8% and produces promising results on the COVID-19 dataset. Our analysis highlights the impact of each modality and provides valuable insights into users&#x2019; mental and emotional states. IEEE,Human mental health (HMH) is a pervasive and impactful condition that profoundly affects an individual’s cognitive, emotional, and behavioural aspects in a negative manner. Among various mental health disorders, depression is particularly prevalent, with approximately 20% of women experiencing at least one depressive episode during their lifetime. Identifying depression early on is crucial for timely intervention and support. This study examines user-generated content from major social platforms like Twitter, Facebook, and Instagram, aiming to detect potential signs of depression through behavioural symptoms such as mood changes, loss of interest, altered sleep patterns, focus difficulties, and impaired decision-making. Leveraging natural language processing and machine learning, sentiment analysis deciphers emotional context in posts and comments. A new efficient methodology utilizing Bidirectional Encoder Representations from Transformers (BERT) is proposed for efficient analysis of the posts and comments. Knowledge distillation transfers insights from a large BERT model to a smaller one, enhancing accuracy. Integrating word2vec and BERT with bidirectional long short-term memory (Bi-LSTM), the approach effectively analyses depression and anxiety indicators in social media data. Comparative assessments highlight the system’s excellence, achieving a remarkable 98.5% accuracy through knowledge distillation. The proposed methodology marks a substantial stride in identifying mental health signals from social media, facilitating better early intervention and support for those facing depression and anxiety-related challenges. © 2023 by author(s).,The mental health of college students is a growing concern, and gauging the mental health needs of college students is difficult to assess in real-time and in scale. To address this gap, researchers and practitioners have encouraged the use of passive technologies. Social media is one such ""passive sensor"" that has shown potential as a viable ""passive sensor"" of mental health. However, the construct validity and in-practice reliability of computational assessments of mental health constructs with social media data remain largely unexplored. Towards this goal, we study how assessing the mental health of college students using social media data correspond with ground-truth data of on-campus mental health consultations. For a large U.S. public university, we obtained ground-truth data of on-campus mental health consultations between 2011–2016, and collected 66,000 posts from the university’s Reddit community. We adopted machine learning and natural language methodologies to measure symptomatic mental health expressions of depression, anxiety, stress, suicidal ideation, and psychosis on the social media data. Seasonal auto-regressive integrated moving average (SARIMA) models of forecasting on-campus mental health consultations showed that incorporating social media data led to predictions with r = 0.86 and SMAPE = 13.30, outperforming models without social media data by 41%. Our language analyses revealed that social media discussions during high mental health consultations months consisted of discussions on academics and career, whereas months of low mental health consultations saliently show expressions of positive affect, collective identity, and socialization. This study reveals that social media data can improve our understanding of college students’ mental health, particularly their mental health treatment needs. © 2022, The Author(s)."
211,210,18,"210_The label for this topic is ""No abstract available"".","The label for this topic is ""No abstract available"".","[No abstract available],[No abstract available],[No abstract available]"
212,211,18,211_Urban Heat Island Effects,Urban Heat Island Effects,"Large-scale long-period urban heat island (UHI) intensity (UHII) prediction with high spatiotemporal resolutions, satisfactory accuracy, and calculation efficiency is crucial and challenging for UHI mitigation studies. This study proposes a framework combining an urban weather generator (UWG), local climate zone (LCZ), deep-learning method, and Python automatic cyclical calculation to obtain hourly UHII throughout one year and over a total of 1920 blocks in Hangzhou City. The spatial-averaged hourly UHII are between ?2 °C and 6 °C in more than 96 % time, and those at 0 °C–1 °C contribute to 43.80 % of the total time. Spring gives the most intense nocturnal UHII, while winter has the weakest one. A significant diurnal UCI phenomenon could accompany the strong nocturnal UHI phenomenon. From the synthetic (based on seasonal data) 24-h curves, mean UHII and UCII (average of the positive and negative values, respectively) drop by approximately 25 % in winter compared to spring. Spatially, UHII is higher in the central regions with compact buildings but significantly lower in the high-vegetation-coverage regions. Based on LCZ framework, LCZ 1 (compact high-rise configurations) has the highest UHII, independent of examined periods. UHII relations to building coverage, vegetation ratio, and building height are individually quantified. The building coverage has the highest influence on annual UHII, with a correlation coefficient as high as 0.76. Results indicate that, for UHI mitigation purposes, the percentage of compact high-rise configurations (LCZ 1) in the urban area shall be limited, and the vegetation ratio is better to be greater than 20 %. © 2023 Elsevier Ltd,This study presents a fully reproducible clustering-based methodology for the assessment of the urban heat island intensity (UHII) at the territory scale, using parametric microclimate models and limited computational resources. In large-scale climate modeling, a common preliminary operation is to utilize the well-established Local Climate Zone classification to characterize the thermal response of urban areas based on morphology. With the increasing availability of urban datasets, data-driven approaches can be implemented to quantitatively derive meaningful urban features without relying on a standardized classification. The proposed methodology employs a Gaussian Mixture Model clustering algorithm to partition the urban territory into a suitable number of homogeneous microclimate zones, enabling the calculation and mapping of the UHII for each zone through the Urban Weather Generator (UWG) tool. The developed approach is applied to the Canton of Geneva, Switzerland, identifying ten microclimatic areas and analyzing the spatiotemporal variation of UHII. Results show yearly average values of UHII ranging from 1.7 °C to 2.2 °C, depending on urban morphology. The simulated values are partially validated by comparison with on-site measurements from two urban weather stations, yielding a satisfactory agreement. The methodology can support urban planning with the goal of avoid overheating through a large-scale mapping. © 2023,Estimating urban evapotranspiration (ET) is of great significance for urban water resource allocation and assessing the urban heat island effect. However, most current urban ET models are based on the energy balance theory to estimate urban ET. These models lead to significant errors in urban ET simulation due to the surface heterogeneity and the existence of anthropogenic heat fluxes in urban areas. To solve this issue, this study proposes a modified machine learning-based urban ET method that can estimate urban ET at the site and regional scales. To better characterize the heterogeneity of urban surfaces, the flux footprint of in-situ ET and physical mechanism of ET process are integrated into the convolutional neural network (CNN) model. The modified CNN model is tested in a fast-developing city: Shenzhen, China based on two Eddy Correlation (EC) observations. The verification results indicated that coupling flux footprint and physical mechanism into the CNN model could effectively improve the accuracy of urban ET simulation at the site scale. The modified CNN model significantly reduced the root-mean-square-error (RMSE) of 25.8 W/m2 and increased the determination coefficient (R2) of 0.17 compared to the CNN-O model (The CNN-O model is defined as the CNN model do not integrate flux footprint and physical mechanism of ET). Further analyses suggested that fusing flux footprint data into a machine learning model helps enhance ET estimation in regions with high heterogeneity and highly variable wind directions. Moreover, the integration of physical mechanisms significantly enhanced the model capability to simulate extreme ET events. The modified CNN model is further applied to map the spatial distribution of urban ET and reconstruct long-term urban ET changes. The spatial pattern of urban ET exhibited large spatial variability, where the urban ET in water bodies (mean ?ET larger than 480 W/m2) and vegetation-covered areas (mean ?ET larger than 260 W/m2) are substantially higher than the impervious surfaces (mean ?ET less than 30 W/m2). Long-term trend analyses demonstrated that urbanization resulted in decline in urban ET. The average decreasing rate of urban ET is 1.61 mm/yr (P < 0.05), with a 18 % decrease relative to the long-term ET average. The leading causes for the decline of urban ET are the increased impervious surfaces and the decreased radiation. This study improved the simulation accuracy of urban ET and revealed the response of urban ET to urbanization. © 2022"
213,212,18,212_Epitope prediction and T-cell receptor specificity,Epitope prediction and T-cell receptor specificity,"Understanding how a T-cell receptor (TCR) recognizes its specific ligand peptide is crucial for gaining an insight into biological functions and disease mechanisms. Despite its importance, experimentally determining TCR–peptide–major histocompatibility complex (TCR–pMHC) interactions is expensive and time-consuming. To address this challenge, computational methods have been proposed, but they are typically evaluated by internal retrospective validation only, and few researchers have incorporated and tested an attention layer from language models into structural information. Therefore, in this study, we developed a machine learning model based on a modified version of Transformer, a source–target attention neural network, to predict the TCR–pMHC interaction solely from the amino acid sequences of the TCR complementarity-determining region (CDR) 3 and the peptide. This model achieved competitive performance on a benchmark dataset of the TCR–pMHC interaction, as well as on a truly new external dataset. Additionally, by analyzing the results of binding predictions, we associated the neural network weights with protein structural properties. By classifying the residues into large- and small-attention groups, we identified statistically significant properties associated with the largely attended residues such as hydrogen bonds within CDR3. The dataset that we created and the ability of our model to provide an interpretable prediction of TCR–peptide binding should increase our knowledge about molecular recognition and pave the way for designing new therapeutics. Copyright © 2023 Koyama, Hashimoto, Nagao and Mizuguchi.,Cognate target identification for T-cell receptors (TCRs) is a significant barrier in T-cell therapy development, which may be overcome by accurately predicting TCR interaction with peptide-bound major histocompatibility complex (pMHC). In this study, we have employed peptide embeddings learned from a large protein language model- Evolutionary Scale Modeling (ESM), to predict TCR-pMHC binding. The TCR-ESM model presented outperforms existing predictors. The complementarity-determining region 3 (CDR3) of the hypervariable TCR is located at the center of the paratope and plays a crucial role in peptide recognition. TCR-ESM trained on paired TCR data with both CDR3? and CDR3? chain information performs significantly better than those trained on data with only CDR3?, suggesting that both TCR chains contribute to specificity, the relative importance however depends on the specific peptide-MHC targeted. The study illuminates the importance of MHC information in TCR-peptide binding which remained inconclusive so far and was thought dependent on the dataset characteristics. TCR-ESM outperforms existing approaches on external datasets, suggesting generalizability. Overall, the potential of deep learning for predicting TCR-pMHC interactions and improving the understanding of factors driving TCR specificity are highlighted. The prediction model is available at http://tcresm.dhanjal-lab.iiitd.edu.in/ as an online tool. © 2023 The Authors,Introduction: T-cell receptor (TCR) recognition of foreign peptides presented by the major histocompatibility complex (MHC) initiates the adaptive immune response against pathogens. While a large number of TCR sequences specific to different antigenic peptides are known to date, the structural data describing the conformation and contacting residues for TCR-peptide-MHC complexes is relatively limited. In the present study we aim to extend and analyze the set of available structures by performing highly accurate template-based modeling of these complexes using TCR sequences with known specificity. Methods: Identification of CDR3 sequences and their further clustering, based on available spatial structures, V- and J-genes of corresponding T-cell receptors, and epitopes, was performed using the VDJdb database. Modeling of the selected CDR3 loops was conducted using a stepwise introduction of single amino acid substitutions to the template PDB structures, followed by optimization of the TCR-peptide-MHC contacting interface using the Rosetta package applications. Statistical analysis and recursive feature elimination procedures were carried out on computed energy values and properties of contacting amino acid residues between CDR3 loops and peptides, using R. Results: Using the set of 29 complex templates (including a template with SARS-CoV-2 antigen) and 732 specificity records, we built a database of 1585 model structures carrying substitutions in either TCR? or TCR? chains with some models representing the result of different mutation pathways for the same final structure. This database allowed us to analyze features of amino acid contacts in TCR - peptide interfaces that govern antigen recognition preferences and interpret these interactions in terms of physicochemical properties of interacting residues. Conclusion: Our results provide a methodology for creating high-quality TCR-peptide-MHC models for antigens of interest that can be utilized to predict TCR specificity. Copyright © 2023 Shcherbinin, Karnaukhov, Zvyagin, Chudakov and Shugay."
214,213,18,213_Generative models for structure-based molecular design and drug discovery,Generative models for structure-based molecular design and drug discovery,"In the past few years, a number of machine learning (ML)-based molecular generative models have been proposed for generating molecules with desirable properties, but they all require a large amount of label data of pharmacological and physicochemical properties. However, experimental determination of these labels, especially bioactivity labels, is very expensive. In this study, we analyze the dependence of various multi-property molecule generation models on biological activity label data and propose Frag-G/M, a fragment-based multi-constraint molecular generation framework based on conditional transformer, recurrent neural networks (RNNs), and reinforcement learning (RL). The experimental results illustrate that, using the same number of labels, Frag-G/M can generate more desired molecules than the baselines (several times more than the baselines). Moreover, compared with the known active compounds, the molecules generated by Frag-G/M exhibit higher scaffold diversity than those generated by the baselines, thus making it more promising to be used in real-world drug discovery scenarios.  © 2023 American Chemical Society.,Motivation: In the field of pharmacochemistry, it is a time-consuming and expensive process for the new drug development. The existing drug design methods face a significant challenge in terms of generation efficiency and quality. Results: In this paper, we proposed a novel molecular generation strategy and optimization based on A2C reinforcement learning. In molecular generation strategy, we adopted transformer-DNN to retain the scaffolds advantages, while accounting for the generated molecules' similarity and internal diversity by dynamic parameter adjustment, further improving the overall quality of molecule generation. In molecular optimization, we introduced heterogeneous parallel supercomputing for large-scale molecular docking based on message passing interface communication technology to rapidly obtain bioactive information, thereby enhancing the efficiency of drug design. Experiments show that our model can generate high-quality molecules with multi-objective properties at a high generation efficiency, with effectiveness and novelty close to 100%. Moreover, we used our method to assist shandong university school of pharmacy to find several candidate drugs molecules of anti-PEDV.  © 2023 The Author(s). Published by Oxford University Press.,Rational drug design often starts from specific scaffolds to which side chains/substituents are added or modified due to the large drug-like chemical space available to search for novel drug-like molecules. With the rapid growth of deep learning in drug discovery, a variety of effective approaches have been developed for de novo drug design. In previous work we proposed a method named DrugEx, which can be applied in polypharmacology based on multi-objective deep reinforcement learning. However, the previous version is trained under fixed objectives and does not allow users to input any prior information (i.e. a desired scaffold). In order to improve the general applicability, we updated DrugEx to design drug molecules based on scaffolds which consist of multiple fragments provided by users. Here, a Transformer model was employed to generate molecular structures. The Transformer is a multi-head self-attention deep learning model containing an encoder to receive scaffolds as input and a decoder to generate molecules as output. In order to deal with the graph representation of molecules a novel positional encoding for each atom and bond based on an adjacency matrix was proposed, extending the architecture of the Transformer. The graph Transformer model contains growing and connecting procedures for molecule generation starting from a given scaffold based on fragments. Moreover, the generator was trained under a reinforcement learning framework to increase the number of desired ligands. As a proof of concept, the method was applied to design ligands for the adenosine A2A receptor (A2AAR) and compared with SMILES-based methods. The results show that 100% of the generated molecules are valid and most of them had a high predicted affinity value towards A2AAR with given scaffolds. © 2023, The Author(s)."
215,214,17,214_Neurodevelopmental effects of stress in rodents and potential interventions,Neurodevelopmental effects of stress in rodents and potential interventions,"Stress is an important environmental factor affecting mental health that cannot be ignored. Moreover, due to the great physiological differences between males and females, the effects of stress may vary by sex. Previous studies have shown that terrified-sound stress, meaning exposed mice to the recorded vocalizations in response to the electric shock by their kind to induce psychological stress, can cause cognitive impairment in male. In the study, we investigated the effects of the terrified-sound stress on adult female mice. Methods: 32 adults female C57BL/6 mice were randomly divided into control (n = 16) and stress group (n = 16). Sucrose preference test (SPT)was carried out to evaluate the depressive-like behavior. Using Open field test (OFT) to evaluate locomotor and exploratory alterations in mice. Spatial learning and memory ability were measured in Morris Water maze test (MWM), Golgi staining and western blotting showed dendritic remodeling after stress. In addition, serum hormone quantifications were performed by ELISA. Results: we found the sucrose preference of stress group was significantly decreased (p < 0.05) compared with control group; the escape latency of the stress group was significantly prolonged (p < 0.05), the total swimming distance and the number of target crossings(p < 0.05) were significantly increased (p < 0.05) in MWM; Endocrine hormone, Testosterone (T) (p < 0.05), GnRH (p < 0.05), FSH and LH levels was decreased; Golgi staining and western blotting showed a significant decrease in dendritic arborization, spine density and synaptic plasticity related proteins PSD95 and BDNF in the stress group. Conclusion: Terrified-sound stress induced depressive-like behaviors, locomotor and exploratory alterations. And impaired cognitive by altering dendritic remodeling and the expression of synaptic plasticity-related proteins. However, females are resilient to terrified-sound stress from a hormonal point of view. © 2023 The Authors,Objective: Electroacupuncture (EA) in the treating principle of “soothing the liver and regulating the kidney” was applied to intervion the rats with post-traumatic stress disorder (PTSD), and its effect on anxiety-like behavior, spatial learning and memory ability, and expressions of synaptophysin (SYN) and postsynaptic dense 95 (PSD95) in hippocampus of rats were observed to explore the underlying mechanism. Methods: Twenty-four male SD rats were randomly divided into control group, model group, and EA group, with 8 rats in each group. There was no intervention in the control group. The rest 16 rats were prepared for modeling. The single-prolonged stress & shock (SPS&S) method was used to establish the PTSD models. There was no intervention in the model group after modeling. In the EA group, “B?ihuì (??GV20)”“ Shéntíng (??GV24)”“G?nsh? (??BL18)”“Shènsh? (??BL23)” were manipulated with EA stimulation, intensity 1 mA, frequency 2/100 Hz, disperse-dense wave, and treatment was performed once a day, 20 min each time, for a total of 21 days. Open field test, elevated plus maze and Morris water maze tests were used to observe the behavioral differences of rats in each group. Immunohistochemical method was used to observe the differences of positive expressions of proteins SYN and PSD95 in hippocampus. Results: In the open field test, compared with the control group, the total traveling distance, the percentage of the time spent in the central cell and the numbers of the central cells crossing in the model group were all decreased (all P < 0.05). Compared with the model group, these indicators in the EA group were significantly all increased (all P < 0.05). In the elevated plus maze test, compared with the control group, the percentages of open arm staying time and entering times in the model group were both decreased (both P < 0.05). Compared with the model group, these indicators in the EA group were both significantly increased (both P < 0.05). The results of Morris water maze test showed that compared with the control group, the escape latency time and travelled distance of rats in the model group were all increased from day1 to day 4 (all P < 0.05), and the percentage of staying time in the target quadrant was decreased (P < 0.05). Compared with the model group, the escape latency time and travelled distance of EA group were both significantly shortened (both P < 0.05), and the percentage of staying time in the target quadrant was significantly increased (P < 0.05). The immunohistochemical results showed that, compared with the control group, the positive expressions of SYN and PSD95 in the hippocampus of the model group were significantly down-regulated (both P < 0.05). Compared with the model group, the positive expressions of SYN and PSD95 in the hippocampus of the EA group were significantly up-regulated (both P < 0.05). Conclusions: EA in the treating principle of “soothing the liver and regulating the kidney” can effectively relieve anxiety-like behavior and improve spatial learning and memory ability of rats with PTSD, and the mechanism is related to the up-regulation of the expressions of SYN and PSD95 in the hippocampus. © 2022,Introduction: Sleep abnormalities are highly correlated with neurodevelopmental disorders, such as intellectual disability, attention deficit hyperactivity disorder, and autism spectrum disorders (ASD). The severity of behavioral abnormalities is correlated with the presence of sleep abnormalities. Based on previous research, we investigated that Ctnnd2 gene deletion in mice lead to ASD-like behaviors and cognitive defects. Given the importance of sleep in individuals with ASD, this study aimed to determine the effects of chronic sleep restriction (SR) on wild-type (WT) mice and on Ctnnd2 deletion-induced, neurologically related phenotypes in mice. Method: WT and Ctnnd2 knockout (KO) mice were both subjected to manual SR (5 h per day) for 21 consecutively days separately, then we compared neurologically related phenotypes of WT mice, WT mice subjected to SR, KO mice, and KO mice subjected to SR using a three-chamber assay, direct social interaction test, open-field test, Morris water maze, Golgi staining, and Western blotting. Results: The effects of SR on WT and KO mice were different. After SR, social ability and cognition were impaired in both WT and KO mice. Repetitive behaviors were increased, and exploration abilities were decreased in KO mice but not in WT mice. Moreover, SR reduced the density and area of mushroom-type dendritic spines in WT rather than KO mice. Finally, the PI3K/Akt-mTOR pathway was found to be involved in the effects induced by SR-impaired phenotypes in WT and KO mice. Conclusion: Overall, results of the present study may have implications for the role of disrupted sleep in patients with CTNND2 gene-related autism and the evolution of neurodevelopmental disorders. © 2023 The Authors. Brain and Behavior published by Wiley Periodicals LLC."
216,215,17,215_Human Pose Estimation,Human Pose Estimation,"Background: In computer vision, simultaneously estimating human pose, shape, and clothing is a practical issue in real life, but remains a challenging task owing to the variety of clothing, complexity of deformation, shortage of large-scale datasets, and difficulty in estimating clothing style. Methods: We propose a multistage weakly supervised method that makes full use of data with less labeled information for learning to estimate human body shape, pose, and clothing deformation. In the first stage, the SMPL human-body model parameters were regressed using the multi-view 2D key points of the human body. Using multi-view information as weakly supervised information can avoid the deep ambiguity problem of a single view, obtain a more accurate human posture, and access supervisory information easily. In the second stage, clothing is represented by a PCAbased model that uses two-dimensional key points of clothing as supervised information to regress the parameters. In the third stage, we predefine an embedding graph for each type of clothing to describe the deformation. Then, the mask information of the clothing is used to further adjust the deformation of the clothing. To facilitate training, we constructed a multi-view synthetic dataset that included BCNet and SURREAL. Results: The Experiments show that the accuracy of our method reaches the same level as that of SOTA methods using strong supervision information while only using weakly supervised information. Because this study uses only weakly supervised information, which is much easier to obtain, it has the advantage of utilizing existing data as training data. Experiments on the DeepFashion2 dataset show that our method can make full use of the existing weak supervision information for fine-tuning on a dataset with little supervision information, compared with the strong supervision information that cannot be trained or adjusted owing to the lack of exact annotation information. Conclusions: Our weak supervision method can accurately estimate human body size, pose, and several common types of clothing and overcome the issues of the current shortage of clothing data. © 2022 Beijing Zhongke Journal Publishing Co. Ltd,3D pose estimation has recently gained substantial interests in computer vision domain. Existing 3D pose estimation methods have a strong reliance on large size well-annotated 3D pose datasets, and they suffer poor model generalization on unseen poses due to limited diversity of 3D poses in training sets. In this work, we propose PoseGU, a novel human pose generator that generates diverse poses with access only to a small size of seed samples, while equipping the Counterfactual Risk Minimization to pursue an unbiased evaluation objective. Extensive experiments demonstrate PoseGU outperforms almost all the state-of-the-art 3D human pose methods under consideration over three popular benchmark datasets. Empirical analysis also proves PoseGU generates 3D poses with improved data diversity and better generalization ability. © 2023 Elsevier Inc.,Deep learning advances have made it possible to recoverfull 3-D meshes of human models from individual images. However, the extension of this notion to videos for recovering temporally coherent poses is still underexplored. A major challenge in this direction is the lack of appropriately annotated video data for learning the desired computational models. The existing human pose datasets only provide 2-D or 3-D skeleton joint annotations, whereas the datasets are also insufficiently recorded in constrained environments. We first contribute a technique to synthesize monocular action videos with rich 3-D annotations that are suitable for learning computational models for full mesh 3-D human pose recovery. Compared to the existing methods that simply 'texture map' clothes onto the 3-D human pose models, our approach incorporates Physics-based realistic cloth deformations with human body movements. The generated videos cover a large variety of human actions, poses, and visual appearances, while the annotations record accurate human pose dynamics and human body surface information. Our second major contribution is an end-to-end trainable recurrent neural network for full pose mesh recovery from monocular videos. Using the proposed video data and a long short-term memory recurrent structure, our network explicitly learns to model the temporal coherence in videos and imposes geometric consistency over the recovered meshes. We establish the effectiveness of the proposed model with quantitative and qualitative analysis using the proposed and benchmark datasets.  © 2020 IEEE."
217,216,17,216_Underwater Image Enhancement Methods,Underwater Image Enhancement Methods,"Optical imaging instruments have been widely deployed in underwater-engineering systems, playing an important role in underwater-object localization, detection, and recognition. However, underwater images suffer from the adverse effects of local distortion and global haze. Consequently, raw underwater images have limitations when used for display and vision tasks. Super resolution (SR) has been increasingly exploited for perceptual image quality improvement. However, underwater-image SR is a relatively underexplored area, most existing methods being unable to reduce the adverse effects of underwater images. Moreover, in contrast to land-based high-performance platforms, the power supply and computational resources of underwater platforms are limited, making them difficult to use in large-scale models. To solve these problems, this study developed a novel range-dependency learning network to present the short- and long-range dependency of multiscale features. Such a mechanism could provide more detailed and accurate texture information for underwater-image SR, improving underwater-image SR performance. Moreover, a channel-splitting module was designed to generate the channel bands which could extract texture details and global structural information at different scales while reducing the number of parameters, thus accelerating the training speed of the model and maintaining good performance. Our novel network could reach an optimal tradeoff between the underwater-image SR performance and efficiency, which was demonstrated by experimental comparisons and an ablation study. © 2023,The absorption and scattering properties of the water medium cause various types of distortion in underwater images, which seriously affects the accuracy and effectiveness of subsequent processing. The application of supervised learning algorithms in underwater image enhancement is limited by the difficulty of obtaining a large number of underwater paired images in practical applications. As a solution, we propose an unsupervised representation disentanglement based underwater image enhancement method (URD-UIE). URD-UIE disentangles content information (e.g., texture, semantics) and style information (e.g., chromatic aberration, blur, noise, and clarity) from underwater images and then employs the disentangled information to generate the target distortion-free image. Our proposed method URD-UIE adopts an unsupervised cycle-consistent adversarial translation architecture and combines multiple loss functions to impose specific constraints on the output results of each module to ensure the structural consistency of underwater images before and after enhancement. The experimental results demonstrate that the URD-UIE technique effectively enhances the quality of underwater images when training with unpaired data, resulting in a significant improvement in the performance of the standard model for underwater object detection and semantic segmentation. © 2023 Elsevier Ltd,Due to underwater light absorption and scattering, underwater images usually suffer from severe color attenuation and contrast reduction. Most mainstream underwater image processing methods based on deep learning require a large amount of underwater paired training data, leading to a complex network structure, longer training time, and higher computational cost. To address this problem, a novel Zero-Reference Deep Network for Underwater Image Enhancement (Zero-UIE) is proposed in this paper, which transforms the enhancement of an underwater image into a specific parameter map estimation by using a deep network. The underwater curve model based on the classical haze image formation principle is specially designed to remove underwater color dispersion and cast. A lightweight deep network is designed to estimate the dynamic adjustment parameters of the underwater curve model, and then adjust the dynamic range of the given image pixels according to the model. A set of non-reference loss functions are designed according to the characteristics of underwater images, which can implicitly drive the network learning. In addition, adaptive color compensation can be optionally used as the pre-processing step to further improve the robustness and visual performance. The significant contribution of the proposed method is zero reference, i.e., it does not require any paired or unpaired reference data for training. Extensive experiments on various benchmarks demonstrate that the proposed method is superior to state-of-the-art methods subjectively and objectively, which is competitive and applicable to diverse underwater conditions. Most importantly, it is an innovative exploration of zero reference for underwater image enhancement. © 1976-2012 IEEE."
218,217,17,"217_Health assessment of equipment under unsupervised conditions using lightweight network (EM-PASCAL), RUL prediction of rolling bearings with competitive temporal convolutional network (CTCN), Data-driven approach for RUL prediction with LSTM and data augmentation, Refined transfer for RUL prediction with multi-channel transfer network, XAI methods for time series regression models in PM, Multi-task spatio-temporal augmented net (MTSTAN) for RUL prediction, Nonlinear target function with NT-TCN for RUL prediction, Res-HSA for RUL prediction using health indicators, GAT-DAT for RUL prediction with deep adaptive transformer, Stacked integration method for RUL prediction with TCN and CNN-Bi-GRU, Contrastive learning for RUL prediction with self-supervised health representation decomposition learning (SHRDL), Attention-GRU model for RUL prediction with attention mechanism, TCNAAM for RUL prediction with adaptive activation function and attention mechanism, STAIRnet for RUL prediction with spatial-temporal attention and information reinforcement network.","Health assessment of equipment under unsupervised conditions using lightweight network (EM-PASCAL), RUL prediction of rolling bearings with competitive temporal convolutional network (CTCN), Data-driven approach for RUL prediction with LSTM and data augmentation, Refined transfer for RUL prediction with multi-channel transfer network, XAI methods for time series regression models in PM, Multi-task spatio-temporal augmented net (MTSTAN) for RUL prediction, Nonlinear target function with NT-TCN for RUL prediction, Res-HSA for RUL prediction using health indicators, GAT-DAT for RUL prediction with deep adaptive transformer, Stacked integration method for RUL prediction with TCN and CNN-Bi-GRU, Contrastive learning for RUL prediction with self-supervised health representation decomposition learning (SHRDL), Attention-GRU model for RUL prediction with attention mechanism, TCNAAM for RUL prediction with adaptive activation function and attention mechanism, STAIRnet for RUL prediction with spatial-temporal attention and information reinforcement network.","Deep learning (DL)-based methods for remaining useful life (RUL) prediction have received increasing research attention due to excellent feature extraction abilities. Most DL methods rely on abundant labeled samples for supervised training. However, because of the adoption of the over-maintenance strategy of equipment, the monitored data for the degradation of equipment usually consists of few labeled samples and a large amount of unlabeled samples, which limits the performance of DL methods. To take advantage of the value of unlabeled samples, this paper proposed a contrastive learning framework for RUL prediction. First, an unlabeled sample augmentation is developed firstly to extend the sample set. Then, an unlabeled sample learning (USL) architecture is proposed to learn the information of degradation from unlabeled samples to promote general DL models’ performance on RUL prediction. Based on the proposed framework, USL-convolutional neural network and USL-long short-term memory network are used to validate its performance based on datasets of turbofan engine and bearing. Results show that the performance of RUL prediction based on the proposed framework can be enhanced by unlabeled samples and verify the good scalability and generalization ability of the proposed framework. © 2023,Data imbalance and large data probability distribution discrepancies are major factors that reduce the accuracy of remaining useful life (RUL) prediction of high-reliability rotating machinery. In feature extraction, most deep transfer learning models consider the overall features but rarely attend to the local target features that are useful for RUL prediction; insufficient attention paid to local features reduces the accuracy and reliability of prediction. By considering the contribution of input data to the modeling output, a deep learning model that incorporates the attention mechanism in feature selection and extraction is proposed in our work; an unsupervised clustering method for classification of rotating machinery performance state evolution is put forward, and a similarity function is used to calculate the expected attention of input data to build an input data extraction attention module; the module is then fused with a gated recurrent unit (GRU), a variant of a recurrent neural network, to construct an attention-GRU model that combines prediction calculation and weight calculation for RUL prediction. Tests on public datasets show that the attention-GRU model outperforms traditional GRU and LSTM in RUL prediction, achieves less prediction error, and improves the performance and stability of the model. © 2023 by the authors.,Remaining useful life (RUL) prediction is of great significance for improving maintenance efficiency and ensuring the reliability of rotating machinery. In recent years, there are a large number of deep-learning-based methods for RUL prediction of rotating machinery. However, the effect of conventional end-to-end RUL prediction methods relies on the distribution consistency of training data and test data, and conventional health indicator extrapolation RUL prediction methods are susceptible to interference from abnormal fluctuations in the health curve. To overcome the problem, this paper proposes a new RUL prediction method for rotating machinery using health indicators constructed by the residual hybrid network with self-attention mechanism (Res-HSA). First of all, we propose the residual hybrid network combined with self-attention mechanism to extract the high-level degenerate feature. Then, the health assessment model based on Res-HSA is proposed to generate the health indicators of the equipment. To assist in network training, the segmented data labels based on the degradation rule are applied to optimize the labels of training sets. Finally, to address the problem of abnormal fluctuations in the health curve, a fitting interval selection method is used to optimize conventional curve fitting schemes to calculate RUL. Two public datasets, IEEE-PHM-2012-challenge datasets and C-MAPSS datasets, are used to verify the effectiveness of the proposed method. The experiment results on two public datasets show that the RUL prediction method proposed in this paper has good prediction performance. Compared to the state-of-the-art method, the method proposed in this article reaches the most advanced level in some test projects, while the rest of the projects can be very close to the most advanced method. © 2023 Elsevier Ltd"
219,218,17,218_Automated classification of white blood cells from microscopic images using deep learning models and data augmentation techniques.,Automated classification of white blood cells from microscopic images using deep learning models and data augmentation techniques.,"Background and objectives: Visual analysis of cell morphology has an important role in the diagnosis of hematological diseases. Morphological cell recognition is a challenge that requires experience and in-depth review by clinical pathologists. Within the new trend of introducing computer-aided diagnostic tools in laboratory medicine, models based on deep learning are being developed for the automatic identification of different types of cells in peripheral blood. In general, well-annotated large image sets are needed to train the models to reach a desired classification performance. This is especially relevant when it comes to discerning between cell images in which morphological differences are subtle and when it comes to low prevalent diseases with the consequent difficulty in collecting cell images. The objective of this work is to develop, train and validate SyntheticCellGAN (SCG), a new system for the automatic generation of artificial images of white blood cells, maintaining morphological characteristics very close to real cells found in practice in clinical laboratories. Methods: SCG is designed with two sequential generative adversarial networks. First, a Wasserstein structure is used to transform random noise vectors into low resolution images of basic mononuclear cells. Second, the concept of image-to-image translation is used to build specific models that transform the basic images into high-resolution final images with the realistic morphology of each cell type target: 1) the five groups of normal leukocytes (lymphocytes, monocytes, eosinophils, neutrophils and basophils); 2) atypical promyelocytes and hairy cells, which are two relevant cell types of complex morphology with low abundance in blood smears. Results: The images of the SCG system are evaluated with four experimental tests. In the first test we evaluated the generated images with quantitative metrics for GANs. In the second test, morphological verification of the artificial images is performed by expert clinical pathologists with 100% accuracy. In the third test, two classifiers based on convolutional neural networks (CNN) previously trained with images of real cells are used. Two sets of artificial images of the SCG system are classified with an accuracy of 95.36% and 94%, respectively. In the fourth test, three CNN classifiers are trained with artificial images of the SCG system. Real cells are identified with an accuracy ranging from 87.7% to 100%. Conclusions: The SCG system has proven effective in creating images of all normal leukocytes and two low-prevalence cell classes associated with diseases such as acute promyelocyte leukemia and hairy cell leukemia. Once trained, the system requires low computational cost and can help augment high-quality image datasets to improve automatic recognition model training for clinical laboratory practice. © 2022 The Author(s),Accurate and early detection of anomalies in peripheral white blood cells plays a crucial role in the evaluation of well-being in individuals and the diagnosis and prognosis of hematologic diseases. For example, some blood disorders and immune system-related diseases are diagnosed by the differential count of white blood cells, which is one of the common laboratory tests. Data is one of the most important ingredients in the development and testing of many commercial and successful automatic or semi-automatic systems. To this end, this study introduces a free access dataset of normal peripheral white blood cells called Raabin-WBC containing about 40,000 images of white blood cells and color spots. For ensuring the validity of the data, a significant number of cells were labeled by two experts. Also, the ground truths of the nuclei and cytoplasm are extracted for 1145 selected cells. To provide the necessary diversity, various smears have been imaged, and two different cameras and two different microscopes were used. We did some preliminary deep learning experiments on Raabin-WBC to demonstrate how the generalization power of machine learning methods, especially deep neural networks, can be affected by the mentioned diversity. Raabin-WBC as a public data in the field of health can be used for the model development and testing in different machine learning tasks including classification, detection, segmentation, and localization. © 2022, The Author(s).,The use of deep learning techniques for White Blood Cell (WBC) classification has garnered significant attention on medical image analysis due to its potential to automate and enhance the accuracy of WBC classification, which is critical for disease diagnosis and infection detection. Convolutional neural networks (CNNs) have revolutionized image analysis tasks, including WBC classification effectively capturing intricate spatial patterns and distinguishing between different cell types. A key advantage of deep learning-based WBC classification is its capability to handle large datasets, enabling models to learn the diverse variations and characteristics of different cell types. This facilitates robust generalization and accurate classification of previously unseen samples. In this paper, a novel approach called Red Deer Optimization with Deep Learning for Robust White Blood Cell Detection and Classification was presented. The proposed model incorporates various components to improve performance and robustness. Image pre-processing involves the utilization of median filtering, while U-Net++ is employed for segmentation, facilitating accurate delineation of WBCs. Feature extraction is performed using the Xception model, which effectively captures informative representations of the WBCs. For classification, BiGRU model is employed, leveraging its ability to model temporal dependencies in the WBC sequences. To optimize the performance of the BiGRU model, the RDO is utilized for parameter tuning, resulting in enhanced accuracy and faster convergence of the deep learning models. The integration of RDO contributes to more reliable detection and classification of WBCs, further improving the overall performance and robustness of the approach. Experimental results demonstrate the superiority of our Red Deer Optimization with deep learning-based approach over traditional methods and standalone deep learning models in achieving robust WBC detection and classification. This research highlights the possibility of combining deep learning techniques with optimization algorithms for improving WBC analysis, offering valuable insights for medical professionals and medical image analysis. © 2023 Auricle Global Society of Education and Research."
220,219,17,219_Low-Light Image Enhancement and Denoising,Low-Light Image Enhancement and Denoising,"Deep convolutional neural networks (CNNs) have achieved significant milestones in low-light image enhancement. We note that existing methods still struggle to recover lost details. (1) They employ image-to-image mapping for enhancement, which may excel at global adjustment but lack attention to detail recovery. (2) They train CNN models with low-light/normal-light (L/N) image pairs that are generally non-pixel-level fully aligned, which causes the enhanced result to be prone to the problem of blurred details. To this end, we propose a novel CNN framework for learning to recover lost details from the dark. Specifically, we adopt a point-to-point mapping for better recovering lost details by estimating color affine transformations. And we design an associated group-codec mechanism to decouple the alignment problem from the enhancing process, thus preventing blurred details. Besides, data scarcity is an outstanding problem in low-light image enhancement; although some unsupervised models may deal with it, they would produce artifacts that cover real details. Hence, we propose a dynamic data collection method to acquire L/N image pairs. And we have collected a large and diverse low-light dataset to supplement existing benchmarks. Experiments conducted on our and public datasets show that our method is very competitive compared with state-of-the-art methods. © 2022,Deep Neural Networks (DNNs) have achieved impressive results on the task of image denoising, but there are two serious problems. First, the denoising ability of DNNs-based image denoising models using traditional training strategies heavily relies on extensive training on clean-noise image pairs. Second, image denoising models based on DNNs usually have large parameters and high computational complexity. To address these issues, this paper proposes a two-stage Few-Shot Learning for Image Denoising (FSLID). Our FSLID is a two-stage denoising strategy integrating Basic Feature Learner (BFL), Denoising Feature Inducer (DFI), and Shared Image Reconstructor (SIR). BFL and SIR are first jointly unsupervised to train on the base image dataset Dbase consisting of easily collected high-quality clean images. Following this, the trained BFL extracts the guided features and constraint features for the noisy and corresponding clean images in the novel image dataset Dnovel, respectively. Furthermore, DFI encodes the noisy features of the noisy images in Dnovel. Then, inducing both the guided features and noisy features, DFI can generate the denoising prior features for the SIR with frozen weights to adaptively denoise the noisy images. Furthermore, we propose refined, low-channel-count, recursive multi-branch Multi-Scale Feature Recursive (MSFR) to modularly formulate an efficient DFI to capture more diverse contextual features information under a limited number of feature channels. Thus, compared with the baseline models, the FSLID composed of the proposed MSFR can significantly reduce the number of model parameters and computational complexity. Extensive experimental results demonstrate our FSLID significantly outperforms well-established baselines on multiple datasets and settings. We hope that our work will encourage further research to explore the field of few-shot image denoising. © 1991-2012 IEEE.,With the development of the field of deep learning, image recognition, enhancement and other technologies have been widely used.However, dark lighting environments in reality, such as insufficient light at night, cause or block photographic images in low brightness, severe noise, and a large number of details are lost, resulting in a huge loss of image content and information, which hinders further analysis and use. Such problems not only exist in the traditional deep learning field, but also exist in criminal investigation, scientific photography and other fields, such as the accuracy of low-light image. However, in the current research results, there is no perfect means to deal with the above problems. Therefore, the study of low-light image enhancement has important theoretical significance and practical application value for the development of smart cities. In order to improve the quality of low-light enhanced images, this paper tries to introduce the luminance attention mechanism to improve the enhancement efficiency. The main contents of this paper are summarized as follows: using the attention mechanism, we proposed a method of low-light image enhancement based on the brightness attention mechanism and generative adversarial networks. This method uses brightness attention mechanism to predict the illumination distribution of low-light image and guides the enhancement network to enhance the image adaptiveness in different luminance regions. At the same time, u-NET network is designed and constructed to improve the modeling process of low-light image. We verified the performance of the algorithm on the synthetic data set and compared it with traditional image enhancement methods (HE, SRIE) and deep learning methods (DSLR). The experimental results show that our proposed network model has relatively good enhancement quality for low-light images, and improves the overall robustness, which has practical significance for solving the problem of low-light image enhancement. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
221,220,17,220_Neurodegenerative Diseases and Biomarkers,Neurodegenerative Diseases and Biomarkers,"The increasing incidence of Alzheimer's disease (AD) has been leading towards a significant growth in socioeconomic challenges. A reliable prediction of AD might be useful to mitigate or at-least slow down its progression for which, identification of the factors affecting the AD and its accurate diagnoses, are vital. In this study, we use Genome-Wide Association Studies (GWAS) dataset which comprises significant genetic markers of complex diseases. The original dataset contains large number of attributes (620901) for which we propose a hybrid feature selection approach based on association test, principal component analysis, and the Boruta algorithm, to identify the most promising predictors of AD. The selected features are then forwarded to a wide and deep neural network models to classify the AD cases and healthy controls. The experimental outcomes indicate that our approach outperformed the existing methods when evaluated on standard dataset, producing an accuracy and f1-score of 99%. The outcomes from this study are impactful particularly, the identified features comprising AD-associated genes and a reliable classification model that might be useful for other chronic diseases. Copyright: © 2023 Alatrany et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.,Background: The progression rates of Alzheimer’s disease (AD) are variable and dynamic, yet the mechanisms that contribute to heterogeneity in progression rates remain ill-understood. Particularly, the role of synergies in pathological processes reflected by biomarkers for amyloid-beta (‘A’), tau (‘T’), and neurodegeneration (‘N’) in progression along the AD continuum is not fully understood. Methods: Here, we used a combination of model and data-driven approaches to address this question. Working with a large dataset (N = 321 across the training and testing cohorts), we first applied unsupervised clustering on longitudinal cognitive assessments to divide individuals on the AD continuum into those showing fast vs. moderate decline. Next, we developed a deep learning model that differentiated fast vs. moderate decline using baseline AT(N) biomarkers. Results: Training the model with AT(N) biomarker combination revealed more prognostic utility than any individual biomarkers alone. We additionally found little overlap between the model-driven progression phenotypes and established atrophy-based AD subtypes. Our model showed that the combination of all AT(N) biomarkers had the most prognostic utility in predicting progression along the AD continuum. A comprehensive AT(N) model showed better predictive performance than biomarker pairs (A(N) and T(N)) and individual biomarkers (A, T, or N). Conclusions: This study combined data and model-driven methods to uncover the role of AT(N) biomarker synergies in the progression of cognitive decline along the AD continuum. The results suggest a synergistic relationship between AT(N) biomarkers in determining this progression, extending previous evidence of A-T synergistic mechanisms. © 2022, The Author(s).,Neuroimaging-based brain-age estimation via machine learning has emerged as an important new approach for studying brain aging. The difference between one's estimated brain age and chronological age, the brain age gap (BAG), has been proposed as an Alzheimer's Disease (AD) biomarker. However, most past studies on the BAG have been cross-sectional. Quantifying longitudinal changes in an individual's BAG temporal pattern would likely improve prediction of AD progression and clinical outcome based on neurophysiological changes. To fill this gap, our study conducted predictive modeling using a large neuroimaging dataset with up to 8 years of follow-up to examine the temporal patterns of the BAG's trajectory and how it varies by subject-level characteristics (sex, APOE?4 carriership) and disease status. Specifically, we explored the pattern and rate of change in BAG over time in individuals who remain stable with normal cognition or mild cognitive impairment (MCI), as well as individuals who progress to clinical AD. Combining multimodal imaging data in a support vector regression model to estimate brain age yielded improved performance over single modality. Multilevel modeling results showed the BAG followed a linear increasing trajectory with a significantly faster rate in individuals with MCI who progressed to AD compared to cognitively normal or MCI individuals who did not progress. The dynamic changes in the BAG during AD progression were further moderated by sex and APOE?4 carriership. Our findings demonstrate the BAG as a potential biomarker for understanding individual specific temporal patterns related to AD progression. © 2022"
222,221,17,"221_Water monitoring using satellite imagery: lakes, bloom detection, bathymetry, waterline extraction, coastal shoreline mapping, LFSI monitoring, reservoir water level retrieval, Sargassum detection, Arctic permafrost degradation, super-resolution of Landsat imagery.","Water monitoring using satellite imagery: lakes, bloom detection, bathymetry, waterline extraction, coastal shoreline mapping, LFSI monitoring, reservoir water level retrieval, Sargassum detection, Arctic permafrost degradation, super-resolution of Landsat imagery.","Shallow water bathymetry is of great significance in understanding, managing, and protecting coastal ecological environments. Many studies have shown that both empirical models and deep learning models can achieve promising results from satellite imagery bathymetry inversion. However, the spectral information available today in multispectral or/and hyperspectral satellite images has not been explored thoroughly in many models. The Band-optimized Bidirectional Long Short-Term Memory (BoBiLSTM) model proposed in this paper feeds only the optimized bands and band ratios to the deep learning model, and a series of experiments were conducted in the shallow waters of Molokai Island, Hawaii, using hyperspectral satellite imagery (PRISMA) and multispectral satellite imagery (Sentinel-2) with ICESat-2 data and multibeam scan data as training data, respectively. The experimental results of the BoBiLSTM model demonstrate its robustness over other compared models. For example, using PRISMA data as the source image, the BoBiLSTM model achieves RMSE values of 0.82 m (using ICESat-2 as the training data) and 1.43 m (using multibeam as the training data), respectively, and because of using the bidirectional strategy, the inverted bathymetry reaches as far as a depth of 25 m. More importantly, the BoBiLSTM model does not overfit the data in general, which is one of its advantages over many other deep learning models. Unlike other deep learning models, which require a large amount of training data and all available bands as the inputs, the BoBiLSTM model can perform very well using equivalently less training data and a handful of bands and band ratios. With ICESat-2 data becoming commonly available and covering many shallow water regions around the world, the proposed BoBiLSTM model holds potential for bathymetry inversion for any region around the world where satellite images and ICESat-2 data are available. © 2023 by the authors.,Widespread eutrophication has been considered as the most serious environment problems in the world. Given the critical roles of lakes in human society and serious negative effects of water eutrophication on lake ecosystems, it is thus fundamentally important to monitor and assess water trophic status of lakes. However, a reliable model for accurately estimating the trophic state index (TSI) of lakes across a large-scale region is still lacking due to their high complexity. Here, we proposed an optical mechanism-based deep learning approach to remotely estimate TSI of lakes based on Landsat images. The approach consists of two steps: (1) determining the optical indicators of TSI and modeling the relationship between them, and (2) developing an approach for remotely deriving the determined optical indicator from Landsat images. With a large number of in situ datasets measured from lakes (2804 samples from 88 lakes) across China with various optical properties, we trained and validated three machine learning methods including deep neural network (DNN), k-nearest neighbors (KNN) and random forest (RF) to model TSI with the optical indicators and TSI and derive the determined optical indicator from Landsat images. The results showed that (1) the total absorption coefficients of optically active constituents at 440 nm (at-w(440)) performs best in characterizing TSI, and (2) DNN outperforms other models in the inversion of both TSI and at-w(440). Overall, our proposed optical mechanism-based deep learning approach demonstrated a robust and satisfactory performance in assessing TSI using Landsat images (root mean squared error (RMSE) = 5.95, mean absolute error (MAE) = 4.81). This highlights its merit as a nationally-adopted method in lake water TSI estimation, enabling the convenience of the acquisition of water eutrophic information in large scale, thereby assisting us in managing lake ecology. Therefore, we assessed water TSI of 961 lakes (>10 km2) across China using the proposed approach. The resulting at-w(440) and TSI ranged from 0.01 m?1 to 31.42 m?1 and from 6 to 96, respectively. Of all these studied lakes, 96 lakes (11.40 %) were oligotrophic, 338 lakes were mesotrophic (40.14 %), 360 lakes were eutrophic (42.76 %), and 48 were hypertrophic (5.70 %) in 2020. © 2024 Elsevier Ltd,Landsat is the longest-running environmental satellite program and has been used for surface water mapping since its launch in 1972. However, its sustained 30 m resolution since 1982 prohibits the detection of small water bodies, which are globally far more prevalent than large. Remote sensing image resolution is increasingly being enhanced through single image super resolution (SR), a machine learning task typically performed by neural networks. Here, we show that a 10× SR model (Enhanced Super Resolution Generative Adversarial Network, or ESRGAN) trained entirely with Planet SmallSat imagery (3 m resolution) improves the detection of small and sub-pixel lakes in Landsat imagery (30 m) and produces images (3 m resolution) with preserved radiometric properties. We test the utility of these Landsat SR images for small lake detection by applying a simple water classification to SR and original Landsat imagery and comparing their lake counts, sizes, and locations with independent, high-resolution water maps made from coincident airborne camera imagery. SR images appear realistic and have fewer missed detections (type II error) compared to low resolution (LR), but exhibit errors in lake location and shape, and yield increasing false detections (type I error) with decreasing lake size. Even so, lakes between ~500 and ~10,000 m2 in area are better detected with SR than with native-resolution Landsat 8 imagery. SR transformation achieves an F-1 score for water detection of 0.75 compared to 0.73 from native resolution Landsat. We conclude that SR enhancement improves the detection of small lakes sized several Landsat pixels or less, with a minimum mapping unit (MMU) of ~ 2/3 of a Landsat pixel–a significant improvement from previous studies. We also apply the SR model to a historical Landsat 5 image and find similar performance gains, using an independent 1985 air photo map of 242 small Alaskan lakes. This demonstration of retroactively generated 3 m imagery dating to 1985 has exciting applications beyond water detection and paves the way for further SR land cover classification and small object detection from the historical Landsat archive. However, we caution that the approach presented is suitable for landscape-scale inventories of lake counts and lake size distributions, but not for specific geolocational positions of individual lakes. Much work remains to be done surrounding technical and ethical guidelines for the creation, use, and dissemination of SR satellite imagery. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group."
223,222,17,222_Semantic Segmentation of Remote Sensing Images for Land Cover Classification,Semantic Segmentation of Remote Sensing Images for Land Cover Classification,"Deep convolutional neural network (CNN) has been increasingly applied in interpretation of remote sensing image such as automatically mapping land cover. Although the automatic CNN method achieves relatively high accuracy, there are still many misclassified areas. Considering that it is still far from practical application, this paper proposes a semi-automatic auxiliary scheme for land cover classification whose core idea is to use an interactive segmentation network. To infer the rough positions and categories of objects, a CNN is relied on to classify images in a patch-wise manner. Then an interactive segmentation method is proposed by accepting user-clicks on the inside and outside of object to guide the model for the segmentation task in the patches. This model also introduces different interactive modules to better integrate features of different scales. In addition, we create a large-scale sample library containing five common land cover categories which covers Jiangsu Province, China, and includes both aerial and satellite imagery. On our sample, we gave a thorough evaluation of most recent deep learning-based methods. The experimental results shown by our interactive segmentation also far outperform the recent semantic segmentation method, which provides a reference for semi-automatic land cover mapping. © 2013 IEEE.,The application of deep learning methods to remote sensing data has produced good results in recent studies. A promising application area is automatic land cover classification (semantic segmentation) from very high-resolution satellite imagery. However, the deep learning methods require large, labelled training datasets that are suitable for the study area. Map data can be used as training data, but it is often insufficiently detailed for very high-resolution satellite imagery. National airborne laser scanner (lidar) datasets provide additional details and are available in many countries. Successful land cover classifications from lidar datasets have been reached, e.g., by object-based image analysis. In the present study, we investigated the feasibility of using airborne laser scanner data and object-based image analysis to automatically generate labelled training data for a deep neural network -based land cover classification of a VHR satellite image. Input data for the object-based classification included digital surface models, intensity and pulse information derived from the lidar data. The resulting land cover classification was then utilized as training data for deep learning. A state-of-the-art deep learning architecture, UnetFormer, was trained and applied to the land cover classification of a WorldView-3 stereo dataset. For the semantic segmentation, three different input data composites were produced using the red, green, blue, NIR and digital surface model bands derived from the satellite data. The quality of the generated training data and the semantic segmentation results was estimated using an independent test set of ground truth points. The results show that final satellite image classification accuracy (94–96%) close to the training data accuracy (97%) was obtained. It was also demonstrated that the resulting maps could be used for land cover change detection. © 2023 The Authors,Global land cover map provides fundamental information for understanding the relationship between global environmental change and human settlement. With the development of data-driven deep learning theory, semantic segmentation network has largely facilitated the global land cover mapping activity. However, the performance of semantic segmentation network is closely related to the number and quality of training data, and the existing annotation data are usually insufficient in quantity, quality, and spatial resolution, and are usually sampled at local region and lack diversity and variability, making data-driven model difficult to extend to global scale. Therefore, we proposed a large-scale annotation dataset (Globe230k) for semantic segmentation of remote sensing image, which has 3 superiorities: (a) large scale: the Globe230k dataset includes 232,819 annotated images with a size of 512 × 512 and a spatial resolution of 1 m, including 10 first-level categories; (b) rich diversity: the annotated images are sampled from worldwide regions, with coverage area of over 60,000 km2, indicating a high variability and diversity; (c) multimodal: the Globe230k dataset not only contains RGB bands but also includes other important features for Earth system research, such as normalized differential vegetation index (NDVI), digital elevation model (DEM), vertical-vertical polarization (VV) bands, and vertical-horizontal polarization (VH) bands, which can facilitate the multimodal data fusion research. We used the Globe230k dataset to test several state-of-the-art semantic segmentation algorithms and found that it is able to evaluate algorithms in multiple aspects that are crucial for characterizing land covers, including multiscale modeling, detail reconstruction, and generalization ability. The dataset has been made public and can be used as a benchmark to promote further development of global land cover mapping and semantic segmentation algorithm development. Copyright © 2023 Qian Shi et al. Exclusive licensee Aerospace Information Research Institute, Chinese Academy of Sciences. Distributed under a Creative Commons Attribution License 4.0 (CC BY 4.0)."
224,223,16,"223_Cognitive Impairment and Risk Factors: PTSD, Aging, HIV, and Menopausal Hormone Therapy","Cognitive Impairment and Risk Factors: PTSD, Aging, HIV, and Menopausal Hormone Therapy","Background Post-traumatic stress disorder (PTSD) is associated with cognitive impairments. It is unclear whether problems persist after PTSD symptoms remit. Methods Data came from 12 270 trauma-exposed women in the Nurses' Health Study II. Trauma and PTSD symptoms were assessed using validated scales to determine PTSD status as of 2008 (trauma/no PTSD, remitted PTSD, unresolved PTSD) and symptom severity (lifetime and past-month). Starting in 2014, cognitive function was assessed using the Cogstate Brief Battery every 6 or 12 months for up to 24 months. PTSD associations with baseline cognition and longitudinal cognitive changes were estimated by covariate-adjusted linear regression and linear mixed-effects models, respectively. Results Compared to women with trauma/no PTSD, women with remitted PTSD symptoms had a similar cognitive function at baseline, while women with unresolved PTSD symptoms had worse psychomotor speed/attention and learning/working memory. In women with unresolved PTSD symptoms, past-month PTSD symptom severity was inversely associated with baseline cognition. Over follow-up, both women with remitted and unresolved PTSD symptoms in 2008, especially those with high levels of symptoms, had a faster decline in learning/working memory than women with trauma/no PTSD. In women with remitted PTSD symptoms, higher lifetime PTSD symptom severity was associated with a faster decline in learning/working memory. Results were robust to the adjustment for sociodemographic, biobehavioral, and health factors and were partially attenuated when adjusted for depression. Conclusion Unresolved but not remitted PTSD was associated with worse cognitive function assessed six years later. Accelerated cognitive decline was observed among women with either unresolved or remitted PTSD symptoms.  Copyright © The Author(s), 2023. Published by Cambridge University Press.,Background and aims: Exponential population aging has led to an increased prevalence of cognitive impairment worldwide. Hand grip strength, which may be associated with physical activity, could be a useful predictor of cognitive impairment. However, few studies have reported the association, if any, between hand grip strength and cognitive function. Methods: We used data obtained from the National Health and Nutrition Examination Survey between 2011–2012 and 2013–2014 to investigate the association between hand grip strength and cognitive impairment. Cognitive impairment was assessed using the Consortium to Establish a Registry for Alzheimer's Disease (CERAD), animal fluency (AF), and digit symbol substitution test (DSST) scores. Cutoff values of CERAD < 5, AF < 14, and DSST < 34 were used to define cognitive impairment. In this cross-sectional study, we used odds ratios to determine the potential usefulness of hand grip strength for the prediction of cognitive impairment. Results: This study included 2,623 participants aged ?60 years. The DSST results showed that hand grip strength was associated with a low risk of cognitive impairment and that subgroup analysis showed that male sex, 60–69 years of age, and the Non-Hispanic (NH)-White, NH Black, and Asian were associated with a significantly low risk of cognitive impairment. The CERAD test results showed that 70–79 years of age and the NH White were significantly associated with a low risk of cognitive impairment. By following full adjustment, we did not observe statistically significant differences between hand grip strength and cognitive impairment based on the CERAD test. The AF test results showed that >80 years of age, female sex, and the NH White were associated with a significantly low risk of cognitive impairment. The most important finding is that a linear association lies between grip strength and cognitive impairment, as well as a sex-based linear association. Machine learning of the XGBoost model suggests that grip strength is one of the top two most important negative predictor variables. Conclusion: We observed an inverse relationship between hand grip strength and cognitive impairment, which might suggest a shared underlying mechanism that needs to be further investigated using a large-scale prospective clinical trial to validate our findings. Copyright © 2022 Huang, Wang, Zhu, Huang, Li, Wang and Liu.,Importance: Understanding how socioeconomic factors are associated with cognitive aging is important for addressing health disparities in Alzheimer disease. Objective: To examine the association of neighborhood disadvantage with cognition among a multiethnic cohort of older adults. Design, Setting, and Participants: In this cross-sectional study, data were collected between September 1, 2017, and May 31, 2022. Participants were from the Health and Aging Brain Study-Health Disparities, which is a community-based single-center study in the Dallas/Fort Worth area of Texas. A total of 1614 Mexican American and non-Hispanic White adults 50 years and older were included. Exposure: Neighborhood disadvantage for participants' current residence was measured by the validated Area Deprivation Index (ADI); ADI Texas state deciles were converted to quintiles, with quintile 1 representing the least disadvantaged area and quintile 5 the most disadvantaged area. Covariates included age, sex, and educational level. Main Outcomes and Measures: Performance on cognitive tests assessing memory, language, attention, processing speed, and executive functioning; measures included the Spanish-English Verbal Learning Test (SEVLT) Learning and Delayed Recall subscales; Wechsler Memory Scale, third edition (WMS-III) Digit Span Forward, Digit Span Backward, and Logical Memory 1 and 2 subscales; Trail Making Test (TMT) parts A and B; Digit Symbol Substitution Test (DSST); Letter Fluency; and Animal Naming. Raw scores were used for analyses. Associations between neighborhood disadvantage and neuropsychological performance were examined via demographically adjusted linear regression models stratified by ethnic group. Results: Among 1614 older adults (mean [SD] age, 66.3 [8.7] years; 980 women [60.7%]), 853 were Mexican American (mean [SD] age, 63.9 [7.9] years; 566 women [66.4%]), and 761 were non-Hispanic White (mean [SD] age, 69.1 [8.7] years; 414 women [54.4%]). Older Mexican American adults were more likely to reside in the most disadvantaged areas (ADI quintiles 3-5), with 280 individuals (32.8%) living in ADI quintile 5, whereas a large proportion of older non-Hispanic White adults resided in ADI quintile 1 (296 individuals [38.9%]). Mexican American individuals living in more disadvantaged areas had worse performance than those living in ADI quintile 1 on 7 of 11 cognitive tests, including SEVLT Learning (ADI quintile 5: ? = -2.50; 95% CI, -4.46 to -0.54), SEVLT Delayed Recall (eg, ADI quintile 3: ? = -1.11; 95% CI, -1.97 to -0.24), WMS-III Digit Span Forward (eg, ADI quintile 4: ? = -1.14; 95% CI, -1.60 to -0.67), TMT part A (ADI quintile 5: ? = 7.85; 95% CI, 1.28-14.42), TMT part B (eg, ADI quintile 5: ? = 31.5; 95% CI, 12.16-51.35), Letter Fluency (ADI quintile 4: ? = -2.91; 95% CI, -5.39 to -0.43), and DSST (eg, ADI quintile 5: ? = -4.45; 95% CI, -6.77 to -2.14). In contrast, only non-Hispanic White individuals living in ADI quintile 4 had worse performance than those living in ADI quintile 1 on 4 of 11 cognitive tests, including SEVLT Learning (? = -2.35; 95% CI, -4.40 to -0.30), SEVLT Delayed Recall (? = -0.95; 95% CI, -1.73 to -0.17), TMT part B (? = 15.95; 95% CI, 2.47-29.44), and DSST (? = -3.96; 95% CI, -6.49 to -1.43). Conclusions and Relevance: In this cross-sectional study, aging in a disadvantaged area was associated with worse cognitive functioning, particularly for older Mexican American adults. Future studies examining the implications of exposure to neighborhood disadvantage across the life span will be important for improving cognitive outcomes in diverse populations. © 2023 American Medical Association. All rights reserved."
225,224,16,224_Object Detection Techniques with Attention and Fusion for Saliency and Multimodal Image Analysis,Object Detection Techniques with Attention and Fusion for Saliency and Multimodal Image Analysis,"Although deep learning-based techniques for salient object detection have considerably improved over recent years, estimated saliency maps still exhibit imprecise predictions owing to the internal complexity and indefinite boundaries of salient objects of varying sizes. Existing methods emphasize the design of an exemplary structure to integrate multi-level features by employing multi-scale features and attention modules to filter salient regions from cluttered scenarios. We propose a saliency detection network based on three novel contributions. First, we use a dense feature extraction unit (DFEU) by introducing large kernels of asymmetric and grouped-wise convolutions with channel reshuffling. The DFEU extracts semantically enriched features with large receptive fields and reduces the gridding problem and parameter sizes for subsequent operations. Second, we suggest a cross-feature integration unit (CFIU) that extracts semantically enriched features from their high resolutions using dense short connections and sub-samples the integrated information into different attentional branches based on the inputs received for each stage of the backbone. The embedded independent attentional branches can observe the importance of the sub-regions for a salient object. With the constraint-wise growth of the sub-attentional branches at various stages, the CFIU can efficiently avoid global and local feature dilution effects by extracting semantically enriched features via dense short-connections from high and low levels. Finally, a contour-aware saliency refinement unit (CSRU) was devised by blending the contour and contextual features in a progressive dense connected fashion to assist the model toward obtaining more accurate saliency maps with precise boundaries in complex and perplexing scenarios. Our proposed model was analyzed with ResNet-50 and VGG-16 and outperforms most contemporary techniques with fewer parameters. © 2022 by the authors.,Compared with the annotated data for the 2D image saliency prediction task, the annotated data for training omnidirectional image (or 360° image) saliency prediction models are not sufficient. Most existing fully-supervised saliency prediction methods for omnidirectional images (ODIs) adopt a scheme, first training the methods on a labeled large 2D image saliency prediction dataset and then fine-tuning the methods on the labeled tiny ODI saliency prediction dataset. However, this strategy is time-consuming and may not inadequately mine the visual features built in ODIs. To explore the visual attributes targeted at ODIs and address the shortage of labels on these ODIs, in this paper, we propose an end-to-end semi-supervised network, namely VFNet, which relies on viewport features and only utilizes ODIs as training data, for ODI saliency prediction. Concretely, we adopt consistency regularization as our semi-supervised learning framework. The predictions between main and auxiliary saliency inference networks in the VFNet enforce consistency. Aiming at ODIs, we introduce a new form of perturbation, i.e., DropView, to improve the effectiveness of consistency regularization. By randomly dropping out different 360° cubemap viewport features before the auxiliary saliency inference network, the proposed DropView enhances the robustness of the final ODI saliency prediction. To adaptively interact with the equirectangular and different cubemap viewport features according to their contributions, we introduce a Viewport Feature Adaptive Integration (VFAI) module and deploy the VFAI module at different levels in the VFNet to raise the capacity of feature encoding of our VFNet. Compared with state-of-the-art fully-supervised methods, our VFNet with fewer labeled training data achieves competitive performance demonstrated by extensive experiments on two publicly available datasets. © 2022 Elsevier B.V.,Co-saliency detection targets at segmenting the common salient objects in a group of relevant images. The current co-salient object detection methods based on deep learning have two limitations: (1) There is only a single target in training images, which can not provide adversarial samples for the model, making the model have poor generalization performance. When facing the interference of unknown class targets, similar salient objects, noisy background environments and so on, the model is greatly limited; (2) The existing methods usually use convolution neural networks (CNNs) to extract features. However, the CNNs can not obtain a large receptive field which makes the model unable to fully model the long-range dependencies, resulting in poor discriminative capability of the model. To this end, we propose a co-saliency detection transformer guided by intra-group adversarial mixup. Aiming at building the co-saliency detection network from a perspective of sequence-to-sequencc and training the model on mixup adversarial data, making the model more generic. Our network mainly contains two parts, a mixup subnetwork and a co-saliency detection transformer. Specifically, in the mixup sub-network, we propose an object refinement module: we set input class activation maps(CAMs) as guidance to segment salient objects with smooth edges as the adversarial objects in an unsupervised way; a distance adjusting module: the adversarial objects are mixed into another group of images with the minimum overlap, constructing the mixed training data. In the co-saliency detection transformer, we construct the model from sequence-to-sequence. In this part, we design a task injector, which can inject group information and saliency information into the feature sequence, and we adopt self-attention to fully capture global information between features. Finally, we mix the group information and saliency information by self-attention, further enhancing the discriminative capability of the feature and generating the Precise results of co-saliency detection. Extensive experiments are carried out on three benchmark datasets including Cosal2015, CoCA, and CoS()D3k, demonstrating superiority of our method to state-of-the-art methods. © 2023 Science Press. All rights reserved."
226,225,16,225_Plasticity and Synchronization in Neural Networks,Plasticity and Synchronization in Neural Networks,"Synchronization is a key feature of the brain dynamics and is necessary for information transmission across brain regions and in higher brain functions like cognition, learning and memory. Experimental findings demonstrated that in cortical microcircuits there are multiple synapses between pairs of connected neurons. Synchronization of neurons in the presence of multiple synaptic connections may be relevant for optimal learning and memory, however, its effect on the dynamics of the neurons is not adequately studied. Here, we address the question that how changes in the strength of the synaptic connections and transmission delays between neurons impact synchronization in a two-neuron system with multiple synapses. To this end, we analytically and computationally investigated synchronization dynamics by considering both phase oscillator model and conductance-based Hodgkin–Huxley (HH) model. Our results show that symmetry/asymmetry of feedforward and feedback connections crucially determines stability of the phase locking of the system based on the strength of connections and delays. In both models, the two-neuron system with multiple synapses achieves in-phase synchrony in the presence of small and large delays, whereas an anti-phase synchronization state is favored for median delays. Our findings can expand the understanding of the functional role of multisynaptic contacts in neuronal synchronization and may shed light on the dynamical consequences of pathological multisynaptic connectivity in a number of brain disorders. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.,Attractor networks are an influential theory for memory storage in brain systems. This theory has recently been challenged by the observation of strong temporal variability in neuronal recordings during memory tasks. In this work, we study a sparsely connected attractor network where memories are learned according to a Hebbian synaptic plasticity rule. After recapitulating known results for the continuous, sparsely connected Hopfield model, we investigate a model in which new memories are learned continuously and old memories are forgotten, using an online synaptic plasticity rule. We show that for a forgetting timescale that optimizes storage capacity, the qualitative features of the network's memory retrieval dynamics are age dependent: most recent memories are retrieved as fixed-point attractors while older memories are retrieved as chaotic attractors characterized by strong heterogeneity and temporal fluctuations. Therefore, fixed-point and chaotic attractors coexist in the network phase space. The network presents a continuum of statistically distinguishable memory states, where chaotic fluctuations appear abruptly above a critical age and then increase gradually until the memory disappears. We develop a dynamical mean field theory to analyze the age-dependent dynamics and compare the theory with simulations of large networks. We compute the optimal forgetting timescale for which the number of stored memories is maximized. We found that the maximum age at which memories can be retrieved is given by an instability at which old memories destabilize and the network converges instead to a more recent one. Our numerical simulations show that a high degree of sparsity is necessary for the dynamical mean field theory to accurately predict the network capacity. To test the robustness and biological plausibility of our results, we study numerically the dynamics of a network with learning rules and transfer function inferred from in vivo data in the online learning scenario. We found that all aspects of the network's dynamics characterized analytically in the simpler model also hold in this model. These results are highly robust to noise. Finally, our theory provides specific predictions for delay response tasks with aging memoranda. In particular, it predicts a higher degree of temporal fluctuations in retrieval states associated with older memories, and it also predicts fluctuations should be faster in older memories. Overall, our theory of attractor networks that continuously learn new information at the price of forgetting old memories can account for the observed diversity of retrieval states in the cortex, and in particular, the strong temporal fluctuations of cortical activity.  © 2023 authors. Published by the American Physical Society.,Memories may be encoded in the brain via strongly interconnected groups of neurons, called assemblies. The concept of Hebbian plasticity suggests that these assemblies are generated through synaptic plasticity, strengthening the recurrent connections within select groups of neurons that receive correlated stimulation. To remain stable in absence of such stimulation, the assemblies need to be self-reinforcing under the plasticity rule. Previous models of such assembly maintenance require additional mechanisms of fast homeostatic plasticity often with biologically implausible timescales. Here we provide a model of neuronal assembly generation and maintenance purely based on spike-timing-dependent plasticity (STDP) between excitatory neurons. It uses irregularly and stochastically spiking neurons and STDP that depresses connections of uncorrelated neurons. We find that assemblies do not grow beyond a certain size, because temporally imprecisely correlated spikes dominate the plasticity in large assemblies. Assemblies in the model can be learned or spontaneously emerge. The model allows for prominent, stable overlap structures between static assemblies. Further, assemblies can drift, particularly according to a novel, transient overlap-based mechanism. Finally the model indicates that assemblies grow in the aging brain, where connectivity decreases. © 2023 Manz, Memmesheimer. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
227,226,16,226_DDoS attack detection and defense in software-defined networking,DDoS attack detection and defense in software-defined networking,"Smart cities have experienced significant growth in recent years, transforming people’s lives into a more futuristic version. The smart city initiative includes a diverse collection of specifications, encompassing a large number of users whose requirements vary significantly and heterogeneously. Each device in smart cities generates a significant amount of data, which places a load on the gateways. Smart cities face a major challenge due to the enormous amount of data they generate. Through software-defined networking (SDN), network information paths are optimized, ensuring that traffic flow is evenly distributed across all network nodes. A considerable number of IoT devices with limited resources are susceptible to various security threats, such as device hijacking, ransomware, man-in-the-middle (MiM) attacks, and distributed denial-of-service (DDoS) attacks. These threats can pose a severe challenge to network security. Additionally, DDoS attacks have disrupted web businesses, resulting in the loss of valuable data. To counter DDoS attacks in a smart city, several options exist, yet many challenges remain. This research presents a secure and intelligent system to combat DDoS attacks on smart cities. SDN security controllers and machine learning models with optimization are employed in this study to reduce the impact of common DDoS attacks on smart cities. This work utilizes an SDN based on security controllers and a detection mechanism rooted in a machine learning model with optimization to mitigate various types of prevalent DDoS attacks within smart cities. Employing binary classification, XGBoost achieved an accuracy of 99.99%, precision of 97%, recall of 99%, an F1 score of 98%, and a false-positive rate of 0.05. In multiclass classification, the average accuracy is 99.29%, precision is 97.7%, recall is 96.69%, and the F1 score is 97.51%. These results highlight the superiority of this approach over other existing machine learning techniques. © 2023 by the author.,The major problem of internet security is a Distributed Denial-of-Service (DDoS) attack, which can’t be detected easily. This attack is said to have occurred when lots of service requests are simultaneously received at a server on the internet. This makes the server too busy to provide normal services for others. The Distributed Denial of Service (DDoS) attacks nature on large networks on the Internet demanding to develop the effective detection and response methods. The deployment of these technique should perform not only at the network core but also at the edge. A DDoS attack detection framework is presented based on transfer learning model consisting of 1D Convolution Neural Network (CNN) and decision tree classifier. The 1D CNN model utilizes for features extraction from the input network traffic data. This operation also reduces the dimension of the data thereby removing the redundancy in the data. These features are given to the decision tree model for classification. The proposed framework identified the DDoS attacks with good accuracy. This system could identify attacks in real-time and provide network security. © 2023, Penerbit Akademia Baru. All rights reserved.,Distributed Denial of Service (DDoS) attack is one of the most destructive internet network attacks, denying legitimate users access to resources and networks by maliciously blocking available computing resources. Intruders send a large number of packets to the network in order to create a crowding effect. Unlike a Denial of Service (DoS) attack, where a single compromised source generates all of the traffic, a Distributed Denial of Service (DDoS) attack generates traffic from multiple compromised nodes spread across multiple geographies. To address the challenges posed by the Distributed Denial of Service (DDoS) attack, several researchers proposed a variety of solutions for early detection and prevention of the attack. Effective solutions for the prevention and early detection of Distributed Denial of Service (DDoS) attacks, on the other hand, have yet to be developed, and the problem remains a prominent research focus area. This paper tries to present a novel and optimal solution for detecting Distributed Denial of Service (DDoS) attacks on internet networks more quickly and accurately. The proposed model is an anomaly-based real-time prevention model for web networks. The model is based on machine learning principles and can effectively counter new types of Distributed Denial of Service (DDoS) attacks. To demonstrate the efficiency, accuracy, model robustness, and relative of the proposed model, a simulation study was run on an LLDOS session log, and the results indicated that the model performed better than benchmark models found in the literature. © 2022 The Authors"
228,227,16,227_Deep learning accelerated topology optimization with boundary-oriented graph embedding and morphable components.,Deep learning accelerated topology optimization with boundary-oriented graph embedding and morphable components.,"Classical optimization methods require finite element analysis in iterations, which increase the computing time and decrease the algorithmic efficiency. The deep learning model can potentially realize real-time topology optimization design, but it normally requires large training set. This paper presents a real-time topology optimization algorithm based on the Moving Morphable Component (MMC) method using a Convolutional Neural Network (CNN). The optimization algorithm uses a new data pre-processing method, which can preserve the numerical characteristics and smoothness of the structure boundary, hence it can help CNN to capture data features with a limited sample set. The topology optimization boundary information of the optimized result is used as the sample set label to avoid the components dislocation phenomenon. The new algorithm effectiveness has been verified with several examples. The trained model can significantly improve the optimization efficiency of the MMC method and offer accurate results with a clear structure boundary. © 2023 Elsevier Ltd,In the context of mechanical engineering design, the field of machine learning accelerated topology optimization is dominated by image-based models trained in a supervised manner on datasets with a limited diversity of boundary conditions. State-of-the-art methods show poor generalization capabilities and are strongly coupled to finite element mesh resolution, hindering scalability. In this paper, we leverage the explicit topology parameterization of the moving morphable components (MMC) framework to train a deep learning model that directly generates geometric design variables using a model architecture that is independent of the finite element mesh used for structural analysis. The developed model is trained on a large dataset of boundary conditions. Despite achieving state-of-the-art regression loss, evaluations reveal that direct-design approaches generate topologies with poor mechanical performance. Specifically, the model-generated topologies have, on average, a stiffness 11.48% lower than conventional MMC designs, as evidenced by in-distribution and out-of-distribution test samples. We demonstrate that this is due to the incompatibility between the regression loss function typically used in literature and the topology optimization objective of compliance minimization. To address this issue, we propose a novel acceleration approach that leverages the trained model to generate improved initial designs for conventional optimization. Specifically, the deep learning model is used to generate an initial design, which is then refined by conventional optimization to arrive at a final, optimal design. This approach shows a computation time-saving of 36.84% without sacrificing the final mechanical performance of the optimal topology compared to conventional optimization starting from a uniform initial layout. © 2023 Elsevier Ltd,Topology optimization design provides innovative structures with excellent thermal, mechanical and acoustic performance for modern engineering. Moving Morphable Component (MMC), as an emerging explicit topology optimization method, can effectively avoid many optimization problems such as the checkerboard phenomenon, however, its optimization iteration process still consumes considerable time, which makes real-time structural topology optimization impossible. Therefore, a lightweight and high-efficiency convolutional neural network, the improved convolutional block attention U-Net (Cba-U-Net) model, is proposed for topology-optimized configuration prediction, which avoids its own tedious iterative computation process and acquires the topology configuration in real-time. It is demonstrated that the proposed network not only obtains accurate topology-optimized configurations in negligible time but also has an accuracy rate of 91.42% compared to other deep learning models. The improved Cba-U-Net model is suitable not only for Moving Morphable Components but also for other optimization algorithms, such as Solid Isotropic Material with Penalization (SIMP) and Evolutionary Structural Optimization Method (ESO). By combining deep learning with topological optimization algorithms, this form of optimization is highly generalizable for practical large-scale projects. © 2022 Elsevier Ltd"
229,228,16,228_Vegetation Drought Responses and Atmospheric Carbon Dynamics in China,Vegetation Drought Responses and Atmospheric Carbon Dynamics in China,"Soil respiration, defined as the total flux of carbon dioxide (CO2) from the soil to the atmosphere, is a key ecosystem process that affects the regional and global carbon (C) cycles and is highly sensitive to temperature and soil moisture. It is challenging to quantify soil respiration at the ecosystem level from commonly used in-situ soil chamber measurements because of large spatial variability. Methods that provide temporally and spatially continuous estimates of soil respiration at various scales are vital to understand the impact of climate change on soil C stock. In this study, we evaluate three commonly used empirical models and a Random Forest machine learning algorithm applied to satellite derived estimates of land surface temperature (LST) and soil moisture to estimate soil respiration in temperate deciduous and coniferous forests in Canada. The models were calibrated using in-situ soil temperature and moisture and validated against in-situ measurements of soil CO2 fluxes (gCm?2day?1) from automatic soil chambers. We separately evaluate the performance of nighttime and daytime satellite-based LST and soil moisture observations in modeling soil respiration. The soil respiration models were also evaluated at daily and monthly time scales against in-situ measurements. Results indicate that models based on satellite LST, and soil moisture can explain more than 70% of the variability in observed soil respiration. Nighttime LST at a monthly time scale resulted in consistently higher accuracy than daytime LST in estimating soil respiration. Satellite observations resulted in comparable accuracy in estimating soil respiration as in-situ measurements. Satellite LST and soil moisture observations are indispensable data sources to estimate soil respiration at ecosystem level and its upscaling to regional and global scales. © 2023,Atmospheric methane (CH4) concentration has been increasing recently, contributing to global warming. As natural sinks, forest soils are expected to mitigate the atmospheric CH4 rise. However, it had been difficult to measure soil CH4 flux continuously and accurately because of the limited stability and precision of CH4 analyzers in the field. In this study, we measured hourly CH4 flux with plant roots (Root) and without roots (Trench) during the growing season in a regenerating deciduous forest using an automated chamber system with an up-to-date analyzer. Combined with a Random Forest (RF) approach, we studied the spatiotemporal variation and control of soil CH4 uptake rates. The results showed that the soil was a CH4 sink throughout the experimental period, and the existence of roots significantly enhanced CH4 uptake, mainly through improving soil aeration. The CH4 uptake rate varied seasonally according to the variations in soil gaseous diffusion caused by soil moisture and temperature differences. In addition, soil CH4 uptake showed a significant spatial variation, mainly resulting from spatial difference in soil porosity, soil carbon and nitrogen contents, and fine root biomass. The RF models showed high performance in soil CH4 flux prediction using the soil O2 diffusion coefficient and soil temperature as explanatory variables. The performance of RF models using ordinary variables of soil water content or water-filled pore space (WFPS) was equal to or slightly better than that of models using the diffusion coefficient. The higher importance of ambient CH4 concentration in Trench chambers indicates an increase in soil CH4 uptake at higher CH4 concentrations, which is predicted in the future. Although there are limitations, we believe that a machine learning approach, such as RF, using a large amount of continuous data with high temporal resolution, has great potential for investigating the dynamic variation in soil CH4 flux. © 2023,China's forests play a vital role in the global carbon cycle through the absorption of atmospheric CO2 to mitigate climate change caused by the increase of anthropogenic CO2. It is essential to evaluate the carbon sink potential (CSP) of China's forest ecosystem. Combining NDVI, field-investigated, and vegetation and soil carbon density data modeled by process-based models, we developed the state-of-the-art learning ensembles model of process-based models (the multi-model random forest ensemble (MMRFE) model) to evaluate the carbon stocks of China's forest ecosystem in historical (1982–2021) and future (2022–2081, without NDVI-driven data) periods. Meanwhile, we proposed a new carbon sink index (CSindex) to scientifically and accurately evaluate carbon sink status and identify carbon sink intensity zones, reducing the probability of random misjudgments as a carbon sink. The new MMRFE models showed good simulation results in simulating forest vegetation and soil carbon density in China (significant positive correlation with the observed values, r = 0.94, P < 0.001). The modeled results show that a cumulative increase of 1.33 Pg C in historical carbon stocks of forest ecosystem is equivalent to 48.62 Bt CO2, which is approximately 52.03% of the cumulative increased CO2 emissions in China from 1959 to 2018. In the next 60 years, China's forest ecosystem will absorb annually 1.69 (RCP45 scenario) to 1.85 (RCP85 scenario) Bt CO2. Compared with the carbon stock in the historical period, the cumulative absorption of CO2 by China's forest ecosystem in 2032–2036, 2062–2066, and 2077–2081 are approximately 11.25–39.68, 110.66–121.49 and 101.31–111.11 Bt CO2, respectively. In historical and future periods, the medium and strong carbon sink intensity regions identified by the historical CSindex covered 65% of the total forest area, cumulative absorbing approximately 31.60 and 65.83–72.22 Bt CO2, respectively. In the future, China's forest ecosystem has a large CSP with a non-continuous increasing trend. However, the CSP should not be underestimated. Notably, the medium carbon sink intensity region should be the priority for natural carbon sequestration action. This study not only provides an important methodological basis for accurately estimating the future CSP of forest ecosystem but also provides important decision support for future forest ecosystem carbon sequestration action. © 2023 The Authors"
230,229,16,229_Multimode Process Monitoring with Soft Sensors and Fault-Introducing Tool Groups,Multimode Process Monitoring with Soft Sensors and Fault-Introducing Tool Groups,"In industrial processes, long short-term memory (LSTM) is usually used for temporal dynamic modeling of soft sensor. The process data usually have various temporal correlations under different time scales due to the continuous physical and chemical reactions. However, LSTM model can only extract the dynamic features at a specific time scale, which affects the feature learning capability and modeling accuracy. In this article, a new hierarchical sequential generative network (HSGN) is proposed for mining multiscale dynamic features using large amount of unlabeled process data for soft sensor. To extract multiscale dynamic features for quality prediction, the process data are resampled with different sampling rates and then used to pretrain the corresponding self-learning LSTM models at different time scales. Subsequently, they can used to calculate the multiscale hidden feature states for labeled samples, which are further integrated with the original input information and input into a deep belief network (DBN) to construct the prediction model for the output variable. Thus, the HSGN method can take advantage of large number of unlabeled samples to mine multiscale dynamic hidden features and overcome the irregular sampling problem in industrial processes. The application in a real industrial scene shows the effectiveness of the proposed method.  © 2001-2012 IEEE.,In real industrial processes, factors, such as the change in manufacturing strategy and production technology lead to the creation of multimode industrial processes and the continuous emergence of new modes. Although the industrial SCADA system has accumulated a large amount of historical data, which can be used for modeling and monitoring multimode processes to a certain extent, it is difficult for the model learned from historical data to adapt to emerging modes, resulting in the model mismatch. On the other hand, updating the model with data from new modes allows the model to continuously match the new modes, but it may cause the model to lose the ability to represent the historical modes, resulting in 'catastrophic forgetting.' To address these problems, this article proposed a jointly mode-matching and similarity-preserving dictionary learning (JMSDL) method, which updated the model by learning the data of new modes, so that the model can adaptively match the newly emerged modes. At the same time, a similarity metric was put forward to guarantee the representation ability of the proposed method for historical data. A numerical simulation experiment, the CSTH process experiment, and an industrial roasting process experiment indicated that the proposed JMSDL method can match new modes while maintaining its performance on the historical modes accurately. In addition, the proposed method significantly outperforms the state-of-the-art methods in terms of fault detection and false alarm rate.  © 2013 IEEE.,This paper introduces a novel sparse dynamic inner principal component analysis (SDiPCA) based monitoring for multimode dynamic processes. Different from traditional multimode monitoring algorithms, a model is updated for sequential modes by memorizing the significant features of existing modes. By adopting the concept of intelligent synapses in continual learning, a loss of quadratic term is introduced to penalize the changes of mode-relevant parameters, where modified synaptic intelligence (MSI) is proposed to estimate the parameter importance. Thus, the proposed algorithm is referred to as SDiPCA-MSI. When a new mode arrives, a set of normal samples should be collected. The previous significant features are consolidated without explicitly storing training samples, while extracting new information from the current mode. Consequently, SDiPCA-MSI can provide outstanding performance for successive modes. Characteristics of the proposed approach are discussed, including the computational complexity, advantages and potential limitations. Compared with several state-of-the-art monitoring methods, the effectiveness and superiorities of the proposed method are demonstrated by a continuous stirred tank heater case and a practical industrial system. Note to Practitioners - Multimode process monitoring is increasingly significant as industrial systems generally operate in varying operating conditions. However, most researches focus on multiple local monitoring models for complex multimode processes and assume that data of all possible modes are available and stored before learning. When similar or new modes arrive, local models are rebuilt corresponding to each mode and the model's capacity would increase with the continuous emergence of modes. Adaptive methods are a branch of multimode monitoring algorithms, but they strive to extract information of the current mode to ensure the monitoring performance while forgetting the previously learned knowledge gradually. This paper proposes a novel sparse dynamic inner principal component analysis with continual learning ability for multimode dynamic process monitoring, where modified synaptic intelligence is developed to measure the parameter importance accurately. It requires limited computation and storage resources for successive modes, which is convenient for practical applications. Similar to current multimode process monitoring algorithms, a set of data should be collected before learning a new mode, which may bring difficulties to real-time monitoring. For industrial systems, such as large-scale power plants and chemical systems, the proposed method has outstanding ability to monitor successive dynamic modes.  © 2004-2012 IEEE."
231,230,16,230_HDI-LFA-Tensor-Decomposition-DWDN,HDI-LFA-Tensor-Decomposition-DWDN,"High-dimensional and incomplete (HDI) data are omnipresent in a variety of Big Data-related applications. Latent feature analysis (LFA) is a typical representation learning method that can extract useful yet latent knowledge from HDI data via low-rank embedding. Existing LFA-based models mostly adopt a single-metric-based modeling strategy, where the representation designed for the embedding Loss function is fixed and exclusive. However, real-world HDI data are commonly heterogeneous and have large diverse underlying patterns, making a single-metric-based model cannot represent such HDI data in a comprehensive and unbiased fashion. Motivated by this discovery, this paper proposes a multi-metric latent feature (MMLF) model whose ideas are two-fold: 1) two vector spaces and three Lp-norms are simultaneously adopted to develop six LFA variants, each of which possesses a unique merit, and 2) all the variants are aggregated with a tailored, self-adaptive weighting strategy. As such, the proposed MMLF enjoys the merits originated from a set of disparate metric spaces all at once, achieving the comprehensive and unbiased representation of HDI data. Theoretical study guarantees that MMLF attains evident performance gain. Extensive experiments on ten real-world HDI matrices, spanning a wide range of industrial and scientific areas, verify that the proposed MMLF significantly outperforms nine state-of-the-art, shallow and deep counterparts. IEEE,High-dimensional and incomplete (HDI) data are commonly encountered in various big data-related applications concerning the complex interactions among numerous nodes, such as the user-item iterations in a recommender system. A stochastic gradient descent (SGD)-based latent factor analysis (LFA) model can perform efficient representation learning to such HDI data, thereby extracting useful knowledge from them. However, a standard SGD algorithm updates a latent factor based on the current stochastic gradient only, without the considerations on the past information, making a resultant model suffer from slow convergence. To address this critical issue, this paper proposes an Adaptive Non-linear PID-incorporated SGD (ANPS) algorithm with two-fold ideas: 1) rebuilding the instant learning error when computing the stochastic gradient following the principle of a nonlinear PID controller to incorporate past update information into the learning scheme efficiently, and 2) implementing gain parameter adaptation following the principle of particle swarm optimization (PSO). Experiments on six widely-adopted HDI datasets demonstrate that compared with state-of-the-art LFA models, an ANPS-based LFA model achieves significant advantage in both efficiency and accuracy. Moreover, its flexible gain parameter adaptation mechanism greatly boosts its practicability for real issues. <italic>Note to Practitioners</italic>&#x2014;In many industrial applications like recommender systems, social network systems, and cloud service systems, people usually encounter numerous nodes and their highly-incomplete relationships. An HDI matrix is commonly adopted to describe such specific relationships. One of the major challenges is to acquire useful knowledge from an HDI matrix efficiently and accurately for various data analysis tasks, e.g., accurate recommendation, community detection, and web service selection. An SGD-based LFA model has been widely adopted to tackle this issue. However, it suffers from slow convergence that leads to considerable time cost on large-scale datasets. This study proposes an ANPS algorithm following the principle of a nonlinear PID controller. With it, an ANPS-based LFA model is achieved, which possesses fast convergence rate on an industrial HDI matrix. The proposed ANPS algorithm can be leveraged for different types of various machine learning models, thereby improving their utility and scalability in practice. IEEE,Latent factor analysis (LFA) is efficient in knowledge discovery from a high-dimensional and incomplete (HDI) matrix frequently encountered in industrial big data-related applications. A stochastic gradient descent (SGD) algorithm is commonly adopted as a learning algorithm for LFA owing to its high efficiency. However, its sequential nature makes it less scalable when processing large-scale data. Although alternating SGD decouples an LFA process to achieve parallelization, its performance relies on its hyper-parameters that are highly expensive to tune. To address this issue, this paper presents three extended alternating SGD algorithms whose hyper-parameters are made adaptive through particle swarm optimization. Correspondingly, three Parallel Adaptive LFA (PAL) models are proposed and achieve highly efficient latent factor acquisition from an HDI matrix. Experiments have been conducted on four HDI matrices collected from industrial applications, and the benchmark models are LFA models based on state-of-the-art parallel SGD algorithms including the alternative SGD, Hogwild!, distributed gradient descent, and sparse matrix factorization parallelization. The results demonstrate that compared with the benchmarks, with 32 threads, the proposed PAL models achieve much speedup gain. They achieve the highest prediction accuracy for missing data on most cases. <italic>Note to Practitioners</italic>&#x2014;HDI data are commonly encountered in many industrial big data-related applications, where rich knowledge and patterns can be extracted efficiently. An SGD based-LFA model is popular in addressing HDI data due to its efficiency. Yet when dealing with large-scale HDI data, its serial nature greatly reduces its scalability. Although alternating SGD can decouple an LFA process to implement parallelization, its performance depends on its hyper-parameter whose tuning is tedious. To address this vital issue, this study proposes three extended alternating SGD algorithms whose hyper-parameters are made via through a particle swarm optimizer. Based on them, three models are realized, which are able to efficiently obtain latent factors from HDI matrices. Compared with the existing and state-of-the-art models, they enjoy their hyper-parameter-adaptive learning process, as well as highly competitive computational efficiency and representation learning ability. Hence, they provide practitioners with more scalable solutions when addressing large HDI data from industrial applications. IEEE"
232,231,16,231_Unsupervised domain adaptation for remote sensing semantic segmentation,Unsupervised domain adaptation for remote sensing semantic segmentation,"Unsupervised domain adaptation (UDA) aims at adapting a model from the source domain to the target domain by tackling the issue of domain shift. Cross-domain segmentation of remote sensing images (RSIs) remains a big challenge due to the unique properties of RSIs. On the one hand, the divergence of data distribution in different local regions leads to negative transfer by directly applying the global alignment method in RSIs. On the other hand, the underlying category-level structure in the target domain is often ignored, which confuses the decision of semantic boundaries on the dispersed category features caused by large intraclass variance and small interclass variance in RSIs. In this study, we propose a novel fine-grained adaptation framework combining two stages of global-local alignment and category-level alignment to solve the above-mentioned problems. In the first stage of global-local adaptation, an attention map is derived from an intermediate discriminator and focuses on hard-to-align regions to mitigate negative transfer due to global adversarial learning. In the second stage of category-level adaptation, the category feature compact module is utilized to address the issue of dispersed features in the target domain attained by the cross-domain network, which will facilitate the fine-grained alignment of categories. Experiments under various scenarios, including geographic location variation and spectral band composition variation, demonstrate that the local adaptation and category-level adaptation of RSIs are complementary in the cross-domain segmentation, and the integrated framework helps achieve outstanding performance for UDA semantic segmentation of RSIs. © 2008-2012 IEEE.,Semantic segmentation techniques for remote sensing images (RSIs) have been widely developed and applied. However, most segmentation methods depend on sufficiently annotated data for specific scenarios. When a large change occurs in the target scenes, model performance drops significantly. Therefore, unsupervised domain adaptation (UDA) for semantic segmentation is proposed to alleviate the reliance on expensive per-pixel densely labeled data. In this paper, two key issues of existing domain adaptive (DA) methods are considered: (1) the factors that cause data distribution shifts in RSIs may be complex and diverse, and existing DA approaches cannot adaptively optimize for different domain discrepancy scenarios; (2) domain-invariant feature alignment, based on adversarial training (AT), is prone to excessive feature perturbation, leading to over robust models. To address these issues, we propose an AdvCDA method that guides the model to adapt adversarial perturbation consistency. We combine consistency regularization to consider interdomain feature alignment as perturbation information in the feature space, and thus propose a joint AT and self-training (ST) DA method to further promote the generalization performance of the model. Additionally, we propose a confidence estimation mechanism that determines network stream training weights so that the model can adaptively adjust the optimization direction. Extensive experiments have been conducted on Potsdam, Vaihingen, and LoveDA remote sensing datasets, and the results demonstrate that the proposed method can significantly improve the UDA performance in various cross-domain scenarios. © 2023 by the authors.,In unsupervised domain adaptation (UDA) of remote sensing images (RSIs), the huge interdomain discrepancies and intradomain variances lead to complicated class-level relations. Specifically, the instances of the same class differ greatly, while instances of different classes are similar, whether across different RSIs domains or within the same RSIs domain. However, existing methods cannot fully consider these problems, limiting the performance of UDA semantic segmentation of RSIs. To this end, this article proposes a novel cross-domain multiprototypes learning method, the core idea of which is to abstract the cross-domain and intradomain class-level relations into multiple prototypes. Specifically, the multiple prototypes belonging to different classes can detailedly describe complex interclass relations, and the multiple prototypes within the same class can better model rich intraclass relations. Furthermore, the source and target samples are jointly used for prototypes calculation, to fully fuse the feature information of different RSIs. In a nutshell, utilizing the samples from different RSIs domains to learn multiple prototypes for each class can achieve better domain alignment at the class level. In addition, considering that RSIs simultaneously contain large targets with wide coverage and important small targets, two masked consistency learning strategies are designed to better explore the contextual structure of target RSIs and improve the quality of pseudo labels for prototype updating. The global consistency strategy can strengthen the utilization of global context relations, while the local consistency strategy can further improve the learning of local context details. Therefore, the proposed method is actually a prototype and context-enhanced learning method for UDA semantic segmentation of RSIs. Extensive experiments demonstrate that the proposed method can achieve better performance than existing state-of-the-art UDA methods.  © 1980-2012 IEEE."
233,232,16,232_Large-scale Game Dynamics and Learning,Large-scale Game Dynamics and Learning,"The emergence of animal societies is a major evolutionary transition, but its implications for learning-dependent innovations are insufficiently understood. Bees, with lifestyles ranging from solitary to eusocial, are ideal models for exploring social evolution. Here, we ask how and why bees may acquire a new 'technology', foraging on morphologically complex flowers, and whether eusociality facilitates this technological shift. We consider 'complex' flowers that produce high food rewards but are difficult to access, versus 'simple' flowers offering easily accessible yet lower rewards. Complex flowers are less profitable than simple flowers to naive bees but become more rewarding after a learning period. We model how social bees optimally choose between simple and complex flowers over time, to maximize their colony's food balance. The model predicts no effect of colony size on the bees' flower choices. More foraging on complex flowers is predicted as colony longevity, its proportion of foragers, individual longevity and learning ability increase. Of these traits, only long-lived colonies and abundant foragers characterize eusocial bees. Thus, we predict that eusociality supports, but is not mandatory for, learning to exploit complex flowers. A re-analysis of a large published dataset of bee-flower interactions supports these conclusions. We discuss parallels between the evolution of insect sociality and other major transitions that provide scaffolds for learning innovations. This article is part of the theme issue 'Human socio-cultural evolution in light of evolutionary transitions'.  © 2023 The Author(s).,In evolutionary game theory, the emergence and maintenance of group cooperative behavior is usually challenged by the lure of high-payoff defection behavior.Recently, besides imitation rules, the adaptive ability of individuals under limited information is also the key to adjust strategies, for example, individuals based on reinforcement learning rules by judging whether previous performance is satisfactory. In realistic scenarios, individuals with rich experience usually lead those who are inexperienced and play a guiding role in the group. Here we propose a multi-guider game model. Players on each layer of the network play different roles and follow different strategy update rules. Specifically, guiders use reinforcement learning rules to update their strategies in the upper network, and guided players use payoff-based imitation rules to update their strategies in the lower network. As the evolution progresses, guided players in the lower layer begin to reference the experienced guiders in the upper layer to update their strategies. A large number of Monte Carlo simulation results show that inexperienced individuals in the group are able to learn from the experience of others with the experienced guidance of multiple guiders. In addition to the improvement of group decision-making, the cooperative behavior can also be maintained at a higher level in the simulation of social dilemma. © 2023,Imitation and aspiration learning rules are frequently observed in humans and animals. The former is an act of copying other’s action, whereas the latter is characterized by the self-evaluation. Here we study the coexistence of these learning mechanisms in structured populations. Both rules have been combined focusing on two different scenarios: (I) adoption of either update rule with a certain probability, and (II) grouping the entire population according to the update rules. We present two pair approximation models, illustrating both scenarios, which yield a nice agreement—under weak selection—with that of agent-based simulations. For weak selection and large population size, we find that the condition for cooperation to dominate defection is similar in both heterogeneous and homogeneous update rules. We examine several variants of the mixed model such as time-evolving aspirations alongside strategies and the coevolution of strategies and update rules. In the former case, our simulation reveals that Prisoner’s dilemma and, in some cases, Stag-hunt experience overall less aspiration levels compared to other games such as Chicken or Trivial. The coevolution of strategies and update rules demonstrates a better cooperation, in contrast to the fixed update rule case, exhibiting the possibility of asymptotic coexistence of both learning mechanisms. © 2023 IOP Publishing Ltd & London Mathematical Society."
234,233,16,233_Video-based Surveillance and Crime Detection,Video-based Surveillance and Crime Detection,"Due to the ever increasing number of closed circuit television (CCTV) cameras worldwide, it is the need of the hour to automate the screening of video content. Still, the majority of video content is manually screened to detect some anomalous incidence or activity. Automatic abnormal event detection such as theft, burglary, or accidents may be helpful in many situations. However, there are significant difficulties in processing video data acquired by several cameras at a central location, such as bandwidth, latency, large computing resource needs, and so on. To address this issue, an edge-based visual surveillance technique has been implemented, in which video analytics are performed on the edge nodes to detect aberrant incidents in the video stream. Various deep learning models were trained to distinguish 13 different categories of aberrant incidences in video. A customized Bi-LSTM model outperforms existing cutting-edge approaches. This approach is used on edge nodes to process video locally. The user can receive analytics reports and notifications. The experimental findings suggest that the proposed system is appropriate for visual surveillance with increased accuracy and lower cost and processing resources. © 2024 by the authors.,Surveillance cameras are increasingly being used worldwide due to the proliferation of digital video capturing, storage, and processing technologies. However, the large volume of video data generated makes it difficult for humans to perform real-time analysis, and even manual approaches can result in delayed detection of events. Automatic violence detection in surveillance footage has therefore gained significant attention in the scientific community as a way to address this challenge. With the advancement of machine learning algorithms, automatic video recognition tasks such as violence detection have become increasingly feasible. In this study, we investigate the use of smart networks that model the dynamic relationships between actors and/or objects using 3D convolutions to capture both the spatial and temporal structure of the data. We also leverage the knowledge learned by a pre-trained action recognition model for efficient and accurate violence detection in surveillance footage. We extend and evaluate several public datasets featuring diverse and challenging video content to assess the effectiveness of our proposed methods. Our results show that our approach outperforms state-of-the-art methods, achieving approximately a 2% improvement in accuracy with fewer model parameters. Additionally, our experiments demonstrate the robustness of our approach under common compression artifacts encountered in remote server processing applications.  © 2013 IEEE.,Purpose: This paper aims to implement and extend the You Only Live Once (YOLO) algorithm for detection of objects and activities. The advantage of YOLO is that it only runs a neural network once to detect the objects in an image, which is why it is powerful and fast. Cameras are found at many different crossroads and locations, but video processing of the feed through an object detection algorithm allows determining and tracking what is captured. Video Surveillance has many applications such as Car Tracking and tracking of people related to crime prevention. This paper provides exhaustive comparison between the existing methods and proposed method. Proposed method is found to have highest object detection accuracy. Design/methodology/approach: The goal of this research is to develop a deep learning framework to automate the task of analyzing video footage through object detection in images. This framework processes video feed or image frames from CCTV, webcam or a DroidCam, which allows the camera in a mobile phone to be used as a webcam for a laptop. The object detection algorithm, with its model trained on a large data set of images, is able to load in each image given as an input, process the image and determine the categories of the matching objects that it finds. As a proof of concept, this research demonstrates the algorithm on images of several different objects. This research implements and extends the YOLO algorithm for detection of objects and activities. The advantage of YOLO is that it only runs a neural network once to detect the objects in an image, which is why it is powerful and fast. Cameras are found at many different crossroads and locations, but video processing of the feed through an object detection algorithm allows determining and tracking what is captured. For video surveillance of traffic cameras, this has many applications, such as car tracking and person tracking for crime prevention. In this research, the implemented algorithm with the proposed methodology is compared against several different prior existing methods in literature. The proposed method was found to have the highest object detection accuracy for object detection and activity recognition, better than other existing methods. Findings: The results indicate that the proposed deep learning–based model can be implemented in real-time for object detection and activity recognition. The added features of car crash detection, fall detection and social distancing detection can be used to implement a real-time video surveillance system that can help save lives and protect people. Such a real-time video surveillance system could be installed at street and traffic cameras and in CCTV systems. When this system would detect a car crash or a fatal human or pedestrian fall with injury, it can be programmed to send automatic messages to the nearest local police, emergency and fire stations. When this system would detect a social distancing violation, it can be programmed to inform the local authorities or sound an alarm with a warning message to alert the public to maintain their distance and avoid spreading their aerosol particles that may cause the spread of viruses, including the COVID-19 virus. Originality/value: This paper proposes an improved and augmented version of the YOLOv3 model that has been extended to perform activity recognition, such as car crash detection, human fall detection and social distancing detection. The proposed model is based on a deep learning convolutional neural network model used to detect objects in images. The model is trained using the widely used and publicly available Common Objects in Context data set. The proposed model, being an extension of YOLO, can be implemented for real-time object and activity recognition. The proposed model had higher accuracies for both large-scale and all-scale object detection. This proposed model also exceeded all the other previous methods that were compared in extending and augmenting the object detection to activity recognition. The proposed model resulted in the highest accuracy for car crash detection, fall detection and social distancing detection. © 2023, Emerald Publishing Limited."
235,234,16,234_Deep learning-based Super-Resolution (SR) methods with advanced techniques and models,Deep learning-based Super-Resolution (SR) methods with advanced techniques and models,"Machine learning-based image super-resolution (SR) has garnered increasing research interest in recent years. However, there are two issues that have not been adequately addressed. The first issue is that existing SR methods often overlook the importance of improving the quality of the training dataset, which is a crucial factor in determining SR performance, regardless of the training method employed. The second issue is that while some studies report high numerical metrics, the visual results remain unsatisfactory. To address the first problem, we propose a new image down-sampling method to obtain higher-quality training datasets. To tackle the second problem, we present a new image super-resolution model based on a large-size convolution kernel and a multi-path algorithm. Specifically, we use an adaptive large-size convolutional kernel to extract features from the image based on the size of the input image, and a residual network to generate a deeper model to retain more details of the original input image. Experimental results demonstrate that the proposed multilayer downsampling method (MDM) can significantly improve the visual quality compared to traditional downsampling methods. Moreover, our proposed method achieves the best peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) values compared to several typical SR algorithms. Furthermore, subjective evaluation by human observers reveals that our method retains more details of the original image and produces smoother high-resolution images. Our proposed method effectively addresses the two aforementioned issues, which leads to improved SR performance in terms of both quantitative and qualitative measures. © 2024, The Author(s).,In single image super-resolution (SISR), deep neural networks learn mainly nonlinear functions to obtain promising high-resolution (HR) images. However, there are usually two undesired limitations to recovered images of existing SR methods. First, since this task is typically an ill-posed problem, recovering the structural information for the SR process is usually sharp edges but distorted. Second, since this task generally has an extremely large space for the mapping function, learning this mapping from low-resolution (LR) to HR images is typically difficult. For the most part, SR models usually suffer from the lack of structural information for the objects in images and poor performance. To address the above issues, we propose a dual gradient regression scheme in the first stage of the network to recover the structure information of objects in images and the corresponding loss function to learn the difference between structural maps. In the second stage of the network, we restore HR gradient maps by the first stage to provide additional structural information for the second stage, further constraining the structure of the SR image and reducing the space of the possible functions. Extensive experiments demonstrate our results outperform the state-of-the-art SR methods in the SSIM, and achieve better visual effects than the state-of-the-art SR methods. © 2022,In recent years, deep learning-based methods have emerged as dominant players in the field of super-resolution (SR), owing to their exceptional reconstruction performance. The primary driver of their effectiveness lies in their utilization of extensive sets of paired low-resolution and high-resolution images for training deep learning models. This training enables the models to effectively replicate the intricate mapping relationship between low-resolution and high-resolution images. Nevertheless, at present, acquiring a sufficient quantity of such image pairs that satisfy the requirements remains a formidable obstacle. Therefore, in order to break the restriction of limited training sets, self-supervised learning has been introduced to train a model for each low-quality image, without requiring pairwise ground-truths. However, they generally presuppose the generation of low-resolution (LR) images from their high-resolution (HR) counterparts using a pre-defined kernel, such as Bicubic downscaling. Such an assumption is seldom valid for real-world LR images, where degradation processes in practical applications are diverse, intricate, and often undisclosed. Therefore, when the presumed downscaling kernel does not match the actual one, the outcomes of state-of-the-art approaches degrade substantially. In this paper, we introduce KGSR, a kernel-guided network for addressing real-world blind SR, effectively avoiding requiring large training image pairs and transforming the blind image super-resolution problem into a supervised learning and non-blind scenario. Specifically, KGSR trains two networks, namely Upscaling and Downscaling, utilizing only patches extracted from the input test image. On one hand, owing to the cross-scale recurrence property of the SR kernel within a single image, the Downscaling network acquires knowledge of the image-specific degradation process through a generative adversarial network. Consequently, the Downscaling network is capable of generating a downsampled version of the LR test image even when the acquisition process is unknown or less than ideal. Additionally, we employ a dedicated discriminator to compel the Downscaling network to prioritize the characterization of kernel orientations. Conversely, a precise blur kernel has the potential to yield superior performance. Guided by the accurate image-specific SR kernel acquired from the Downscaling network and the downsampled LR input, the Upscaling network is capable of producing a high-quality HR image from the LR input. Within the Upscaling network, we additionally introduce an effective module for harnessing the acquired image-specific SR kernel. KGSR operates as a fully unsupervised approach, yet it can concurrently produce both the image-specific SR kernel and high-quality HR images. Comprehensive experiments conducted on standard benchmarks validate the efficacy of the proposed approach compared to state-of-the-art methodologies. Moreover, the suggested method can deliver visually appealing SR outcomes while exhibiting shorter processing times when applied to real-world LR images. © 2023 Elsevier Ltd"
236,235,16,"235_High-resolution remote sensing methods for mapping water bodies, terraces, snow areas, and reservoirs using deep learning algorithms and DTM analysis.","High-resolution remote sensing methods for mapping water bodies, terraces, snow areas, and reservoirs using deep learning algorithms and DTM analysis.","Solifluction terraces are a topographic and sedimentary signature for understanding the mechanism of solifluction occurrence and the past climate (e.g., paleoclimate), and have also impacts on hillslope stability. Traditional field surveys and aerial remote sensing have been primarily employed to map solifluction terraces, but they have challenges in large-scale mapping, which results in little knowledge of spatial distribution of solifluction terraces in many areas of the world. In this study, we made a first attempt at large-scale mapping of solifluction terraces based on high-resolution satellite remote sensing images and deep learning in the Luhuo and Xinduqiao areas, located in the southeastern Tibetan Plateau. Enriched training samples were firstly generated through manually delineated 1057 solifluction terrace and non-solifluction terrace polygons in Luhuo and data augmentation. The DeepLabv3+ model (Chen et al., 2018) was then trained by these diverse and abundant samples, and employed to Gaofen-2 satellite images for extraction of solifluction terraces in both the Luhuo and Xinduqiao areas. The performance estimation showed that the F1-score, precision, and recall reached 91.15 %, 84.56 %, and 98.85 % in the Luhuo area, and 84.30 %, 84.90 %, and 83.71 % in the Xinduqiao area, respectively. The extracted results of solifluction terraces were also validated by field investigations in the Xinduqiao area. Moreover, we found that the solifluction terraces were mainly distributed around the slope angle of 21° and concentrated in the altitude range of 3000–3900 m, but the slope orientations were heterogeneous in both areas. Our results demonstrated that the combination of high-resolution satellite images and deep learning had a significant potential for mapping solifluction terraces at regional and global scales. © 2023,Introduction: Monitoring surface water through the extraction of water bodies from high-resolution remote sensing images is of significant importance. With the advancements in deep learning, deep neural networks have been increasingly applied to high-resolution remote sensing image segmentation. However, conventional convolutional models face challenges in water body extraction, including issues like unclear water boundaries and a high number of training parameters. Methods: In this study, we employed the DeeplabV3+ network for water body extraction in high-resolution remote sensing images. However, the traditional DeeplabV3+ network exhibited limitations in segmentation accuracy for high-resolution remote sensing images and incurred high training costs due to a large number of parameters. To address these issues, we made several improvements to the traditional DeeplabV3+ network: Replaced the backbone network with MobileNetV2. Added a Channel Attention (CA) module to the MobileNetV2 feature extraction network. Introduced an Atrous Spatial Pyramid Pooling (ASPP) module. Implemented Focal loss for balanced loss computation. Results: Our proposed method yielded significant enhancements. It not only improved the segmentation accuracy of water bodies in high-resolution remote sensing images but also effectively reduced the number of network parameters and training time. Experimental results on the Water dataset demonstrated superior performance compared to other networks: Outperformed the U-Net network by 3.06% in terms of mean Intersection over Union (mIoU). Outperformed the MACU-Net network by 1.03%. Outperformed the traditional DeeplabV3+ network by 2.05%. The proposed method surpassed not only the traditional DeeplabV3+ but also U-Net, PSP-Net, and MACU-Net networks. Discussion: These results highlight the effectiveness of our modified DeeplabV3+ network with MobileNetV2 backbone, CA module, ASPP module, and Focal loss for water body extraction in high-resolution remote sensing images. The reduction in training time and parameters makes our approach a promising solution for accurate and efficient water body segmentation in remote sensing applications. Copyright © 2023 Wu, Fu, Yi, Wang, Mo, Maponde, Liang, Tao, Ge, Jiang and Ren.,Floods are one of the most devastating natural calamities affecting millions of people and causing damage all around the globe. Flood models and remote sensing imagery are often used to predict and understand flooding. An increasing number of earth observation satellites are producing data at a rate that far outpaces our ability to manually extract meaningful information from it, motivating a surge in research on automatic feature detection in satellite imagery using machine learning and deep learning algorithms to automate flood mapping so that information from large streams of data can be extracted in near-real time and used for disaster response at landscape scale. The development of such an algorithm is predicated on exposure to training datasets that are representative of the full range of diversity in the spatial and spectral signature of surface water as it is sampled by space-based instruments. To address these needs, we developed a semantically labeled dataset of high-resolution multispectral imagery (Maxar WorldView 2/3) strategically sampled to be representative of North American surface water variability along five spatiotemporal strata: latitude, topographic complexity, land use, and day of year. This dataset was utilized to train a convolutional neural network (CNN) to automatically detect inundation extents using the Deep Earth Learning, Tools, and Analysis (DELTA) framework, an open source TensorFlow/Keras interpreter for satellite imagery. Our research objective was to demonstrate the out-of-sample accuracy of our trained CNN at landscape scale. The model performed well, with 98% precision and 94% recall for the water class during validation. We then evaluated the accuracy of our satellite-derived flood maps from trained machine learning model against a hydraulic model. For this, we compared predicted inundation extents against the USGS Flood Inundation Mapping (FIM) Program's flood map library at 17 different locations, where the FIM library provides flood inundation extents based on hydraulic models built for river reaches and corresponding to stage measurements at a nearby USGS gaging site. Compared to the hydraulic model, we estimated the underprediction of flood inundation by optical remote sensing data in our areas of interest to be 62%. We used land use data from National Land Cover Database (NLCD) and cloud masks to estimate that 79% of underprediction was due to these obstructions, with 74% belonging to vegetation, 9% to clouds, and 4% to both. A significant amount of inundation is missed when only optical remote sensing data is considered, and we suggest the use of flood models along with remote sensing data for getting the most realistic flood inundation extents. © 2023 The Authors"
237,236,15,236_Music Generation with Deep Learning,Music Generation with Deep Learning,"The last two decades have seen the emergence of a brand-new kind of music known as digital brain stimulant, also known as instrumental music or music without lyrics, which mostly comprises entrainment beats. While listening to it has the same ability to affect the brain as taking medication, it also has the risk of having a negative impact or encouraging unwanted behavior. This sparked the interest of a large number of studies in the psychological and physiological effects of music's brainwave entrainment beats on listeners. These studies started to categorize and examine how musical beats affected brainwave entrainment by looking at electroencephalogram (EEG) signals. Although this categorization represents a step forward for the early research efforts, it is constrained by the difficulty of having each musical track and conducting EEG tests on humans exposed to distortion due to noise in order to determine its influence. The work proposed in this article continues to explore this topic but in a novel, simple, accurate, and reliable categorization procedure based on the music signal elements themselves rather than dependent on EEG. VGGish and YAMNET based transfer deep learning models, are tuned to handle a straightforward, accurate real-time detector for the existence of the music beats inside music files with accuracy of 98.5 and 98.4, respectively. Despite the fact that they yield results that are equivalent, the YAMNET model is more suited for use with mobile devices due to its low power consumption and low latency. The article also proposes modified version of VGGish and YAMNET binary classifying models called BW-VGGish and BW-YAMNET respectively. The modification was to turn the binary classification into multi-classification. These multi-classifiers handle the classification of the influence of music beats (five different brain waves) on human brainwave entrainment with average accuracy of 94.5% and 94.5%, respectively. Since there was a lack of datasets addressing this kind of music, two datasets, the Brainwave Entrainment Beats (BWEB) dataset and the Brainwave Music Manipulation (BWMM) dataset, were generated for classification training and testing. The re-testing on a sample of music files that have their impact on brain waves (with their EEG) in an earlier study is done to strengthen the validity of the proposed work and to overcome the potential limitation of utilizing a music dataset that is not proved with its EEG. The success of the suggested models was demonstrated. © 2023 A. Sadek et al.,With the development of artificial intelligence and deep learning, a large number of music generation methods have been proposed. Recently, Transformer has been widely used in music generation. However, the structural complexity of music puts forward higher requirements for music generation. In this paper, we propose a new automatic music generation network which consists of a Recursive Skip Connection with Layer Normalization (RSCLN) model, a Transformer-XL model and a multi-head attention mechanism. Our method not only alleviates the gradient vanishing problem in the model training, but also increases the ability of the model to capture the correlation of music information before and after, so as to generate music works closer to the original music style. Effectiveness of the RSCLN_Transformer-XL music automatic generation method is verified through music similarity evaluation experiments using music structure similarity and listening test. The experimental results show that the RSCLN_Transformer-XL music automatic generation model can generate better music than the Transformer-XL model. © 2024, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,This paper utilizes deep learning algorithms to informally integrate modern vocal music teaching with traditional music culture and extracts audio time-domain features and frequency-domain features through neural network self-learning. Secondly, a large number of music tracks are decomposed into music patterns, which constitute a music pattern library, and a music training model is generated through the automatic music audio synthesis algorithm based on a recurrent neural network, and the GRU model is used for music training and model prediction. The strategy of integrating artificial intelligence and modern vocal music teaching mode through traditional music culture in modern vocal music teaching is informatized, and a controlled experiment is carried out with H Music Academy as an example. The results show that the average degree of completion of the learning objectives of the students in the two experimental classes is 89.32 and 87.16, respectively, which is 14.15 and 11.99 higher than the average degree of completion of the control class. This study demonstrates that the teaching mode of traditional music culture integration in modern vocal music teaching can enhance the student’s ability of vocal music skills and practically improve the students’ artistic literacy, which can improve the degree of completion of the student’s learning objectives and in turn, improve the overall level of vocal music teaching. © 2023 Ni Zhang, published by Sciendo."
238,237,15,237_Video Anomaly Detection in Railway Surveillance,Video Anomaly Detection in Railway Surveillance,"Visual surface anomaly detection focuses on the classification (CLS) and location (LOC) of regions that deviate from the normal appearance, and generally, only normal samples are provided for training. The reconstruction-based method is widely used, which locates the anomalies by analyzing the reconstruction error. However, there are two problems unsettled in the reconstruction-based method. First, the reconstruction error in the normal regions is sometimes large. This might mislead the model to take the normal regions as anomalies, which is named an overkill problem. Second, it has been observed that the anomalous regions sometimes cannot be repaired to normal, which results in a small reconstruction error in the anomalous regions. This misleads the model to take the anomalies as normal, which is called an anomaly escape problem. Aiming at the above two problems, we propose a model named dual-branch autoencoder with prior information (DBPI) which is mainly composed of a dual-branch AE structure and a GA unit. To alleviate the overkill problem, a natural idea is to reduce the reconstruction error in the normal regions, and therefore a dual-branch AE is proposed. The dual-branch AE reconstructs two images with consistent normal regions and different anomalous regions. By analyzing the reconstruction error between the above two reconstructed images, the anomalies can be detected without causing overkill. For the anomaly escape problem, an effective solution is to add prior information of normal appearance to the reconstructive network, which assists in repairing the anomalous regions and increasing the reconstruction error in the anomalous regions. Since the mathematical expectation map of the training data contains crucial features of the normal appearance, we utilize it as the prior information of the normal appearance. And the prior information is selectively introduced by the proposed gated attention (GA) unit, which effectively assists in reconstructing a normal image and further mitigates the anomaly escape problem. On the average precision (AP) metric for the anomaly detection benchmark dataset MVTec, the proposed unsupervised method outperforms the current state-of-the-art reconstruction-based method self-supervised predictive convolutional attentive block (SSPCAB) by 7.4%. Meanwhile, our unsupervised method also exhibits comparable performance to the best supervised methods on the surface defect detection DAGM dataset.  © 1963-2012 IEEE.,Anomaly detection refers to the problem of uncovering patterns in a given data set that do not conform to the expected behavior. Recently, owing to the continuous development of deep representation learning, a large number of anomaly detection approaches based on deep learning models have been developed and achieved promising performance. In this work, an image anomaly detection approach based on contrastive learning framework is proposed. Rather than adopting ResNet or other CNN-based deep neural networks as in most of the previous deep learning-based image anomaly detection approaches to learn representations from training samples, a contrastive learning framework is developed for anomaly detection in which Transformer is adopted for extracting better representations. Then, we develop a triple contrastive loss function and embed it into the proposed contrastive learning framework to alleviate the problem of catastrophic collapse that is often encountered in many anomaly detection approaches. Furthermore, a nonlinear Projector is integrated with our model to improve the performance of anomaly detection. The effectiveness of our image anomaly detection approach is validated through experiments on multiple benchmark data sets. According to the experimental results, our approach can obtain better or comparative performance in comparison with state-of-the-art anomaly detection approaches. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Large-scale sensor and data acquisition systems, integrated with deep learning methodologies, play a pivotal role in enhancing the sustainability and security of smart city environments, exemplifying the critical significance of anomaly detection techniques. Anomaly detection in complex industrial scenarios presents various challenges, such as intricate working environments, limited anomaly samples, and lack of a priori information. Unsupervised anomaly detection based on knowledge distillation enables anomaly detection using only normal samples. However, the similarity in structure between teacher and student models, along with identical input data flow, hampers accurate anomaly detection and localization. To address these issues, we propose MNMC, an unsupervised anomaly detection model consisting of a mixed noise generation module emulating real defects, a mutual constraint module, and an anomaly segmentation module. Firstly, to enhance the student network's ability to learn robust features, we construct a hybrid noise model comprising dead-leaves noise and perlin noise. This generates features with structural texture and distributional characteristics closer to real anomalies. Secondly, we design a mutual constraint framework to further improve the learning ability of the student network for normal features by constraining representations containing only a single noise. Lastly, for the detection of anomalies at different scales, we propose a new evaluation metric based on equal importance of normal and anomalous regions. Through ablation experiments, we demonstrate the effectiveness of the simulated real defect generation module and the mutual constraints module. Performance experiments on the MVTec dataset show that our method achieves competitive results compared to the current state-of-the-art anomaly detection methods. © 2023 Elsevier B.V."
239,238,15,238_Electric Vehicle Charging and Scheduling Optimization,Electric Vehicle Charging and Scheduling Optimization,"With the rapid growth of the number of Electric Vehicles (EVs), access to large-scale EVs will bring serious safety hazards to the operation planning of the power system. It needs to be supported by an effective EV charging and discharging behavior control strategy to meet the operation demand of the power system. An optimization model with the objectives of minimizing grid load variance and minimizing user charging cost is established. An improved hybrid algorithm is proposed for the optimal allocation of charging and discharging power of EVs by combining particle swarm optimization (PSO) algorithm and gravitational search algorithm (GSA). The performance of variant algorithm is tested using CEC2005 benchmarking functions sets and applied to the solution of the ordered charge–discharge optimal scheduling model. The results show that the convergence accuracy of the algorithm is better than the traditional algorithm, and it can effectively balance exploration and exploitation ability of the particles. In addition, the scheduling analysis is performed for different charging strategies of EVs. The scheduling results show that with the same optimization weights, implementing the ordered charging and discharging strategy can significantly reduce the charging cost of users and the load variance of the grid. Thus, the operational stability of the grid and the economic benefits for users are improved. © 2024 The Authors,With the growing popularity of electric vehicles (EVs), it is a new challenge for the residential microgrid system to conduct charging scheduling to meet the charging demands of EVs while maximizing its profit. In this work, a safe reinforcement learning (RL)-based charging scheduling strategy is proposed to meet this challenge. We construct a complete microgrid system equipped with a large charging station and consider different types of EVs, as well as the vehicle-to-grid (V2G) mode and nonlinear charging characteristics of EVs. Subsequently, the charging scheduling problem is formulated as a constrained Markov decision process (CMDP) due to the various limitations of power and demands. To effectively capture the uncertainties of the supply side and demand side of the microgrid, a model-free RL framework is employed. However, the curse of dimensionality of the action space is inevitable as EVs increase. To solve this problem, a charging and discharging strategy based on a general ladder electricity pricing scheme is designed. Different EVs are divided into different sets according to their states under this strategy, and the agent gives control signals of different sets instead of controlling each EV individually, which effectively reduces the dimension of the action space. Subsequently, a constrained soft actor-critic (CSAC) algorithm is designed to solve the established CMDP, and a safety filter is introduced to ensure safety. In the end, a numerical case is conducted to verify the effectiveness of the proposed method. © 2023 Elsevier Ltd,The rapid growth of electric vehicles (EVs) is an unstoppable worldwide development trend. An optimal charging strategy for large-scale EVs is able to deal with the randomness of EVs charging and satisfy charging demands of users while ensuring safe and economic operation of the power system. The current centralized and model-based methods failed to overcome the randomness charging problem of the large-scale EVs. Thus, the paper proposes a decentralized approach based on model-free deep reinforcement learning (DRL) to determine the optimal strategy for reducing EVs charging cost considering power limit of the charging station (CS), users' charging demands and fair charging fees. First, a decentralized framework and a dynamic energy boundary (DEB) model of single EV which discretizes the charging demand are proposed. Second, the problem as a Markov Decision Process (MDP) with unknown transition probability is formulated. Moreover, the recurrent deep deterministic policy gradient (RDDPG) based approach is proposed to determine the charging strategy for all charging piles in the CS. Finally, digital simulation studies are conducted to demonstrate the effectiveness of the proposed approach in charging cost reduction and fair charging fees. In addition, the RDDPG-based approach has great scalability which can apply a small-scale model to solve a large-scale problem without being retrained. © 2022"
240,239,15,239_Online Course Recommendation,Online Course Recommendation,"The exponential growth of Massive Open Online Courses (MOOCs) surges the needs of advanced models for personalized Online Education Services (OES). Existing solutions successfully recommend MOOCs courses via deep learning models, they however generate weak 'course embeddings' with original profiles, which contain noisy and few enrolled courses. On the other hand, existing algorithms provide recommendation orders according to the score of each course while ignoring personalized demands of users. To tackle the above challenges, we propose a Meta Hierarchical Reinforced Ranking approach MHRR, which consists of a meta hierarchical reinforcement learning pre-trained mechanism and an over-parameterized ranking regressor to enhance the representation learning of courses and learners while refining the ranking result of recommended courses. Specifically, MHRR combines a user profile reviser and a meta embedding generator to provide course embedding representation enhancement for recommender services. Furthermore, MHRR transforms learned representations generated from recommender services with Gaussian kernel approximation to over-parameterize the downstream learning to rank (LTR) models with representations in ultra-high dimensionality. We deploy MHRR on a real-world MOOCs platform and evaluate it with a large number of baseline models. The results show that MHRR outperforms baseline algorithms on two major metrics, including Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). Also, we conduct a 7-day online evaluation using the realistic traffic of a large-scale real-world MOOCs platform, where we can still observe significant improvement in real-world applications. MHRR performs consistently both in the online and offline evaluation.  © 2008-2012 IEEE.,Massive Open Online Courses (MOOCs) have received unprecedented attention, in which learners can obtain a large number of learning objects anytime and anywhere. However, the increasing information overload on MOOCs inhibits the appropriate choice of learning objects by learners, leading to a low efficiency and high dropout rates in the learning process of this human-computer interaction scenario. E-learning recommendation systems have been studied to present learning objects directly to learners, thereby relieving such problem. However, in MOOC platforms, recommendation network structures which can selectively extract implicit feature such as heterogeneous learning preference and knowledge organization of learning objects are still not comprehensively studied. To this end, we propose a learning object recommendation model based on heterogeneous learning behavior and knowledge graph. To generate a unified representation of each entity and relation, we first propose an Attentive Composition based Graph Convolutional Network (ACGCN). By introducing an attention mechanism, information is amplified when updating the representation of the heterogeneous graph, which eliminates the impact of noise and improves the robustness of the model. Then, a Dense Feature based Operation-Aware Network (DFOAN) is utilized to capture implicit and complex learners' interactive behaviors, and to further provide a recommendation. Experimental results using two real-world datasets revealed that our proposed model has the best precision, recall, F1, and accuracy scores compared to those of several existing models.  © 1989-2012 IEEE.,Massive Open Online Courses (MOOCs), which provide learners with a large-scale, open-access learning opportunity, have drawn a lot of attention recently. The amount of information available on MOOCs is increasing, making it challenging for learners to choose the best course materials and leading to low learning efficiency and high dropout rates. To address these problems, Recommendation Systems (RSs) have been researched as a direct approach of delivering educational content to learners while also attracting their interest. However, a course often consists of various learning concepts, each of which covers a different topic. Explicitly proposing courses can cause the lack of learners' attention to a particular knowledge concept. We introduce a Top-N Knowledge Concept Recommendations in MOOCs using a Neural Co-Attention Model, called NCO-A, that integrates significant heterogeneous data with recommendations based on knowledge concepts. The NCO-A model's effectiveness has been proven by extensive experiments on three real-world datasets.  © 2013 IEEE."
241,240,15,240_Binary Code Similarity Detection and Vulnerability Analysis,Binary Code Similarity Detection and Vulnerability Analysis,"Deep learning has achieved great progress in automated code vulnerability detection. Several code vulnerability detection approaches based on deep learning have been proposed. However, few studies empirically studied the impacts of different deep learning models on code vulnerability detection in Python. For this reason, we strive to cover many more code representation learning models and classification models for vulnerability detection. We design and conduct an empirical study for evaluating the effects of the eighteen deep learning architectures derived from combinations of three representation learning models, i.e., Word2Vec, fastText, and CodeBERT, and six classification models, i.e., random forest, XGBoost, Multi-Layer Perception (MLP), Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Gate Recurrent Unit (GRU) on code vulnerability detection in total. Additionally, two machine learning strategies i.e., the attention and bi-directional mechanisms are also empirically compared. The statistical significance and effect size analysis between different models are also conducted. In terms of precision, recall, and F-score, Word2Vec is better than Bidirectional Encoder Representations from Transformers CodeBERT and fastText. Likewise, long short-term memory (LSTM) and gated recurrent unit (GRU) are superior to other classification models we studied. The bi-directional LSTM and GRU with attention using Word2Vec are two optimal models for solving code vulnerability detection for Python code. Moreover, they have medium or large effect sizes on LSTM and GRU using only a single mechanism. Both the representation learning models and classification models have important influences on vulnerability detection in Python code. Likewise, the bi-directional and attention mechanisms can impact the performance of code vulnerability detection. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Machine learning-based fine-grained vulnerability detection is an important technique for locating vulnerable statements, which assists engineers in efficiently analyzing and fixing the vulnerabilities. However, due to insufficient code representations, code embeddings, and neural network design, current methods suffer low vulnerability localization performance. In this paper, we propose to address these shortcomings by presenting SlicedLocator, a novel fine-grained code vulnerability detection model that is trained in a dual-grained manner and can predict both program-level and statement-level vulnerabilities. We design the sliced dependence graph, a new code representation that not only preserves rich interprocedural relations but also eliminates vulnerability-irrelevant statements. We create attention-based code embedding networks that are trained with the entire model to extract vulnerability-aware code features. In addition, we present a new LSTM-GNN model as a fusion of semantic modeling and structural modeling. Experiment results on a large-scale C/C++ vulnerability dataset reveal that SlicedLocator outperforms state-of-the-art machine learning-based vulnerability detectors, especially in terms of localization metrics. © 2023 Elsevier Ltd,Vulnerability detection is essential to protect software systems. Various approaches based on deep learning have been proposed to learn the pattern of vulnerabilities and identify them. Although these approaches have shown vast potential in this task, they still suffer from the following issues: (1) It is difficult for them to distinguish vulnerability-related information from a large amount of irrelevant information, which hinders their effectiveness in capturing vulnerability features. (2) They are less effective in handling long code because many neural models would limit the input length, which hinders their ability to represent the long vulnerable code snippets. To mitigate these two issues, in this work, we proposed to decompose the syntax-based Control Flow Graph (CFG) of the code snippet into multiple execution paths to detect the vulnerability. Specifically, given a code snippet, we first build its CFG based on its Abstract Syntax Tree (AST), refer to such CFG as syntax-based CFG, and decompose the CFG into multiple paths from an entry node to its exit node. Next, we adopt a pre-trained code model and a convolutional neural network to learn the path representations with intra- and inter-path attention. The feature vectors of the paths are combined as the representation of the code snippet and fed into the classifier to detect the vulnerability. Decomposing the code snippet into multiple paths can filter out some redundant information unrelated to the vulnerability and help the model focus on the vulnerability features. Besides, since the decomposed paths are usually shorter than the code snippet, the information located in the tail of the long code is more likely to be processed and learned. To evaluate the effectiveness of our model, we build a dataset with over 231 k code snippets, in which there are 24 k vulnerabilities. Experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines by at least 22.30%, 42.92%, and 32.58% in terms of Precision, Recall, and F1-Score, respectively. Our further analysis investigates the reason for the proposed approach's superiority.  © 1976-2012 IEEE."
242,241,15,241_Emergency Fleet Scheduling and Optimization,Emergency Fleet Scheduling and Optimization,"A high-intensity storm surge hazard exposes residents and facilities to inundation danger. A continuously operating robust inundation emergency logistics system is vital to guarantee lives and support the control of hazardous materials. A massive inundation area, large number of affected facilities, and instantly changing flooding situation bring tremendous challenges to rescue resource allocation. In this article, a rescue resource distribution scheduling of storm surge inundation logistics is proposed to quantitatively formulate the rescue time minimization problem in emergency logistics. The mixed-integer linear programming (MILP) method is proposed for the emergency logistics scheduling model validation and optimality comparison. To enhance the efficiency of creating a good quality allocation strategy when facing large-scale problems, a deep reinforcement learning algorithm - deep deterministic policy gradient (DDPG) - is utilized to search the solutions. Based on a rescue resource scheduling model of storm surge inundation logistics targeting storm surge Mangkhut in September, 2018, a case study of the Futian District, Shenzhen, China, was conducted to verify the correctness and efficiency of the MILP and the DDPG. The optimal schedule solution designed by the MILP had a duration of 3.5138 h, while the solution calculated by DDPG had a duration of 5.065 h for the rescue. The execution time of DDPG was stable and under a second, while the execution time of the MILP was over two hours.  © 2005-2012 IEEE.,Hospitals play a key role in providing medical assistance in a post-disaster phase. However, depending on the nature, scale, and severity of disasters, they may also be affected and may need to evacuate their patients. Current evacuation response plans for hospitals are predominantly based on the use of land vehicles, whereas little methodological advancements have been reported for when aerial hospital evacuations become necessary. To address this problem, this study develops a model that facilitates evacuation planning for moving patients from at-risk hospitals to remote locations using limited aerial transportation resources. A subset of the model parameters is treated as stochastic in order to reflect the uncertainties involved in real-world emergency conditions. The complexities of the model are handled via the use of a metaheuristic approach, the Variable neighbourhood search (VNS) algorithm, to provide a suitable solution within a reasonable amount of time. A relaxed mathematical model is used in the proposed VNS to generate an initial solution, which is then transformed into a feasible solution through a try-and-error procedure. The proposed method applies a reinforcement learning procedure that uses different local search strategies within the loop of the VNS. A hypothetical evacuation scenario of real-life scale, from an island state in Australia to the mainland states, is used to evaluate the ability of the model to generate efficient plans. In this case study, Tasmania is considered as an evacuation point, while hospitals located in other major cities host evacuated patients. Comparing the proposed method with a conventional method shows superior performance based on all relevant metrics. Emergency response agencies across many countries that can adopt the proposed methodology, when facing large-scale emergencies caused by wars, epidemics, or natural hazards such as wildfires and earthquakes. © 2023 Elsevier Ltd,Evacuation planning and emergency routing systems are crucial in saving lives during disasters. Traditional emergency routing systems, despite their best efforts, often struggle to accurately capture the dynamic nature of flood conditions, road closures, and other real-time changes inherent in urban disaster logistics. This paper introduces the ReinforceRouting model, a novel approach to optimizing evacuation routes using reinforcement learning (RL). The model incorporates a unique RL environment that considers multiple criteria, such as traffic conditions, hazardous situations, and the availability of safe routes. The RL agent in this model learns optimal actions through interaction with the environment, receiving feedback in the form of rewards or penalties. The ReinforceRouting model excels in executing prompt and accurate route planning on large road networks, outperforming traditional RL algorithms and shortest-path-based algorithms. A higher safety score and episode reward of the model are demonstrated when compared to these classical methods. This innovative approach to disaster evacuation planning offers a promising avenue for enhancing the efficiency, safety, and reliability of emergency responses in dynamic urban environments. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group."
243,242,15,242_Dialogue generation with pretrained models and variational training,Dialogue generation with pretrained models and variational training,"Task-oriented dialogue systems continue to face significant challenges as they require not only an understanding of dialogue history but also domain-specific knowledge. However, knowledge is often dynamic, making it difficult to effectively integrate into the learning process. Existing large language model approaches primarily treat knowledge bases as textual resources, neglecting to capture the underlying relationships between facts within the knowledge base. To address this limitation, we propose a novel dialogue system called PluDG. We regard the knowledge as a knowledge graph and propose a knowledge extraction plug-in, Kg-Plug, to capture the features of the graph and generate prompt entities to assist the system's dialogue generation. Besides, we propose Unified Memory Integration, a module that enhances the comprehension of the sentence's internal structure and optimizes the knowledge base's encoding location. We conduct experiments on three public datasets and compare PluDG with several state-of-the-art dialogue models. The experimental results indicate that PluDG achieves significant improvements in both accuracy and diversity, outperforming the current state-of-the-art dialogue system models and achieving state-of-the-art performance. © 2023 Dong and Chen,An effective dialogue system needs amount of training data, but the existing training data is insufficient. Although the pre-trained model has made great progress in recent years, which can alleviate the problem of low resource dialogue to a certain extent, the pre-trained model is large and difficult to deploy. How to improve the performance of dialogue model without additional annotation data and decreasing the model volume has become a new challenge. We propose a multi-source data augmentation method for low-resource dialogue generation by utilizing inverse curriculum learning (inverse CL). Firstly, we adopt three data augmentation methods, including round-trip translation, paraphrasing and pre-trained model, to generate augmentation data. Next, we propose a new training strategy based on inverse CL to utilize different augmentation data. Comparing with the baselines, our method comprehensively outperform the baselines on all evaluation metrics, which shows the effectiveness of our proposed training strategy for dialogue generation. To the best of our knowledge, this is the first systematic investigation of data augmentation in the dialogue generation. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Achieving high performance in a multi-domain dialogue system with low computation is undoubtedly challenging. Previous works applying an end-To-end approach have been very successful. However, the computational cost remains a major issue since the large-sized language model using GPT-2 is required. Meanwhile, the optimization for individual components in the dialogue system has not shown promising result, especially for the component of dialogue management due to the complexity of multi-domain state and action representation. To cope with these issues, this article presents an efficient guidance learning where the imitation learning and the hierarchical reinforcement learning (HRL) with human-in-The-loop are performed to achieve high performance via an inexpensive dialogue agent. The behavior cloning with auxiliary tasks is exploited to identify the important features in latent representation. In particular, the proposed HRL is designed to treat each goal of a dialogue with the corresponding sub-policy so as to provide efficient dialogue policy learning by utilizing the guidance from human through action pruning and action evaluation, as well as the reward obtained from the interaction with the simulated user in the environment. Experimental results on ConvLab-2 framework show that the proposed method achieves state-of-The-Art performance in dialogue policy optimization and outperforms the GPT-2 based solutions in end-To-end system evaluation. © 2014 IEEE."
244,243,15,243_Tree species mapping and forest monitoring using aerial and spectral-temporal imagery,Tree species mapping and forest monitoring using aerial and spectral-temporal imagery,"Airborne laser scanning (ALS) data is increasingly being used for accurate estimations of forest inventory attributes, such as tree height and volume. However, determining tree species information, an important attribute required by inventories for a range of forest management applications, is challenging to extract from ALS data. This is due to complex species assemblages and vertical structures in certain forest environments, and a lack of spectral information for most ALS sensors. Our objective was to estimate tree species compositions in a Canadian boreal forest environment using ALS data and point-based deep learning techniques. To accomplish this, we utilised existing polygonal forest resource information to develop a large comprehensive dataset of tree species compositions across a 630,000 ha forest management area. We then used an adapted PointAugment generative adversarial network (GAN) coupled with the dynamic graph convolutional neural network to estimate tree species proportions. This innovative deep learning approach resulted in an overall R2adj of 0.61 for all tree species proportions. A weighted F1 score of 63% was observed when classifying the leading species of a plot, 66% for leading genus, and 85% when distinguishing between coniferous and deciduous dominated plots. Furthermore, the PointAugment GAN successfully generated augmented point clouds of forest plots which can alleviate issues associated with limited training samples for use in deep learning. This approach demonstrates the capability of point-based deep learning techniques to accurately estimate tree species compositions from ALS point clouds within an expansive forested region, characterized by a complex management history and a diversity of tree species. The source code along with the pretrained models are available at https://github.com/Brent-Murray/point-dl/tree/main/Pytorch/models/PointAugment. © 2023,Tropical forests are a major component of the global carbon cycle and home to two-thirds of terrestrial species. Upper-canopy trees store the majority of forest carbon and can be vulnerable to drought events and storms. Monitoring their growth and mortality is essential to understanding forest resilience to climate change, but in the context of forest carbon storage, large trees are underrepresented in traditional field surveys, so estimates are poorly constrained. Aerial photographs provide spectral and textural information to discriminate between tree crowns in diverse, complex tropical canopies, potentially opening the door to landscape monitoring of large trees. Here we describe a new deep convolutional neural network method, Detectree2, which builds on the Mask R-CNN computer vision framework to recognize the irregular edges of individual tree crowns from airborne RGB imagery. We trained and evaluated this model with 3797 manually delineated tree crowns at three sites in Malaysian Borneo and one site in French Guiana. As an example application, we combined the delineations with repeat lidar surveys (taken between 3 and 6 years apart) of the four sites to estimate the growth and mortality of upper-canopy trees. Detectree2 delineated 65 000 upper-canopy trees across 14 km2 of aerial images. The skill of the automatic method in delineating unseen test trees was good (F1 score = 0.64) and for the tallest category of trees was excellent (F1 score = 0.74). As predicted from previous field studies, we found that growth rate declined with tree height and tall trees had higher mortality rates than intermediate-size trees. Our approach demonstrates that deep learning methods can automatically segment trees in widely accessible RGB imagery. This tool (provided as an open-source Python package) has many potential applications in forest ecology and conservation, from estimating carbon stocks to monitoring forest phenology and restoration. Python package available to install at https://github.com/PatBall1/Detectree2. © 2023 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London.,Automatic identification and mapping of tree species is an essential task in forestry and conservation. However, applications that can geolocate individual trees and identify their species in heterogeneous forests on a large scale are lacking. Here, we assessed the potential of the Convolutional Neural Network algorithm, Faster R-CNN, which is an efficient end-to-end object detection approach, combined with open-source aerial RGB imagery for the identification and geolocation of tree species in the upper canopy layer of heterogeneous temperate forests. We studied four tree species, i.e., Norway spruce (Picea abies (L.) H. Karst.), silver fir (Abies alba Mill.), Scots pine (Pinus sylvestris L.), and European beech (Fagus sylvatica L.), growing in heterogeneous temperate forests. To fully explore the potential of the approach for tree species identification, we trained single-species and multi-species models. For the single-species models, the average detection accuracy (F1 score) was 0.76. Picea abies was detected with the highest accuracy, with an average F1 of 0.86, followed by A. alba (F1 = 0.84), F. sylvatica (F1 = 0.75), and Pinus sylvestris (F1 = 0.59). Detection accuracy increased in multi-species models for Pinus sylvestris (F1 = 0.92), while it remained the same or decreased slightly for the other species. Model performance was more influenced by site conditions, such as forest stand structure, and less by illumination. Moreover, the misidentification of tree species decreased as the number of species included in the models increased. In conclusion, the presented method can accurately map the location of four individual tree species in heterogeneous forests and may serve as a basis for future inventories and targeted management actions to support more resilient forests. © 2023 by the authors."
245,244,15,244_Industrial Manufacturing and Artificial Intelligence,Industrial Manufacturing and Artificial Intelligence,"Entering the second decade of the Industrie 4.0 vision, the production sector is facing challenges in taking full advantage of global digitalization. Production research has focused on sophisticated mathematical models ranging from molecular materials modeling to production control to supply chain logistics. These models help simulate and control the related physical system but the variety of individual situations and behaviors is captured only as statistical uncertainty. The emergence of data-driven methods adds statistical or AI models learned from real-time production data to Digital Twins, and ideally allows for continuous synchronization (twinning) between physical and virtual system. However, the complexity of today's production systems precludes Digital Twins covering more than just a few system perspectives, especially if realtime performance is required. To achieve better performance and more precise context adaptation, the interdisciplinary research cluster “Internet of Production” at RWTH Aachen University is exploring the concept of Digital Shadows. We conceptualize Digital Shadows as a generalization of compact views on dynamic processes, whose defining “query” combines condensed measurement data with efficient simplified mathematical models. Their small size makes Digital Shadows amenable to dynamic function allocation in hybrid cloud–edge settings. In addition to showing the similarities and differences to the traditional view concept, we also present a conceptual embedding of Digital Shadows in the context of large distributed system architectures, and sovereign data exchange in international Data Space communities. Two production use case experiences demonstrate that Digital Shadows can be valuable carriers of deep and reusable engineering knowledge for technical and ecological progress. © 2023 The Author(s),Many industry sectors have been pursuing the adoption of Industry 4.0 (I4.0) ideas and technologies, which promise to realize lean and just-in-time production through digitization and the use of smart machines. This shift is driven by technological advances, including Artificial Intelligence (AI) and machine learning, sensor networks and Internet of Things technologies, cloud computing, additive manufacturing, and the availability of large amounts of data that can be exploited by these technologies. However, the adoption of AI technologies for I4.0 varies considerably among industry sectors. This article complements broader reviews of I4.0 by examining the specific applications of IAI in several industry sectors, highlighting the issues and concerns encountered in and across different industry sectors, and discussing potential solutions that have been introduced along with opportunities and challenges for adoption. In this article, we review the literature to identify common themes and concerns related to the adoption of AI technologies in the context of I4.0 in several industry sectors. AI solutions are discussed in the context of an AI adoption pipeline that spans data collection, processing, model construction, and interpretation of results. Our findings indicate that although different industries share common issues, the adopted solutions are often specific to a particular industry sector, which may be difficult to transfer to other sectors. Moreover, industry sectors may pursue different adoption strategies due to varying experience and maturity of AI practices. These findings may inform managers, practitioners, and decision-makers who are involved in the adaptation of Industry 4.0 transformation in their respective industry sectors. © 2022 Elsevier Ltd,The whole cycle for manufacturing aerospace thin-walled shells is a lengthy and sophisticated process. A large amount of quality-related data exists within and between processes, involving many types of quality defects and influencing factors. However, there are ambiguous causal associations among quality-related data affecting the shape-properties of the shell. Also, the coupling of long processes and multiple factors makes it hard to analyze the main factors that affect the quality defects in shell manufacturing. In this paper, taking into account the advantages of causal Scientology and the large language model (LLM), we propose an industrial structure causal knowledge-enhanced large language model for the cause analysis of quality defects in aerospace product manufacturing. To reinforce the causal associations among quality-related data deriving from manufacturing documents (product defect survey sheets, quality inspection, and maintenance reports), a structure causal graph-based sum-product network (SCG-SPN) model is designed to model machining quality-related knowledge and eliminate pseudo-association confounding factors by doing an intervention. Thus, a causal quality-related knowledge graph (CQKG) with high-quality causal associations is constructed. With this, to provide a trustworthy guarantee in responding to quality problem solving, we construct a quality-related prompt dataset with multi-round conversations based on CQKG. Then, a novel P-tuning that adapts to utilize external CQKG instructions is designed to fine-tune an open-source ChatGLM base model. Based on this, a causal knowledge graph-augmented LLM, named CausalKGPT, is developed to enable reasoning and responding to quality defects in both Chinese and English. It uses natural text descriptions related to quality defects as input and takes a quality-related causal knowledge graph as an additional corpus. Finally, the case study shows that the CausalKGPT performs with more expertise and reliability in responding to quality question solving of aerospace shell manufacturing than the classic commercial models like ChatGPT and GPT4. The results indicate that the proposed method may provide a trustworthy guide in assisting workers to analyze quality defects in aerospace products. © 2023 Elsevier Ltd"
246,245,15,245_Detection of Hate Speech and Offensive Language on Social Media Platforms,Detection of Hate Speech and Offensive Language on Social Media Platforms,"With increasing number of social media users and online engagement, it is essential to study hate speech propagation on social media platforms (SMPs). Automatic hate speech detection on social media is of utmost importance as hate speech can create discomfort among users and potentially generate a strong reaction in society. Ensemble learning algorithms are helpful in addressing sentiment-based classification due to their fault tolerance and efficiency. However, a simple, scalable, and robust framework is required to deal with large-scale data efficiently and accurately. Therefore, we propose parallelization to the standard ensemble learning algorithms to speed up the automatic hate speech detection on SMPs. In this study, we parallelize bagging, A-stacking, and random sub-space algorithms and test their serial and parallel versions on the standard high-dimensional datasets for hate speech detection. The experiments are performed using six datasets that address hate speech propagation during events like the COVID-19 pandemic, the US presidential election (2020), and the farmers’ protest in India (2021). Our parallel models observe a significant speedup with high efficiency, claiming that the proposed models are suitable for the considered application. Also, one of the main motivations of this study is to highlight the importance of generalization by testing the models under the cross-dataset environment. We observed that the accuracy is not affected while parallelizing the algorithms compared with serial algorithms executing on a single machine. © 2023 Elsevier Ltd,Online hate speech has flourished on social networking sites due to the widespread availability of mobile computers and other Web knowledge. Extensive research has shown that online exposure to hate speech has real-world effects on marginalized communities. Research into methods of automatically identifying hate speech has garnered significant attention. Hate speech can affect any demographic, while some populations are more vulnerable than others. Relying solely on progressive learning is insufficient for achieving the goal of automatic hate speech identification. It need access to large amounts of labelled data to train a model. Inaccurate statistics on hate speech and preconceived notions have been the biggest obstacles in the field of hate speech research for a long time. This research provides a novel strategy for meeting these needs by combining a transfer-learning attitude-based BERT (Bidirectional Encoder Representations from Transformers) with a coral reef optimization-based approach (CROA). A feature selection (FC) optimization strategy for coral reefs, a coral reefs optimization method mimics coral behaviours for reef location and development. We might think of each potential answer to the problem as a coral trying to establish itself in the reefs. The results are refined at each stage by applying specialized operators from the coral reefs optimization algorithm. When everything is said and done, the optimal solution is chosen. We also use a cutting-edge fine-tuning method based on transfer learning to assess BERT’s ability to recognize hostile contexts in social media communications. The paper evaluates the proposed approach using Twitter datasets tagged for racist, sexist, homophobic, or otherwise offensive content. The numbers show that our strategy achieves 5%–10% higher precision and recall compared to other approaches. © 2024 by author(s).,There is an increased demand for detecting online hate speech, especially with the recent changing policies of hate content and free-of-speech right of online social media platforms. Detecting hate speech will reduce its negative impact on social media users. A lot of effort in the Natural Language Processing (NLP) field aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, which adds more challenges to hate speech detection tasks. Word representation from its domain will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibility of leveraging domain-specific word embedding as features and a bidirectional LSTM-based deep model as a classifier to automatically detect hate speech. This approach guarantees that the word is assigned its negative meaning, which is a very helpful technique to detect coded words. Furthermore, we investigate the use of the transfer learning language model (BERT) on the hate speech problem as a binary classification task as it provides high-performance results for many NLP tasks. The experiments showed that domain-specific word embedding with the bidirectional LSTM-based deep model achieved a 93% f1-score, while BERT achieved 96% f1-score on a combined balanced dataset from available hate speech datasets. The results proved that the performance of pre-trained models is influenced by the size of the trained data. Although there is a huge variation in the corpus size, the first approach achieved a very close result compared to BERT, which is trained on a huge data corpus, this is because it is trained on data related to the same domain. The first approach was very helpful to detect coded words while the second approach achieved better performance because it is trained on much larger data. To conclude, it is very helpful to build large pre-trained models from rich domains specific content in current social media platforms. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC."
247,246,15,246_Landslide Detection in Remote Sensing Images,Landslide Detection in Remote Sensing Images,"Accurately detecting landslides over a large area with complex background objects is a challenging task. Research in the area suffers from three drawbacks in general. First, the models are mostly modified from typical networks, and are not designed specifically for landslide detection. Second, the images used to construct and evaluate models of landslide detection are limited to one spatial resolution, which struggles to meet the requirements of such relevant applications as emergency response. Third, assessments are primarily carried out by using the training data on different parts of the same study area. This makes it difficult to objectively evaluate the transferability of the model, because ground objects in the same area are distributed with similar spectral characteristics. To respond to the challenges above, this study proposes DeenNet, specifically designed for landslide detection. Different from the widely used encoder–decoder networks, DeenNet maintains multi-scale landslide features by decoding the input feature maps to a large scale before encoding a module. The decoding operation is conducted by deconvolution of the input feature maps, while encoding is conducted by convolution. Our model is trained on two earthquake-triggered landslide datasets, constructed using images with different spatial resolutions from different sensor platforms. Two other landslide datasets of different study areas with different spatial resolutions were used to evaluate the trained model. The experimental results demonstrated an at least 6.17% F1-measure improvement by DeenNet compared with three widely used typical encoder–decoder-based networks. The decoder–encoder network structure of DeenNet proves to be effective in maintaining landslide features, regardless of the size of the landslides in different evaluation images. It further validated the capacity of DeenNet in maintaining landslide features, which provides a strong applicability in the context of applications. © 2022 by the authors.,Loess landslides pose a severe threat of destruction, and detecting them is crucial for minimizing their impact on society. They typically consist of wind-deposited clay and silt, which makes them challenging to detect using conventional methods. Techniques like visual interpretation and field surveys are the most useful, yet these methods can be laborious, expensive, and require a certain level of prior knowledge. Furthermore, remote sensing approaches face the challenge of distinguishing between natural erosion and landslides. Recently, deep learning for landslide detection has the potential to speed up and improve detection accuracy. Nonetheless, deep learning models require large amounts of labeled data and the development of robust algorithms capable of extracting meaningful features from remote sensing data. In this article, a novel approach is introduced to improve the Mask Regional Convolutional Neural Network (Mask-RCNN) algorithm for accurately detecting landslides when the number of available segmentation mask samples is limited. Specifically, A novel loess landslides dataset is established using high-resolution remote sensing images in Gansu Province. In the context of partially supervised learning, a neural network branch containing a weight transfer function is developed to capture mask information from bounding boxes. Additionally, a mask-scoring network block is used to learn the quality of predicted instance masks. Our modified algorithm achieves an average precision improvement of 20.7% compared to the original Mask R-CNN algorithm in small landslide detection. The mask IoU threshold value of 0.5 is used to estimate the average accuracy higher than 0.75. The average precision of the segmentation mask is improved by 16.7% in test set. By proposing a solution that can achieve accurate landslide detection while using limited labeled samples, this study makes a valuable contribution to the application of deep learning in the domains of remote sensing and landslide detection. © 2023 Elsevier B.V.,Loess landslides are one of the geological hazards prevalent in mountainous areas of Loess Plateau, seriously threatening people’s lives and property safety. Accurate identification of landslides is a prerequisite for reducing the risk of landslide hazards. Traditional landslide interpretation methods often have the disadvantage of being laborious and difficult to use on a large scale compared with the recently developed deep learning-based landslide detection methods. In this study, we propose an improved deep learning model, landslide detection- you only look once (LD-YOLO), based on the existing you only look once (YOLO) model for the intelligent identification of old and new landslides in loess areas. Specifically, remote sensing images of landslides in Baoji City, Shaanxi Province, China are acquired from the Google Earth Engine platform. The landslide images of Baoji City (excluding Qianyang County) are used to establish a loess landslide dataset for training the model. The landslide data of Qianyang County is used to verify the detection performance of the model. The focal and efficient IoU (Focal-EIoU) loss function and efficient channel attention (ECA) mechanism are incorporated into the 7th version of YOLO (YOLOv7) model to construct the LD-YOLO model, which makes it more suitable for the landslide detection task. The experiments yielded an improved LD-YOLO model with average precision of 92.05%, precision of 92.31%, recall of 90.28%, and F1-score of 91.28% for loess landslide detection. The landslides in Qianyang County were divided into two test sets, new landslides and old landslides, which were used to test the detection performance of LD-YOLO for both types of landslides. The results show that LD-YOLO detects old landslides with a detection precision of 82.75% and a recall of 80%. When detecting new landslides, the detection precision is 94.29% and the recall is 91.67%. It indicates that our proposed LD-YOLO model has strong detection performance for both new and old landslides in loess areas. Through a proposed solution that can realize the accurate detection of landslides in loess areas, this paper provides a valuable reference for the application of deep learning methods in landslide identification. © 2023, Science Press, Institute of Mountain Hazards and Environment, CAS and Springer-Verlag GmbH Germany, part of Springer Nature."
248,247,14,247_Adolescent Hormones and Well-being,Adolescent Hormones and Well-being,"This paper uses a large scale and nationally representative dataset, Chinese General Social Survey, to empirically examine the role of physical activity in reducing the negative effects of depression among people with mental disorders. Empirical results demonstrate that physical exercise could help to alleviate depression's adverse consequences on work and life for depressed individuals. The impact mechanism is that physical activity may decrease the severity of depression, enhance life satisfaction, improve mood, and make people have a better sense of purpose and meaning in life. Therefore, from the perspective of multidimensional subjective wellbeing, evaluative wellbeing, experienced wellbeing and eudaimonic wellbeing all play mediating roles in the reduction of depression's adverse effects. Heterogeneity analysis shows that there are no significant gender differences in the health benefits of physical exercise, but its impact tends to be more prominent for depressed individuals who are younger and higher educated, with better health status, and live in urban areas. It is also found that socioeconomic status may play an important moderating role. The health benefits of physical activity seem to be greater for depressed people who have lower income, work in the secondary labor market, and have lower levels of social capital and assets. In addition, the instrumental variable approach is used to identify the causal impact of physical activity, which further proves a significant effect of it based on tackling the endogeneity problem. Meanwhile, this paper uses different explanatory and explained variables, different statistical models, as well as machine learning and placebo techniques to conduct robustness tests, all of which lend credence to above findings. Copyright © 2022 Li, Ning, Xia and Liu.,This paper uses a nationally representative and large-scale dataset from China to empirically examine the relationship between exercise participation and happiness. To address the problem of reverse causality between the two factors, the instrumental variable (IV) approach is used to deal with endogeneity to some extent. It is demonstrated that higher frequencies of exercise participation are positively related to happiness. Findings also demonstrate that physical exercise could significantly decrease depressive disorders, improves self-rated health conditions and reduces the frequency of health problems affecting people's work and life. At the same time, all of above health factors significantly influence subjective wellbeing. When these health variables are included in regressions, the correlation between exercise participation and happiness declines. This confirms that physical activity helps to improve happiness by enhancing mental and overall health conditions. In addition, results show that physical activities are more prominently related to happiness for male, older and unmarried individuals and those living in rural areas, lacking social security and with higher levels of depression as well as lower socioeconomic status. Furthermore, a series of robustness checks are carried out and exercise participation's positive role in improving happiness is further confirmed using different happiness measures and instrumental variables, various IV models, as well as penalized machine learning methods and placebo tests. With the increasing emphasis of improving happiness as an important goal in the global public health policy, findings of this paper have important policy implications for enhancing subjective wellbeing. Copyright © 2023 Li, Ning and Xia.,Background: Comorbidity of psychiatric disorders such as depression and anxiety is very common among children and adolescents. Few studies have examined how comorbid anxiety and depression are associated with health risk behaviors (HRBs) in adolescents, which could inform preventative approaches for mental health. Objective: We evaluated the association between HRBs and comorbid anxiety and depression in a large adolescent cohort. Methods: We used data from 22,868 adolescents in the National Youth Cohort (China). Anxiety and depression symptoms were assessed using the 9-item Patient Health Questionnaire scale and the 7-item Generalized Anxiety Disorder scale, respectively. Comorbidity was determined by the coexistence of anxiety and depression. HRBs including poor diet, smoking, physical inactivity, and poor sleep, as well as the above HRB scores, were added to obtain the total HRB score (HRB risk index). Based on single and total HRB scores, we divided participants into low-, medium-, and high-risk groups. Potential confounders included gender, presence of siblings, regional economic level, educational status, self-rated health, parental education level, self-reported family income, number of friends, learning burden, and family history of psychosis. Correlation analysis was used to explore associations between single risk behaviors. Binary logistic regression estimated the association between HRBs and anxiety-depression comorbidity before and after adjusting for potential confounders. Results: The comorbidity rate of anxiety and depression among Chinese adolescents was 31.6% (7236/22,868). There was a statistically significant association between each HRB (P<.05), and HRBs were positively associated with comorbid anxiety and depression in the above population. For single HRBs, adolescents with poor diet, smoking, and poor sleep (medium-risk) were more prone to anxiety-depression comorbidity after adjusting for confounders compared to low-risk adolescents. However, adolescents with all high-risk HRBs were more likely to have comorbid anxiety and depression after adjusting for confounders (poor diet odds ratio [OR] 1.50, 95% CI 1.39-1.62; smoking OR 2.17, 95% CI 1.67-2.81; physical inactivity OR 1.16, 95% CI 1.06-1.28; poor sleep OR 1.84, 95% CI 1.70-2.01). Moreover, in both unadjusted (medium risk OR 1.79, 95% CI 1.56-2.05; high risk OR 3.09, 95% CI 2.72-3.52) and adjusted (medium risk OR 1.57, 95% CI 1.37-1.80; high risk OR 2.33, 95% CI 2.03-2.68) models, HRB risk index, like clustered HRBs, was positively associated with anxiety-depression comorbidity, and the strength of the association was stronger than for any single HRB. In addition, we found that compared to girls, the association between clustered HRBs and anxiety-depression comorbidity was stronger in boys after adjustment. Conclusions: We provide evidence that HRBs are related to comorbid anxiety and depression. Interventions that decrease HRBs may support mental health development in adolescence, with the potential to improve health and well-being through to adulthood. © Meng Wang, Xingyue Mou, Tingting Li, Yi Zhang, Yang Xie, Shuman Tao, Yuhui Wan, Fangbiao Tao, Xiaoyan Wu."
249,248,14,248_Forecasting chaotic time series using Multi-level Information Aggregation Network (MIANet) and Echo State Networks (ESNs),Forecasting chaotic time series using Multi-level Information Aggregation Network (MIANet) and Echo State Networks (ESNs),"Electroencephalographic signals (EEG) are main examination of brain disease, because of their simple portability and their important temporal resolution of milliseconds or less. Nevertheless, the study of EEGs is confronted mainly with two problems: its complex behavior, which is considered in this paper due to its chaotic origin, and being contaminated by different types of noise from different sources. Our paper is thus carried out dealing with mentioned issues, we propose an algorithm for Largest Lyapunov Exponent (LLE) determination based on that of A. Wolf. In fact, LLE is an efficient tool for chaotic signal analysis. Our algorithm permits to study noisy chaotic signal. The proposed method was evaluated using a chaotic signal, obtained from the Logistic Map, for different values of the bifurcation parameter, we reach a low error rate in LLE determination using the proposed method (PLLE). Later, we use a noisy chaotic signal obtained from an additive noise to the previous signal, again PLLE leads to a perfect estimation of LLE, with a weak dependence on noise. The performance of the proposed PLLE, for LLE estimation, is also confirmed through other chaotic attractors, Hénon, Rössler and Lorenz. We then propose a supervised machine learning model for epilepsy and seizure detection based on PLLE, kernel tricks and features reduction using the benchmark EEG database provided by the University of Bonn. Comparisons with various state-of-the-art methods demonstrate the importance of our proposed method, which achieves 100% accuracy in different classification cases, runs fast and uses 4 features. © 2022 Elsevier Ltd,A deep learning method is developed for chaotic time series classification. We investigate the chaotic state of a dynamical system, based on the output of the system. One of the main obstacles in time series classification is mapping a high-dimensional vector into a scalar value. To reduce the dimensions, it is common to use an average pooling layer block after feature extraction block. This blind process results in models with high computational complexity and potent to overfitting. One alternative is to extract the features manually, then apply shallow learning models to classify the time series. In fact, since complexity lies between the chaos and order, it is a sound idea to refer to complex systems characteristics to explore the chaotic region entrance. Therefore, chaotic state of a dynamical system can be recognized solely based on these characteristics. Inspired by this concept, we conclude that there is a feature space in which the output vector can be sparsified. Thus, we propose a deep learning method which the feature space dimensions successively are reduced in the feature extraction process. Specifically, we employ a fully convolutional network and add on two maximum pooling layers to the relevant feature extraction block. To validate the proposed model, the Lorenz system is employed which exhibits chaotic/non-chaotic states. We generate a labeled dataset containing 10000 samples each with 20000 features of the output of Lorenz system. The proposed model achieves 99.45 percent accuracy over 2000 unseen samples, higher than all the other competitor methods. © 2023 Materials and Energy Research Center. All rights reserved.,From one side, Evolutionary Algorithms have enabled enormous progress over the last years in the optimization field. They have been applied to a variety of problems, including optimization of Neural Networks’ architectures. On the other side, the Echo State Network (ESN) model has become increasingly popular in time series prediction, for instance when modeling chaotic sequences. The network has numerous hidden neurons forming a recurrent topology, so-called reservoir, which is fixed during the learning process. Initial reservoir design has mostly been made by human experts; as a consequence, it is prone to errors and bias, and it is a time consuming task. In this paper, we introduce an automatic general neuroevolutionary framework for ESNs, on which we develop a computational tool for evolving reservoirs, called EVOlutionary Echo State Network (EvoESN). To increase efficiency, we represent the large matrix of reservoir weights in the Fourier space, where we perform the evolutionary search strategy. This frequency space has major advantages compared with the original weight space. After updating the Fourier coefficients, we go back to the weight space and perform a conventional training phase for full setting the reservoir architecture. We analyze the evolutionary search employing genetic algorithms and particle swarm optimization, obtaining promising results with the latter over three well-known chaotic time series. The proposed framework leads fast to very good results compared with modern ESN models. Hence, this contribution positions an important family of recurrent systems in the promising neuroevolutionary domain. © 2023 Elsevier B.V."
250,249,14,249_Crop breeding using genomic markers and machine learning,Crop breeding using genomic markers and machine learning,"Background: Many studies have demonstrated the utility of machine learning (ML) methods for genomic prediction (GP) of various plant traits, but a clear rationale for choosing ML over conventionally used, often simpler parametric methods, is still lacking. Predictive performance of GP models might depend on a plethora of factors including sample size, number of markers, population structure and genetic architecture. Methods: Here, we investigate which problem and dataset characteristics are related to good performance of ML methods for genomic prediction. We compare the predictive performance of two frequently used ensemble ML methods (Random Forest and Extreme Gradient Boosting) with parametric methods including genomic best linear unbiased prediction (GBLUP), reproducing kernel Hilbert space regression (RKHS), BayesA and BayesB. To explore problem characteristics, we use simulated and real plant traits under different genetic complexity levels determined by the number of Quantitative Trait Loci (QTLs), heritability (h 2 and h 2e), population structure and linkage disequilibrium between causal nucleotides and other SNPs. Results: Decision tree based ensemble ML methods are a better choice for nonlinear phenotypes and are comparable to Bayesian methods for linear phenotypes in the case of large effect Quantitative Trait Nucleotides (QTNs). Furthermore, we find that ML methods are susceptible to confounding due to population structure but less sensitive to low linkage disequilibrium than linear parametric methods. Conclusions: Overall, this provides insights into the role of ML in GP as well as guidelines for practitioners. Copyright: © 2023 Farooq M et al.,In modern plant breeding, genomic selection is becoming the gold standard to select superior genotypes in large breeding populations that are only partially phenotyped. Many breeding programs commonly rely on single-nucleotide polymorphism (SNP) markers to capture genome-wide data for selection candidates. For this purpose, SNP arrays with moderate to high marker density represent a robust and cost-effective tool to generate reproducible, easy-to-handle, high-throughput genotype data from large-scale breeding populations. However, SNP arrays are prone to technical errors that lead to failed allele calls. To overcome this problem, failed calls are often imputed, based on the assumption that failed SNP calls are purely technical. However, this ignores the biological causes for failed calls—for example: deletions—and there is increasing evidence that gene presence–absence and other kinds of genome structural variants can play a role in phenotypic expression. Because deletions are frequently not in linkage disequilibrium with their flanking SNPs, permutation of missing SNP calls can potentially obscure valuable marker–trait associations. In this study, we analyze published datasets for canola and maize using four parametric and two machine learning models and demonstrate that failed allele calls in genomic prediction are highly predictive for important agronomic traits. We present two statistical pipelines, based on population structure and linkage disequilibrium, that enable the filtering of failed SNP calls that are likely caused by biological reasons. For the population and trait examined, prediction accuracy based on these filtered failed allele calls was competitive to standard SNP-based prediction, underlying the potential value of missing data in genomic prediction approaches. The combination of SNPs with all failed allele calls or the filtered allele calls did not outperform predictions with only SNP-based prediction due to redundancy in genomic relationship estimates. Copyright © 2023 Weber, Chawla, Ehrig, Hickey, Frisch and Snowdon.,Background: Genomewide prediction estimates the genomic breeding values of selection candidates which can be utilized for population improvement and cultivar development. Ridge regression and deep learning-based selection models were implemented for yield and agronomic traits of 204 chile pepper genotypes evaluated in multi-environment trials in New Mexico, USA. Results: Accuracy of prediction differed across different models under ten-fold cross-validations, where high prediction accuracy was observed for highly heritable traits such as plant height and plant width. No model was superior across traits using 14,922 SNP markers for genomewide selection. Bayesian ridge regression had the highest average accuracy for first pod date (0.77) and total yield per plant (0.33). Multilayer perceptron (MLP) was the most superior for flowering time (0.76) and plant height (0.73), whereas the genomic BLUP model had the highest accuracy for plant width (0.62). Using a subset of 7,690 SNP loci resulting from grouping markers based on linkage disequilibrium coefficients resulted in improved accuracy for first pod date, ten pod weight, and total yield per plant, even under a relatively small training population size for MLP and random forest models. Genomic and ridge regression BLUP models were sufficient for optimal prediction accuracies for small training population size. Combining phenotypic selection and genomewide selection resulted in improved selection response for yield-related traits, indicating that integrated approaches can result in improved gains achieved through selection. Conclusions: Accuracy values for ridge regression and deep learning prediction models demonstrate the potential of implementing genomewide selection for genetic improvement in chile pepper breeding programs. Ultimately, a large training data is relevant for improved genomic selection accuracy for the deep learning models. © 2023, The Author(s)."
251,250,14,250_Edge Computing in IoT-based Applications,Edge Computing in IoT-based Applications,"In recent years, more and more applied industries have relied on data collection by IoT devices. Various IoT devices generate vast volumes of data that require efficient processing. Usually, the intellectual analysis of such data takes place in data centers in cloud environments. However, the problems of transferring large volumes of data and the long wait for a response from the data center for further corrective actions in the system led to the search for new processing methods. One possible option is Edge computing. Intelligent data analysis in the places of their collection eliminates the disadvantages mentioned above, revealing many advantages of using such an approach in practice. However, the Edge computing approach is challenging to implement when different IoT devices collect the independent attributes required for classification/regression. In order to overcome this limitation, the authors developed a new cascade ensemble-learning model for the deployment at the Edge. It is based on the principles of cascading machine learning methods, where each IoT device that collects data performs its analysis based on the attributes it contains. The results of its work are transmitted to the next IoT device, which analyzes the attributes it collects, taking into account the output of the previous device. All independent at-tributes are taken into account in this way. Because of this, the proposed approach provides: 1) The possibility of effective implementation of Edge computing for intelligent data analysis, that is, even before their transmission to the data center; 2) increasing, and in some cases maintaining, classification/regression accuracy at the same level that can be achieved in the data center; 3) significantly reducing the duration of training procedures due to the processing of a smaller number of attributes by each of the IoT devices. The simulation of the proposed approach was performed on a real-world set of IoT data. The missing data recovery task in the atmospheric air state data was solved. The authors selected the optimal parameters of the proposed approach. It was established that the developed model provides a slight increase in prediction accuracy while significantly reducing the duration of the training procedure. However, in this case, the main advantage is that all this happens within the bounds of Edge computing, which opens up several benefits of using the developed model in practice. Copyright © 2023 Izonin, Tkachenko, Krak, Berezsky, Shevchuk and Shandilya.,Due to Industry 4.0, machines can be connected to their manufacturing processes with the ability to react faster and smarter to changing conditions in a factory. Previously, Internet of Things (IoT) devices could only collect and send data to the cloud for analysis. However, the increasing computing capacity of today's devices allows them to perform complex computations on-device, resulting in edge computing. Edge devices are a fundamental component of modern, distributed real-world artificial intelligence (AI) systems in Industry 4.0 environments. As a result, edge computing extends cloud computing capabilities by bringing services near the edge of a network and thus supports a new variety of AI services and machine learning (ML) applications. However, there is a large difference between designing and training an ML model, potentially in the cloud, to create ML services that can be deployed and consumed on the edge. This article presents an ML workflow based on ML operations (MLOps) over the Thinger.io IoT platform to streamline the transition from model training to model deployment on edge devices. The proposed workflow is composed of different elements, such as the ML training pipeline, ML deployment pipeline, and ML workspace. Similarly, this article describes the ease of design and deployment of the proposed solution in a real environment, where an anomaly detection service is implemented for detecting outliers on temperature and humidity measurements. The performance tests performed over the ML pipeline steps and the ML service throughput on the edge indicate that this workflow adds minimum overhead to the process, providing a more reliable, reusable, and productive environment.  © 2014 IEEE.,Internet-of-Things (IoT)-based cyber–physical systems are increasingly being adopted because of the recent technological advancements in sensor technology, edge computing, machine learning, and big data. Integrating machine learning into designing IoT-based cyber–physical systems is essential. However, it is considered a challenging problem. This stems from the fact that IoT devices generate extensive data that requires extensive processing to achieve adequate learning. Relying on local learning by each IoT device is not feasible in most cases due to its limited resources. On the contrary, relying on all cloud-based learning requires transmitting a large amount of data to the cloud to perform the learning process, which is inefficient in large-scale IoT deployments. Therefore, this paper proposes a novel edge-computing architecture that employs the concept of distributed multi-task learning over EC networks in large-scale IoT-based cyber–physical systems. The architecture develops multiple distributed learning algorithms, a data placement architecture, task allocation algorithms, and a network protocol. In addition, it considers the problem of learning model parameters from IoT data distributed over different edge nodes in a large geographical area without sending raw data to the cloud. The architecture supports several distributed machine models that are trained using a combination of machine learning algorithms and population-based search algorithms to optimize the learning process. Population-based search algorithms allow for maintaining a set of candidate solutions, with each solution corresponding to a unique point in the search space for an optimal solution. Having the dataset distributed over several edge nodes, with each node having its own unique set of candidate solutions, increases the chance of finding a solution that generalizes well for the overall dataset combined. Simulation experiments with real IoT datasets are conducted to evaluate the accuracy of the proposed learning models. Results show the ability to achieve high-accuracy results that are close to single-machine models but with significantly efficient edge computing resource utilization. © 2022 Elsevier B.V."
252,251,14,251_Underwater Acoustic Signal Processing and Recognition,Underwater Acoustic Signal Processing and Recognition,"In underwater acoustic target recognition, there is a lack of massive high-quality labeled samples to train robust deep neural networks, and it is difficult to collect and annotate a large amount of base class data in advance unlike the image recognition field. Therefore, conventional few-shot learning methods are difficult to apply in underwater acoustic target recognition. In this report, following advanced self-supervised learning frameworks, a learning framework for underwater acoustic target recognition model with few samples is proposed. Meanwhile, a semi-supervised fine-tuning method is proposed to improve the fine-tuning performance by mining and labeling partial unlabeled samples based on the similarity of deep features. A set of small sample datasets with different amounts of labeled data are constructed, and the performance baselines of four underwater acoustic target recognition models are established based on these datasets. Compared with the baselines, using the proposed framework effectively improves the recognition effect of four models. Especially for the joint model, the recognition accuracy has increased by 2.04% to 12.14% compared with the baselines. The model performance on only 10 percent of the labeled data can exceed that on the full dataset, effectively reducing the dependence of model on the number of labeled samples. The problem of lack of labeled samples in underwater acoustic target recognition is alleviated. © 2023, Springer Nature Limited.,The research of underwater sonar image classification is beneficial to the development of marine resources. In recent years, deep neural networks have achieved great success in the fields of image classification and image recognition. However, deep neural networks require a large amount of training data. Due to the difficulty of obtaining sonar image datasets, the existing public sonar datasets are generally small, and sonar image classification can be regarded as a small sample problem. To solve this problem, we designed a deep adaptive sonar image classification network (DASCN) based on deep learning and domain adaptation. The feature extraction module in DASCN extracts multiscale features of images; the attention module learns the importance of different channel features; and the domain adaptation module reduces the difference between the source domain and the target domain. It is worth noting that our DASCN does not require a large number of training samples, and sonar images used for training do not need to be labeled. As demonstrated in comprehensive experiments, the classification accuracy of the DASCN model reached 89.4% on the sonar image dataset. Our DASCN achieves unsupervised accurate classification of small sample sonar images. In addition, our DASCN has good classification results on the Office-31 dataset and the Office Home dataset and has good generalization performance.  © 2023 SPIE and IS&T.,The importance of active sonar is increasing due to the quieting of submarines and the increase in maritime traffic. However, the multipath propagation of sound waves and the low signal-to-noise ratio due to multiple clutter make it difficult to detect, track, and identify underwater targets using active sonar. To solve this problem, machine learning and deep learning techniques that have recently been in the spotlight are being applied, but these techniques require a large amount of data. In order to supplement insufficient active sonar data, methods based on mathematical modeling are primarily utilized. However, mathematical modeling-based methods have limitations in accurately simulating complicated underwater phenomena. Therefore, an artificial intelligence-based sonar signal synthesis technique is proposed in this paper. The proposed method modified the major modules of the Tacotron model, which is widely used in the field of speech synthesis, in order to apply the Tacotron model to the field of sonar signal synthesis. To prove the validity of the proposed method, spectrograms of synthesized sonar signals are analyzed and the mean opinion score was measured. Through the evaluation, we confirmed that the proposed method can synthesize active sonar data similar to the trained one. © 2022 by the authors."
253,252,14,252_Mental health and suicide-related research using language analysis,Mental health and suicide-related research using language analysis,"Suicide is the 10th leading cause of death in the USA and globally. Despite decades of research, the ability to predict who will die by suicide is still no better than 50%. Traditional screening instruments have helped identify risk factors for suicide, but they have not provided accurate predictive power for reducing death rates. Over the past decade, natural language processing (NLP), a form of machine learning (ML), has been used to identify suicide risk by analyzing language data. Recent work has demonstrated the successful integration of a suicide risk screening interview to collect language data for NLP analysis from patients in two emergency departments (ED) of a large healthcare system. Results indicated that ML/NLP models performed well identifying patients that came to the ED for suicide risk. However, little is known about the clinician’s perspective of how a qualitative brief interview suicide risk screening tool to collect language data for NLP integrates into an ED workflow. This report highlights the feedback and observations of patient experiences obtained from clinicians using brief suicide screening interviews. The investigator used an open-ended, narrative interview approach to inquire about the qualitative interview process. Three overarching themes were identified: behavioral health workflow, clinical implications of interview probes, and integration of an application into provider patient experience. Results suggest a brief, qualitative interview method was feasible, person-centered, and useful as a suicide risk detection approach. © 2023, National Council for Mental Wellbeing.,Background: Social media platforms have been increasingly used to express suicidal thoughts, feelings, and acts, raising public concerns over time. A large body of literature has explored the suicide risks identified by people's expressions on social media. However, there is not enough evidence to conclude that social media provides public surveillance for suicide without aligning suicide risks detected on social media with actual suicidal behaviors. Corroborating this alignment is a crucial foundation for suicide prevention and intervention through social media and for estimating and predicting suicide in countries with no reliable suicide statistics. Objective: This study aimed to corroborate whether the suicide risks identified on social media align with actual suicidal behaviors. This aim was achieved by tracking suicide risks detected by 62 million tweets posted in Japan over a 10-year period and assessing the locational and temporal alignment of such suicide risks with actual suicide behaviors recorded in national suicide statistics. Methods: This study used a human-in-the-loop approach to identify suicide-risk tweets posted in Japan from January 2013 to December 2022. This approach involved keyword-filtered data mining, data scanning by human efforts, and data refinement via an advanced natural language processing model termed Bidirectional Encoder Representations from Transformers. The tweet-identified suicide risks were then compared with actual suicide records in both temporal and spatial dimensions to validate if they were statistically correlated. Results: Twitter-identified suicide risks and actual suicide records were temporally correlated by month in the 10 years from 2013 to 2022 (correlation coefficient=0.533; P < .001); this correlation coefficient is higher at 0.652 when we advanced the Twitter-identified suicide risks 1 month earlier to compare with the actual suicide records. These 2 indicators were also spatially correlated by city with a correlation coefficient of 0.699 (P < .001) for the 10-year period. Among the 267 cities with the top quintile of suicide risks identified from both tweets and actual suicide records, 73.5% (n=196) of cities overlapped. In addition, Twitter-identified suicide risks were at a relatively lower level after midnight compared to a higher level in the afternoon, as well as a higher level on Sundays and Saturdays compared to weekdays. Conclusions: Social media platforms provide an anonymous space where people express their suicidal thoughts, ideation, and acts. Such expressions can serve as an alternative source to estimating and predicting suicide in countries without reliable suicide statistics. It can also provide real-time tracking of suicide risks, serving as an early warning for suicide. The identification of areas where suicide risks are highly concentrated is crucial for location-based mental health planning, enabling suicide prevention and intervention through social media in a spatially and temporally explicit manner. © Siqin Wang, Huan Ning, Xiao Huang, Yunyu Xiao, Mengxi Zhang, Ellie Fan Yang, Yukio Sadahiro, Yan Liu, Zhenlong Li, Tao Hu, Xiaokang Fu, Zi Li, Ye Zeng. Originally published in the Journal of Medical Internet Research (https://www.jmir.org),02.06.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.,Background: Current depression, anxiety, and suicide screening techniques rely on retrospective patient reported symptoms to standardized scales. A qualitative approach to screening combined with the innovation of natural language processing (NLP) and machine learning (ML) methods have shown promise to enhance person-centeredness while detecting depression, anxiety, and suicide risk from in-the-moment patient language derived from an open-ended brief interview. Objective: To evaluate the performance of NLP/ML models to identify depression, anxiety, and suicide risk from a single 5–10-min semi-structured interview with a large, national sample. Method: Two thousand four hundred sixteen interviews were conducted with 1,433 participants over a teleconference platform, with 861 (35.6%), 863 (35.7%), and 838 (34.7%) sessions screening positive for depression, anxiety, and suicide risk, respectively. Participants completed an interview over a teleconference platform to collect language about the participants’ feelings and emotional state. Logistic regression (LR), support vector machine (SVM), and extreme gradient boosting (XGB) models were trained for each condition using term frequency-inverse document frequency features from the participants’ language. Models were primarily evaluated with the area under the receiver operating characteristic curve (AUC). Results: The best discriminative ability was found when identifying depression with an SVM model (AUC = 0.77; 95% CI = 0.75–0.79), followed by anxiety with an LR model (AUC = 0.74; 95% CI = 0.72–0.76), and an SVM for suicide risk (AUC = 0.70; 95% CI = 0.68–0.72). Model performance was generally best with more severe depression, anxiety, or suicide risk. Performance improved when individuals with lifetime but no suicide risk in the past 3 months were considered controls. Conclusion: It is feasible to use a virtual platform to simultaneously screen for depression, anxiety, and suicide risk using a 5-to-10-min interview. The NLP/ML models performed with good discrimination in the identification of depression, anxiety, and suicide risk. Although the utility of suicide risk classification in clinical settings is still undetermined and suicide risk classification had the lowest performance, the result taken together with the qualitative responses from the interview can better inform clinical decision-making by providing additional drivers associated with suicide risk. Copyright © 2023 Wright-Berryman, Cohen, Haq, Black and Pease."
254,253,14,253_Deep Learning-Based Survival Prediction in Lymphoma Patients,Deep Learning-Based Survival Prediction in Lymphoma Patients,"Background: Gallbladder cancer is the sixth most common malignant gastrointestinal tumor. Radical surgery is currently the only effective treatment, but patient prognosis is poor, with a 5-year survival rate of only 5–10%. Establishing an effective survival prediction model for gallbladder cancer patients is crucial for disease status assessment, early intervention, and individualized treatment approaches. The existing gallbladder cancer survival prediction model uses clinical data—radiotherapy and chemotherapy, pathology, and surgical scope—but fails to utilize laboratory examination and imaging data, limiting its prediction accuracy and preventing sufficient treatment plan guidance. Aims: The aim of this work is to propose an accurate survival prediction model, based on the deep learning 3D-DenseNet network, integrated with multimodal medical data (enhanced CT imaging, laboratory test results, and data regarding systemic treatments). Methods: Data were collected from 195 gallbladder cancer patients at two large tertiary hospitals in Shanghai. The 3D-DenseNet network extracted deep imaging features and constructed prognostic factors, from which a multimodal survival prediction model was established, based on the Cox regression model and incorporating patients’ laboratory test and systemic treatment data. Results: The model had a C-index of 0.787 in predicting patients’ survival rate. Moreover, the area under the curve (AUC) of predicting patients’ 1-, 3-, and 5-year survival rates reached 0.827, 0.865, and 0.926, respectively. Conclusions: Compared with the monomodal model based on deep imaging features and the tumor–node–metastasis (TNM) staging system—widely used in clinical practice—our model’s prediction accuracy was greatly improved, aiding the prognostic assessment of gallbladder cancer patients. © 2022, The Author(s).,Objectives: To assess the performance of DeepSurv, a deep learning-based model in the survival prediction of laryngeal squamous cell carcinoma (LSCC) using the Surveillance, Epidemiology, and End Results (SEER) database. Methods: In this large population-based study, we developed and validated a deep learning survival neural network using pathologically diagnosed patients with LSCC from the SEER database between January 2010 and December 2018. Totally 13 variables were included in this network, including patients baseline characteristics, stage, grade, site, tumor extension and treatment details. Based on the total risk score derived from this algorithm, a three-knot restricted cubic spline was plotted to exhibit the difference of survival benefits from two treatment modalities. Results: Totally 6316 patients with LSCC were included in the study, of which 4237 cases diagnosed between 2010 and 2015 were selected as the development cohort, and the rest (2079 cases diagnosed from 2016 to 2018) were the validation cohort. A state-of-the-art deep learning-based model based on 23 features (i.e., 13 variables) was generated, which showed more superior performance in the prediction of overall survival (OS) than the tumor, node, and metastasis (TNM) staging system (C-index for DeepSurv vs TNM staging = 0.71; 95% CI 0.69–0.74 vs 0.61; 95% CI 0.60–0.63). Interestingly, a significantly nonlinear association between total risk score and treatment effectiveness was observed. When the total risk score ranges 0.1–1.5, surgical treatment brought more survival benefits than nonsurgical one for LSCC patients, especially in 70.5% of patients staged III–IV. Conclusions: The deep learning-based model shows more potential benefits in survival estimation for patients with LSCC, which may potentially serve as an auxiliary approach to provide reliable treatment recommendations. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Background: For the purpose to examine lower limb melanoma (LLM) and its long-term survival rate, we used data from the Surveillance, Epidemiology and End Results (SEER) database. To estimate the prognosis of LLM patients and assess its efficacy, we used a powerful deep learning and neural network approach called DeepSurv. Methods: We gathered data on those who had an LLM diagnosis between 2000 and 2019 from the SEER database. We divided the people into training and testing cohorts at a 7:3 ratio using a random selection technique. To assess the likelihood that LLM patients would survive, we compared the results of the DeepSurv model with those of the Cox proportional-hazards (CoxPH) model. Calibration curves, the time-dependent area under the receiver operating characteristic curve (AUC), and the concordance index (C-index) were all used to assess how accurate the predictions were. Results: In this study, a total of 26,243 LLM patients were enrolled, with 7873 serving as the testing cohort and 18,370 as the training cohort. Significant correlations with age, gender, AJCC stage, chemotherapy status, surgery status, regional lymph node removal and the survival outcomes of LLM patients were found by the CoxPH model. The CoxPH model’s C-index was 0.766, which signifies a good degree of predicted accuracy. Additionally, we created the DeepSurv model using the training cohort data, which had a higher C-index of 0.852. In addition to calculating the 3-, 5-, and 8-year AUC values, the predictive performance of both models was evaluated. The equivalent AUC values for the CoxPH model were 0.795, 0.767, and 0.847, respectively. The DeepSurv model, in comparison, had better AUC values of 0.872, 0.858, and 0.847. In comparison to the CoxPH model, the DeepSurv model demonstrated greater prediction performance for LLM patients, as shown by the AUC values and the calibration curve. Conclusion: We created the DeepSurv model using LLM patient data from the SEER database, which performed better than the CoxPH model in predicting the survival time of LLM patients. © 2023, The Author(s)."
255,254,13,254_Privacy-preserving federated load forecasting and management for smart energy systems,Privacy-preserving federated load forecasting and management for smart energy systems,"In this paper, we propose privacy-friendly electricity consumption prediction models based on Federated Learning (FL). Federated Learning provides a novel framework for Artificial Neural Network training. It decouples data storage from model training from decentralized data sources. A central server does not need to collect raw training data from individual clients but only the model parameters, like the gradients or weights updates. The sensitive raw data can be safely stored and used by end-users themselves. We improve the accuracy of the FL model by first clustering households, and training a personalized model for each cluster. We also analyse the Deep Leakage from Gradients (DLG) attack in our study case with three scenarios. Our simulations suggest that the DLG attack can barely succeed in consumption prediction. In this way, the FL model can guarantee clients’ privacy by design. We use a large-scale dataset that contains 3590 households’ 1.5 years of consumption to test the FL model's performance. Several clustering algorithms are tested for the following experiments. To comprehensively test the FL model's performance, we propose several popular neural network models: the simple Deep Neural Network, Long Short-Term Memory, Convolutional Neural Network, and WaveNet. These models are both trained under the centralized and federated framework. The federated trained models slightly sacrifice the model's accuracy while guaranteeing the client's data privacy. Meanwhile, the federated model shows remarkable scalability. Under the FL framework, new clients can obtain their prediction 6 times faster. Moreover, the federated model has strong robustness against missing or damaged training data. With a certain percentage of missing data in the training set, the centralized model's accuracy gets 27% worse, while the federated model's accuracy only gets 2% worse. © 2023 Elsevier Ltd,Accurate load forecasting is critical for electricity production, transmission, and maintenance. Deep learning (DL) model has replaced other classical models as the most popular prediction models. However, the deep prediction model requires users to provide a large amount of private electricity consumption data, which has potential privacy risks. Edge nodes can federally train a global model through aggregation using federated learning (FL). As a novel distributed machine learning (ML) technique, it only exchanges model parameters without sharing raw data. However, existing forecasting methods based on FL still face challenges from data heterogeneity and privacy disclosure. Accordingly, we propose a user-level load forecasting system based on personalized federated learning (PFL) to address these issues. The obtained personalized model outperforms the global model on local data. Further, we introduce a novel differential privacy (DP) algorithm in the proposed system to provide an additional privacy guarantee. Based on the principle of generative adversarial network (GAN), the algorithm achieves the balance between privacy and prediction accuracy throughout the game. We perform simulation experiments on the real-world dataset and the experimental results show that the proposed system can comply with the requirement for accuracy and privacy in real load forecasting scenarios. © 2018 Tsinghua University Press.,With high levels of intermittent power generation and dynamic demand patterns, accurate forecasts for residential loads have become essential. Smart meters can play an important role when making these forecasts as they provide detailed load data. However, using smart meter data for load forecasting is challenging due to data privacy requirements. This paper investigates how these requirements can be addressed through a combination of federated learning and privacy preserving techniques such as differential privacy and secure aggregation. For our analysis, we employ a large set of residential load data and simulate how different federated learning models and privacy preserving techniques affect performance and privacy. Our simulations reveal that combining federated learning and privacy preserving techniques can secure both high forecasting accuracy and near-complete privacy. Specifically, we find that such combinations enable a high level of information sharing while ensuring privacy of both the processed load data and forecasting models. Moreover, we identify and discuss challenges of applying federated learning, differential privacy and secure aggregation for residential short-term load forecasting. © 2022 The Author(s)"
256,255,13,"255_Peptide Screening and Prediction for Bioactive, Antimicrobial, Umami, and Therapeutic Peptides","Peptide Screening and Prediction for Bioactive, Antimicrobial, Umami, and Therapeutic Peptides","Functional peptides are easy to absorb and have low side effects, which has attracted increasing interest from pharmaceutical scientists. However, due to the limitations in the laboratory funding and human resources, it is difficult to screen the functional peptides from a large number of peptides with unknown functions. With the development of machine learning and Deep learning, the combination of computational methods and biological information provides an effective method for identifying peptide functions. To explore the value of multi-functional active peptides, a new deep learning method named Deep2Pep (Deep learning to Peptides) was constructed, which was based on sequence encoding, embedding, and language tokenizer. It can achieve predictions of peptides on antimicrobial, antihypertensive, antioxidant and antihyperglycemic by converting sequence information into digital vectors, combined BiLSTM, attention-residual algorithm, and BERT Encoder. The results showed that Deep2Pep had a Hamming Loss of 0.095, subset Accuracy of 0.737, and Macro F1-Score of 0.734. which outperformed other models. BiLSTM played a primary role in Deep2Pep, which BERT encoder was in an auxiliary position. Deep learning algorithms was used in this study to accurately predict the four active functions of peptides, and it was expected to provide effective references for predicting multi-functional peptides. © 2024 Elsevier Ltd,Amyloid-like nanofibers from self-assembling peptides can promote viral gene transfer for therapeutic applications. Traditionally, new sequences are discovered either from screening large libraries or by creating derivatives of known active peptides. However, the discovery of de novo peptides, which are sequence-wise not related to any known active peptides, is limited by the difficulty to rationally predict structure-activity relationships because their activities typically have multi-scale and multi-parameter dependencies. Here, we used a small library of 163 peptides as a training set to predict de novo sequences for viral infectivity enhancement using a machine learning (ML) approach based on natural language processing. Specifically, we trained an ML model using continuous vector representations of the peptides, which were previously shown to retain relevant information embedded in the sequences. We used the trained ML model to sample the sequence space of peptides with 6 amino acids to identify promising candidates. These 6-mers were then further screened for charge and aggregation propensity. The resulting 16 new 6-mers were tested and found to be active with a 25% hit rate. Strikingly, these de novo sequences are the shortest active peptides for infectivity enhancement reported so far and show no sequence relation to the training set. Moreover, by screening the sequence space, we discovered the first hydrophobic peptide fibrils with a moderately negative surface charge that can enhance infectivity. Hence, this ML strategy is a time- and cost-efficient way for expanding the sequence space of short functional self-assembling peptides exemplified for therapeutic viral gene delivery. © 2023 The Royal Society of Chemistry,Many bioactive peptides demonstrated therapeutic effects over complicated diseases, such as antiviral, antibacterial, anticancer, etc. It is possible to generate a large number of potentially bioactive peptides using deep learning in a manner analogous to the generation of de novo chemical compounds using the acquired bioactive peptides as a training set. Such generative techniques would be significant for drug development since peptides are much easier and cheaper to synthesize than compounds. Despite the limited availability of deep learning-based peptide-generating models, we have built an LSTM model (called LSTM_Pep) to generate de novo peptides and fine-tuned the model to generate de novo peptides with specific prospective therapeutic benefits. Remarkably, the Antimicrobial Peptide Database has been effectively utilized to generate various kinds of potential active de novo peptides. We proposed a pipeline for screening those generated peptides for a given target and used the main protease of SARS-COV-2 as a proof-of-concept. Moreover, we have developed a deep learning-based protein-peptide prediction model (DeepPep) for rapid screening of the generated peptides for the given targets. Together with the generating model, we have demonstrated that iteratively fine-tuning training, generating, and screening peptides for higher-predicted binding affinity peptides can be achieved. Our work sheds light on developing deep learning-based methods and pipelines to effectively generate and obtain bioactive peptides with a specific therapeutic effect and showcases how artificial intelligence can help discover de novo bioactive peptides that can bind to a particular target. © 2023 American Chemical Society."
257,256,13,256_Traffic classification for cybersecurity using machine learning and deep learning techniques,Traffic classification for cybersecurity using machine learning and deep learning techniques,"The proliferation of smart devices in the 5G era of industrial IoT (IIoT) produces significant traffic data, some of which is encrypted malicious traffic, creating a significant problem for malicious traffic detection. Malicious traffic classification is one of the most efficient techniques for detecting malicious traffic. Although it is a labor-intensive and time-consuming process to gather large labeled datasets, the majority of prior studies on the classification of malicious traffic use supervised learning approaches and provide decent classification results when a substantial quantity of labeled data is available. This paper proposes a semi-supervised learning approach for classifying malicious IIoT traffic. The approach utilizes the encoder–decoder model framework to classify the traffic, even with a limited amount of labeled data available. We sample and normalize the data during the data-processing stage. In the semi-supervised model-building stage, we first pre-train a model on a large unlabeled dataset. Subsequently, we transfer the learned weights to a new model, which is then retrained using a small labeled dataset. We also offer an edge intelligence model that considers aspects such as computation latency, transmission latency, and privacy protection to improve the model’s performance. To achieve the lowest total latency and to reduce the risk of privacy leakage, we first create latency and privacy-protection models for each local, edge, and cloud. Then, we optimize the total latency and overall privacy level. In the study of IIoT malicious traffic classification, experimental results demonstrate that our method reduces the model training and classification time with 97.55% accuracy; moreover, our approach boosts the privacy-protection factor. © 2023 by the authors.,An enterprise's private cloud may be attacked by attackers when communicating with the public cloud. Although traffic detection methods based on deep learning have been widely used, these methods rely on a large amount of sample data and cannot quickly detect new attacks such as Zero-day Attacks. Moreover, deep learning has a black-box nature and cannot interpret the detection results, which has certain security risks. This paper proposes an interpretable abnormal traffic detection method, which can complete the detection task with only a few malicious traffic samples. Specifically, it uses the covariance matrix to characterize each traffic category and then calculates the similarity between the query traffic and each category according to the covariance metric function to realize the traffic detection based on few-shot learning. After that, the traffic images processed by the random masks are input into the model to obtain the predicted probability of the corresponding traffic category. Finally, the predicted probability is linearly summed with each mask to generate the final saliency map to interpret and analyze the model decision. In this paper, experiments are carried out by simulating only 15 and 25 malicious traffic samples. The results show that the proposed method can obtain good accuracy and recall, and the interpretation analysis shows that the model is reliable and interpretable. © 2022 Elsevier B.V.,Encrypted traffic classification requires identifying the services and programs running behind the content-invisible traffic data for improving quality of service and providing security assurance. Mainstream solutions achieve reliable performance by training on large-scale datasets. However, with the continuous emergence and development of encryption services, collecting and labeling sufficient amounts of encrypted traffic becomes impractical. Therefore, it is critical to utilize the few labeled data for accurate encrypted traffic classification. In this paper, we propose a Multi-task Representation Enhanced Meta-learning model (MetaMRE) for few-shot encrypted traffic classification. Specifically, we design a flow discrepancy enhancement module that combines supervised learning and clustering-based unsupervised learning to boost the discrepancy of encrypted traffic representations from a few labeled data. Moreover, MetaMRE introduces a multi-task collaborative meta-learning module that makes full use of non-target task data to learn the optimal initialization parameters suitable for the encrypted traffic classification, and then only a small amount of labeled encrypted traffic is required to adapt to the target classification task. Extensive evaluations on various real-world datasets show that the MetaMRE outperforms existing state-of-the-art methods and copes well with version updates and cross-domain problems in encrypted traffic classification. © 2023 Elsevier B.V."
258,257,13,257_MRI Radiomics and Machine Learning for Stroke Outcome Prediction,MRI Radiomics and Machine Learning for Stroke Outcome Prediction,"Background: The selection of patients with large-vessel occlusion (LVO) stroke for endovascular treatment (EVT) depends on patient characteristics and procedural metrics. The relation of these variables to functional outcome after EVT has been assessed in numerous datasets from both randomized controlled trials (RCT) and real-world registries, but whether differences in their case mix modulate outcome prediction is unknown. Methods: We leveraged data from individual patients with anterior LVO stroke treated with EVT from completed RCTs from the Virtual International Stroke Trials Archive (N = 479) and from the German Stroke Registry (N = 4079). Cohorts were compared regarding (i) patient characteristics and procedural pre-EVT metrics, (ii) these variables’ relation to functional outcome, and (iii) the performance of derived outcome prediction models. Relation to outcome (functional dependence defined by a modified Rankin Scale score of 3–6 at 90 days) was analyzed by logistic regression models and a machine learning algorithm. Results: Ten out of 11 analyzed baseline variables differed between the RCT and real-world cohort: RCT patients were younger, had higher admission NIHSS scores, and received thrombolysis more often (all p < 0.0001). Largest differences at the level of individual outcome predictors were observed for age (RCT: adjusted odds ratio (aOR), 1.29 (95% CI, 1.10–1.53) vs real-world aOR, 1.65 (95% CI, 1.54–1.78) per 10-year increments, p < 0.001). Treatment with intravenous thrombolysis was not significantly associated with functional outcome in the RCT cohort (aOR, 1.64 (95 % CI, 0.91–3.00)), but in the real-world cohort (aOR, 0.81 (95% CI, 0.69–0.96); p for cohort heterogeneity = 0.056). Outcome prediction was more accurate when constructing and testing the model using real-world data compared to construction with RCT data and testing on real-world data (area under the curve, 0.82 (95% CI, 0.79–0.85) vs 0.79 (95% CI, 0.77–0.80), p = 0.004). Conclusions: RCT and real-world cohorts considerably differ in patient characteristics, individual outcome predictor strength, and overall outcome prediction model performance. © European Stroke Organisation 2022.,Purpose: Individual regions of the Alberta Stroke Programme Early CT Score (ASPECTS) may contribute differently to the clinical symptoms in large vessel occlusion (LVO). Here, we investigated whether the predictive performance on clinical outcome can be increased by considering specific ASPECTS subregions. Methods: A consecutive series of patients with LVO affecting the middle cerebral artery territory and subsequent endovascular treatment (EVT) between January 2015 and July 2020 was analyzed, including affected ASPECTS regions. A multivariate logistic regression was performed to assess the individual impact of ASPECTS regions on good clinical outcome (defined as modified Rankin scale after 90 days of 0–2). Machine-learning-driven logistic regression models were trained (training?= 70%, testing?= 30%) to predict good clinical outcome using i) cumulative ASPECTS and ii) location-specific ASPECTS, and their performance compared using deLong’s test. Furthermore, additional analyses using binarized as well as linear clinical outcomes using regression and machine-learning techniques were applied to thoroughly assess the potential predictive properties of individual ASPECTS regions and their combinations. Results: Of 1109 patients (77.3 years?± 11.6, 43.8% male), 419 achieved a good clinical outcome and a median NIHSS after 24?h of 12 (interquartile range, IQR 4–21). Individual ASPECTS regions showed different impact on good clinical outcome in the multivariate logistic regression, with strongest effects for insula (odds ratio, OR 0.56, 95% confidence interval, CI 0.42–0.75) and M5 (OR 0.53, 95% CI 0.29–0.97) regions. Accuracy (ACC) in predicting good clinical outcome of the test set did not differ between when considering i) cumulative ASPECTS and ii) location-specific ASPECTS (ACC?= 0.619, 95% CI 0.58–0.64 vs. ACC?= 0.629, 95% CI 0.60–0.65; p?= 0.933). Conclusion: Cumulative ASPECTS assessment in LVO remains a stable and reliable predictor for clinical outcome and is not inferior to a weighted (location-specific) ASPECTS assessment. © 2023, The Author(s).,Background Quantitative and automated volumetric evaluation of early ischemic changes on non-contrast CT (NCCT) has recently been proposed as a new tool to improve prognostic performance in patients undergoing endovascular therapy (EVT) for acute ischemic stroke (AIS). We aimed to test its clinical value compared with the Alberta Stroke Program Early CT Score (ASPECTS) in a large single-institutional patient cohort. Methods A total of 1103 patients with AIS due to large vessel occlusion in the M1 or proximal M2 segments who underwent NCCT and EVT between January 2013 and November 2019 were retrospectively enrolled. Acute ischemic volumes (AIV) and ASPECTS were generated from the baseline NCCT through e-ASPECTS (Brainomix). Correlations were tested using Spearman's coefficient. The predictive capabilities of AIV for a favorable outcome (modified Rankin Scale score at 90 days ?2) were tested using multivariable logistic regression as well as machine-learning models. Performance of the models was assessed using receiver operating characteristic (ROC) curves and differences were tested using DeLong's test. Results Patients with a favorable outcome had a significantly lower AIV (median 12.0 mL (IQR 5.7-21.7) vs 18.8 mL (IQR 9.4-33.9), p<0.001). AIV was highly correlated with ASPECTS (rho=0.78, p<0.001) and weakly correlated with the National Institutes of Health Stroke Scale score at baseline (rho=0.22, p<0.001), and was an independent predictor of an unfavorable clinical outcome (adjusted OR 0.97, 95% CI 0.96 to 0.98). No significant difference was found between machine-learning models using either AIV or ASPECTS or both metrics for predicting a good clinical outcome (p>0.05). Conclusion AIV is an independent predictor of clinical outcome and presented a non-inferior performance compared with ASPECTS, without clear advantages for prognostic modelling. © Author(s) (or their employer(s)) 2023. No commercial re-use. See rights and permissions. Published by BMJ."
259,258,13,258_Cardiac Substructure Segmentation,Cardiac Substructure Segmentation,"Cardiac computed tomography angiography (CTA) is an emerging imaging modality for assessing coronary artery as well as various cardiovascular structures. Recently, deep learning (DL) methods have been successfully applied to many applications of medical image analysis including cardiac CTA structure segmentation. However, DL requires a large amounts of data and high-quality labels for training which can be burdensome to obtain due to its labor-intensive nature. In this study, we aim to develop a fully automatic artificial intelligence (AI) system, named DeepHeartCT, for accurate and rapid cardiac CTA segmentation based on DL. The proposed system was trained using a large clinical dataset with computer-generated labels to segment various cardiovascular structures including left and right ventricles (LV, RV), left and right atria (LA, RA), and LV myocardium (LVM). This new system was trained directly using high-quality computer labels generated from our previously developed multi-atlas based AI system. In addition, a reverse ranking strategy was proposed to assess the segmentation quality in the absence of manual reference labels. This strategy allowed the new framework to assemble optimal computer-generated labels from a large dataset for effective training of a deep convolutional neural network (CNN). A large clinical cardiac CTA studies (n = 1,064) were used to train and validate our framework. The trained model was then tested on another independent dataset with manual labels (n = 60). The Dice score, Hausdorff distance and mean surface distance were used to quantify the segmentation accuracy. The proposed DeepHeartCT framework yields a high median Dice score of 0.90 [interquartile range (IQR), 0.90–0.91], a low median Hausdorff distance of 7 mm (IQR, 4–15 mm) and a low mean surface distance of 0.80 mm (IQR, 0.57–1.29 mm) across all segmented structures. An additional experiment was conducted to evaluate the proposed DL-based AI framework trained with a small vs. large dataset. The results show our framework also performed well when trained on a small optimal training dataset (n = 110) with a significantly reduced training time. These results demonstrated that the proposed DeepHeartCT framework provides accurate and rapid cardiac CTA segmentation that can be readily generalized for handling large-scale medical imaging applications. Copyright © 2022 Bui, Hsu, Chang, Sun, Tran, Shanbhag, Zhou, Mehta and Chen.,Cardiac substructure segmentation is a prerequisite for cardiac diagnosis and treatment, providing a basis for accurate calculation, modeling, and analysis of the entire cardiac structure. CT (computed tomography) imaging can be used for a noninvasive qualitative and quantitative evaluation of the cardiac anatomy and function. Cardiac substructures have diverse grayscales, fuzzy boundaries, irregular shapes, and variable locations. We designed a deep learning-based framework to improve the accuracy of the automatic segmentation of cardiac substructures. This framework integrates cardiac anatomical knowledge; it uses prior knowledge of the location, shape, and scale of cardiac substructures and separately processes the structures of different scales. Through two successive segmentation steps with a coarse-to-fine cascaded network, the more easily segmented substructures were coarsely segmented first; then, the more difficult substructures were finely segmented. The coarse segmentation result was used as prior information and combined with the original image as the input for the model. Anatomical knowledge of the large-scale substructures was embedded into the fine segmentation network to guide and train the small-scale substructures, achieving efficient and accurate segmentation of ten cardiac substructures. Sixty cardiac CT images and ten substructures manually delineated by experienced radiologists were retrospectively collected; the model was evaluated using the DSC (Dice similarity coefficient), Recall, Precision, and the Hausdorff distance. Compared with current mainstream segmentation models, our approach demonstrated significantly higher segmentation accuracy, with accurate segmentation of ten substructures of different shapes and sizes, indicating that the segmentation framework fused with prior anatomical knowledge has superior segmentation performance and can better segment small targets in multi-target segmentation tasks. © 2023 by the authors.,Background and objective: The sheer volume of data generated by population imaging studies is unparalleled by current capabilities to extract objective and quantitative cardiac phenotypes; subjective and time-consuming manual image analysis remains the gold standard. Automated image analytics to compute quantitative imaging biomarkers of cardiac function are desperately needed. Data volumes and their variability pose a challenge to most state-of-the-art methods for endo- and epicardial contours, which lack robustness when applied to very large datasets. Our aim is to develop an analysis pipeline for the automatic quantification of cardiac function from cine magnetic resonance imaging data. Method: This work adopt 4,638 cardiac MRI cases coming from UK Biobank with ground truth available for left and RV contours. A hybrid and robust algorithm is proposed to improve the accuracy of automatic left and right ventricle segmentation by harnessing the localization accuracy of deep learning and the morphological accuracy of 3D-ASM (three-dimensional active shape models). The contributions of this paper are three-fold. First, a fully automatic method is proposed for left and right ventricle initialization and cardiac MRI segmentation by taking full advantage of spatiotemporal constraint. Second, a deeply supervised network is introduced to train and segment the heart. Third, the 3D-ASM image search procedure is improved by combining image intensity models with convolutional neural network (CNN) derived distance maps improving endo- and epicardial edge localization. Results: The proposed architecture outperformed the state of the art for cardiac MRI segmentation from UK Biobank. The statistics of RV landmarks detection errors for Triscuspid valve and RV apex are 4.17 mm and 5.58 mm separately. The overlap metric, mean contour distance, Hausdorff distance and cardiac functional parameters are calculated for the LV (Left Ventricle) and RV (Right Ventricle) contour segmentation. Bland–Altman analysis for clinical parameters shows that the results from our automated image analysis pipelines are in good agreement with results from expert manual analysis. Conclusions: Our hybrid scheme combines deep learning and statistical shape modeling for automatic segmentation of the LV/RV from cardiac MRI datasets is effective and robust and can compute cardiac functional indexes from population imaging. © 2023 Elsevier B.V."
260,259,12,259_Efficient Transfer Learning for Medical Visual Question Answering (Med-VQA),Efficient Transfer Learning for Medical Visual Question Answering (Med-VQA),"Objective: In the last two decades, there has been a growing interest in exploring surgical procedures with statistical models to analyze operations at different semantic levels. This information is necessary for developing context-aware intelligent systems, which can assist the physicians during operations, evaluate procedures afterward or help the management team to effectively utilize the operating room. The objective is to extract reliable patterns from surgical data for the robust estimation of surgical activities performed during operations. The purpose of this article is to review the state-of-the-art deep learning methods that have been published after 2018 for analyzing surgical workflows, with a focus on phase and step recognition. Methods: Three databases, IEEE Xplore, Scopus, and PubMed were searched, and additional studies are added through a manual search. After the database search, 343 studies were screened and a total of 44 studies are selected for this review. Conclusion: The use of temporal information is essential for identifying the next surgical action. Contemporary methods used mainly RNNs, hierarchical CNNs, and Transformers to preserve long-distance temporal relations. The lack of large publicly available datasets for various procedures is a great challenge for the development of new and robust models. As supervised learning strategies are used to show proof-of-concept, self-supervised, semi-supervised, or active learning methods are used to mitigate dependency on annotated data. Significance: The present study provides a comprehensive review of recent methods in surgical workflow analysis, summarizes commonly used architectures, datasets, and discusses challenges.  © 2013 IEEE.,Background: Surgical phase recognition using computer vision presents an essential requirement for artificial intelligence-assisted analysis of surgical workflow. Its performance is heavily dependent on large amounts of annotated video data, which remain a limited resource, especially concerning highly specialized procedures. Knowledge transfer from common to more complex procedures can promote data efficiency. Phase recognition models trained on large, readily available datasets may be extrapolated and transferred to smaller datasets of different procedures to improve generalizability. The conditions under which transfer learning is appropriate and feasible remain to be established. Methods: We defined ten operative phases for the laparoscopic part of Ivor-Lewis Esophagectomy through expert consensus. A dataset of 40 videos was annotated accordingly. The knowledge transfer capability of an established model architecture for phase recognition (CNN + LSTM) was adapted to generate a “Transferal Esophagectomy Network” (TEsoNet) for co-training and transfer learning from laparoscopic Sleeve Gastrectomy to the laparoscopic part of Ivor-Lewis Esophagectomy, exploring different training set compositions and training weights. Results: The explored model architecture is capable of accurate phase detection in complex procedures, such as Esophagectomy, even with low quantities of training data. Knowledge transfer between two upper gastrointestinal procedures is feasible and achieves reasonable accuracy with respect to operative phases with high procedural overlap. Conclusion: Robust phase recognition models can achieve reasonable yet phase-specific accuracy through transfer learning and co-training between two related procedures, even when exposed to small amounts of training data of the target procedure. Further exploration is required to determine appropriate data amounts, key characteristics of the training procedure and temporal annotation methods required for successful transferal phase recognition. Transfer learning across different procedures addressing small datasets may increase data efficiency. Finally, to enable the surgical application of AI for intraoperative risk mitigation, coverage of rare, specialized procedures needs to be explored. Graphical abstract: [Figure not available: see fulltext.]. © 2023, The Author(s).,Purpose:: Automatic surgical instruction generation is a crucial part for intra-operative surgical assistance. However, understanding and translating surgical activities into human-like sentences are particularly challenging due to the complexity of surgical environment and the modal gap between images and natural languages. To this end, we introduce SIG-Former, a transformer-backboned generation network to predict surgical instructions from monocular RGB images. Methods:: Taking a surgical image as input, we first extract its visual attentive feature map with a fine-tuned ResNet-101 model, followed by transformer attention blocks to correspondingly model its visual representation, text embedding and visual–textual relational feature. To tackle the loss-metric inconsistency between training and inference in sequence generation, we additionally apply a self-critical reinforcement learning approach to directly optimize the CIDEr score after regular training. Results:: We validate our proposed method on DAISI dataset, which contains 290 clinical procedures from diverse medical subjects. Extensive experiments demonstrate that our method outperforms the baselines and achieves promising performance on both quantitative and qualitative evaluations. Conclusion:: Our experiments demonstrate that SIG-Former is capable of mapping dependencies between visual feature and textual information. Besides, surgical instruction generation is still at its preliminary stage. Future works include collecting large clinical dataset, annotating more reference instructions and preparing pre-trained models on medical images. © 2022, The Author(s)."
261,260,12,260_Plasma Disruption Prediction and Analysis in Tokamaks,Plasma Disruption Prediction and Analysis in Tokamaks,"Disruption prediction has made rapid progress in recent years, especially in machine learning (ML)-based methods. If a disruption prediction model can be interpreted, it can tell why certain samples are classified as disruption precursors. This allows us to tell the types of incoming disruption for disruption avoidance and gives us insight into the mechanism of disruption. This paper presents a disruption predictor called interpretable disruption predictor based on physics-guided feature extraction (IDP-PGFE) and its results on J-TEXT experiment data. The prediction performance of IDP-PGFE with physics-guided features is effectively improved (true positive rate = 97.27%, false positive rate = 5.45%, area under the ROC curve = 0.98) compared to the models with raw signal input. The validity of the interpretation results is ensured by the high performance of the model. The interpretability study using an attribution technique provides an understanding of J-TEXT disruption and conforms to our prior comprehension of disruption. Furthermore, IDP-PGFE gives a possible mean on inferring the underlying cause of the disruption and how interventions affect the disruption process in J-TEXT. The interpretation results and the experimental phenomenon have a high degree of conformity. The interpretation results also gives a possible experimental analysis direction that the resonant magnetic perturbations delays the density limit disruption by affecting both the MHD instabilities and the radiation profile. PGFE could also reduce the data requirement of IDP-PGFE to 10% of the training data required to train a model on raw signals. This made it possible to be transferred to the next-generation tokamaks, which cannot provide large amounts of data. Therefore, IDP-PGFE is an effective approach to exploring disruption mechanisms and transferring disruption prediction models to future tokamaks. © 2023 The Author(s). Published on behalf of IAEA by IOP Publishing Ltd.,Predicting disruptions across different tokamaks is necessary for next generation device. Future large-scale tokamaks can hardly tolerate disruptions at high performance discharge, which makes it difficult for current data-driven methods to obtain an acceptable result. A machine learning method capable of transferring a disruption prediction model trained on one tokamak to another is required to solve the problem. The key is a feature extractor which is able to extract common disruption precursor traces in tokamak diagnostic data, and can be easily transferred to other tokamaks. Based on the concerns above, this paper presents a deep feature extractor, namely, the fusion feature extractor (FFE), which is designed specifically for extracting disruption precursor features from common diagnostics on tokamaks. Furthermore, an FFE-based disruption predictor on J-TEXT is demonstrated. The feature extractor is aimed to extracting disruption-related precursors and is designed according to the precursors of disruption and their representations in common tokamak diagnostics. Strong inductive bias on tokamak diagnostics data is introduced. The paper presents the evolution of the neural network feature extractor and its comparison against general deep neural networks, as well as a physics-based feature extraction with a traditional machine learning method. Results demonstrate that the FFE may reach a similar effect with physics-guided manual feature extraction, and obtain a better result compared with other deep learning methods. © 2023 Chinese Physical Society and IOP Publishing Ltd.,The ability to identify underlying disruption precursors is key to disruption avoidance. In this paper, we present an integrated deep learning (DL) based model that combines disruption prediction with the identification of several disruption precursors like rotating modes, locked modes, H-to-L back transitions and radiative collapses. The first part of our study demonstrates that the DL-based unstable event identifier trained on 160 manually labeled DIII-D shots can achieve, on average, 84% event identification rate of various frequent unstable events (like H-L back transition, locked mode, radiative collapse, rotating MHD mode, large sawtooth crash), and the trained identifier can be adapted to label unseen discharges, thus expanding the original manually labeled database. Based on these results, the integrated DL-based framework is developed using a combined database of manually labeled and automatically labeled DIII-D data, and it shows state-of-the-art (AUC = 0.940) disruption prediction and event identification abilities on DIII-D. Through cross-machine numerical disruption prediction studies using this new integrated model and leveraging the C-Mod, DIII-D, and EAST disruption warning databases, we demonstrate the improved cross-machine disruption prediction ability and extended warning time of the new model compared with a baseline predictor. In addition, the trained integrated model shows qualitatively good cross-machine event identification ability. Given a labeled dataset, the strategy presented in this paper, i.e. one that combines a disruption predictor with an event identifier module, can be applied to upgrade any neural network based disruption predictor. The results presented here inform possible development strategies of machine learning based disruption avoidance algorithms for future tokamaks and highlight the importance of building comprehensive databases with unstable event information on current machines. © 2023 The Author(s). Published on behalf of IAEA by IOP Publishing Ltd."
262,261,12,"261_Cardiac Imaging Techniques: CT Angiography, SPECT, and OCT with Deep Learning for Atherosclerosis and Ischemic Stroke Diagnosis","Cardiac Imaging Techniques: CT Angiography, SPECT, and OCT with Deep Learning for Atherosclerosis and Ischemic Stroke Diagnosis","(1) Background: The CT-based attenuation correction of SPECT images is essential for obtaining accurate quantitative images in cardiovascular imaging. However, there are still many SPECT cameras without associated CT scanners throughout the world, especially in developing countries. Performing additional CT scans implies troublesome planning logistics and larger radiation doses for patients, making it a suboptimal solution. Deep learning (DL) offers a revolutionary way to generate complementary images for individual patients at a large scale. Hence, we aimed to generate linear attenuation coefficient maps from SPECT emission images reconstructed without attenuation correction using deep learning. (2) Methods: A total of 384 SPECT myocardial perfusion studies that used 99mTc-sestamibi were included. A DL model based on a 2D U-Net architecture was trained using information from 312 patients. The quality of the generated synthetic attenuation correction maps (ACMs) and reconstructed emission values were evaluated using three metrics and compared to standard-of-care data using Bland–Altman plots. Finally, a quantitative evaluation of myocardial uptake was performed, followed by a semi-quantitative evaluation of myocardial perfusion. (3) Results: In a test set of 66 test patients, the ACM quality metrics were MSSIM = 0.97 ± 0.001 and NMAE = 3.08 ± 1.26 (%), and the reconstructed emission quality metrics were MSSIM = 0.99 ± 0.003 and NMAE = 0.23 ± 0.13 (%). The 95% limits of agreement (LoAs) at the voxel level for reconstructed SPECT images were: [?9.04; 9.00]%, and for the segment level, they were [?11; 10]%. The 95% LoAs for the Summed Stress Score values between the images reconstructed were [?2.8, 3.0]. When global perfusion scores were assessed, only 2 out of 66 patients showed changes in perfusion categories. (4) Conclusion: Deep learning can generate accurate attenuation correction maps from non-attenuation-corrected cardiac SPECT images. These high-quality attenuation maps are suitable for attenuation correction in myocardial perfusion SPECT imaging and could obviate the need for additional imaging in standalone SPECT scanners. © 2023 by the authors.,Background: Plaque analysis with coronary computed tomography angiography (CCTA) is a promising tool to identify high risk of future coronary events. The analysis process is time-consuming, and requires highly trained readers. Deep learning models have proved to excel at similar tasks, however, training these models requires large sets of expert-annotated training data. The aims of this study were to generate a large, high-quality annotated CCTA dataset derived from Swedish CArdioPulmonary BioImage Study (SCAPIS), report the reproducibility of the annotation core lab and describe the plaque characteristics and their association with established risk factors. Methods and results: The coronary artery tree was manually segmented using semi-automatic software by four primary and one senior secondary reader. A randomly selected sample of 469 subjects, all with coronary plaques and stratified for cardiovascular risk using the Systematic Coronary Risk Evaluation (SCORE), were analyzed. The reproducibility study (n = 78) showed an agreement for plaque detection of 0.91 (0.84–0.97). The mean percentage difference for plaque volumes was ?0.6% the mean absolute percentage difference 19.4% (CV 13.7%, ICC 0.94). There was a positive correlation between SCORE and total plaque volume (rho = 0.30, p < 0.001) and total low attenuation plaque volume (rho = 0.29, p < 0.001). Conclusions: We have generated a CCTA dataset with high-quality plaque annotations showing good reproducibility and an expected correlation between plaque features and cardiovascular risk. The stratified data sampling has enriched high-risk plaques making the data well suited as training, validation and test data for a fully automatic analysis tool based on deep learning. © 2023 The Authors,Purpose: Although SPECT myocardial perfusion imaging (MPI) is susceptible to artifacts from soft tissue attenuation, most scans are performed without attenuation correction. Deep learning-based attenuation corrected (DLAC) polar maps improved diagnostic accuracy for detection of coronary artery disease (CAD) beyond non-attenuation-corrected (NAC) polar maps in a large single center study. However, the generalizability of this approach to other institutions with different scanner models and protocols is uncertain. In this study, we evaluated the diagnostic performance of DLAC compared to NAC for detection of CAD as defined by invasive coronary angiography (ICA) in a large multi-center trial. Methods: During the phase 3 flurpiridaz multi-center diagnostic clinical trial, conducted over 74 international sites, patients with known or suspected CAD who were referred for a clinically indicated ICA were enrolled. Using receiver operating characteristic (ROC) analysis, we evaluated the detectability of obstructive CAD, defined by quantitative coronary angiography by a core laboratory, using total perfusion deficit (TPD) as an integrated measure of defect extent and severity on DLAC polar maps compared to NAC polar maps. This was also compared against the visual scoring of three expert core lab readers. Results: Out of 755 patients, 722 (69% male) had evaluable SPECT and ICA for this study. ROC analysis demonstrated significant improvement in detecting per-patient obstructive CAD with DLAC over NAC with area under the curve (AUC) of 0.752 (95% CI: 0.711–0.792) for DLAC compared to 0.717 (0.675–0.759) for NAC (p value = 0.016). Compared to the consensus of expert readers AUC = 0.743 (0.701–0.784), DLAC was comparable (p value = 0.913), whereas NAC underperformed (p value = 0.051). Conclusion: DL-based attenuation correction improves diagnostic performance of SPECT MPI for detecting CAD in data from a large multi-center clinical trial regardless of SPECT camera model or protocol. Trial registration: A Phase 3 Multi-center Study to Assess PET Imaging of Flurpiridaz F 18 Injection in Patients With CAD, ClinicalTrials.gov Identifier: NCT01347710, registered on 4 May 2011. https://clinicaltrials.gov/ct2/show/study/NCT01347710 © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
263,262,12,262_Time series forecasting using neural network methods and evaluation of informatization system,Time series forecasting using neural network methods and evaluation of informatization system,"The key to the accuracy of time series forecasting is to find the most appropriate forecasting method. Therefore, the forecasting model selection of time series has become a new research hotspot in the data analysis field. However, most of the existing meta learning forecasting model selection methods rely on manual selection of features, which leads to low efficiency and lack of objectivity. Therefore, this paper proposes an improved meta learning framework for deep learning time series forecasting model selection. Inspired by computer vision, we transform one -dimensional time series into two-dimensional images, and use convolution neural network to train and classify time series images (model selection). Moreover, in order to deal with the over fitting problem caused by small sample datasets, the sliding window data augmentation method is used to improve the accuracy of small datasets model selection. The large-scale empirical study on M3 data sets shows that the framework has better model selection accuracy and smaller forecasting error than the recurrent neural network (RNN), back propagation neural network and traditional time series image algorithms. In addition, compared with the traditional time series image method, RNN and BP, the classification rate (model selection accuracy) of this algorithm is improved by 6.5%, 4.4% and 3.2%, respectively. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Long sequence time-series forecasting (LSTF) problems, such as weather forecasting, stock market forecasting, and power resource management, are widespread in the real world. The LSTF problem requires a model with high prediction accuracy. Recent studies have shown that the transformer model architecture is the most promising model structure for LSTF problems compared with other model architectures. The transformer model has the property of permutation equivalence, which leads to the importance of sequence position encoding, an essential process in model training. Currently, the continuous dynamics models constructed for position encoding using the neural differential equations (neural ODEs) method can model sequence position information well. However, we have found that there are some limitations when neural ODEs are applied to the LSTF problem, including the time cost problem, the baseline drift problem, and the information loss problem; thus, neural ODEs cannot be directly applied to the LSTF problem. To address this problem, we design a binary position encoding-based regularization model for long sequence time-series prediction, named Seformer, which has the following structure: 1) The binary position encoding mechanism, including intrablock and interblock position encoding. For intrablock position encoding, we design a simple ODE method by discretizing the continuum dynamics model, which reduces the time cost required to compute neural ODEs while maintaining their dynamics properties to the maximum extent. In interblock position encoding, a chunked recursive form is adopted to alleviate the baseline drift problem caused by eigenvalue explosion. 2) Information transfer regularization mechanism: By regularizing the model intermediate hidden variables as well as the encoder-decoder connection variables, we can reduce information loss during the model training process while ensuring the smoothness of the position information. Extensive experimental results obtained on six large-scale datasets show a consistent improvement in our approach over the baselines. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,Many real-world applications show growing demand for the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) requires a higher prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to accommodate the capacity requirements. However, three real challenges that may have prevented expanding the prediction capacity in LSTF are that the Transformer is limited by quadratic time complexity, high memory usage, and slow inference speed under the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics. (i) a ProbSparse self-attention mechanism, which achieves O(Llog?L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling promotes dominating attention by convolutional operators. Besides, the halving of layer width is intended to reduce the expense of building a deeper network on extremely long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on ten large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem. © 2023"
264,263,12,263_Wind Turbine Wake Prediction and Optimization,Wind Turbine Wake Prediction and Optimization,"As wind energy continues to be a crucial part of sustainable power generation, the need for precise and efficient modeling of wind turbines, especially under yawed conditions, becomes increasingly significant. Addressing this, the current study introduces a machine learning-based symbolic regression approach for elucidating wake dynamics. Utilizing WindSE’s actuator line method (ALM) and Large Eddy Simulation (LES), we model an NREL 5-MW wind turbine under yaw conditions ranging from no yaw to 40 degrees. Leveraging a hold-out validation strategy, the model achieves robust hyper-parameter optimization, resulting in high predictive accuracy. While the model demonstrates remarkable precision in predicting wake deflection and velocity deficit at both the wake center and hub height, it shows a slight deviation at low downstream distances, which is less critical to our focus on large wind farm design. Nonetheless, our approach sets the stage for advancements in academic research and practical applications in the wind energy sector by providing an accurate and computationally efficient tool for wind farm optimization. This study establishes a new standard, filling a significant gap in the literature on the application of machine learning-based wake models for wind turbine yaw wake prediction. © 2023 by the authors.,Complete and clear global wind turbine wake data is very important for the study of wind turbine wake characteristics in increasingly large offshore wind farms. Existing wake measurement techniques can only obtain local high-resolution (HR) wake flow field, or sacrifice accuracy to obtain larger measurement area, which is insufficient for accurate modeling of wake effect. To overcome this challenge, this paper proposes a novel super-resolution (SR) reconstruction approach that can reconstruct the global HR wake flow field from low-resolution (LR) wake flow field measurement data effectively. The proposed approach utilizes a deep learning framework called down-sampled skip-connection and multi-scale network. The performance of the SR approach is evaluated by enhancing the resolution of the wake flow field at different scale factors, and its potential application is demonstrated by assessing the prediction accuracy of three typical wake models. The results indicate that the resolution of the global wind turbine wake can be improved by 16 times using the SR model, and the reconstructed global SR wake flow fields are consistent with the ground truth in terms of both the spatial distribution and the temporal variation. By comparing the prediction results of three different wake models with the LR or SR wake data, it is shown that the SR flow reconstruction method can be applied to more accurately evaluate the wake model prediction performance, which has the potential to improve wake models. Overall, this study presents an innovative solution to the problem of incomplete and inaccurate wake flow measurement in the wind energy industry, which could reduce the workload of experimental measurements and the cost burden of accurate measuring equipment for engineering applications. © 2023 Elsevier Ltd,Wind turbine wake prediction considering yawed condition is of great importance for wind farm applications. Tremendous efforts have been made on yawed wind turbine wake prediction by using analytical wake models and numerical simulations, while their applications are still limited due to a lack of balanced consideration on both efficiency and precision. In this work, a deep learning-based wake prediction model is developed by integrating the transformer module into the conditional generative adversarial network. The developed model takes the inflow velocity and turbulence field as the input to predict the three-dimensional wake flow under different yawed conditions. The data generation approach by both analytical and numerical ways is used to build a large wake database. Subsequently, a pretraining-finetuning strategy is adopted to improve the model training efficiency and enhance the prediction performance. The validation results show that the proposed model can achieve a good agreement with numerical simulations at different streamwise distances under various inflow conditions, with the mean absolute relative error of 5.0 % and 7.27 % for wake velocity and turbulence intensity, respectively. The model parameters are also investigated to illustrate the wake prediction improvement by transformer-mixed modelling method. The wake prediction performance of the proposed model is validated by a comparison with analytical wake model and other popular machine learning-based methods. Moreover, the power calculation of multiple wind turbines is conducted to demonstrate the easy implementation and good performance in wind farm applications. © 2024 Elsevier Ltd"
265,264,12,264_Image inpainting methods with progressive hole-filling and advanced feature fusion for realistic content completion,Image inpainting methods with progressive hole-filling and advanced feature fusion for realistic content completion,"With the development of image generation and processing techniques, image inpainting techniques based on deep learning have achieved impressive results. Especially the emphasis on global context during inpainting enables the network to generate reasonably coarse inpainting results at low resolutions. However, how to better achieve high-quality texture filling at high resolutions is still a challenging problem. To address this problem, most methods design two-stage networks to achieve structure and texture restoration separately. But in the face of large-scale masks, the generated textures still suffer from blurring and artifacts. Therefore, in order to achieve inpainting of images with large-scale masks and generate fine textures, this paper proposes an end-to-end generative adversarial model for large mask inpainting, called Panoramic Feature Aggregation Network (PFAN). First, this paper designs a Euclidean Attention Mechanism (EAM) which exploits encoder features to generate low-resolution structure restoration. Then a Feature Aggregation Synthesis Block (FASB) is proposed in the decoder to achieve high-resolution complex texture filling. With the global receptive fields of these two modules, texture filling results with satisfactory performance even under large-scale masks. Experiments on CelebA-HQ, Paris Street View and FFHQ datasets show that the proposed method has superior performance. © 2024 Elsevier B.V.,Image inpainting is the process to fill missing pixels in the damaged image and this process has drawn more attraction and gained active and expensive research topic in recent decades, because the high quality in the image inpainting benefits a greater range of applications, like object removal, photo restoration, and so on. Inpainting of larger quality of the image needs to fill the empty regions with plausible content in the damaged image. The existing inpainting methods either fill image regions by stealing the image patches or semantically create coherent patches from the regional context. Most of the traditional models perform well on small holes images, but restoring the image with large holes still results a challenging task. To overcome such issues and to generate effective inpainting results, a proposed method named the hybrid context deep learning approach is designed in order to fill empty regions of crack images. Moreover, the proposed method is more effective by employing a hybrid optimization algorithm for training of classifier to generate a more robust and accurate inpainted result. The developed model includes two different deep learning classifiers to accomplish the process of image inpainting in such a way that the results are fused through the probabilistic model. Moreover, the proposed approach attains higher performance by the metrics such as Peak signal-to-noise ratio (PSNR), Structural Similarity Index (SSIM), Second Derivative like Measure of Enhancement (SDME), and Universal Quality Index (UQI) with the values of 38.02 db, 0.867, 54.32 db, and 0.864, respectively. © 2023 World Scientific Publishing Company.,As the only underground mural in the collection, the tomb murals are subject to damage due to temperature, humidity, and foundation settlement changes. Traditional mural inpainting takes a long time and requires experts to draw it manually. Therefore, the need for digital inpainting is increasing to save time and costs. Due to the scarcity of samples and the variety of damage, the image features are scattered and partially sparse, and the colors are less vivid than in other images. Traditional deep learning inpainting causes information loss and generates irrational structures. The generative adversarial network is, recently, a more effective method. Therefore, this paper presents an inpainting model based on dual-attention multiscale feature aggregation and an improved generator. Firstly, an improved residual prior and attention mechanism is added to the generator module to preserve the image structure. Secondly, the model combines spatial and channel attention with multiscale feature aggregation to change the mapping network structure and improve the inpainting accuracy. Finally, the segmental loss function and its training method are improved.The experimental results show that the results of using signal-to-noise ratio (PSNR), structural similarity (SSIM), and mean square error (MSE) on epitaxial mask, crack mask, random small mask, and random large mask are better than other methods. It demonstrates the performance of this paper in inpainting different diseases of murals. It can be used as a reference for experts in manual inpainting, saving the cost and time of manual inpainting. © 2023 by the authors."
266,265,12,265_Support Vector Machines for Multi-task Learning,Support Vector Machines for Multi-task Learning,"In recent years, various studies have been conducted on SVMs and their applications in different area. They have been developed significantly in many areas. SVM is one of the most robust classification and regression algorithms that plays a significant role in pattern recognition. However, SVM has not been developed significantly in some areas like large-scale datasets, unbalanced datasets, and multiclass classification. Efficient SVM training in large-scale datasets is of great importance in the big data era. However, as the number of samples increases, the time and memory required to train SVM increase, making SVM impractical even for a medium-sized problem. With the emergence of big data, this problem becomes more significant. This paper presents a novel distributed method for SVM training in which a very small subset of training samples is used for classification, which reduces the problem size and thus the required memory and computational resources. The solution of this problem almost converges to standard SVM. This method includes three steps: first, detecting a subset of distributed training samples, second, creating local models of SVM and obtaining partial vectors, and finally combining the partial vectors and obtaining the global vector and the final model. In addition, the datasets which suffer from unbalanced number of samples and tend to the majority class, the proposed method balances the samples of the two classes and it can be used in unbalanced datasets. The empirical results show that using this method is efficient for large-scale problems. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,This study introduces the global-local least-squares support vector machine (GLocal-LS-SVM), a novel machine learning algorithm that combines the strengths of localised and global learning. GLocal-LS-SVM addresses the challenges associated with decentralised data sources, large datasets, and input-space-related issues. The algorithm is a double-layer learning approach that employs multiple local LS-SVM models in the first layer and one global LS-SVM model in the second layer. The key idea behind GLocal-LS-SVM is to extract the most informative data points, known as support vectors, from each local region in the input space. Local LS-SVM models are developed for each region to identify the most contributing data points with the highest support values. The local support vectors are then merged at the final layer to form a reduced training set used to train the global model. We evaluated the performance of GLocal-LS-SVM using both synthetic and real-world datasets. Our results demonstrate that GLocal-LS-SVM achieves comparable or superior classification performance compared to standard LS-SVM and state-of-the-art models. In addition, our experiments show that GLocal-LS-SVM outperforms standard LS-SVM in terms of computational efficiency. For instance, on a training dataset of 9, 000 instances, the average training time for GLocal-LS-SVM was only 2% of the time required to train the LS-SVM model while maintaining classification performance. In summary, the GLocal-LS-SVM algorithm offers a promising solution to address the challenges associated with decentralised data sources and large datasets while maintaining high classification performance. Furthermore, its computational efficiency makes it a valuable tool for practical applications in various domains. Copyright: © 2023 Ahmed Youssef Ali Amer. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.,This study introduces the global-local least-squares support vector machine (GLocal-LS-SVM), a novel machine learning algorithm that combines the strengths of localised and global learning. GLocal-LS-SVM addresses the challenges associated with decentralised data sources, large datasets, and input-space-related issues. The algorithm is a double-layer learning approach that employs multiple local LS-SVM models in the first layer and one global LS-SVM model in the second layer. The key idea behind GLocal-LS-SVM is to extract the most informative data points, known as support vectors, from each local region in the input space. Local LS-SVM models are developed for each region to identify the most contributing data points with the highest support values. The local support vectors are then merged at the final layer to form a reduced training set used to train the global model. We evaluated the performance of GLocal-LS-SVM using both synthetic and real-world datasets. Our results demonstrate that GLocal-LS-SVM achieves comparable or superior classification performance compared to standard LS-SVM and state-of-the-art models. In addition, our experiments show that GLocal-LS-SVM outperforms standard LS-SVM in terms of computational efficiency. For instance, on a training dataset of 9, 000 instances, the average training time for GLocal-LS-SVM was only 2% of the time required to train the LS-SVM model while maintaining classification performance. In summary, the GLocal-LS-SVM algorithm offers a promising solution to address the challenges associated with decentralised data sources and large datasets while maintaining high classification performance. Furthermore, its computational efficiency makes it a valuable tool for practical applications in various domains. © 2023 Ahmed Youssef Ali Amer. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
267,266,12,266_Oil spill detection using SAR and deep learning,Oil spill detection using SAR and deep learning,"Oil spills are the main threats to marine and coastal environments. Due to the increase in the marine transportation and shipping industry, oil spills have increased in recent years. Moreover, the rapid spread of oil spills in open waters seriously affects the fragile marine ecosystem and creates environmental concerns. Effective monitoring, quick identification, and estimation of the volume of oil spills are the first and most crucial steps for a successful cleanup operation and crisis management. Remote Sensing observations, especially from Synthetic Aperture Radar (SAR) sensors, are a very suitable choice for this purpose due to their ability to collect data regardless of the weather and illumination conditions and over far and large areas of the Earth. Owing to the relatively complex nature of SAR observations, machine learning (ML) based algorithms play an important role in accurately detecting and monitoring oil spills and can significantly help experts in faster and more accurate detection. This paper uses SAR images from ESA's Copernicus Sentinel-1 satellite to detect and locate oil spills in open waters under different environmental conditions. To this end, a deep learning framework has been presented to identify oil spills automatically. The SAR images were segmented into two classes, the oil slick and the background, using convolutional neural networks (CNN) and vision transformers (ViT). Various scenarios for the proposed architecture were designed by placing ViT networks in different parts of the CNN backbone. An extensive dataset of oil spill events in various regions across the globe was used to train and assess the performance of the proposed framework. After the detection performance assessments, the F1-score values for the standard DeepLabV3+, FC-DenseNet, and U-Net networks were 75.08 %, 73.94 %, and 60.85, respectively. In the combined networks models (combination of CNN and ViT), the best F1-score results were obtained as 78.48 %. Our results showed that these hybrid models could improve detection accuracy and have a high ability to distinguish oil spill borders even in noisy images. Evaluation metrics are increased in all the combined networks compared to the original CNN networks. © 2023 Elsevier Ltd,Oil spill accidents are one of the major problems causing marine pollution, and thus such accidents require rapid detection for early response. In recent years, deep learning algorithms for oil spill detection have been developed for analyzing SAR images. Nevertheless, to generation of deep learning training data using visual inspection is not only a time-consuming and labor-intensive, but also cause bias and lack of diversity in oil slick training data. This has easily led to false positives in challenging SAR images, such as large scale look-alikes, biologic surface films or oil slicks surrounded by look-alikes. In order to accommodate a broader range of SAR oil spill scenes, including those under challenging conditions, it is essential to continuously enhance the performance of oil spill detection algorithms. In this study, a novel self-evolving algorithm for automatic oil spill detection is proposed, which consists of three inter-connected modules: 1) oil spill detection, 2) generation of new training data, and 3) enhancement of deep learning models. The algorithm detected oil slicks automatically while the new SAR image was added, and then the additional high-quality training data was generated by an adaptive thresholding method to increase the performance of deep learning models. In order to detect oil slicks from whole SAR image with oil look-alikes, the variation of the backscattering coefficients near the oil boundary was considered as a key parameter to distinguish oil spills from look-alikes. As a result, after 21 self-evolving training cycles, 27 new training data were generated with an improved F1-score rising from 0.8423 to 0.8896. Despite the existence of look-alikes in various magnitudes, the algorithm successfully identified oil slicks, and the parameters of deep learning models were automatically updated. It was demonstrated that the performance of the algorithm was gradually improved by self-evolving without any human intervention. Additionally, the fundamental limitations of oil spill detection algorithms were mitigated by the proposed approaches, providing more possibilities for subsequent studies. © 2023,Marine oil spills can cause severe damage to the marine environment and biological resources. Using satellite remote sensing technology is one of the best ways to monitor the sea surface in near real-time to obtain oil spill information. The existing methods in the literature either use deep convolutional neural networks in synthetic aperture radar (SAR) images to directly identify oil spills or use traditional methods based on artificial features sequentially to distinguish oil spills from sea surface. However, both approaches currently only use image information and ignore some valuable auxiliary information, such as marine weather conditions, distances from oil spill candidates to oil spill sources, etc. In this study, we proposed a novel method to help detect marine oil spills by constructing a multi-source knowledge graph, which was the first one specifically designed for oil spill detection in the remote sensing field. Our method can rationally organize and utilize various oil spill-related information obtained from multiple data sources, such as remote sensing images, vectors, texts, and atmosphere-ocean model data, which can be stored in a graph database for user-friendly query and management. In order to identify oil spills more effectively, we also proposed 13 new dark spot features and then used a feature selection technique to create a feature subset that was favorable to oil spill detection. Furthermore, we proposed a knowledge graph-based oil spill reasoning method that combines rule inference and graph neural network technology, which pre-inferred and eliminated most non-oil spills using statistical rules to alleviate the problem of imbalanced data categories (oil slick and non-oil slick). Entity recognition is ultimately performed on the remaining oil spill candidates using a graph neural network algorithm. To verify the effectiveness of our knowledge graph approach, we collected 35 large SAR images to construct a new dataset, for which the training set contained 110 oil slicks and 66264 non-oil slicks from 18 SAR images, the validation set contained 35 oil slicks and 69005 non-oil slicks from 10 SAR images, and the testing set contained 36 oil slicks and 36281 non-oil slicks from the remaining 7 SAR images. The results showed that some traditional oil spill detection methods and deep learning models failed when the dataset suffered a severe imbalance, while our proposed method identified oil spills with a sensitivity of 0.8428, specificity of 0.9985, and precision of 0.2781 under those same conditions. The knowledge graph method we proposed using multi-source data can not only help solve the problem of information island in oil spill detection, but serve as a guide for construction of remote sensing knowledge graphs in many other applications as well. The dataset gathered has been made freely available online (https://pan.baidu.com/s/1DDaqIljhjSMEUHyaATDIYA?pwd=qmt6). © 2022 The Author(s)"
268,267,12,267_Water treatment process optimization using deep learning and soft sensors,Water treatment process optimization using deep learning and soft sensors,"Developing advanced onsite wastewater treatment systems (OWTS) requires accurate and consistent water quality monitoring to evaluate treatment efficiency and ensure regulatory compliance. However, off-line parameters such as chemical oxygen demand (COD), total suspended solids (TSS), and Escherichia coli (E. coli) require sample collection and time-consuming laboratory analyses that do not provide real-time information of system performance or component failure. While real-time COD analyzers have emerged in recent years, they are not economically viable for onsite systems due to cost and chemical consumables. This study aimed to design and implement a real-time remote monitoring system for OWTS by developing several multi-input and single-output soft sensors. The soft sensor integrates data that can be obtained from well-established in-line sensors to accurately predict key water quality parameters, including COD, TSS, and E. coli concentrations. The temporal and spatial water quality data of an existing field-tested OWTS operated for almost two years (n = 56 data points) were used to evaluate the prediction performance of four machine learning algorithms. These algorithms, namely, partial least square regression (PLS), support vector regression (SVR), cubist regression (CUB), and quantile regression neural network (QRNN), were chosen as candidate algorithms for their prior application and effectiveness in wastewater treatment predictions. Water quality parameters that can be measured in-line, including turbidity, color, pH, NH4+, NO3-, and electrical conductivity, were selected as model inputs for predicting COD, TSS, and E. coli. The results revealed that the trained SVR model provided a statistically significant prediction for COD with a mean absolute percentage error (MAPE) of 14.5% and R2 of 0.96. The CUB model provided the optimal predictive performance for TSS, with a MAPE of 24.8% and R2 of 0.99. None of the models were able to achieve optimal prediction results for E. coli; however, the CUB model performed the best with a MAPE of 71.4% and R2 of 0.22. Given the large fluctuation in the concentrations of COD, TSS, and E. coli within the OWTS wastewater dataset, the proposed soft sensor models adequately predicted COD and TSS, while E. coli prediction was comparatively less accurate and requires further improvement. These results indicate that although water quality datasets for the OWTS are relatively small, machine learning-based soft sensors can provide useful predictive estimates of off-line parameters and provide real-time monitoring capabilities that can be used to make adjustments to OWTS operations. © 2023 The Authors. Published by American Chemical Society.,Machine learning has been applied to the modeling of water treatment processes. While machine learning models have a great ability to handle nonlinear relationships in the process, changes in raw water quality and process operations can make predictions difficult. This study investigated the use of machine learning models, including traditional and deep learning approaches, for predicting both coagulant dosage and settled water turbidity in the water treatment process using six years of operating data. The study found that deep learning models, which process temporal sequential data, significantly improved prediction accuracies in response to changing dynamics of water treatment processes. The results emphasize the importance of collecting large datasets for modeling water treatment processes to capture rapid changes in raw water quality, thereby increasing prediction accuracies. The modeling results provide suggestions for model selection, data collection, and monitoring implementation in water treatment plants, which can enhance the accuracy of predictions and ensure high-quality treated water. © 2023,Chemical demand prediction is important for water management and the environment. This study aimed to select and apply suitable data-driven models based on real-world big data for dosage prediction towards improved automated control of water treatment plant management. Coagulation is a prominent process in normal water treatment plants (WTP). The chemical reactions are complex and the amount of coagulant dosage required was affected by many factors which makes it difficult to determine the optimal dosage effectively. Additionally, the coagulant process is a typical non-linear, multi-variable, large time-delay, non-stationary, strong coupling and time-varying system. Accurately determining the amount of coagulant added has become one of the most significant challenges. Some studies build a prediction model that only uses current water quality parameters, the previous time sequences were ignored and lacked consideration of multivariate time series and multiple water quality parameters simultaneously, resulting in unsatisfactory prediction accuracy. This study not only takes current water quality parameters into account during the modelling but also considers historical time-series water quality features. We found that the attention-based encoder-decoder of the recurrent neural network framework is an effective model in the area of intelligent water management. In this paper, we studied real-world data with 4 different machine learning models. Compared to the other three potential competitive machine learning algorithm models (random forest, multiple linear regression, and long short term memory), the experiment results demonstrated the best performance for predictive analysis with a highest coefficient of determination (R2) of 0.9908 and lowest values of root mean squared error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE) (1.2524%, 1.1263%, and 1.01%, respectively) in the DA_RNN algorithm. Consequently, this study provides a more reliable and accurate approach for forecasting wastewater coagulation dosage, which is pivotal in terms of the socio-economic aspects of wastewater management. © 2023 The Royal Society of Chemistry."
269,268,12,268_Legal NLP and Case Retrieval,Legal NLP and Case Retrieval,"Legal case retrieval has received increasing attention in recent years. However, compared to ad hoc retrieval tasks, legal case retrieval has its unique challenges. First, case documents are rather lengthy and contain complex legal structures. Therefore, it is difficult for most existing dense retrieval models to encode an entire document and capture its inherent complex structure information. Most existing methods simply truncate part of the document content to meet the input length limit of PLMs, which will lead to information loss. Additionally, the definition of relevance in the legal domain differs from that in the general domain. Previous semantic-based or lexical-based methods fail to provide a comprehensive understanding of the relevance of legal cases. In this article, we propose a Structured Legal case Retrieval (SLR) framework, which incorporates internal and external structural information to address the above two challenges. Specifically, to avoid the truncation of long legal documents, the internal structural information, which is the organization pattern of legal documents, can be utilized to split a case document into segments. By dividing the document-level semantic matching task into segment-level subtasks, SLR can separately process segments using different methods based on the characteristic of each segment. In this way, the key elements of a case document can be highlighted without losing other content information. Second, toward a better understanding of relevance in the legal domain, we investigate the connections between criminal charges appearing in large-scale case corpus to generate a chargewise relation graph. Then, the similarity between criminal charges can be pre-computed as the external structural information to enhance the recognition of relevant cases. Finally, a learning-to-rank algorithm integrates the features collected from internal and external structures to output the final retrieval results. Experimental results on public legal case retrieval benchmarks demonstrate the superior effectiveness of SLR over existing state-of-the-art baselines, including traditional bag-of-words and neural-based methods. Furthermore, we conduct a case study to visualize how the proposed model focuses on key elements and improves retrieval performance. © 2023 Association for Computing Machinery. All rights reserved.,Technology has substantially transformed the way legal services operate in many different countries. With a large and complex collection of digitized legal documents, the judiciary system worldwide presents a promising scenario for the development of intelligent tools. In this work, we tackle the challenging task of organizing and summarizing the constantly growing collection of legal documents, uncovering hidden topics, or themes that later can support tasks such as legal case retrieval and legal judgment prediction. Our approach to this problem relies on topic discovery techniques combined with a variety of preprocessing techniques and learning-based vector representations of words, such as Doc2Vec and BERT-like models. The proposed method was validated using four different datasets composed of short and long legal documents in Brazilian Portuguese, from legal decisions to chapters in legal books. Analysis conducted by a team of legal specialists revealed the effectiveness of the proposed approach to uncover unique and relevant topics from large collections of legal documents, serving many purposes, such as giving support to legal case retrieval tools and also providing the team of legal specialists with a tool that can accelerate their work of labeling/tagging legal documents. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.,The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal NLP). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models (LLMs), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study (SMS) to bridge this gap.We aim to provide a descriptive statistical analysis of the Legal NLP research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal NLP field.We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing (NLP), understanding the capabilities and limitations of Legal NLP becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal NLP researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns. Authors"
270,269,12,269_Query-based Recommendation Algorithm for Expert Knowledge in Digital Archives: M-CRBMs,Query-based Recommendation Algorithm for Expert Knowledge in Digital Archives: M-CRBMs,"While learning to rank (LTR) is widely employed in web searches to prioritize pertinent webpages from the retrieved contents based on input queries, traditional LTR models stumble over two principal stumbling blocks leading to subpar performance: (1) the lack of well-annotated query-webpage pairs with ranking scores to cover search queries of various popularity, debilitating their coverage of search queries across the popularity spectrum, and (2) ill-trained models that are incapable of inducing generalized representations for LTR, culminating in overfitting. To tackle above challenges, we proposed a G?enerativeS?emi - S?upervisedP?re -trained (GS 2 P) Learning to Rank model. Specifically, GS 2 P first generates pseudo-labels for the unlabeled samples using tree-based LTR models after a series of co-training procedures, then learns the representations of query-webpage pairs with self-attentive transformers via both discriminative (LTR) and generative (denoising autoencoding for reconstruction) losses. Finally, GS 2 P boosts the performance of LTR through incorporating Random Fourier Features to over-parameterize the models into “interpolating regime”, so as to enjoy the further descent of generalization errors with learned representations. We conduct extensive offline experiments on a publicly available dataset and a real-world dataset collected from a large-scale search engine. The results show that GS 2 P can achieve the best performance on both datasets, compared to baselines. We also deploy GS 2 P at a large-scale web search engine with realistic traffic, where we can still observe significant improvement in real-world applications. GS 2 P performs consistently in both online and offline experiments. © 2024, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.,While China has become the largest online market in the world with approximately 1 billion internet users, Baidu runs the world’s largest Chinese search engine serving more than hundreds of millions of daily active users and responding to billions of queries per day. To handle the diverse query requests from users at the web-scale, Baidu has made tremendous efforts in understanding users’ queries, retrieving relevant content from a pool of trillions of webpages, and ranking the most relevant webpages on the top of the results. Among the components used in Baidu search, learning to rank (LTR) plays a critical role and we need to timely label an extremely large number of queries together with relevant webpages to train and update the online LTR models. To reduce the costs and time consumption of query/webpage labelling, we study the problem of active learning to rank (active LTR) that selects unlabeled queries for annotation and training in this work. Specifically, we first investigate the criterion–Ranking entropy (RE) characterizing the entropy of relevant webpages under a query produced by a sequence of online LTR models updated by different checkpoints, using a query-by-committee (QBC) method. Then, we explore a new criterion namely prediction variances (PV) that measures the variance of prediction results for all relevant webpages under a query. Our empirical studies find that RE may favor low-frequency queries from the pool for labelling while PV prioritizes high-frequency queries more. Finally, we combine these two complementary criteria as the sample selection strategies for active learning. Extensive experiments with comparisons to baseline algorithms show that the proposed approach could train LTR models to achieve higher discounted cumulative gain (i.e., the relative improvement ?DCG 4 = 1.38%) with the same budgeted labelling efforts. © 2024, Institute of Automation, Chinese Academy of Sciences and Springer-Verlag GmbH Germany, part of Springer Nature.,While learning to rank (LTR) has been widely used in web search to prioritize most relevant webpages among the retrieved contents subject to the input queries, the traditional LTR models fail to deliver decent performance due to two main reasons: 1) the lack of well-annotated query-webpage pairs with ranking scores to cover search queries of various popularity, and 2) ill-trained models based on a limited number of training samples with poor generalization performance. To improve the performance of LTR models, tremendous efforts have been done from above two aspects, such as enlarging training sets with pseudo-labels of ranking scores by self-training, or refining the features used for LTR through feature extraction and dimension reduction. Though LTR performance has been marginally increased, we still believe these methods could be further improved in the newly-fashioned 'interpolating regime'. Specifically, instead of lowering the number of features used for LTR models, our work proposes to transform original data with random Fourier feature, so as to over-parameterize the downstream LTR models (e.g., GBRank or LightGBM) with features in ultra-high dimensionality and achieve superb generalization performance. Furthermore, rather than self-training with pseudo-labels produced by the same LTR model in a 'self-tuned' fashion, the proposed method incorporates the diversity of prediction results between the listwise and pointwise LTR models while co-training both models with a cyclic labeling-prediction pipeline in a 'ping-pong' manner. We deploy the proposed Co-trained and Over-parameterized LTR system COLTR at Baidu search and evaluate COLTR with a large number of baseline methods. The results show that COLTR could achieve ? NDCG 4 ?NDCG4 = 3.64% ?4.92%, compared to baselines, under various ratios of labeled samples. We also conduct a 7-day A/B Test using the realistic web traffics of Baidu Search, where we can still observe significant performance improvement around ?NDCG4 = 0.17%?0.92% in real-world applications. COLTR performs consistently both in online and offline experiments.  © 1989-2012 IEEE."
271,270,11,270_Machine Learning in Medical Diagnostics,Machine Learning in Medical Diagnostics,"Clinicians need improved prediction models to estimate time to kidney replacement therapy (KRT) for children with chronic kidney disease (CKD). Here, we aimed to develop and validate a prediction tool based on common clinical variables for time to KRT in children using statistical learning methods and design a corresponding online calculator for clinical use. Among 890 children with CKD in the Chronic Kidney Disease in Children (CKiD) study, 172 variables related to sociodemographics, kidney/cardiovascular health, and therapy use, including longitudinal changes over one year were evaluated as candidate predictors in a random survival forest for time to KRT. An elementary model was specified with diagnosis, estimated glomerular filtration rate and proteinuria as predictors and then random survival forest identified nine additional candidate predictors for further evaluation. Best subset selection using these nine additional candidate predictors yielded an enriched model additionally based on blood pressure, change in estimated glomerular filtration rate over one year, anemia, albumin, chloride and bicarbonate. Four additional partially enriched models were constructed for clinical situations with incomplete data. Models performed well in cross-validation, and the elementary model was then externally validated using data from a European pediatric CKD cohort. A corresponding user-friendly online tool was developed for clinicians. Thus, our clinical prediction tool for time to KRT in children was developed in a large, representative pediatric CKD cohort with an exhaustive evaluation of potential predictors and supervised statistical learning methods. While our models performed well internally and externally, further external validation of enriched models is needed. © 2023 International Society of Nephrology,Chronic kidney disease (CKD) remains one of the most prominent global causes of mortality worldwide, necessitating accurate prediction models for early detection and prevention. In recent years, machine learning (ML) techniques have exhibited promising outcomes across various medical applications. This study introduces a novel ML-driven monogram approach for early identification of individuals at risk for developing CKD stages 3–5. This retrospective study employed a comprehensive dataset comprised of clinical and laboratory variables from a large cohort of diagnosed CKD patients. Advanced ML algorithms, including feature selection and regression models, were applied to build a predictive model. Among 467 participants, 11.56% developed CKD stages 3–5 over a 9-year follow-up. Several factors, such as age, gender, medical history, and laboratory results, independently exhibited significant associations with CKD (p < 0.05) and were utilized to create a risk function. The Linear regression (LR)-based model achieved an impressive R-score (coefficient of determination) of 0.954079, while the support vector machine (SVM) achieved a slightly lower value. An LR-based monogram was developed to facilitate the process of risk identification and management. The ML-driven nomogram demonstrated superior performance when compared to traditional prediction models, showcasing its potential as a valuable clinical tool for the early detection and prevention of CKD. Further studies should focus on refining the model and validating its performance in diverse populations. © 2023, The Author(s).,Rationale & Objective: Chronic kidney disease (CKD) is a major cause of morbidity and mortality. To date, there are no widely used machine-learning models that can predict progressive CKD across the entire disease spectrum, including the earliest stages. The objective of this study was to use readily available demographic and laboratory data from Sonic Healthcare USA laboratories to train and test the performance of machine learning-based predictive risk models for CKD progression. Study Design: Retrospective observational study Setting & Participants: The study population was composed of deidentified laboratory information services data procured from a large US outpatient laboratory network. The retrospective data set included 110,264 adult patients over a 5-year period with initial estimated glomerular filtration rate (eGFR) values between 15-89 mL/min/1.73 m2. Predictors: Patient demographic and laboratory characteristics. Outcomes: Accelerated (ie, >30%) eGFR decline associated with CKD progression within 5 years. Analytical Approach: Machine-learning models were developed using random forest survival methods, with laboratory-based risk factors analyzed as potential predictors of significant eGFR decline. Results: The 7-variable risk classifier model accurately predicted an eGFR decline of >30% within 5 years and achieved an area under the curve receiver-operator characteristic of 0.85. The most important predictor of progressive decline in kidney function was the eGFR slope. Other key contributors to the model included initial eGFR, urine albumin-creatinine ratio, serum albumin (initial and slope), age, and sex. Limitations: The cohort study did not evaluate the role of clinical variables (eg, blood pressure) on the performance of the model. Conclusions: Our progressive CKD classifier accurately predicts significant eGFR decline in patients with early, mid, and advanced disease using readily obtainable laboratory data. Although prospective studies are warranted, our results support the clinical utility of the model to improve timely recognition and optimal management for patients at risk for CKD progression. Plain-Language Summary: Defined by a significant decrease in estimated glomerular filtration rate (eGFR), chronic kidney disease (CKD) progression is strongly associated with kidney failure. However, to date, there are no broadly used resources that can predict this clinically significant event. Using machine-learning techniques on a diverse US population, this cohort study aimed to address this deficiency and found that a 5-year risk prediction model for CKD progression was accurate. The most important predictor of progressive decline in kidney function was the eGFR slope, followed by the urine albumin-creatinine ratio and serum albumin slope. Although further study is warranted, the results showed that a machine-learning model using readily obtainable laboratory information accurately predicts CKD progression, which may inform clinical diagnosis and management for this at-risk population. © 2023 The Authors"
272,271,11,271_Short-term vessel speed prediction and trajectory fitting in maritime industry using AIS data and deep learning models,Short-term vessel speed prediction and trajectory fitting in maritime industry using AIS data and deep learning models,"Maritime situational awareness tasks such as port management, collision avoidance, and search-and-rescue missions rely on accurate knowledge of vessel locations. The availability of historical vessel trajectory data through the Automatic Identification System (AIS) has enabled the development of prediction methods, with a recent focus on trajectory prediction via recurrent neural networks (RNNs) and other deep learning architectures. While these methods have shown promising performance benefits over kinematic and clustering-based models, comparing among RNN-based models remains difficult due to variations in evaluation datasets, region sizes, vessel types, and numerous other design choices. As a result, it is not clear whether recent methods based on highly-sophisticated network architectures are necessary to achieve strong prediction performance. In this work, we present a simple fusion-based RNN approach to vessel trajectory prediction that allows for easy incorporation of exogenous variables. We perform an extensive ablation study to measure the impact of various modeling choices, including preprocessing, loss functions, and the choice of features, as well as the first usage of surface current information in vessel trajectory prediction. We demonstrate that our approach achieves state-of-the-art performance on three large regions off the United States coast, obtaining an improvement of up to 0.88 km over competing methods when predicting three hours into the future. We conclude that our simple architecture can outperform more complicated architectures while incurring a lower memory cost. Further, we show that the choice of loss function and the inclusion of surface current information both have significant impact on prediction performance. © 2024,The marine vessel Automatic Identification System (AIS) broadcast is a system with no-response mechanism, so there are missing data from the received AIS vessel trajectory. Currently, the mainstream vessel trajectory curve fitting method is achieved by means of deep learning algorithm cyclic training, however, the quality of the original data has a large impact on the trajectory fitting results, and the current method does not consider the interference of missing trajectory points on the fitted trajectory curve. Therefore, based on the data features of AIS, this paper proposes a lightweight deep learning method: Forward Backward Bidirectional Gated Recurrent Unit (FB-BiGRU). The method in this paper consists of a forward Gated Recurrent Unit (GRU) network and a backward Bidirectional Gated Recurrent Unit (BiGRU) network, reducing the range of the trajectory to be fitted by scaling in two directions simultaneously, thus gradually realizing the trajectory fitting function. The characteristics of our method are that training the trajectory data by bi-directional scaling can maximize the linear features of the trajectory curves to enhance the accuracy and efficiency of linear regression with no additional computing resources. The trajectory fitting performance of this method is verified on actual Denmark trajectory datasets, we prove that the accuracy of our proposed method in fitting short-term trajectories has increased by 49.16% and 29.89% on average compared with the (Long Short-Term Memory) LSTM and BiGRU. Furthermore, the average fitting accuracy of our method is 96 m, and the minimum fitting error is 64 m. © 2022 Elsevier Ltd,With the exponential growth in vessel traffic and the increasing complexity of maritime operations, there is a pressing need for reliable and efficient methods to forecast vessel movements. The accurate prediction of vessel trajectories plays a pivotal role in various maritime applications, including route planning, collision avoidance, and maritime traffic management. Traditional statistical and machine learning approaches have shown limitations in capturing the complex spatial–temporal patterns of vessel movements. Deep learning techniques have emerged as a promising solution due to their ability to handle large-scale datasets and capture nonlinear relationships. This study proposes a novel deep learning-based vessel trajectory prediction framework for AIS data using Auxiliary tasks and Convolutional encoders (AIS-ACNet). The framework utilizes various features of Automatic Identification System (AIS) data, including geographical positions, and vessel dynamics such as Speed Over Ground (SOG), and Course Over Ground (COG), for trajectory prediction. The AIS-ACNet employs parallel convolutional encoder networks with feature fusion layers to control the weight of auxiliary features. The model is trained with a multi-task learning objective that includes auxiliary SOG and COG prediction tasks. This framework enhances the model's vessel trajectory prediction performance by efficiently incorporating vessel dynamics. The proposed framework is evaluated on a real-world AIS dataset retrieved from the Port of Houston, Texas, USA. The result shows that AIS-ACNet achieves a 5.31% increase in average displacement error compared to the best performing baseline model. Also, the model demonstrates ability to perform robustly on various types of trajectories. © 2024"
273,272,11,272_Heart Failure Subtypes and Predictive Modeling,Heart Failure Subtypes and Predictive Modeling,"A heart failure (HF) condition is a type of chronic cardiovascular disease that affects millions of people globally. It can lead to various symptoms and has a significant impact on the quality of life. Despite the advancements that have been made in treati ng this condition, it remains a major public health issue. One of the biggest challenges that HF management faces is the high number of readmissions. This issue contributes to the increasing of patients' outcomes and costs the healthcare system. Implementing effective interventions and identifying those at high risk of returning to the hospital can help lower the financial burden on the system. Through the use of machine learning techniques, researchers can now predict the likelihood of HF readmissions. These tools can analyze large datasets and provide a personalized diagnosis and treatment plan. There have been various studies that have examined the use of ML for predicting HF readmissions. The goal of this study is to analyze the various techniques used in predicting HF readmissions and provide a comprehensive analysis of their performance. Through a combination of data collected from various sources, including a diverse set of patients, we will be able to explore the performance of various ML algorithms. In addition to the algorithms' performance, we will also look into their impact on various parameters, such as model evaluation metrics, optimization techniques, and feature selection. The findings of this study will be used to inform policymakers and healthcare providers about the use of ML techniques to identify patients at high risk of HF readmissions. These insights can help them improve the quality of care for those with this condition and develop effective interventions. The objective of this study is to use the power of ML to improve the management of HF and reduce the burden of readmissions on both the patients and the healthcare systems. © 2024, Ismail Saritas. All rights reserved.,Introduction Heart failure (HF) is increasingly common and associated with excess morbidity, mortality, and healthcare costs. Treatment of HF can alter the disease trajectory and reduce clinical events in HF. However, many cases of HF remain undetected until presentation with more advanced symptoms, often requiring hospitalisation. Predicting incident HF is challenging and statistical models are limited by performance and scalability in routine clinical practice. An HF prediction model implementable in nationwide electronic health records (EHRs) could enable targeted diagnostics to enable earlier identification of HF. Methods and analysis We will investigate a range of development techniques (including logistic regression and supervised machine learning methods) on routinely collected primary care EHRs to predict risk of new-onset HF over 1, 5 and 10 years prediction horizons. The Clinical Practice Research Datalink (CPRD)-GOLD dataset will be used for derivation (training and testing) and the CPRD-AURUM dataset for external validation. Both comprise large cohorts of patients, representative of the population of England in terms of age, sex and ethnicity. Primary care records are linked at patient level to secondary care and mortality data. The performance of the prediction model will be assessed by discrimination, calibration and clinical utility. We will only use variables routinely accessible in primary care. Ethics and dissemination Permissions for CPRD-GOLD and CPRD-AURUM datasets were obtained from CPRD (ref no: 21_000324). The CPRD ethical approval committee approved the study. The results will be submitted as a research paper for publication to a peer-reviewed journal and presented at peer-reviewed conferences. Trial registration details The study was registered on Clinical Trials.gov (NCT 05756127). A systematic review for the project was registered on PROSPERO (registration number: CRD42022380892).  © 2024 BMJ Publishing Group. All rights reserved.,Background: Most risk prediction models are confined to specific medical conditions, thus limiting their application to general medical populations. Objectives: The MARKER-HF (Machine learning Assessment of RisK and EaRly mortality in Heart Failure) risk model was developed in heart failure (HF) patients. We assessed the ability of MARKER-HF to predict 1-year mortality in a large community-based hospital registry database including patients with and without HF. Methods: This study included 41,749 consecutive patients who underwent echocardiography in a tertiary referral hospital (4,640 patients with and 37,109 without HF). Patients without HF were further subdivided into those with (n = 22,946) and without cardiovascular disease (n = 14,163) and also into cohorts based on recent acute coronary syndrome or history of atrial fibrillation, chronic obstructive pulmonary disease, chronic kidney disease, diabetes mellitus, hypertension, or malignancy. Results: The median age of the 41,749 patients was 65 years, and 56.2% were male. The receiver operated area under the curves for MARKER-HF prediction of 1-year mortality of patients with HF was 0.729 (95% CI: 0.706-0.752) and for patients without HF was 0.770 (95% CI: 0.760-0.780). MARKER-HF prediction of mortality was consistent across subgroups with and without cardiovascular disease and in patients diagnosed with acute coronary syndrome, atrial fibrillation, chronic obstructive pulmonary disease, chronic kidney disease, diabetes mellitus, or hypertension. Patients with malignancy demonstrated higher mortality at a given MARKER-HF score than did patients in the other groups. Conclusions: MARKER-HF predicts mortality for patients with HF as well as for patients suffering from a variety of diseases. © 2023 The Authors"
274,273,11,273_Deep Learning Approaches for Weakly Supervised Object Detection and Salient Object Detection with Scale-aware Augmentation and Pseudo Labels,Deep Learning Approaches for Weakly Supervised Object Detection and Salient Object Detection with Scale-aware Augmentation and Pseudo Labels,"Fully-supervised salient object detection (SOD) methods have made great progress, but such methods often rely on a large number of pixel-level annotations, which are time-consuming and labour-intensive. In this paper, we focus on a new weakly-supervised SOD task under hybrid labels, where the supervision labels include a large number of coarse labels generated by the traditional unsupervised method and a small number of real labels. To address the issues of label noise and quantity imbalance in this task, we design a new pipeline framework with three sophisticated training strategies. In terms of model framework, we decouple the task into label refinement sub-task and salient object detection sub-task, which cooperate with each other and train alternately. Specifically, the R-Net is designed as a two-stream encoder-decoder model equipped with Blender with Guidance and Aggregation Mechanisms (BGA), aiming to rectify the coarse labels for more reliable pseudo-labels, while the S-Net is a replaceable SOD network supervised by the pseudo labels generated by the current R-Net. Note that, we only need to use the trained S-Net for testing. Moreover, in order to guarantee the effectiveness and efficiency of network training, we design three training strategies, including alternate iteration mechanism, group-wise incremental mechanism, and credibility verification mechanism. Experiments on five SOD benchmarks show that our method achieves competitive performance against weakly-supervised/unsupervised methods both qualitatively and quantitatively. The code and results can be found from the link of https://rmcong.github.io/proj_Hybrid-Label-SOD.html.  © 1991-2012 IEEE.,Object annotation is essential for computer vision tasks, and more high-quality annotated data can effectively improve the performance of vision models. However, manual annotation is time-consuming (annotating a box takes 35s). Recent studies have explored faster automated annotation, among which weakly supervised methods stand out. Weakly supervised methods learn to automatically localize objects in images from weakly labeled annotations, e.g., class tags or points, replacing manual bounding box annotations. Although using a single weakly labeled annotation can reduce a large amount of time, it leads to poor annotation quality, particularly for the complex scenes containing multiple objects. To balance annotation time and quality, we propose a weakly semi-supervised automated annotation method. Its main idea is to incorporate point-labeled and fully labeled annotations into a teacher-student framework for training, to jointly localize the object bounding boxes on all point-labeled images. We also propose two effective techniques within this framework to better use of these mixed annotations. The first is a point-guided sample assignment technique which optimizes the loss calculation. The second is a pseudo-label filtering technique which generate accurate pseudo labels for model training by utilizing the points and boxes localization confidences. Extensive experiments on MSCOCO demonstrate that our method outperforms existing automated annotation methods. In particular, when using 95% point-labeled and 5% fully labeled data, our approach reduces the annotation time by approximately 52% and achieves an annotation quality of 87.4% mIoU. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.,The rapid development of deep learning has made a great progress in salient object detection task. Fully supervised methods need a large number of pixel-level annotations. To avoid laborious and consuming annotation, weakly supervised methods consider low-cost annotations such as category, bounding-box, scribble, etc. Due to simple annotation and existing large-scale classification datasets, the category annotation based methods have received more attention while still suffering from inaccurate detection. In this work, we proposed one weakly supervised method with category annotation. First, we proposed one coarse object location network (COLN) to roughly locate the object of an image with category annotation. Second, we refined the coarse object location to generate pixel-level pseudo-labels and proposed one quality check strategy to select high quality pseudo labels. To this end, we studied COLN twice followed by refinement to obtain a pseudo-labels pair and calculated the consistency of pseudo-label pairs to select high quality labels. Third, we proposed one multi-decoder neural network (MDN) for saliency detection supervised by pseudo-label pairs. The loss of each decoder and between decoders are both considered. Last but not least, we proposed one pseudo-labels update strategy to iteratively optimize pseudo-labels and saliency detection models. Performance evaluation on four public datasets shows that our method outperforms other image category annotation based work. © 2023 the Author(s)"
275,274,11,274_Cybersecurity in Power Systems with ML and Penetration Testing,Cybersecurity in Power Systems with ML and Penetration Testing,"Machine learning (ML) sees an increasing prevalence of being used in the internet-of-things (IoT)-based smart grid. However, the trustworthiness of ML is a severe issue that must be addressed to accommodate the trend of ML-based smart grid applications (MLsgAPPs). The adversarial distortion injected into the power signal will greatly affect the system&#x2019;s normal control and operation. Therefore, it is imperative to conduct vulnerability assessment for MLsgAPPs applied in the safety-critical power systems. In this paper, we provide a comprehensive review of the recent progress in designing attack and defense methods for MLsgAPPs. Unlike the traditional survey about ML security, this is the first review work about the security of MLsgAPPs that focuses on the characteristics of power systems. We first highlight the specifics for constructing adversarial attacks on MLsgAPPs. Then, the vulnerability of MLsgAPP is analyzed from the perspective of the power system and ML model, respectively. Afterward, a comprehensive survey is conducted to review and compare existing studies about the adversarial attacks on MLsgAPPs in scenarios of generation, transmission, distribution, and consumption, and the countermeasures are reviewed according to the attacks that they defend against. Finally, the future research directions are discussed on the attacker&#x2019;s and defender&#x2019;s side, respectively. We also analyze the potential vulnerability of large language model-based (e.g., ChatGPT) smart grid applications. Overall, our purpose is to encourage more researchers to contribute to investigating the adversarial issues of MLsgAPPs. IEEE,Penetration testing (PT) is an effective method to assess the security of a network, mainly carried out by experienced human experts, and is widely applied in practice. It is urgent to develop automated tools to alleviate the pressure of talent shortages. Reinforcement learning (RL) is a promising approach to achieving automated PT. However, the high complexity of PT scenarios and the low sample efficiency of RL hinder its applications in practice. Specifically, it faces two dilemmas: (1) vast state and action spaces and (2) highly ineffective exploration. We propose a hierarchical deep reinforcement learning (HDRL) model with expert prior knowledge to overcome the above dilemmas. The HDRL model mitigates the first dilemma. According to the characteristics of PT, we design the model as a hierarchical structure containing two layers of agents, and the agents as a deep neural network to decompose PT tasks and reduce their complexity. Expert prior knowledge mitigates the second dilemma. It is used as rules and knowledge graphs, carries out action constraints according to the rules, and obtains action advice according to knowledge graphs. The two jointly guide the decision-making of agents to reduce invalid exploration. To verify the effectiveness of the proposed method, we design scenarios based on actual network environments. The experimental results show that our model significantly improves the sample efficiency, greatly reduces the learning time of the agents, and shows good performance on large-scale network scenarios, which has the potential to promote the practical application of intelligent PT based on RL. © 2023 Elsevier Ltd,Penetration testing (PT) is a method for assessing and evaluating the security of digital assets by planning, generating, and executing possible attacks that aim to discover and exploit vulnerabilities. In large networks, penetration testing becomes repetitive, complex and resource consuming despite the use of automated tools. This paper investigates reinforcement learning (RL) to make penetration testing more intelligent, targeted, and efficient. The proposed approach called Intelligent Automated Penetration Testing Framework (IAPTF) utilizes model-based RL to automate sequential decision making. Penetration testing tasks are treated as a partially observed Markov decision process (POMDP) which is solved with an external POMDP-solver using different algorithms to identify the most efficient options. A major difficulty encountered was solving large POMDPs resulting from large networks. This was overcome by representing networks hierarchically as a group of clusters and treating each cluster separately. This approach is tested through simulations of networks of various sizes. The results show that IAPTF with hierarchical network modeling outperforms previous approaches as well as human performance in terms of time, number of tested vectors and accuracy, and the advantage increases with the network size. Another advantage of IAPTF is the ease of repetition for retesting similar networks, which is often encountered in real PT. The results suggest that IAPTF is a promising approach to offload work from and ultimately replace human pen testing. © 2022, The Author(s)."
276,275,11,"275_Deep learning methods for RNA sequence analysis including motif discovery, clustering, and secondary structure prediction","Deep learning methods for RNA sequence analysis including motif discovery, clustering, and secondary structure prediction","Background: Tools for accurately clustering biological sequences are among the most important tools in computational biology. Two pioneering tools for clustering sequences are CD-HIT and UCLUST, both of which are fast and consume reasonable amounts of memory; however, there is a big room for improvement in terms of cluster quality. Motivated by this opportunity for improving cluster quality, we applied the mean shift algorithm in MeShClust v1.0. The mean shift algorithm is an instance of unsupervised learning. Its strong theoretical foundation guarantees the convergence to the true cluster centers. Our implementation of the mean shift algorithm in MeShClust v1.0 was a step forward. In this work, we scale up the algorithm by adapting an out-of-core strategy while utilizing alignment-free identity scores in a new tool: MeShClust v3.0. Results: We evaluated CD-HIT, MeShClust v1.0, MeShClust v3.0, and UCLUST on 22 synthetic sets and five real sets. These data sets were designed or selected for testing the tools in terms of scalability and different similarity levels among sequences comprising clusters. On the synthetic data sets, MeShClust v3.0 outperformed the related tools on all sets in terms of cluster quality. On two real data sets obtained from human microbiome and maize transposons, MeShClust v3.0 outperformed the related tools by wide margins, achieving 55%–300% improvement in cluster quality. On another set that includes degenerate viral sequences, MeShClust v3.0 came third. On two bacterial sets, MeShClust v3.0 was the only applicable tool because of the long sequences in these sets. MeShClust v3.0 requires more time and memory than the related tools; almost all personal computers at the time of this writing can accommodate such requirements. MeShClust v3.0 can estimate an important parameter that controls cluster membership with high accuracy. Conclusions: These results demonstrate the high quality of clusters produced by MeShClust v3.0 and its ability to apply the mean shift algorithm to large data sets and long sequences. Because clustering tools are utilized in many studies, providing high-quality clusters will help with deriving accurate biological knowledge. © 2022, The Author(s).,RNA-binding proteins (RBPs) play significant roles in many biological life activities, many algorithms and tools are proposed to predict RBPs for researching biological mechanisms of RNA-protein binding sites. Deep learning algorithms based on traditional machine learning get better result for predicting RBPs. Recently, deep learning method fused with attention mechanism has attracted huge attention in many fields and gets competitive result. Thus, attention mechanism module may also improve model performance for predicting RNA-protein binding sites. In this study, we propose convolutional residual multi-head self-attention network (CRMSNet) that combines convolutional neural network (CNN), ResNet, and multi-head self-attention blocks to find RBPs for RNA sequence. First, CRMSNet incorporates convolutional neural networks, recurrent neural networks, and multi-head self-attention block. Second, CRMSNet can draw binding motif pictures from the convolutional layer parameters. Third, attention mechanism module combines the local and global RNA sequence information for capturing long sequence feature. CRMSNet gets competitive AUC (area under the receiver operating characteristic [ROC] curve) result in a large-scale dataset RBP-24. And CRMSNet experiment result is also compared with other state-of-the-art methods. The source code of our proposed CRMSNet method can be found in https://github.com/biomg/CRMSNet. © 2023 Wiley Periodicals LLC.,An important question in evolutionary biology is whether (and in what ways) genotype–phenotype (GP) map biases can influence evolutionary trajectories. Untangling the relative roles of natural selection and biases (and other factors) in shaping phenotypes can be difficult. Because the RNA secondary structure (SS) can be analyzed in detail mathematically and computationally, is biologically relevant, and a wealth of bioinformatic data are available, it offers a good model system for studying the role of bias. For quite short RNA (length (Formula presented.)), it has recently been shown that natural and random RNA types are structurally very similar, suggesting that bias strongly constrains evolutionary dynamics. Here, we extend these results with emphasis on much larger RNA with lengths up to 3000 nucleotides. By examining both abstract shapes and structural motif frequencies (i.e., the number of helices, bonds, bulges, junctions, and loops), we find that large natural and random structures are also very similar, especially when contrasted to typical structures sampled from the spaces of all possible RNA structures. Our motif frequency study yields another result, where the frequencies of different motifs can be used in machine learning algorithms to classify random and natural RNA with high accuracy, especially for longer RNA (e.g., ROC AUC 0.86 for L = 1000). The most important motifs for classification are the number of bulges, loops, and bonds. This finding may be useful in using SS to detect candidates for functional RNA within ‘junk’ DNA regions. © 2023 by the authors."
277,276,11,276_Pose Estimation for Robotic Grasping of Objects,Pose Estimation for Robotic Grasping of Objects,"6D pose recognition has been a crucial factor in the success of robotic grasping, and recent deep learning based approaches have achieved remarkable results on benchmarks. However, their generalization capabilities in real-world applications remain unclear. To overcome this gap, we introduce 6IMPOSE, a novel framework for sim-to-real data generation and 6D pose estimation. 6IMPOSE consists of four modules: First, a data generation pipeline that employs the 3D software suite Blender to create synthetic RGBD image datasets with 6D pose annotations. Second, an annotated RGBD dataset of five household objects was generated using the proposed pipeline. Third, a real-time two-stage 6D pose estimation approach that integrates the object detector YOLO-V4 and a streamlined, real-time version of the 6D pose estimation algorithm PVN3D optimized for time-sensitive robotics applications. Fourth, a codebase designed to facilitate the integration of the vision system into a robotic grasping experiment. Our approach demonstrates the efficient generation of large amounts of photo-realistic RGBD images and the successful transfer of the trained inference model to robotic grasping experiments, achieving an overall success rate of 87% in grasping five different household objects from cluttered backgrounds under varying lighting conditions. This is made possible by fine-tuning data generation and domain randomization techniques and optimizing the inference pipeline, overcoming the generalization and performance shortcomings of the original PVN3D algorithm. Finally, we make the code, synthetic dataset, and all the pre-trained models available on GitHub. Copyright © 2023 Cao, Dirnberger, Bernardini, Piazza and Caccamo.,6-D object pose estimation is widely used in the robotic grasp, and a series of object pose estimation methods have been proposed. Among them, category-level object pose estimation methods are widely researched in recent years. Category-level object pose estimation is mainly used to estimate the pose of unknown objects in the same class, and has been used in robotic grasping and augmented reality. Most of the current methods tend to rely on large datasets as well as labels, which pose challenges. To address this problem, a new category-level object pose estimation network, SCNet, is proposed, which not only enables the network to transfer from the simulation environment to the real world but also allows us to train the network with a self-supervised learning way, which can well compensate for the lack of large-scale labeled datasets. Since the network lacks 3-D models of unknown objects, we introduce the prior point cloud of objects in the same category and propose a deformation module based on RGB images and the prior point cloud, which enables the prior point cloud to be well deformed into target objects in the scene. Moreover, a transformer-based recurrent refinement module is proposed to further refine the deformation structure to better fit target objects. We have performed evaluation experiments on CAMERA25 dataset and REAL275 dataset, and our experimental results show that the proposed method outperforms current self-supervised category-based pose estimation methods, and outperforms some supervised category pose estimation methods. Finally, we apply the SCNet to the object pose estimation in the real world, and perform a series of robotic grasp tasks on a Baxter robot. IEEE,Estimation of 6D object poses is a key issue in robotic grasping tasks. Recently, many high-performance learning-based methods have been introduced using robust deep learning techniques; however, applying these methods to real robot environments requires many ground truth 6D pose annotations for training. To address this problem, we propose a template matching-based particle filter approach for 6D pose estimation; the proposed method does not require ground truth 6D poses. Although particle filter approaches can stochastically avoid local optima, they require adequate initial pose hypotheses for estimating an accurate 6D object pose. Therefore, we estimated an initial translation of the target object for accurately initializing a particle filter by developing a new deep network. Once the proposed centroid prediction network (CPN) is trained with a specific dataset, no additional training is required for new objects not in the dataset. We evaluated the performance of the CPN and the proposed 6D pose estimation method on benchmark datasets, which demonstrated that the CPN can predict the centroid for any object, including those not in the training data, and that our 6D pose estimation method outperforms existing methods for partially occluded objects. Finally, we tested a grasping task based on our proposed method using a real robot platform to demonstrate an application of our method to a downstream task. This experiment shows that our method can be applied to part assembly, bin picking, and object manipulation without large training datasets with 6D pose annotations. The code and models are available at: https://github.com/oorrppp2/Particle_filter_approach_6D_pose_estimation.  © 2013 IEEE."
278,277,10,277_Aerodynamic modeling and prediction using ROM and ML techniques for aircraft and hypersonic vehicles.,Aerodynamic modeling and prediction using ROM and ML techniques for aircraft and hypersonic vehicles.,"Pulse wind tunnel aerodynamic tests are an important part of hypersonic aircraft design and development. With the development of hypersonic aircraft technology, the aircraft model in pulsed wind tunnels has gradually become large-scale and heavy-loaded. During the force measurement test, due to the influence of the increased size of the aircraft model, the real aerodynamic signal of the force measurement system is submerged by the interference signal. During the effective test time of several hundred milliseconds, it is impossible to obtain high-precision aerodynamic signals of the force measuring system using traditional signal-processing methods. Therefore, for the in-depth study of aerodynamic identification of short-time hypersonic pulsed wind tunnels, there is an urgent need for an identification algorithm that can adapt to the force measurement system of large-scale aircraft. In this paper, a new aerodynamic identification algorithm that combines a traditional signal-processing method and a deep neural network is proposed and applied to pulse combustion wind tunnels. The algorithm is mainly divided into signal preprocessing and deep learning. First, the original signal is decomposed into different sub-signals via variational modal decomposition (VMD), and then the real aerodynamic signal is obtained via convolutional neural network (CNN)-long short-term memory (LSTM) training. For different interference signals in the pulsed wind tunnel test, this algorithm innovatively designs VMD preprocessing and optimizes hyperparameters, to extract signal features for subsequent deep learning and to filter out interference components. To ensure the consistency between the validation and the application, the algorithm was verified using the suspension test bench during training, and satisfactory results were obtained. Finally, the algorithm is applied to the suspension force measurement system of a pulse combustion wind tunnel, and the aerodynamic identification results present satisfactory accuracy. © 2022 IOP Publishing Ltd.,The modeling of dynamic stall aerodynamics is essential to stall flutter, due to the flow separation in a large-amplitude pitching oscillation process. A newly neural network based Reduced Order Model (ROM) framework for predicting the aerodynamic forces of an airfoil undergoing large-amplitude pitching oscillation at various velocities is presented in this work. First, the dynamic stall aerodynamics is calculated by solving RANS equations and the transitional SST-? model. Afterwards, the stall flutter bifurcation behavior is calculated by the above CFD solver coupled with structural dynamic equation. The critical flutter speed and limit-cycle oscillation amplitudes are consistent with those obtained by experiments. A newly multi-layer Gated Recurrent Unit (GRU) neural network based ROM is constructed to accelerate the calculation of aerodynamic forces. The training and validation process are carried out upon the unsteady aerodynamic data obtained by the proposed CFD method. The well-trained ROM is then coupled with the structure equation at a specific velocity, the Limit-Cycle Oscillation (LCO) of stall flutter under this flow condition is predicted precisely and more quickly. In order to predict both the critical flutter velocity and LCO amplitudes after bifurcation at different velocities, a new ROM with GRU neural network considering the variation of flow velocities is developed. The stall flutter results predicted by ROM agree well with the CFD ones at different velocities. Finally, a brief sensitivity analysis of two structural parameters of ROM is carried out. It infers the potential of the presented modeling method to depict the nonlinearity of dynamic stall and stall flutter phenomenon. © 2022 Chinese Society of Aeronautics and Astronautics,In this paper, a reduced-order model (ROM) based on data-driven machine learning algorithm is constructed to identify the aerodynamic forces of airfoil undergoing large-amplitude pitching oscillation. Strong nonlinearity and unsteadiness in aerodynamics is a major challenge in the prediction of aerodynamic forces. To deal with this problem, the recurrent neural network (RNN) with gated recurrent unit (GRU) is applied for nonlinear and unsteady aerodynamic identification. A motion input signal which covers a wide range of frequency and amplitude is designed to enable the ROM with generalization capability. Shear stress transport (SST) model with low-Reynolds number modification is introduced into the computational fluid dynamics (CFD) method to calculate the aerodynamic forces as the training data. The time step size and lag order of the model are determined by the frequency domain characteristics of the training data. The results suggest that the proposed ROM has a high identification precision on nonlinear unsteady aerodynamics. The well-trained ROM could accurately predict the aerodynamic forces of airfoil undergoing sinusoidal oscillations with various frequencies and amplitudes. The proposed ROM shows advantages in accuracy over other ROM techniques. The calculation speed of ROM is 69 times faster than that of CFD method on the premise of accuracy, which can be expected a good application in engineering. © IMechE 2022."
279,278,10,278_Internet-based Treatment Research for Depression and Chronic Pain Management,Internet-based Treatment Research for Depression and Chronic Pain Management,"Development of back pain is multifactorial, and it is not well understood which factors are the main drivers of the disease. We therefore applied a machine-learning approach to an existing large cohort study data set and sought to identify and rank the most important contributors to the presence of back pain amongst the documented parameters of the cohort. Data from 399 participants in the KORA-MRI (Cooperative health research in the region Augsburg-magnetic resonance imaging) (Cooperative Health Research in the Region Augsburg) study was analyzed. The data set included MRI images of the whole body, including the spine, metabolic, sociodemographic, anthropometric, and cardiovascular data. The presence of back pain was one of the documented items in this data set. Applying a machine-learning approach to this preexisting data set, we sought to identify the variables that were most strongly associated with back pain. Mediation analysis was performed to evaluate the underlying mechanisms of the identified associations. We found that depression and anxiety were the 2 most selected predictors for back pain in our model. Additionally, body mass index, spinal canal width and disc generation, medium and heavy physical work as well as cardiovascular factors were among the top 10 most selected predictors. Using mediation analysis, we found that the effects of anxiety and depression on the presence of back pain were mainly direct effects that were not mediated by spinal imaging. In summary, we found that psychological factors were the most important predictors of back pain in our cohort. This supports the notion that back pain should be treated in a personalized multidimensional framework. Perspective: This article presents a wholistic approach to the problem of back pain. We found that depression and anxiety were the top predictors of back pain in our cohort. This strengthens the case for a multidimensional treatment approach to back pain, possibly with a special emphasis on psychological factors. © 2024 United States Association for the Study of Pain, Inc.,Chronic pain remains a serious healthcare challenge, particularly for older adults who suffer substantial disability and are susceptible to serious risks from pain medications and invasive procedures. Psychotherapy is a promising option for older adults with chronic pain, since it does not contribute to medical or surgical risks. However, standard psychotherapies for chronic pain, including cognitive-behavioral therapy (CBT), acceptance and commitment therapy, and mindfulness-based interventions, produce only modest and time-limited benefits for older adults. In this article, we describe a novel, evidence-based psychological assessment and treatment approach for older adults with chronic pain, including a detailed case example. The approach begins with reviewing patients’ pain, psychosocial, and medical histories to elicit evidence of a subtype of chronic pain called centralized (primary, nociplastic, or psychophysiologic) pain, which is highly influenced and may even be caused by life stress, emotions, and alterations in brain function. Patients then undertake a novel psychotherapy approach called emotional awareness and expression therapy (EAET) that aims to reduce or eliminate centralized pain by resolving trauma and emotional conflicts and learning healthy communication of adaptive emotions. Our published preliminary clinical trial (n = 53) indicated that EAET produced statistically significant and large effect size advantages over CBT in pain reduction and marginally greater improvements in pain interference than CBT for older adults with chronic musculoskeletal pain. Geriatric mental healthcare providers may learn this assessment and treatment approach to benefit many of their patients with chronic pain. © 2022,Background: Chronic low back pain (cLBP) is the most common cause of years lived with disability (YLD). Chronic overlapping pain conditions (COPCs) is a relatively new taxonomy for widespread pain. Researchers have postulated that patients with COPCs have more pain-related impact than those with isolated pain conditions. We know little about the combination of COPCs with cLBP. This study aims to characterize patients with isolated cLBP compared to those with cLBP and associated COPCs across multiple domains of physical, psychological, and social functioning. Methods: Using Stanford's CHOIR registry-based learning health system, we performed a cross-sectional study on patients with localized cLBP (group L) versus cLBP with COPCs (group W). We used demographic, PROMIS (Patient-Reported Outcomes Measurement Information System), and legacy survey data to characterize the physical, psychological, social, and global health outcomes. We further subdivided the COPCs into intermediate and severe based on the number of body regions involved. We used descriptive statistics and generalized linear regression models to characterize and compare the pain groups. Results: Among 8783 patients with cLBP, 485 (5.5%) had localized cLBP (Group L) without widespread pain. Compared to Group L, patients in Group W were more likely to be females, younger, and reported longer duration of pain. Although the mean pain scores were significantly higher in group W, this difference did not appear clinically significant (average pain scores MD ?0.73, 95% CI [?0.91 to ?0.55]). Group W had significantly worse outcomes in all PROMIS outcomes. However, outcomes with large clinical differences (Cohen's d > 0.5) were fatigue (MD = ?7.0, 95% CI [?8.0 to ?6.1]); sleep impairment (MD = ?6.2, 95% CI [?7.1 to ?5.3]); sleep disturbance (MD = ?5.3, 95% CI [?6.2 to ?4.5]); pain behavior (MD = ?2.2, 95% CI [?2.5 to ?1.8]); physical function (MD = 4.0, 95% CI [3.2–5.0]); pain interference (MD = ?3.4, 95% CI [?4.0 to ?2.8]); and anxiety (MD = ?4.9, 95% CI [?5.7 to ?4.0]). Adjusted analysis controlling for age, gender, BMI category, and duration of pain confirmed worsening of all outcomes with more widespread pain. Conclusion: COPCs are a common presentation with cLBP. The combination of COPCs with cLBP is associated with significantly worse physical, psychological, social, and global health outcomes. This information may identify patients with COPCs and cLBP to optimally risk and treatment stratify their care and individualize their management. © 2023 World Institute of Pain."
280,279,10,279_Sports video analysis and machine learning for player performance and match-fixing detection.,Sports video analysis and machine learning for player performance and match-fixing detection.,"With the popularity of sports wearable smart devices, it is no longer difficult to obtain human movement data. A series of running fitness software came into being, leading the nation's running wave and greatly promoting the rapid development of the sports industry. However, a large amount of sports data has not been deeply mined, resulting in a huge waste of its value. In order to make the data collected by smart sports equipment better serve the sports enthusiasts, thereby more effectively improving the degree of informatization of the sports industry, this paper selects the design and implementation of the human motion recognition information processing system as the main research content. This article combs the previous research results of human motion recognition information processing systems related to sports wearable intelligence and proposes a three-layer human motion recognition information processing system architecture, including data collection layer, data calculation layer, and data application layer. In the data calculation layer, different from the traditional classification algorithm, this paper proposes a classifier based on the recurrent neural network algorithm. The mechanical motion capture method mainly uses mechanical devices to track and measure motion. A typical system consists of multiple joints and rigid links. Inertial measurement units are bound to the joints to obtain angles and accelerations, and then analyze the human body motion based on these angles and accelerations. From the perspective of optical motion capture, the Kinect somatosensory camera is researched, and the method of human motion capture based on depth images, and the principle and method of human motion information are analyzed. At the same time, research on the application of Kinect's motion capture data. As a deep learning algorithm, convolutional action recognition model has the characteristics of being good at processing long and interrelated data and automatically learning features in the data. It solves the defect that the traditional recognition method needs to manually extract the motion features from the data, the whole system structure is streamlined, and the recognition efficiency is higher. The overall evaluation is as high as 99.4%. It avoids the manual extraction of time-domain and frequency-domain features of time series data, and at the same time avoids the loss of data information caused by dimensionality reduction. © 2021, The Author(s) under exclusive licence to The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.,The deep fusion of sports and machine vision has become a research hot spot in sports video target detection, athlete state recovery and sports promotion. On the basis of in-depth study, it can detect a large number of sports videos, complete the drawing and analysis of human body detection model, and detect and evaluate the posture of corresponding athletes in the video, which can save a lot of costs and maximize the more professional training of athletes. In order to solve the above problems, this paper innovatively completes the automatic language description of sports video based on time-sharing memory algorithm. Its principle is to realize the accurate decomposition of athletes' sports data through the mapping relationship between the corresponding letter sequence and video sequence in time-sharing memory. In order to capture the key posture of athletes' sports video, this paper innovatively proposes an object extraction algorithm based on athletes' skeleton motion enhancement. In practical application, based on the key pose capture, it is necessary to train the depth selection network in time to extract the key pose of the skeleton. Based on this network, it can enhance the key posture of bone information and accurately express its related features. After extracting the actual athlete's bone information, we need to fine-tune the training network to realize the accurate recognition of key features. Based on the above key algorithms, this paper designs a sports video athlete detection system based on deep learning and makes an experimental research on the related sports video. The experimental results show that the detection accuracy of athletes' sports video is improved by nearly 10% compared with the traditional convolution network recognition algorithm, so the algorithm has obvious advantages in recognition accuracy. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.,Sports video courses, as a teaching method and means, are now widely used in the field of physical education and teaching, enabling people to make better use of sports course resources. To study the utilization rate and the effectiveness of the distribution channel of sports course video resources, optimize the application channel for enhancing sports video resources, and improve the exposure rate and the accuracy rate of the sports video courses being retrieved to facilitate users' query and browsing. This paper investigates the impact of neural network and migration learning models on the application of sports video courses by comparing the experimental results of GRU4Rec and Caser algorithm frameworks and migration learning algorithms with computer processing, analysis, and understanding techniques for sports video content. The results show that too small an embedding vector for sports videos affects the accuracy of the whole embedding vector, and when the relevant behavioral data is small, it is not possible to learn an embedding vector representation with very low dimensionality, when a larger vector d is a better choice. However, the experimental effect becomes poor when d is too large due to the sparse user behavior data and the loss of behavior information caused by too large d. The method proposed in this paper can be better applied in sports video courses. © 2023 Jing Wang et al.;published by Sciendo."
281,280,10,280_Deep Learning Methods for Evaluating Data Augmentation Techniques,Deep Learning Methods for Evaluating Data Augmentation Techniques,"High-velocity data streams present a challenge to deep learning-based computer vision models due to the resources needed to retrain for new incremental data. This study presents a novel staggered training approach using an ensemble model comprising the following: (i) a resource-intensive high-accuracy vision transformer; and (ii) a fast training, but less accurate, low parameter-count convolutional neural network. The vision transformer provides a scalable and accurate base model. A convolutional neural network (CNN) quickly incorporates new data into the ensemble model. Incremental data are simulated by dividing the very large So2Sat LCZ42 satellite image dataset into four intervals. The CNN is trained every interval and the vision transformer trained every half interval. We call this combination of a complementary ensemble with staggered training a “two-speed” network. The novelty of this approach is in the use of a staggered training schedule that allows the ensemble model to efficiently incorporate new data by retraining the high-speed CNN in advance of the resource-intensive vision transformer, thereby allowing for stable continuous improvement of the ensemble. Additionally, the ensemble models for each data increment out-perform each of the component models, with best accuracy of 65% against a holdout test partition of the RGB version of the So2Sat dataset. © 2023, The Author(s).,In recent years, artificial intelligence has gradually become the core driving force of a new round of scientific and technological revolution and industrial transformation, and is exerting a profound impact on all aspects of human life. With the rapid development of Internet big data and high-performance parallel computing, relevant research in computer vision has made significant progress in the past few years, becoming one of the important application branches in the field of artificial intelligence. The exercise of image classification forming part of computer vision tasks involves a large amount of computation, and training based on traditional deep learning (DL) classification models typically involves slow training and low accuracy in many parameters. Thus, in order to solve these problems, an image classification model based on DL and SAE network was proposed. Firstly, the main research of computer vision task-image classification is introduced in detail. Then, the combination framework of deep neural network and SAE network is built. At the same time, the deep neural network was used to carry out convolution operation of the parameters learned by SAE and extract each feature of the image with neurons, so as to improve the training accuracy of the deep neural network. Finally, the traditional deep neural network and SAE network were used for comparative experiment and analysis. Experimental results show that the proposed method has a certain degree of improvement in image classification accuracy compared with traditional deep neural network and SAE network, and the accuracy reaches 97.13%.  © 2023 Shijia Ling et al., published by Sciendo.,Image recognition and classification is a significant research topic in computational vision and widely used computer technology. The methods often used in image classification and recognition tasks are based on deep learning, like Convolutional Neural Networks (CNNs), LeNet, and Long Short-Term Memory networks (LSTM). Unfortunately, the classification accuracy of these methods is unsatisfactory. In recent years, using large-scale deep learning networks to achieve image recognition and classification can improve classification accuracy, such as VGG16 and Residual Network (ResNet). However, due to the deep network hierarchy and complex parameter settings, these models take more time in the training phase, especially when the sample number is small, which can easily lead to overfitting. This paper suggested a deep learningbased image classification technique based on a CNN model and improved convolutional and pooling layers. Furthermore, the study adopted the approximate dynamic learning rate update algorithm in the model training to realize the learning rate’s self-adaptation, ensure the model’s rapid convergence, and shorten the training time. Using the proposed model, an experiment was conducted on the Fashion-MNIST dataset, taking 6,000 images as the training dataset and 1,000 images as the testing dataset. In actual experiments, the classification accuracy of the suggested method was 93 percent, 4.6 percent higher than that of the basic CNN model. Simultaneously, the study compared the influence of the batch size of model training on classification accuracy. Experimental outcomes showed this model is very generalized in fashion clothing image classification tasks. © 2023,Journal of Information and Communication Technology.All Rights Reserved."
282,281,10,281_Cognitive Models of Movement Adaptation and Sensorimotor Feedback,Cognitive Models of Movement Adaptation and Sensorimotor Feedback,"Motor adaptation maintains movement accuracy. To evaluate movement accuracy, motor adaptation relies on an error signal, generated by the movement target, while suppressing error signals from irrelevant objects in the vicinity. Previous work used static testing environments, where all information required to evaluate movement accuracy was available simultaneously. Using saccadic eye movements as a model for motor adaptation, we tested how movement accuracy is maintained in dynamic environments, where the availability of conflicting error signals varied over time. Participants made a vertical saccade toward a target (either a small square or a large ring). Upon saccade detection, two candidate stimuli were shown left and right of the target, and participants were instructed to discriminate a feature on one of the candidates. Critically, candidate stimuli were presented sequentially, and saccade adaptation, thus, had to resolve a conflict between a task-relevant and a task-irrelevant error signal that were separated in space and time. We found that the saccade target influenced several aspects of oculomotor learning. In presence of a small target, saccade adaptation evaluated movement accuracy based on the first available error signal after the saccade, irrespective of its task relevance. However, a large target not only allowed for greater flexibility when evaluating movement accuracy, but it also promoted a stronger contribution of strategic behavior when compensating inaccurate saccades. Our results demonstrate how motor adaptation maintains movement accuracy in dynamic environments, and how properties of the visual environment modulate the relative contribution of different learning processes. ©2023 The Authors.,We examined the extent to which intentionally underperforming a goal-directed reaching task impacts how memories of recent performance contribute to sensorimotor adaptation. Healthy human subjects performed computerized cognition testing and an assessment of sensorimotor adaptation, wherein they grasped the handle of a horizontal planar robot while making goal-directed out-and-back reaching movements. The robot exerted forces that resisted hand motion with a spring-like load that changed unpredictably between movements. The robotic test assessed how implicit and explicit memories of sensorimotor performance contribute to the compensation for the unpredictable changes in the hand-held load. After each movement, subjects were to recall and report how far the hand moved on the previous trial (peak extent of the out-and-back movement). Subjects performed the tests under two counter-balanced conditions: one where they performed with their best effort, and one where they intentionally sabotaged (i.e., suppressed) kinematic performance. Results from the computerized cognition tests confirmed that subjects understood and complied with task instructions. When suppressing performance during the robotic assessment, subjects demonstrated marked changes in reach precision, time to capture the target, and reaction time. We fit a set of limited memory models to the data to identify how subjects used implicit and explicit memories of recent performance to compensate for the changing loads. In both sessions, subjects used implicit, but not explicit, memories from the most recent trial to adapt reaches to unpredictable spring-like loads. Subjects did not “give up” on large errors, nor did they discount small errors deemed “good enough”. Although subjects clearly suppressed kinematic performance (response timing, movement variability, and self-reporting of reach error), the relative contributions of sensorimotor memories to trial-by-trial variations in task performance did not differ significantly between the two testing conditions. We conclude that intentional performance suppression had minimal impact on how implicit sensorimotor memories contribute to adaptation of unpredictable mechanical loads applied to the hand. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.,Integrating sensory information during movement and adapting motor plans over successive movements are both essential for accurate, flexible motor behaviour. When an ongoing movement is off target, feedback control mechanisms update the descending motor commands to counter the sensed error. Over longer timescales, errors induce adaptation in feedforward planning so that future movements become more accurate and require less online adjustment from feedback control processes. Both the degree to which sensory feedback is integrated into an ongoing movement and the degree to which movement errors drive adaptive changes in feedforward motor plans have been shown to scale inversely with sensory uncertainty. However, since these processes have only been studied in isolation from one another, little is known about how they are influenced by sensory uncertainty in real-world movement contexts where they co-occur. Here, we show that sensory uncertainty may impact feedforward adaptation of reaching movements differently when feedback integration is present versus when it is absent. In particular, participants gradually adjust their movements from trial-to-trial in a manner that is well characterised by a slow and consistent envelope of error reduction. Riding on top of this slow envelope, participants exhibit large and abrupt changes in their initial movement vectors that are strongly correlated with the degree of sensory uncertainty present on the previous trial. However, these abrupt changes are insensitive to the magnitude and direction of the sensed movement error. These results prompt important questions for current models of sensorimotor learning under uncertainty and open up new avenues for future exploration in the field. © 2023 Hewitson et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
283,282,10,282_Radiomics-Based Predictive Models for Cancer Staging and Treatment Outcomes,Radiomics-Based Predictive Models for Cancer Staging and Treatment Outcomes,"Purpose: To investigate the predictability of local tumor progression (LTP) after microwave ablation (MWA) in colorectal carcinoma liver metastases (CRLM) patients by magnetic resonance imaging (MRI) radiomics and clinical characteristics-based combined model. Materials and Methods: Forty-two consecutive CRLM patients (67 tumors) with post-MWA complete response at 1st month MRI were included in this retrospective study. One hundred and eleven radiomics features were extracted for each tumor and for each phase by manual segmentation from pre-treatment MRI T2 fat-suppressed (Phase 2) and early arterial phase T1 fat-suppressed sequences (Phase 1). A clinical model was constructed using clinical data, two combined models were created with feature reduction and machine learning by combining clinical data and Phase 2 and Phase 1 radiomics features. The predicting performance for LTP development was investigated. Results: LTP developed in 7 patients (16.6%) and 11 tumors (16.4%). In the clinical model, the presence of extrahepatic metastases before MWA was associated with a high probability of LTP (p < 0.001). The pre-treatment levels of carbohydrate antigen 19–9 and carcinoembryonic antigen were higher in the LTP group (p = 0.010, p = 0.020, respectively). Patients with LTP had statistically significantly higher radiomics scores in both phases (p < 0.001 for Phase 2 and p = 0.001 for Phase 1). The classification performance of the combined model 2, created by using clinical data and Phase 2-based radiomics features, achieved the highest discriminative performance in predicting LTP (p = 0,014; the area under curve (AUC) value 0.981 (95% CI 0.948–0.990). The combined model 1, created using clinical data and Phase 1-based radiomics features (AUC value 0,927 (95% CI 0.860–0.993, p < 0.001)) and the clinical model alone [AUC value of 0.887 (95% CI 0.807–0.967, p < 0.001)] had similar performance. Conclusion: Combined models based on clinical data and radiomics features obtained from T2 fat-suppressed and early arterial-phase T1 fat-suppressed MRI are valuable markers in predicting LTP after MWA in CRLM patients. Large-scale studies with internal and external validations are needed to come to a firm conclusion on the predictability of radiomics models in CRLM patients. © 2023, Springer Science+Business Media, LLC, part of Springer Nature and the Cardiovascular and Interventional Radiological Society of Europe (CIRSE).,Introduction: Predicting checkpoint inhibitors treatment outcomes in melanoma is a relevant task, due to the unpredictable and potentially fatal toxicity and high costs for society. However, accurate biomarkers for treatment outcomes are lacking. Radiomics are a technique to quantitatively capture tumour characteristics on readily available computed tomography (CT) imaging. The purpose of this study was to investigate the added value of radiomics for predicting clinical benefit from checkpoint inhibitors in melanoma in a large, multicenter cohort. Methods: Patients who received first-line anti-PD1±anti-CTLA4 treatment for advanced cutaneous melanoma were retrospectively identified from nine participating hospitals. For every patient, up to five representative lesions were segmented on baseline CT, and radiomics features were extracted. A machine learning pipeline was trained on the radiomics features to predict clinical benefit, defined as stable disease for more than 6 months or response per RECIST 1.1 criteria. This approach was evaluated using a leave-one-centre-out cross validation and compared to a model based on previously discovered clinical predictors. Lastly, a combination model was built on the radiomics and clinical model. Results: A total of 620 patients were included, of which 59.2% experienced clinical benefit. The radiomics model achieved an area under the receiver operator characteristic curve (AUROC) of 0.607 [95% CI, 0.562–0.652], lower than that of the clinical model (AUROC=0.646 [95% CI, 0.600–0.692]). The combination model yielded no improvement over the clinical model in terms of discrimination (AUROC=0.636 [95% CI, 0.592–0.680]) or calibration. The output of the radiomics model was significantly correlated with three out of five input variables of the clinical model (p < 0.001). Discussion: The radiomics model achieved a moderate predictive value of clinical benefit, which was statistically significant. However, a radiomics approach was unable to add value to a simpler clinical model, most likely due to the overlap in predictive information learned by both models. Future research should focus on the application of deep learning, spectral CT-derived radiomics, and a multimodal approach for accurately predicting benefit to checkpoint inhibitor treatment in advanced melanoma. © 2023 The Author(s),Objectives Optimizing a machine learning (ML) pipeline for radiomics analysis involves numerous choices in data set composition, preprocessing, and model selection. Objective identification of the optimal setup is complicated by correlated features, interdependency structures, and a multitude of available ML algorithms. Therefore, we present a radiomics-based benchmarking framework to optimize a comprehensive ML pipeline for the prediction of overall survival. This study is conducted on an image set of patients with hepatic metastases of colorectal cancer, for which radiomics features of the whole liver and of metastases from computed tomography images were calculated. A mixed model approach was used to find the optimal pipeline configuration and to identify the added prognostic value of radiomics features. Materials and Methods In this study, a large-scale ML benchmark pipeline consisting of preprocessing, feature selection, dimensionality reduction, hyperparameter optimization, and training of different models was developed for radiomics-based survival analysis. Portal-venous computed tomography imaging data from a previous prospective randomized trial evaluating radioembolization of liver metastases of colorectal cancer were quantitatively accessible through a radiomics approach. One thousand two hundred eighteen radiomics features of hepatic metastases and the whole liver were calculated, and 19 clinical parameters (age, sex, laboratory values, and treatment) were available for each patient. Three ML algorithms - a regression model with elastic net regularization (glmnet), a random survival forest (RSF), and a gradient tree-boosting technique (xgboost) - were evaluated for 5 combinations of clinical data, tumor radiomics, and whole-liver features. Hyperparameter optimization and model evaluation were optimized toward the performance metric integrated Brier score via nested cross-validation. To address dependency structures in the benchmark setup, a mixed-model approach was developed to compare ML and data configurations and to identify the best-performing model. Results Within our radiomics-based benchmark experiment, 60 ML pipeline variations were evaluated on clinical data and radiomics features from 491 patients. Descriptive analysis of the benchmark results showed a preference for RSF-based pipelines, especially for the combination of clinical data with radiomics features. This observation was supported by the quantitative analysis via a linear mixed model approach, computed to differentiate the effect of data sets and pipeline configurations on the resulting performance. This revealed the RSF pipelines to consistently perform similar or better than glmnet and xgboost. Further, for the RSF, there was no significantly better-performing pipeline composition regarding the sort of preprocessing or hyperparameter optimization. Conclusions Our study introduces a benchmark framework for radiomics-based survival analysis, aimed at identifying the optimal settings with respect to different radiomics data sources and various ML pipeline variations, including preprocessing techniques and learning algorithms. A suitable analysis tool for the benchmark results is provided via a mixed model approach, which showed for our study on patients with intrahepatic liver metastases, that radiomics features captured the patients' clinical situation in a manner comparable to the provided information solely from clinical parameters. However, we did not observe a relevant additional prognostic value obtained by these radiomics features.  © Wolters Kluwer Health, Inc. All rights reserved."
